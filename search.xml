<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Flink</title>
    <url>/Flink/</url>
    <content><![CDATA[<h2 id="Maven"><a href="#Maven" class="headerlink" title="Maven"></a>Maven</h2><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">flink.version</span>&gt;</span>1.12.0<span class="tag">&lt;/<span class="name">flink.version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">java.version</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">java.version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">scala.binary.version</span>&gt;</span>2.11<span class="tag">&lt;/<span class="name">scala.binary.version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">slf4j.version</span>&gt;</span>1.7.30<span class="tag">&lt;/<span class="name">slf4j.version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-java_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-clients_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-runtime-web_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-api<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;slf4j.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-log4j12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;slf4j.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.logging.log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j-to-slf4j<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.14.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.projectlombok<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>lombok<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.18.12<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.12.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.alibaba<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>fastjson<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.75<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-redis_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.1.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-elasticsearch6_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.12.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.49<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-jdbc_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.12.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-statebackend-rocksdb_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-cep_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-planner_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-planner-blink_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-scala_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-csv<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-json<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-hive_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- Hive Dependency --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="WordCount"><a href="#WordCount" class="headerlink" title="WordCount"></a>WordCount</h2><h3 id="batch"><a href="#batch" class="headerlink" title="batch"></a>batch</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//1.获取执行环境</span></span><br><span class="line">    ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.读取文件</span></span><br><span class="line">    DataSource&lt;String&gt; input = env.readTextFile(<span class="string">&quot;/Users/vincent/Documents/flink/input/word.txt&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.用flatmap压平 1行hello world 变成 1行hello 1行world</span></span><br><span class="line">    FlatMapOperator&lt;String, String&gt; stringObjectFlatMapOperator = input.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String s, Collector&lt;String&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="comment">//按照空格切割</span></span><br><span class="line">            String[] words = s.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">            <span class="keyword">for</span> (String word: words</span><br><span class="line">                 ) &#123;</span><br><span class="line">                collector.collect(word);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//4.单词变元组 hello -&gt; (hello,1)</span></span><br><span class="line">    MapOperator&lt;String, Tuple2&lt;String,Integer&gt;&gt; stringObjectMapOperator = stringObjectFlatMapOperator.map(<span class="keyword">new</span> MapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">map</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="comment">//return new Tuple2&lt;&gt;(s,1);</span></span><br><span class="line">            <span class="keyword">return</span> Tuple2.of(s,<span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//5.分组</span></span><br><span class="line">    UnsortedGrouping&lt;Tuple2&lt;String, Integer&gt;&gt; tuple2UnsortedGrouping = stringObjectMapOperator.groupBy(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//6.聚合</span></span><br><span class="line">    AggregateOperator&lt;Tuple2&lt;String, Integer&gt;&gt; tuple2AggregateOperator = tuple2UnsortedGrouping.sum(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//7.显示</span></span><br><span class="line">    tuple2AggregateOperator.print();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="bounded-stream"><a href="#bounded-stream" class="headerlink" title="bounded_stream"></a>bounded_stream</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//1、创建执行环境，设置并行度为1</span></span><br><span class="line">    StreamExecutionEnvironment executionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    executionEnvironment.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2、读取文件流</span></span><br><span class="line">    DataStreamSource&lt;String&gt; stringDataStreamSource = executionEnvironment.readTextFile(<span class="string">&quot;/Users/vincent/Documents/flink/input/word.txt&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.hello world 转换为 (hello,1) (world,1)</span></span><br><span class="line">    SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; objectSingleOutputStreamOperator = stringDataStreamSource.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Tuple2&lt;String,Integer&gt;&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String s, Collector&lt;Tuple2&lt;String,Integer&gt;&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            String[] words = s.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">            <span class="keyword">for</span> (String word: words</span><br><span class="line">                 ) &#123;</span><br><span class="line">                collector.collect(Tuple2.of(word, <span class="number">1</span>));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//4.分组</span></span><br><span class="line">    KeyedStream&lt;Tuple2&lt;String, Integer&gt;, String&gt; tuple2TupleKeyedStream = objectSingleOutputStreamOperator.keyBy(<span class="keyword">new</span> KeySelector&lt;Tuple2&lt;String, Integer&gt;, String&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">getKey</span><span class="params">(Tuple2&lt;String, Integer&gt; stringIntegerTuple2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> stringIntegerTuple2.f0;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//5.聚合</span></span><br><span class="line">    SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; sum = tuple2TupleKeyedStream.sum(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//6.显示</span></span><br><span class="line">    sum.print();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//7.启动任务</span></span><br><span class="line">    JobExecutionResult execute = executionEnvironment.execute(<span class="string">&quot;start&quot;</span>);</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="unbounded-stream"><a href="#unbounded-stream" class="headerlink" title="unbounded_stream"></a>unbounded_stream</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//1.获取执行环境</span></span><br><span class="line">    StreamExecutionEnvironment executionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    executionEnvironment.setParallelism(<span class="number">1</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//2.读取端口数据流</span></span><br><span class="line">    DataStreamSource&lt;String&gt; stringDataStreamSource = executionEnvironment.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">1234</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//3.转换为元组</span></span><br><span class="line">    SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; objectSingleOutputStreamOperator = stringDataStreamSource.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Tuple2&lt;String,Integer&gt;&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String s, Collector&lt;Tuple2&lt;String,Integer&gt;&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            String[] words = s.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">            <span class="keyword">for</span> (String word: words</span><br><span class="line">            ) &#123;</span><br><span class="line">                collector.collect(Tuple2.of(word, <span class="number">1</span>));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//4.分组</span></span><br><span class="line">    KeyedStream&lt;Tuple2&lt;String, Integer&gt;, String&gt; tuple2TupleKeyedStream = objectSingleOutputStreamOperator.keyBy(<span class="keyword">new</span> KeySelector&lt;Tuple2&lt;String, Integer&gt;, String&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">getKey</span><span class="params">(Tuple2&lt;String, Integer&gt; stringIntegerTuple2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> stringIntegerTuple2.f0;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//5.聚合</span></span><br><span class="line">    SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; sum = tuple2TupleKeyedStream.sum(<span class="number">1</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//6.显示</span></span><br><span class="line">    sum.print();</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//7.启动任务</span></span><br><span class="line">    executionEnvironment.execute();</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Source"><a href="#Source" class="headerlink" title="Source"></a>Source</h2><h3 id="collection"><a href="#collection" class="headerlink" title="collection"></a>collection</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">StreamExecutionEnvironment executionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">executionEnvironment.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">List&lt;WaterSensor&gt; list = Arrays.asList(</span><br><span class="line">  <span class="keyword">new</span> WaterSensor(<span class="string">&quot;a&quot;</span>,<span class="number">12345678910L</span>,<span class="number">56</span>),</span><br><span class="line">  <span class="keyword">new</span> WaterSensor(<span class="string">&quot;b&quot;</span>,<span class="number">12354657829L</span>,<span class="number">57</span>),</span><br><span class="line">  <span class="keyword">new</span> WaterSensor(<span class="string">&quot;c&quot;</span>,<span class="number">12362457829L</span>,<span class="number">52</span>)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">DataStreamSource&lt;WaterSensor&gt; waterSensorDataStreamSource = executionEnvironment.fromCollection(list);</span><br><span class="line"></span><br><span class="line">waterSensorDataStreamSource.print();</span><br><span class="line"></span><br><span class="line">executionEnvironment.execute();</span><br></pre></td></tr></table></figure>

<h3 id="file"><a href="#file" class="headerlink" title="file"></a>file</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">StreamExecutionEnvironment executionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">executionEnvironment.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">DataStreamSource&lt;String&gt; stringDataStreamSource = executionEnvironment.readTextFile(<span class="string">&quot;input/sensor.txt&quot;</span>);</span><br><span class="line"></span><br><span class="line">SingleOutputStreamOperator&lt;WaterSensor&gt; map = stringDataStreamSource.map(<span class="keyword">new</span> MapFunction&lt;String, WaterSensor&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> WaterSensor <span class="title">map</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        String[] split = s.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> WaterSensor(split[<span class="number">0</span>],Long.parseLong(split[<span class="number">1</span>]),Integer.parseInt(split[<span class="number">2</span>]));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">map.print();</span><br><span class="line"></span><br><span class="line">executionEnvironment.execute();</span><br></pre></td></tr></table></figure>

<h3 id="socket"><a href="#socket" class="headerlink" title="socket"></a>socket</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">DataStreamSource&lt;String&gt; socketTextStream = env.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">1234</span>);</span><br><span class="line"></span><br><span class="line">socketTextStream.print();</span><br><span class="line"></span><br><span class="line">env.execute();</span><br></pre></td></tr></table></figure>

<h3 id="kafka"><a href="#kafka" class="headerlink" title="kafka"></a>kafka</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-console-producer.sh --broker-list linux1:9092 --topic tp_order</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;linux1:9092&quot;</span>);      <span class="comment">//kafka集群</span></span><br><span class="line">props.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;xsh&quot;</span>);                       <span class="comment">//消费者组</span></span><br><span class="line"></span><br><span class="line">DataStreamSource&lt;String&gt; tp_order = env.addSource(<span class="keyword">new</span> FlinkKafkaConsumer&lt;String&gt;(<span class="string">&quot;tp_order&quot;</span>, <span class="keyword">new</span> SimpleStringSchema(), props));          <span class="comment">//tp_order是topic</span></span><br><span class="line"></span><br><span class="line">tp_order.print();</span><br><span class="line"></span><br><span class="line">env.execute();</span><br></pre></td></tr></table></figure>

<h3 id="custom-source"><a href="#custom-source" class="headerlink" title="custom_source"></a>custom_source</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    StreamExecutionEnvironment executionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    executionEnvironment.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    DataStreamSource&lt;WaterSensor&gt; objectDataStreamSource = executionEnvironment.addSource(<span class="keyword">new</span> MySource(<span class="string">&quot;localhost&quot;</span>,<span class="number">2345</span>));</span><br><span class="line"></span><br><span class="line">    objectDataStreamSource.print();</span><br><span class="line"></span><br><span class="line">    executionEnvironment.execute();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MySource</span> <span class="keyword">implements</span> <span class="title">SourceFunction</span>&lt;<span class="title">WaterSensor</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String host;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> port;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Boolean running = <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">    Socket socket = <span class="keyword">null</span>;</span><br><span class="line">    BufferedReader reader = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">MySource</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">MySource</span><span class="params">(String host, <span class="keyword">int</span> port)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.host = host;</span><br><span class="line">        <span class="keyword">this</span>.port = port;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        socket = <span class="keyword">new</span> Socket(host,port);</span><br><span class="line">        reader = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(socket.getInputStream(), UTF_8));</span><br><span class="line"></span><br><span class="line">        String s = reader.readLine();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(running &amp;&amp; s != <span class="keyword">null</span>)&#123;</span><br><span class="line"></span><br><span class="line">            String[] split = s.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">            WaterSensor waterSensor = <span class="keyword">new</span> WaterSensor(split[<span class="number">0</span>], Long.parseLong(split[<span class="number">1</span>]), Integer.parseInt(split[<span class="number">2</span>]));</span><br><span class="line">            ctx.collect(waterSensor);</span><br><span class="line">            s = reader.readLine();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        running = <span class="keyword">false</span>;</span><br><span class="line">        <span class="keyword">try</span>&#123;</span><br><span class="line">            reader.close();</span><br><span class="line">        &#125;<span class="keyword">catch</span> (Exception e)&#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            socket.close();</span><br><span class="line">        &#125;<span class="keyword">catch</span> (IOException e)&#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Transform"><a href="#Transform" class="headerlink" title="Transform"></a>Transform</h2><h3 id="map"><a href="#map" class="headerlink" title="map"></a>map</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//1.获取执行环境</span></span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.从文件读取数据</span></span><br><span class="line">    DataStreamSource&lt;String&gt; stringDataStreamSource = env.readTextFile(<span class="string">&quot;input/sensor.txt&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.转换为JavaBean并打印数据</span></span><br><span class="line">    stringDataStreamSource.map((MapFunction&lt;String, WaterSensor&gt;) value -&gt; &#123;</span><br><span class="line">        String[] split = value.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> WaterSensor(split[<span class="number">0</span>], Long.parseLong(split[<span class="number">1</span>]), Integer.parseInt(split[<span class="number">2</span>]));</span><br><span class="line">    &#125;).print();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//4.执行</span></span><br><span class="line">    env.execute();</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="richmap"><a href="#richmap" class="headerlink" title="richmap"></a>richmap</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//1.获取执行环境</span></span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.从文件读取数据</span></span><br><span class="line">    DataStreamSource&lt;String&gt; stringDataStreamSource = env.readTextFile(<span class="string">&quot;input/sensor.txt&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.将每行数据转换为JavaBean</span></span><br><span class="line">    SingleOutputStreamOperator&lt;WaterSensor&gt; waterSensorDS = stringDataStreamSource.map(<span class="keyword">new</span> MyRichMapFunc());</span><br><span class="line"></span><br><span class="line">    <span class="comment">//4.打印结果数据</span></span><br><span class="line">    waterSensorDS.print();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//5.执行任务</span></span><br><span class="line">    env.execute();</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//RichFunction富有的地方在于:1.生命周期方法,2.可以获取上下文执行环境,做状态编程</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyRichMapFunc</span> <span class="keyword">extends</span> <span class="title">RichMapFunction</span>&lt;<span class="title">String</span>, <span class="title">WaterSensor</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;Open方法被调用！！&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> WaterSensor <span class="title">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        String[] split = value.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> WaterSensor(split[<span class="number">0</span>], Long.parseLong(split[<span class="number">1</span>]), Integer.parseInt(split[<span class="number">2</span>]));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;Close方法被调用！！！&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="richflatmap"><a href="#richflatmap" class="headerlink" title="richflatmap"></a>richflatmap</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//1.获取执行环境</span></span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.从文件读取数据</span></span><br><span class="line">    DataStreamSource&lt;String&gt; stringDataStreamSource = env.readTextFile(<span class="string">&quot;input/sensor.txt&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.压平数据</span></span><br><span class="line">    SingleOutputStreamOperator&lt;String&gt; result = stringDataStreamSource.flatMap(<span class="keyword">new</span> MyRichFlatMapFunc());</span><br><span class="line"></span><br><span class="line">    <span class="comment">//4.打印数据</span></span><br><span class="line">    result.print();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//5.执行任务</span></span><br><span class="line">    env.execute();</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyRichFlatMapFunc</span> <span class="keyword">extends</span> <span class="title">RichFlatMapFunction</span>&lt;<span class="title">String</span>, <span class="title">String</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;aaa&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String value, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        String[] split = value.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (String s : split) &#123;</span><br><span class="line">            out.collect(s);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="richfillter"><a href="#richfillter" class="headerlink" title="richfillter"></a>richfillter</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//1.获取执行环境</span></span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.从文件读取数据</span></span><br><span class="line">    DataStreamSource&lt;String&gt; stringDataStreamSource = env.readTextFile(<span class="string">&quot;input/sensor.txt&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.过滤数据,只取水位高于20的</span></span><br><span class="line">    SingleOutputStreamOperator&lt;String&gt; result = stringDataStreamSource.filter(<span class="keyword">new</span> MyRichFilterFunc());</span><br><span class="line"></span><br><span class="line">    <span class="comment">//4.打印数据</span></span><br><span class="line">    result.print();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//5.执行任务</span></span><br><span class="line">    env.execute();</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyRichFilterFunc</span> <span class="keyword">extends</span> <span class="title">RichFilterFunction</span>&lt;<span class="title">String</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;aaaa&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        String[] split = value.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> Integer.parseInt(split[<span class="number">2</span>]) &gt; <span class="number">20</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="connect"><a href="#connect" class="headerlink" title="connect"></a>connect</h3><p><strong>说明：1、只能操作两个流    2、两个流可以是不同类型</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//1.获取执行环境</span></span><br><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//2.读取端口数据创建流</span></span><br><span class="line">DataStreamSource&lt;String&gt; stringDS = env.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">1234</span>);</span><br><span class="line">DataStreamSource&lt;String&gt; socketTextStream2 = env.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">4321</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//3.将socketTextStream2转换为Int类型</span></span><br><span class="line">SingleOutputStreamOperator&lt;Integer&gt; intDS = socketTextStream2.map(String::length);</span><br><span class="line"></span><br><span class="line"><span class="comment">//4.连接两个流</span></span><br><span class="line">ConnectedStreams&lt;String, Integer&gt; connectedStreams = stringDS.connect(intDS);</span><br><span class="line"></span><br><span class="line"><span class="comment">//5.处理连接之后的流</span></span><br><span class="line">SingleOutputStreamOperator&lt;Object&gt; result = connectedStreams.map(<span class="keyword">new</span> CoMapFunction&lt;String, Integer, Object&gt;() &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">map1</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> value;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">map2</span><span class="params">(Integer value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> value;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">//6.打印数据</span></span><br><span class="line">result.print();</span><br><span class="line"></span><br><span class="line"><span class="comment">//7.执行</span></span><br><span class="line">env.execute();</span><br></pre></td></tr></table></figure>

<h3 id="union"><a href="#union" class="headerlink" title="union"></a>union</h3><p><strong>说明：1、可以操作多个流    2、多个流必须是同类型</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//1.获取执行环境</span></span><br><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//2.读取端口数据创建流</span></span><br><span class="line">DataStreamSource&lt;String&gt; socketTextStream1 = env.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">1234</span>);</span><br><span class="line">DataStreamSource&lt;String&gt; socketTextStream2 = env.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">4321</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//3.连接两条流</span></span><br><span class="line">DataStream&lt;String&gt; union = socketTextStream1.union(socketTextStream2);</span><br><span class="line"></span><br><span class="line"><span class="comment">//4.打印</span></span><br><span class="line">union.print();</span><br><span class="line"></span><br><span class="line"><span class="comment">//5.执行任务</span></span><br><span class="line">env.execute();</span><br></pre></td></tr></table></figure>

<h3 id="max-maxBy"><a href="#max-maxBy" class="headerlink" title="max/maxBy"></a>max/maxBy</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//1.获取执行环境</span></span><br><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//2.读取端口数据并转换为JavaBean</span></span><br><span class="line">SingleOutputStreamOperator&lt;WaterSensor&gt; waterSensorDS = env.socketTextStream(<span class="string">&quot;hadoop102&quot;</span>, <span class="number">9999</span>)</span><br><span class="line">        .map(<span class="keyword">new</span> MapFunction&lt;String, WaterSensor&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> WaterSensor <span class="title">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String[] split = value.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> WaterSensor(split[<span class="number">0</span>],</span><br><span class="line">                        Long.parseLong(split[<span class="number">1</span>]),</span><br><span class="line">                        Integer.parseInt(split[<span class="number">2</span>]));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">//3.按照传感器ID分组</span></span><br><span class="line">KeyedStream&lt;WaterSensor, String&gt; keyedStream = waterSensorDS.keyBy(<span class="keyword">new</span> KeySelector&lt;WaterSensor, String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getKey</span><span class="params">(WaterSensor value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> value.getId();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">//4.计算最高水位线</span></span><br><span class="line"><span class="comment">//SingleOutputStreamOperator&lt;WaterSensor&gt; result = keyedStream.max(&quot;vc&quot;);</span></span><br><span class="line">SingleOutputStreamOperator&lt;WaterSensor&gt; result = keyedStream.maxBy(<span class="string">&quot;vc&quot;</span>, <span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//5.打印</span></span><br><span class="line">result.print();</span><br><span class="line"></span><br><span class="line"><span class="comment">//6.执行任务</span></span><br><span class="line">env.execute();</span><br></pre></td></tr></table></figure>

<h3 id="reduce"><a href="#reduce" class="headerlink" title="reduce"></a>reduce</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//1.获取执行环境</span></span><br><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//2.读取端口数据并转换为JavaBean</span></span><br><span class="line">SingleOutputStreamOperator&lt;WaterSensor&gt; waterSensorDS = env.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">1234</span>)</span><br><span class="line">        .map(<span class="keyword">new</span> MapFunction&lt;String, WaterSensor&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> WaterSensor <span class="title">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String[] split = value.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> WaterSensor(split[<span class="number">0</span>],</span><br><span class="line">                        Long.parseLong(split[<span class="number">1</span>]),</span><br><span class="line">                        Integer.parseInt(split[<span class="number">2</span>]));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">//3.按照传感器ID分组</span></span><br><span class="line">KeyedStream&lt;WaterSensor, String&gt; keyedStream = waterSensorDS.keyBy(<span class="keyword">new</span> KeySelector&lt;WaterSensor, String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getKey</span><span class="params">(WaterSensor value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> value.getId();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">//4.计算最高水位线</span></span><br><span class="line">SingleOutputStreamOperator&lt;WaterSensor&gt; result = keyedStream.reduce(<span class="keyword">new</span> ReduceFunction&lt;WaterSensor&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> WaterSensor <span class="title">reduce</span><span class="params">(WaterSensor value1, WaterSensor value2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> WaterSensor(value1.getId(),</span><br><span class="line">                value2.getTs(),</span><br><span class="line">                Math.max(value1.getVc(), value2.getVc()</span><br><span class="line">                ));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">//5.打印</span></span><br><span class="line">result.print();</span><br><span class="line"></span><br><span class="line"><span class="comment">//6.执行任务</span></span><br><span class="line">env.execute();</span><br></pre></td></tr></table></figure>

<h3 id="process"><a href="#process" class="headerlink" title="process"></a>process</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//1.获取执行环境</span></span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.读取端口数据</span></span><br><span class="line">    DataStreamSource&lt;String&gt; socketTextStream = env.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">1234</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.使用Process实现压平功能</span></span><br><span class="line">    SingleOutputStreamOperator&lt;String&gt; wordDS = socketTextStream.process(<span class="keyword">new</span> ProcessFlatMapFunc());</span><br><span class="line"></span><br><span class="line">    <span class="comment">//4.使用Process实现Map功能</span></span><br><span class="line">    SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; wordToOneDS = wordDS.process(<span class="keyword">new</span> ProcessMapFunc());</span><br><span class="line"></span><br><span class="line">    <span class="comment">//5.按照单词分组</span></span><br><span class="line">    KeyedStream&lt;Tuple2&lt;String, Integer&gt;, String&gt; keyedStream = wordToOneDS.keyBy(data -&gt; data.f0);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//6.计算总和</span></span><br><span class="line">    SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; result = keyedStream.sum(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//7.打印</span></span><br><span class="line">    result.print();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//8.执行任务</span></span><br><span class="line">    env.execute();</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">ProcessFlatMapFunc</span> <span class="keyword">extends</span> <span class="title">ProcessFunction</span>&lt;<span class="title">String</span>, <span class="title">String</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//生命周期方法</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(String value, Context ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//运行时上下文,状态编程</span></span><br><span class="line">        RuntimeContext runtimeContext = getRuntimeContext();</span><br><span class="line"></span><br><span class="line">        String[] words = value.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">        <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">            out.collect(word);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//定时器</span></span><br><span class="line">        TimerService timerService = ctx.timerService();</span><br><span class="line">        timerService.registerProcessingTimeTimer(<span class="number">1245L</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取当前处理数据的时间</span></span><br><span class="line">        timerService.currentProcessingTime();</span><br><span class="line">        timerService.currentWatermark();  <span class="comment">//事件时间</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//侧输出流</span></span><br><span class="line">        <span class="comment">//ctx.output();</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">ProcessMapFunc</span> <span class="keyword">extends</span> <span class="title">ProcessFunction</span>&lt;<span class="title">String</span>, <span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Integer</span>&gt;&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(String value, Context ctx, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        out.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(value, <span class="number">1</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="partition"><a href="#partition" class="headerlink" title="partition"></a>partition</h3><p>keyBy/shuffle/rebalance/rescale/global/forward/broadcast</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//1.获取执行环境</span></span><br><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="comment">//env.setParallelism(5);</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//2.读取端口数据</span></span><br><span class="line">DataStreamSource&lt;String&gt; socketTextStream = env.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">1234</span>);</span><br><span class="line">SingleOutputStreamOperator&lt;String&gt; map = socketTextStream.map(x -&gt; x);</span><br><span class="line">        <span class="comment">//.setParallelism(2);</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//map.print(&quot;Map&quot;).setParallelism(2);</span></span><br><span class="line"><span class="comment">//map.rebalance().print(&quot;Rebalance&quot;);</span></span><br><span class="line"><span class="comment">//map.rescale().print(&quot;Rescale&quot;);</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//3.使用不同的重分区策略分区后打印</span></span><br><span class="line">socketTextStream.keyBy(data -&gt; data).print(<span class="string">&quot;KeyBy&quot;</span>);</span><br><span class="line">socketTextStream.shuffle().print(<span class="string">&quot;Shuffle&quot;</span>);</span><br><span class="line">socketTextStream.rebalance().print(<span class="string">&quot;Rebalance&quot;</span>);</span><br><span class="line">socketTextStream.rescale().print(<span class="string">&quot;Rescale&quot;</span>);</span><br><span class="line">socketTextStream.global().print(<span class="string">&quot;Global&quot;</span>);</span><br><span class="line"><span class="comment">//socketTextStream.forward().print(&quot;Forward&quot;); // 报错</span></span><br><span class="line"><span class="comment">//socketTextStream.broadcast().print(&quot;Broadcast&quot;);</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//4.开启任务</span></span><br><span class="line">env.execute();</span><br></pre></td></tr></table></figure>

<h2 id="Sink"><a href="#Sink" class="headerlink" title="Sink"></a>Sink</h2><h3 id="kafka-1"><a href="#kafka-1" class="headerlink" title="kafka"></a>kafka</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//1.获取执行环境</span></span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.读取端口数据并转换为JavaBean</span></span><br><span class="line">    SingleOutputStreamOperator&lt;WaterSensor&gt; waterSensorDS = env.socketTextStream(<span class="string">&quot;linux1&quot;</span>, <span class="number">9999</span>)</span><br><span class="line">            .map(<span class="keyword">new</span> MapFunction&lt;String, WaterSensor&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> WaterSensor <span class="title">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                    String[] split = value.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">new</span> WaterSensor(split[<span class="number">0</span>],</span><br><span class="line">                            Long.parseLong(split[<span class="number">1</span>]),</span><br><span class="line">                            Integer.parseInt(split[<span class="number">2</span>]));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.将数据转换为字符串写入Kafka</span></span><br><span class="line">    Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">    properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;linux1:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">    waterSensorDS.map(<span class="keyword">new</span> MapFunction&lt;WaterSensor, String&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(WaterSensor value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> JSON.toJSONString(value);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;).addSink(<span class="keyword">new</span> FlinkKafkaProducer&lt;String&gt;(<span class="string">&quot;test&quot;</span>, <span class="keyword">new</span> SimpleStringSchema(), properties));</span><br><span class="line"></span><br><span class="line">    <span class="comment">//4.执行任务</span></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="redis"><a href="#redis" class="headerlink" title="redis"></a>redis</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//1.获取执行环境</span></span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.读取端口数据并转换为JavaBean</span></span><br><span class="line">    SingleOutputStreamOperator&lt;WaterSensor&gt; waterSensorDS = env.socketTextStream(<span class="string">&quot;linux1&quot;</span>, <span class="number">9999</span>)</span><br><span class="line">            .map(<span class="keyword">new</span> MapFunction&lt;String, WaterSensor&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> WaterSensor <span class="title">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                    String[] split = value.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">new</span> WaterSensor(split[<span class="number">0</span>],</span><br><span class="line">                            Long.parseLong(split[<span class="number">1</span>]),</span><br><span class="line">                            Integer.parseInt(split[<span class="number">2</span>]));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.将数据写入Redis</span></span><br><span class="line">    FlinkJedisPoolConfig jedisPoolConfig = <span class="keyword">new</span> FlinkJedisPoolConfig.Builder()</span><br><span class="line">            .setHost(<span class="string">&quot;linux1&quot;</span>)</span><br><span class="line">            .setPort(<span class="number">6379</span>)</span><br><span class="line">            .build();</span><br><span class="line">    waterSensorDS.addSink(<span class="keyword">new</span> RedisSink&lt;&gt;(jedisPoolConfig, <span class="keyword">new</span> MyRedisMapper()));</span><br><span class="line"></span><br><span class="line">    <span class="comment">//4.执行任务</span></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyRedisMapper</span> <span class="keyword">implements</span> <span class="title">RedisMapper</span>&lt;<span class="title">WaterSensor</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> RedisCommandDescription <span class="title">getCommandDescription</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> RedisCommandDescription(RedisCommand.HSET, <span class="string">&quot;Sensor&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getKeyFromData</span><span class="params">(WaterSensor data)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> data.getId();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getValueFromData</span><span class="params">(WaterSensor data)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> data.getVc().toString();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="elasticsearch"><a href="#elasticsearch" class="headerlink" title="elasticsearch"></a>elasticsearch</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//1.获取执行环境</span></span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.读取端口数据并转换为JavaBean</span></span><br><span class="line">    SingleOutputStreamOperator&lt;WaterSensor&gt; waterSensorDS = env.socketTextStream(<span class="string">&quot;linux1&quot;</span>, <span class="number">9999</span>)</span><br><span class="line">    <span class="comment">//SingleOutputStreamOperator&lt;WaterSensor&gt; waterSensorDS = env.readTextFile(&quot;input/sensor.txt&quot;)</span></span><br><span class="line">            .map(<span class="keyword">new</span> MapFunction&lt;String, WaterSensor&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> WaterSensor <span class="title">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                    String[] split = value.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">new</span> WaterSensor(split[<span class="number">0</span>],</span><br><span class="line">                            Long.parseLong(split[<span class="number">1</span>]),</span><br><span class="line">                            Integer.parseInt(split[<span class="number">2</span>]));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.将数据写入ES</span></span><br><span class="line">    ArrayList&lt;HttpHost&gt; httpHosts = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    httpHosts.add(<span class="keyword">new</span> HttpHost(<span class="string">&quot;linux1&quot;</span>, <span class="number">9200</span>));</span><br><span class="line"></span><br><span class="line">    ElasticsearchSink.Builder&lt;WaterSensor&gt; waterSensorBuilder =</span><br><span class="line">            <span class="keyword">new</span> ElasticsearchSink.Builder&lt;WaterSensor&gt;(httpHosts, <span class="keyword">new</span> MyEsSinkFunc());</span><br><span class="line"></span><br><span class="line">    <span class="comment">//批量提交参数</span></span><br><span class="line">    waterSensorBuilder.setBulkFlushMaxActions(<span class="number">1</span>);</span><br><span class="line">    ElasticsearchSink&lt;WaterSensor&gt; elasticsearchSink = waterSensorBuilder.build();</span><br><span class="line"></span><br><span class="line">    waterSensorDS.addSink(elasticsearchSink);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//4.执行任务</span></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyEsSinkFunc</span> <span class="keyword">implements</span> <span class="title">ElasticsearchSinkFunction</span>&lt;<span class="title">WaterSensor</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WaterSensor element, RuntimeContext ctx, RequestIndexer indexer)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        HashMap&lt;String, String&gt; source = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        source.put(<span class="string">&quot;ts&quot;</span>, element.getTs().toString());</span><br><span class="line">        source.put(<span class="string">&quot;vc&quot;</span>, element.getVc().toString());</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建Index请求</span></span><br><span class="line">        IndexRequest indexRequest = Requests.indexRequest()</span><br><span class="line">                .index(<span class="string">&quot;sensor1&quot;</span>)</span><br><span class="line">                .type(<span class="string">&quot;_doc&quot;</span>)</span><br><span class="line">                <span class="comment">//.id(element.getId())</span></span><br><span class="line">                .source(source);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//写入ES</span></span><br><span class="line">        indexer.add(indexRequest);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="jdbc"><a href="#jdbc" class="headerlink" title="jdbc"></a>jdbc</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//1.获取执行环境</span></span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.读取端口数据并转换为JavaBean</span></span><br><span class="line">    SingleOutputStreamOperator&lt;WaterSensor&gt; waterSensorDS = env.socketTextStream(<span class="string">&quot;linux1&quot;</span>, <span class="number">9999</span>)</span><br><span class="line">            .map(<span class="keyword">new</span> MapFunction&lt;String, WaterSensor&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> WaterSensor <span class="title">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                    String[] split = value.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">new</span> WaterSensor(split[<span class="number">0</span>],</span><br><span class="line">                            Long.parseLong(split[<span class="number">1</span>]),</span><br><span class="line">                            Integer.parseInt(split[<span class="number">2</span>]));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.将数据写入MySQL</span></span><br><span class="line">    waterSensorDS.addSink(JdbcSink.sink(</span><br><span class="line">            <span class="string">&quot;INSERT INTO `sensor-0821` VALUES(?,?,?) ON DUPLICATE KEY UPDATE `ts`=?,`vc`=?&quot;</span>,</span><br><span class="line">            <span class="keyword">new</span> JdbcStatementBuilder&lt;WaterSensor&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accept</span><span class="params">(PreparedStatement preparedStatement, WaterSensor waterSensor)</span> <span class="keyword">throws</span> SQLException </span>&#123;</span><br><span class="line">                    <span class="comment">//给占位符赋值</span></span><br><span class="line">                    preparedStatement.setString(<span class="number">1</span>, waterSensor.getId());</span><br><span class="line">                    preparedStatement.setLong(<span class="number">2</span>, waterSensor.getTs());</span><br><span class="line">                    preparedStatement.setInt(<span class="number">3</span>, waterSensor.getVc());</span><br><span class="line">                    preparedStatement.setLong(<span class="number">4</span>, waterSensor.getTs());</span><br><span class="line">                    preparedStatement.setInt(<span class="number">5</span>, waterSensor.getVc());</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            JdbcExecutionOptions.builder()</span><br><span class="line">                    .withBatchSize(<span class="number">1</span>)</span><br><span class="line">                    .build(),</span><br><span class="line">            <span class="keyword">new</span> JdbcConnectionOptions.JdbcConnectionOptionsBuilder()</span><br><span class="line">                    .withUrl(<span class="string">&quot;jdbc:mysql://linux1:3306/test?useSSL=false&quot;</span>)</span><br><span class="line">                    .withDriverName(<span class="string">&quot;com.mysql.jdbc.Driver&quot;</span>)</span><br><span class="line">                    .withUsername(<span class="string">&quot;root&quot;</span>)</span><br><span class="line">                    .withPassword(<span class="string">&quot;000000&quot;</span>)</span><br><span class="line">                    .build()</span><br><span class="line">    ));</span><br><span class="line"></span><br><span class="line">    <span class="comment">//4.执行任务</span></span><br><span class="line">    env.execute();</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="custome-sink"><a href="#custome-sink" class="headerlink" title="custome_sink"></a>custome_sink</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//1.获取执行环境</span></span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.读取端口数据并转换为JavaBean</span></span><br><span class="line">    SingleOutputStreamOperator&lt;WaterSensor&gt; waterSensorDS = env.socketTextStream(<span class="string">&quot;linux1&quot;</span>, <span class="number">9999</span>)</span><br><span class="line">            .map(<span class="keyword">new</span> MapFunction&lt;String, WaterSensor&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> WaterSensor <span class="title">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                    String[] split = value.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">new</span> WaterSensor(split[<span class="number">0</span>],</span><br><span class="line">                            Long.parseLong(split[<span class="number">1</span>]),</span><br><span class="line">                            Integer.parseInt(split[<span class="number">2</span>]));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.将数据写入Mysql</span></span><br><span class="line">    waterSensorDS.addSink(<span class="keyword">new</span> MySink());</span><br><span class="line"></span><br><span class="line">    <span class="comment">//4.执行任务</span></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MySink</span> <span class="keyword">extends</span> <span class="title">RichSinkFunction</span>&lt;<span class="title">WaterSensor</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//声明连接</span></span><br><span class="line">    <span class="keyword">private</span> Connection connection;</span><br><span class="line">    <span class="keyword">private</span> PreparedStatement preparedStatement;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//生命周期方法,用于创建连接</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        connection = DriverManager.getConnection(<span class="string">&quot;jdbc:mysql://linux1:3306/test?useSSL=false&quot;</span>, <span class="string">&quot;root&quot;</span>, <span class="string">&quot;000000&quot;</span>);</span><br><span class="line">        preparedStatement = connection.prepareStatement(<span class="string">&quot;INSERT INTO `sensor-0821` VALUES(?,?,?) ON DUPLICATE KEY UPDATE `ts`=?,`vc`=?&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">invoke</span><span class="params">(WaterSensor value, Context context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//给占位符赋值</span></span><br><span class="line">        preparedStatement.setString(<span class="number">1</span>, value.getId());</span><br><span class="line">        preparedStatement.setLong(<span class="number">2</span>, value.getTs());</span><br><span class="line">        preparedStatement.setInt(<span class="number">3</span>, value.getVc());</span><br><span class="line">        preparedStatement.setLong(<span class="number">4</span>, value.getTs());</span><br><span class="line">        preparedStatement.setInt(<span class="number">5</span>, value.getVc());</span><br><span class="line"></span><br><span class="line">        <span class="comment">//执行操作</span></span><br><span class="line">        preparedStatement.execute();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//生命周期方法,用于关闭连接</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        preparedStatement.close();</span><br><span class="line">        connection.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Window"><a href="#Window" class="headerlink" title="Window"></a>Window</h2><h3 id="TimeTumbling"><a href="#TimeTumbling" class="headerlink" title="TimeTumbling"></a>TimeTumbling</h3><h4 id="增量聚合计算"><a href="#增量聚合计算" class="headerlink" title="增量聚合计算"></a>增量聚合计算</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//1.获取执行环境</span></span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.读取端口数据</span></span><br><span class="line">    DataStreamSource&lt;String&gt; socketTextStream = env.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">9999</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.压平并转换为元组</span></span><br><span class="line">    SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; wordToOneDS = socketTextStream.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            String[] words = value.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">            <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">                out.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(word, <span class="number">1</span>));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//4.按照单词分组</span></span><br><span class="line">    KeyedStream&lt;Tuple2&lt;String, Integer&gt;, String&gt; keyedStream = wordToOneDS.keyBy(data -&gt; data.f0);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//5.开窗</span></span><br><span class="line">    WindowedStream&lt;Tuple2&lt;String, Integer&gt;, String, TimeWindow&gt; windowedStream = keyedStream.window(TumblingProcessingTimeWindows.of(Time.seconds(<span class="number">5</span>)));</span><br><span class="line"></span><br><span class="line">    <span class="comment">//6.增量聚合计算(来一条数据计算一次，窗口结束输出结果)</span></span><br><span class="line">    <span class="comment">//SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; result = windowedStream.sum(1);</span></span><br><span class="line">    SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; result = windowedStream.aggregate(<span class="keyword">new</span> MyAggFunc(), <span class="keyword">new</span> MyWindowFunc());</span><br><span class="line"></span><br><span class="line">    <span class="comment">//7.打印</span></span><br><span class="line">    result.print();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//8.执行任务</span></span><br><span class="line">    env.execute();</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyAggFunc</span> <span class="keyword">implements</span> <span class="title">AggregateFunction</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Integer</span>&gt;, <span class="title">Integer</span>, <span class="title">Integer</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">createAccumulator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">add</span><span class="params">(Tuple2&lt;String, Integer&gt; value, Integer accumulator)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> accumulator + <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">getResult</span><span class="params">(Integer accumulator)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> accumulator;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">merge</span><span class="params">(Integer a, Integer b)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> a + b;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyWindowFunc</span> <span class="keyword">implements</span> <span class="title">WindowFunction</span>&lt;<span class="title">Integer</span>, <span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Integer</span>&gt;, <span class="title">String</span>, <span class="title">TimeWindow</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">apply</span><span class="params">(String key, TimeWindow window, Iterable&lt;Integer&gt; input, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//取出迭代器中的数据</span></span><br><span class="line">        Integer next = input.iterator().next();</span><br><span class="line">        <span class="comment">//输出数据</span></span><br><span class="line">        out.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="keyword">new</span> Timestamp(window.getStart()) + <span class="string">&quot;:&quot;</span> + key, next));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="全量窗口计算"><a href="#全量窗口计算" class="headerlink" title="全量窗口计算"></a>全量窗口计算</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//1.获取执行环境</span></span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.读取端口数据</span></span><br><span class="line">    DataStreamSource&lt;String&gt; socketTextStream = env.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">9999</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.压平并转换为元组</span></span><br><span class="line">    SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; wordToOneDS = socketTextStream.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            String[] words = value.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">            <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">                out.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(word, <span class="number">1</span>));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//4.按照单词分组</span></span><br><span class="line">    KeyedStream&lt;Tuple2&lt;String, Integer&gt;, String&gt; keyedStream = wordToOneDS.keyBy(data -&gt; data.f0);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//5.开窗</span></span><br><span class="line">    WindowedStream&lt;Tuple2&lt;String, Integer&gt;, String, TimeWindow&gt; windowedStream = keyedStream.window(TumblingProcessingTimeWindows.of(Time.seconds(<span class="number">5</span>)));</span><br><span class="line"></span><br><span class="line">    <span class="comment">//6.全量窗口计算(将窗口内数据收集，窗口结束时计算)</span></span><br><span class="line">    <span class="comment">//调用apply和process都可以</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; result = windowedStream.apply(new WindowFunction&lt;Tuple2&lt;String, Integer&gt;, Tuple2&lt;String, Integer&gt;, String, TimeWindow&gt;() &#123;</span></span><br><span class="line">    <span class="comment">//    @Override</span></span><br><span class="line">    <span class="comment">//    public void apply(String key, TimeWindow window, Iterable&lt;Tuple2&lt;String, Integer&gt;&gt; input, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) throws Exception &#123;</span></span><br><span class="line">    <span class="comment">//        //取出迭代器的长度</span></span><br><span class="line">    <span class="comment">//        ArrayList&lt;Tuple2&lt;String, Integer&gt;&gt; arrayList = Lists.newArrayList(input.iterator());</span></span><br><span class="line">    <span class="comment">//        //输出数据</span></span><br><span class="line">    <span class="comment">//        out.collect(new Tuple2&lt;&gt;(new Timestamp(window.getStart()) + &quot;:&quot; + key, arrayList.size()));</span></span><br><span class="line">    <span class="comment">//    &#125;</span></span><br><span class="line">    <span class="comment">//&#125;);</span></span><br><span class="line"></span><br><span class="line">    SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; result = windowedStream.process(<span class="keyword">new</span> ProcessWindowFunction&lt;Tuple2&lt;String, Integer&gt;, Tuple2&lt;String, Integer&gt;, String, TimeWindow&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(String key, Context context, Iterable&lt;Tuple2&lt;String, Integer&gt;&gt; elements, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="comment">//取出迭代器的长度</span></span><br><span class="line">            ArrayList&lt;Tuple2&lt;String, Integer&gt;&gt; arrayList = Lists.newArrayList(elements.iterator());</span><br><span class="line">            <span class="comment">//输出数据</span></span><br><span class="line">            out.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(<span class="keyword">new</span> Timestamp(context.window().getStart()) + <span class="string">&quot;:&quot;</span> + key, arrayList.size()));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//7.打印</span></span><br><span class="line">    result.print();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//8.执行任务</span></span><br><span class="line">    env.execute();</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="TimeSliding"><a href="#TimeSliding" class="headerlink" title="TimeSliding"></a>TimeSliding</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">keyedStream.window(SlidingProcessingTimeWindows.of(Time.seconds(<span class="number">5</span>), Time.seconds(<span class="number">2</span>)))</span><br></pre></td></tr></table></figure>

<h3 id="TimeSession"><a href="#TimeSession" class="headerlink" title="TimeSession"></a>TimeSession</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">keyedStream.window(ProcessingTimeSessionWindows.withGap(Time.seconds(<span class="number">5</span>)))</span><br></pre></td></tr></table></figure>

<h3 id="CountTumbling"><a href="#CountTumbling" class="headerlink" title="CountTumbling"></a>CountTumbling</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">keyedStream.countWindow(<span class="number">5L</span>)</span><br></pre></td></tr></table></figure>

<h3 id="CountSliding"><a href="#CountSliding" class="headerlink" title="CountSliding"></a>CountSliding</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">keyedStream.countWindow(<span class="number">5L</span>, <span class="number">2L</span>)</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>实时计算</category>
      </categories>
  </entry>
  <entry>
    <title>Hive</title>
    <url>/Hive/</url>
    <content><![CDATA[<h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>Hive：由Facebook开源用于解决海量结构化日志的数据统计工具。</p>
<p>Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类SQL查询功能。</p>
<p>本质是：将HiveSQL转化成MapReduce程序。</p>
<p>（1）Hive处理的数据存储在HDFS</p>
<p>（2）Hive分析数据底层的实现是MapReduce</p>
<p>（3）执行程序运行在Yarn上</p>
<h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><p>1）Hive的执行延迟比较高，因此Hive常用于数据分析，对实时性要求不高的场合</p>
<p>2）Hive优势在于处理大数据，对于处理小数据没有优势，因为Hive的执行延迟比较高</p>
<p>3）Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数</p>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><p>1）Hive的HQL表达能力有限</p>
<p>（1）迭代式算法无法表达</p>
<p>（2）数据挖掘方面不擅长，由于MapReduce数据处理流程的限制，效率更高的算法却无法实现。</p>
<p>2）Hive的效率比较低</p>
<p>（1）Hive自动生成的MapReduce作业，通常情况下不够智能化</p>
<p>（2）Hive调优比较困难，粒度较粗</p>
<h3 id="Hive和数据库比较"><a href="#Hive和数据库比较" class="headerlink" title="Hive和数据库比较"></a>Hive和数据库比较</h3><p>1.由于Hive是针对数据仓库应用设计的，而数据仓库的内容是读多写少的。因此，Hive中不建议对数据的改写，所有的数据都是在加载的时候确定好的。</p>
<p>2.Hive 在查询数据的时候，由于没有索引，需要扫描整个表，因此延迟较高。另外一个导致 Hive 执行延迟高的因素是 MapReduce框架。由于MapReduce 本身具有较高的延迟，因此在利用MapReduce 执行Hive查询时，也会有较高的延迟。</p>
<h2 id="Hive安装地址"><a href="#Hive安装地址" class="headerlink" title="Hive安装地址"></a>Hive安装地址</h2><p>Hive官网地址</p>
<p><a href="http://hive.apache.org/">http://hive.apache.org/</a></p>
<p>文档查看地址</p>
<p><a href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted">https://cwiki.apache.org/confluence/display/Hive/GettingStarted</a></p>
<p>下载地址</p>
<p><a href="http://archive.apache.org/dist/hive/">http://archive.apache.org/dist/hive/</a></p>
<p>github地址</p>
<p><a href="https://github.com/apache/hive">https://github.com/apache/hive</a></p>
<h2 id="MySql安装"><a href="#MySql安装" class="headerlink" title="MySql安装"></a>MySql安装</h2><p>1）检查当前系统是否安装过mysql</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">rpm -qa|grep mariadb</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mariadb-libs-5.5.56-2.el7.x86_64 //如果存在通过如下命令卸载</span><br><span class="line">sudo rpm -e --nodeps  mariadb-libs  //用此命令卸载mariadb</span><br></pre></td></tr></table></figure>

<p>2）将MySQL安装包拷贝到/opt/software目录下</p>
<p>3）解压MySQL安装包</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mkdir mysql-libs</span><br><span class="line">tar -xf mysql-5.7.28-1.el7.x86_64.rpm-bundle.tar -C ./mysql-libs/</span><br></pre></td></tr></table></figure>

<p>4）在安装目录下执行rpm安装</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd mysql-libs</span><br><span class="line">sudo rpm -ivh mysql-community-common-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">sudo rpm -ivh mysql-community-libs-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">sudo rpm -ivh mysql-community-libs-compat-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">sudo rpm -ivh mysql-community-client-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">sudo rpm -ivh mysql-community-server-5.7.28-1.el7.x86_64.rpm</span><br></pre></td></tr></table></figure>

<p>如果Linux是最小化安装的，在安装mysql-community-server-5.7.28-1.el7.x86_64.rpm时可能会出现如下错误:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">警告：mysql-community-server-5.7.28-1.el7.x86_64.rpm: 头V3 DSA/SHA1 Signature, 密钥 ID 5072e1f5: NOKEY</span><br><span class="line">错误：依赖检测失败：</span><br><span class="line">    libaio.so.1()(64bit) 被 mysql-community-server-5.7.28-1.el7.x86_64 需要</span><br><span class="line"></span><br><span class="line">    libaio.so.1(LIBAIO_0.1)(64bit) 被 mysql-community-server-5.7.28-1.el7.x86_64 需要</span><br><span class="line"></span><br><span class="line">    libaio.so.1(LIBAIO_0.4)(64bit) 被 mysql-community-server-5.7.28-1.el7.x86_64 需要</span><br></pre></td></tr></table></figure>

<p>通过yum安装缺少的依赖,然后重新安装mysql-community-server-5.7.28-1.el7.x86_64 即可</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo yum install -y libaio</span><br></pre></td></tr></table></figure>

<p>5）删除/etc/my.cnf文件中datadir指向的目录下的所有内容，如果有内容的情况下</p>
<p>查看datadir的值：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">datadir=/var/lib/mysql</span><br></pre></td></tr></table></figure>

<p>删除/var/lib/mysql目录下的所有内容:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd /var/lib/mysql</span><br><span class="line">sudo rm -rf ./*   //注意执行命令的位置</span><br></pre></td></tr></table></figure>

<p>6）初始化数据库</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo mysqld --initialize --user=mysql</span><br></pre></td></tr></table></figure>

<p>7）查看临时生成的root用户密码</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo cat /var/log/mysqld.log</span><br></pre></td></tr></table></figure>

<p>8）启动mysql服务</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo systemctl start mysqld</span><br></pre></td></tr></table></figure>

<p>9）登录mysql数据库</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mysql -uroot -p</span><br><span class="line">Enter password:  输入临时生成的密码</span><br></pre></td></tr></table></figure>

<p>10）必须先修改root用户的密码，否则执行其他操作会报错</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; set password = password(&quot;新密码&quot;)</span><br></pre></td></tr></table></figure>

<p>11）修改mysql库下user表中的root用户允许任意ip连接</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; update mysql.user set host=&#x27;%&#x27; where user=&#x27;root&#x27;;</span><br><span class="line">mysql&gt; flush privileges;</span><br></pre></td></tr></table></figure>

<h2 id="Hive安装部署"><a href="#Hive安装部署" class="headerlink" title="Hive安装部署"></a>Hive安装部署</h2><p>1）把 apache-hive-3.1.2-bin.tar.gz 上传到linux的 /opt/software 目录下</p>
<p>2）解压 apache-hive-3.1.2-bin.tar.gz 到/opt/module 目录下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -zxvf /opt/software/apache-hive-3.1.2-bin.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>

<p>3）修改 apache-hive-3.1.2-bin 的名称为 hive</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mv /opt/module/apache-hive-3.1.2-bin/ /opt/module/hive</span><br></pre></td></tr></table></figure>

<p>4）修改etc/profile.d/my_env.sh，添加环境变量</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo vim /etc/profile.d/my_env.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">HIVE_HOME</span></span><br><span class="line">HIVE_HOME=/opt/module/hive</span><br><span class="line">PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HIVE_HOME/bin</span><br><span class="line">export PATH JAVA_HOME HADOOP_HOME HIVE_HOME</span><br></pre></td></tr></table></figure>

<p>5）解决日志jar包冲突</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mv $HIVE_HOME/lib/log4j-slf4j-impl-2.10.0.jar $HIVE_HOME/lib/log4j-slf4j-impl-2.10.0.bak</span><br></pre></td></tr></table></figure>

<h2 id="Hive元数据配置到MySql"><a href="#Hive元数据配置到MySql" class="headerlink" title="Hive元数据配置到MySql"></a>Hive元数据配置到MySql</h2><h3 id="拷贝驱动"><a href="#拷贝驱动" class="headerlink" title="拷贝驱动"></a>拷贝驱动</h3><p>将MySQL的JDBC驱动拷贝到Hive的lib目录下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cp /opt/software/mysql-connector-java-5.1.37.jar $HIVE_HOME/lib</span><br></pre></td></tr></table></figure>

<h3 id="配置Metastore到MySql"><a href="#配置Metastore到MySql" class="headerlink" title="配置Metastore到MySql"></a>配置Metastore到MySql</h3><p>在$HIVE_HOME/conf目录下新建hive-site.xml文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim $HIVE_HOME/conf/hive-site.xml</span><br></pre></td></tr></table></figure>

<p>添加如下内容</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot;?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- jdbc连接的URL --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://linux1:3306/metastore?useSSL=false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- jdbc连接的Driver--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">&lt;!-- jdbc连接的username--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">&lt;!-- jdbc连接的password --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>211819<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">&lt;!-- Hive默认在HDFS的工作目录 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">&lt;!-- Hive元数据存储版本的验证 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.schema.verification<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">&lt;!-- 指定存储元数据要连接的地址 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://linux1:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">&lt;!-- 指定hiveserver2连接的端口号 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">value</span>&gt;</span>10000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">&lt;!-- 指定hiveserver2连接的host --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.bind.host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>linux1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">&lt;!-- 元数据存储授权 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.event.db.notification.api.auth<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="安装Tez引擎"><a href="#安装Tez引擎" class="headerlink" title="安装Tez引擎"></a>安装Tez引擎</h3><p>1）将tez安装包拷贝到集群，并解压tar包</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mkdir /opt/module/tez</span><br><span class="line">tar -zxvf /opt/software/tez-0.10.1-SNAPSHOT-minimal.tar.gz -C /opt/module/tez</span><br></pre></td></tr></table></figure>

<p>2）上传tez依赖到HDFS</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop fs -mkdir /tez</span><br><span class="line">hadoop fs -put /opt/software/tez-0.10.1-SNAPSHOT.tar.gz /tez</span><br></pre></td></tr></table></figure>

<p>3）新建tez-site.xml</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim $HADOOP_HOME/etc/hadoop/tez-site.xml</span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.lib.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;fs.defaultFS&#125;/tez/tez-0.10.1-SNAPSHOT.tar.gz<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.use.cluster.hadoop-libs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.am.resource.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>1024<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.am.resource.cpu.vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.container.max.java.heap.fraction<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>0.4<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.task.resource.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>1024<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.task.resource.cpu.vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>4）修改Hadoop环境变量</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim $HADOOP_HOME/etc/hadoop/shellprofile.d/tez.sh</span><br></pre></td></tr></table></figure>

<p>添加Tez的Jar包相关信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop_add_profile tez</span><br><span class="line">function _tez_hadoop_classpath</span><br><span class="line">&#123;</span><br><span class="line">  hadoop_add_classpath &quot;$HADOOP_HOME/etc/hadoop&quot; after</span><br><span class="line">  hadoop_add_classpath &quot;/opt/module/tez/*&quot; after</span><br><span class="line">  hadoop_add_classpath &quot;/opt/module/tez/lib/*&quot; after</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>5）修改Hive的计算引擎</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim $HIVE_HOME/conf/hive-site.xml</span><br></pre></td></tr></table></figure>

<p>添加</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.execution.engine<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>tez<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.tez.container.size<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1024<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>6）解决日志jar包冲突</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">rm /opt/module/tez/lib/slf4j-log4j12-1.7.10.jar</span><br></pre></td></tr></table></figure>

<h2 id="启动Hive"><a href="#启动Hive" class="headerlink" title="启动Hive"></a>启动Hive</h2><h3 id="初始化元数据库"><a href="#初始化元数据库" class="headerlink" title="初始化元数据库"></a>初始化元数据库</h3><p>登录mysql</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mysql -uroot -p</span><br></pre></td></tr></table></figure>

<p>新建Hive元数据库</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; create database metastore;</span><br><span class="line">mysql&gt; quit;</span><br></pre></td></tr></table></figure>

<p>初始化Hive元数据库</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">schematool -initSchema -dbType mysql -verbose</span><br></pre></td></tr></table></figure>

<h3 id="启动metastore和hiveserver2"><a href="#启动metastore和hiveserver2" class="headerlink" title="启动metastore和hiveserver2"></a>启动metastore和hiveserver2</h3><p><strong>启动metastore（不配置 hive.metastore.uris 的话可以直接启动hive，相当于hive直接访问mysql，不需要先启动metastore服务；配置 hive.metastore.uris 相当于hive通过metastore服务连接mysql）</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive --service metastore</span><br></pre></td></tr></table></figure>

<p>注意: 启动后窗口不能再操作，需打开一个新的shell窗口做别的操作</p>
<p><strong>启动 hiveserver2（通过JDBC连接hive需要启动hiveserver2）</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive --service hiveserver2</span><br></pre></td></tr></table></figure>

<p>注意: 启动后窗口不能再操作，需打开一个新的shell窗口做别的操作</p>
<p>编写hive服务启动脚本（可以只使用一个命令窗口）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim $HIVE_HOME/bin/hiveservices.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">HIVE_LOG_DIR=$HIVE_HOME/logs</span><br><span class="line">if [ ! -d $HIVE_LOG_DIR ]</span><br><span class="line">then</span><br><span class="line">	mkdir -p $HIVE_LOG_DIR</span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span><span class="bash">检查进程是否运行正常，参数1为进程名，参数2为进程端口</span></span><br><span class="line">function check_process()</span><br><span class="line">&#123;</span><br><span class="line">    pid=$(ps -ef 2&gt;/dev/null | grep -v grep | grep -i $1 | awk &#x27;&#123;print $2&#125;&#x27;)</span><br><span class="line">    ppid=$(netstat -nltp 2&gt;/dev/null | grep $2 | awk &#x27;&#123;print $7&#125;&#x27; | cut -d &#x27;/&#x27; -f 1)</span><br><span class="line">    echo $pid</span><br><span class="line">    [[ &quot;$pid&quot; =~ &quot;$ppid&quot; ]] &amp;&amp; [ &quot;$ppid&quot; ] &amp;&amp; return 0 || return 1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">function hive_start()</span><br><span class="line">&#123;</span><br><span class="line">    metapid=$(check_process HiveMetastore 9083)</span><br><span class="line">    cmd=&quot;nohup hive --service metastore &gt;$HIVE_LOG_DIR/metastore.log 2&gt;&amp;1 &amp;&quot;</span><br><span class="line">    cmd=$cmd&quot; sleep 4; hdfs dfsadmin -safemode wait &gt;/dev/null 2&gt;&amp;1&quot;</span><br><span class="line">    [ -z &quot;$metapid&quot; ] &amp;&amp; eval $cmd || echo &quot;Metastroe服务已启动&quot;</span><br><span class="line">    server2pid=$(check_process HiveServer2 10000)</span><br><span class="line">    cmd=&quot;nohup hive --service hiveserver2 &gt;$HIVE_LOG_DIR/hiveServer2.log 2&gt;&amp;1 &amp;&quot;</span><br><span class="line">    [ -z &quot;$server2pid&quot; ] &amp;&amp; eval $cmd || echo &quot;HiveServer2服务已启动&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">function hive_stop()</span><br><span class="line">&#123;</span><br><span class="line">    metapid=$(check_process HiveMetastore 9083)</span><br><span class="line">    [ &quot;$metapid&quot; ] &amp;&amp; kill $metapid || echo &quot;Metastore服务未启动&quot;</span><br><span class="line">    server2pid=$(check_process HiveServer2 10000)</span><br><span class="line">    [ &quot;$server2pid&quot; ] &amp;&amp; kill $server2pid || echo &quot;HiveServer2服务未启动&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)</span><br><span class="line">    hive_start</span><br><span class="line">    ;;</span><br><span class="line">&quot;stop&quot;)</span><br><span class="line">    hive_stop</span><br><span class="line">    ;;</span><br><span class="line">&quot;restart&quot;)</span><br><span class="line">    hive_stop</span><br><span class="line">    sleep 2</span><br><span class="line">    hive_start</span><br><span class="line">    ;;</span><br><span class="line">&quot;status&quot;)</span><br><span class="line">    check_process HiveMetastore 9083 &gt;/dev/null &amp;&amp; echo &quot;Metastore服务运行正常&quot; || echo &quot;Metastore服务运行异常&quot;</span><br><span class="line">    check_process HiveServer2 10000 &gt;/dev/null &amp;&amp; echo &quot;HiveServer2服务运行正常&quot; || echo &quot;HiveServer2服务运行异常&quot;</span><br><span class="line">    ;;</span><br><span class="line">*)</span><br><span class="line">    echo Invalid Args!</span><br><span class="line">    echo &#x27;Usage: &#x27;$(basename $0)&#x27; start|stop|restart|status&#x27;</span><br><span class="line">    ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">chmod +x $HIVE_HOME/bin/hiveservices.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hiveservices.sh start</span><br></pre></td></tr></table></figure>

<h3 id="HiveJDBC访问"><a href="#HiveJDBC访问" class="headerlink" title="HiveJDBC访问"></a>HiveJDBC访问</h3><p>1）启动beeline客户端</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hive]$ bin/beeline -u jdbc:hive2://linux1:10000 -n vincent</span><br></pre></td></tr></table></figure>

<p>2）看到如下界面</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">Connecting</span> to jdbc:hive<span class="number">2</span>://linux<span class="number">1</span>:<span class="number">10000</span></span><br><span class="line"><span class="attribute">Connected</span> to: Apache Hive (version <span class="number">3</span>.<span class="number">1</span>.<span class="number">2</span>)</span><br><span class="line"><span class="attribute">Driver</span>: Hive JDBC (version <span class="number">3</span>.<span class="number">1</span>.<span class="number">2</span>)</span><br><span class="line"><span class="attribute">Transaction</span> isolation: TRANSACTION_REPEATABLE_READ</span><br><span class="line"><span class="attribute">Beeline</span> version <span class="number">3</span>.<span class="number">1</span>.<span class="number">2</span> by Apache Hive</span><br></pre></td></tr></table></figure>

<h3 id="Hive访问"><a href="#Hive访问" class="headerlink" title="Hive访问"></a>Hive访问</h3><p>1）启动hive客户端</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hive]$ bin/hive</span><br></pre></td></tr></table></figure>

<p>2）看到如下界面</p>
<figure class="highlight gradle"><table><tr><td class="code"><pre><span class="line">which: no hbase in (<span class="regexp">/usr/</span>local<span class="regexp">/bin:/u</span>sr<span class="regexp">/bin:/u</span>sr<span class="regexp">/local/</span>sbin:<span class="regexp">/usr/</span>sbin:<span class="regexp">/opt/m</span>odule<span class="regexp">/jdk1.8.0_212/</span>bin:<span class="regexp">/opt/m</span>odule<span class="regexp">/hadoop-3.1.3/</span>bin:<span class="regexp">/opt/m</span>odule<span class="regexp">/hadoop-3.1.3/</span>sbin:<span class="regexp">/opt/m</span>odule<span class="regexp">/hive/</span>bin:<span class="regexp">/home/</span>vincent<span class="regexp">/.local/</span>bin:<span class="regexp">/home/</span>vincent/bin)</span><br><span class="line">Hive Session ID = <span class="number">486</span>a2ca7-<span class="number">44</span>d4-<span class="number">41</span>a1-a5fb-<span class="number">4564</span>f3f3d54f</span><br><span class="line"></span><br><span class="line">Logging initialized using configuration in jar:<span class="keyword">file</span>:<span class="regexp">/opt/m</span>odule<span class="regexp">/hive/</span>lib<span class="regexp">/hive-common-3.1.2.jar!/</span>hive-log4j2.properties Async: <span class="keyword">true</span></span><br><span class="line">Hive Session ID = <span class="number">187</span>d1d0a-b70d-<span class="number">4</span>fe5-<span class="number">92</span>d9-<span class="number">514</span>e256a4dd0</span><br></pre></td></tr></table></figure>

<p>3）打印当前库和表头</p>
<p>在hive-site.xml中加入如下两个配置:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim $HIVE_HOME/conf/hive-site.xml</span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.header<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether to print the names of the columns in query output.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.current.db<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether to include the current database in the Hive prompt.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="Hive常用交互命令"><a href="#Hive常用交互命令" class="headerlink" title="Hive常用交互命令"></a>Hive常用交互命令</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/hive -help</span><br></pre></td></tr></table></figure>

<p>-e 不进入hive的交互窗口执行sql语句</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/hive -e &quot;select id from student;&quot;</span><br></pre></td></tr></table></figure>

<p>-f 执行脚本中的sql语句</p>
<p>1）在/opt/module/hive/下创建datas目录并在datas目录下创建hivef.sql文件</p>
<p>2）文件中写入正确的sql语句</p>
<p>3）执行文件中的sql语句</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hive]$ bin/hive -f /opt/module/hive/datas/hivef.sql</span><br></pre></td></tr></table></figure>

<p>4）执行文件中的sql语句并将结果写入文件中</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hive]$ bin/hive -f /opt/module/hive/datas/hivef.sql  &gt; /opt/module/datas/hive_result.txt</span><br></pre></td></tr></table></figure>

<p>在hive cli命令窗口中如何查看hdfs文件系统</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">hive(default)&gt;</span><span class="bash">dfs -ls /;</span></span><br></pre></td></tr></table></figure>

<p>查看在hive中输入的所有历史命令</p>
<p>进入当前用户的根目录/root或者/home/vincent</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 ~]$ cat .hivehistory</span><br></pre></td></tr></table></figure>

<h2 id="Hive常见属性配置"><a href="#Hive常见属性配置" class="headerlink" title="Hive常见属性配置"></a>Hive常见属性配置</h2><p>1）Hive的log默认存放在/tmp/vincent/hive.log目录下（当前用户名下）</p>
<p>2）修改hive的log存放到/opt/module/hive/logs</p>
<p>修改 /opt/module/hive/conf/hive-log4j2.properties.template 文件名称为 hive-log4j2.properties</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 conf]$ mv hive-log4j2.properties.template hive-log4j2.properties</span><br></pre></td></tr></table></figure>

<p>在hive-log4j2.properties文件中修改log存放位置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive.log.dir=/opt/module/hive/logs</span><br></pre></td></tr></table></figure>

<h2 id="参数配置方式"><a href="#参数配置方式" class="headerlink" title="参数配置方式"></a>参数配置方式</h2><p>查看当前所有的配置信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"><span class="built_in">set</span></span></span><br></pre></td></tr></table></figure>

<p>参数配置的三种方式</p>
<p>1）配置文件方式</p>
<p>默认配置文件：hive-default.xml</p>
<p>用户自定义配置文件：hive-site.xml</p>
<p>注意：用户自定义配置会覆盖默认配置。另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。配置文件的设定对本机启动的所有Hive进程都有效。</p>
<p>2）命令行参数方式</p>
<p>启动Hive时，可以在命令行添加 -hiveconf param=value 来设定参数。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hive]$ bin/hive -hiveconf mapred.reduce.tasks=10;</span><br></pre></td></tr></table></figure>

<p>注意：仅对本次hive启动有效</p>
<p>3）参数声明方式</p>
<p>查看参数设置：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks;</span><br></pre></td></tr></table></figure>

<p>可以在HQL中使用SET关键字设定参数</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks=100;</span><br></pre></td></tr></table></figure>

<p>注意：仅对本次hive启动有效。</p>
<p>上述三种设定方式的优先级依次递增。即 配置文件&lt;命令行参数&lt;参数声明。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在会话建立以前已经完成了。</p>
<h2 id="Hive数据类型"><a href="#Hive数据类型" class="headerlink" title="Hive数据类型"></a>Hive数据类型</h2><h3 id="基本数据类型"><a href="#基本数据类型" class="headerlink" title="基本数据类型"></a>基本数据类型</h3><table>
<thead>
<tr>
<th>Hive数据类型</th>
<th>Java数据类型</th>
<th>长度</th>
<th>例子</th>
</tr>
</thead>
<tbody><tr>
<td>TINYINT</td>
<td>byte</td>
<td>1byte有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>SMALINT</td>
<td>short</td>
<td>2byte有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>INT</td>
<td>int</td>
<td>4byte有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>BIGINT</td>
<td>long</td>
<td>8byte有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>BOOLEAN</td>
<td>boolean</td>
<td>布尔类型，true或者false</td>
<td>TRUE  FALSE</td>
</tr>
<tr>
<td>FLOAT</td>
<td>float</td>
<td>单精度浮点数</td>
<td>3.14159</td>
</tr>
<tr>
<td>DOUBLE</td>
<td>double</td>
<td>双精度浮点数</td>
<td>3.14159</td>
</tr>
<tr>
<td>STRING</td>
<td>string</td>
<td>字符系列。可以指定字符集。可以使用单引号或者双引号。</td>
<td>‘now is the time’ “for all good men”</td>
</tr>
<tr>
<td>TIMESTAMP</td>
<td></td>
<td>时间类型</td>
<td></td>
</tr>
<tr>
<td>BINARY</td>
<td></td>
<td>字节数组</td>
<td></td>
</tr>
</tbody></table>
<p><strong>对于Hive的String类型相当于数据库的varchar类型，该类型是一个可变的字符串，不过它不能声明其中最多能存储多少个字符，理论上它可以存储2GB的字符数。</strong></p>
<h3 id="集合数据类型"><a href="#集合数据类型" class="headerlink" title="集合数据类型"></a>集合数据类型</h3><table>
<thead>
<tr>
<th>数据类型</th>
<th>描述</th>
<th>语法示例</th>
</tr>
</thead>
<tbody><tr>
<td>STRUCT</td>
<td>和c语言中的struct类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是STRUCT{first STRING, last STRING},那么第1个元素可以通过字段.first来引用。</td>
<td>struct()例如struct&lt;street:string, city:string&gt;</td>
</tr>
<tr>
<td>MAP</td>
<td>MAP是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是MAP，其中键-&gt;值对是’first’-&gt;’John’和’last’-&gt;’Doe’，那么可以通过字段名[‘last’]获取最后一个元素</td>
<td>map()例如map&lt;string, int&gt;</td>
</tr>
<tr>
<td>ARRAY</td>
<td>数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’, ‘Doe’]，那么第2个元素可以通过数组名[1]进行引用。</td>
<td>Array()例如array<string></string></td>
</tr>
</tbody></table>
<p>Hive有三种复杂数据类型ARRAY、MAP 和 STRUCT。ARRAY和MAP与Java中的Array和Map类似，而STRUCT与C语言中的Struct类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套。</p>
<p>案例实操</p>
<p>1）假设某表有如下一行，我们用JSON格式来表示其数据结构。在Hive下访问的格式为</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;name&quot;</span>: <span class="string">&quot;songsong&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;friends&quot;</span>: [<span class="string">&quot;bingbing&quot;</span> , <span class="string">&quot;lili&quot;</span>] ,       <span class="comment">//列表Array, </span></span><br><span class="line">    <span class="attr">&quot;children&quot;</span>: &#123;                      			 <span class="comment">//键值Map,</span></span><br><span class="line">        <span class="attr">&quot;xiao song&quot;</span>: <span class="number">18</span> ,</span><br><span class="line">        <span class="attr">&quot;xiaoxiao song&quot;</span>: <span class="number">19</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="string">&quot;address&quot;</span>: &#123;                      			 <span class="comment">//结构Struct,</span></span><br><span class="line">        <span class="attr">&quot;street&quot;</span>: <span class="string">&quot;hui long guan&quot;</span> ,</span><br><span class="line">        <span class="attr">&quot;city&quot;</span>: <span class="string">&quot;beijing&quot;</span> </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>2）基于上述数据结构，我们在Hive里创建对应的表，并导入数据</p>
<p>创建本地测试文件test.txt</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">songsong</span>,bingbing_lili,xiao song:<span class="number">18</span>_xiaoxiao song:<span class="number">19</span>,hui long guan_beijing</span><br><span class="line"><span class="attribute">yangyang</span>,caicai_susu,xiao yang:<span class="number">18</span>_xiaoxiao yang:<span class="number">19</span>,chao yang_beijing</span><br></pre></td></tr></table></figure>

<p>注意：MAP，STRUCT和ARRAY里的元素间关系都可以用同一个字符表示，这里用“_”。</p>
<p>3）Hive上创建测试表test</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test(</span><br><span class="line">name string,</span><br><span class="line">friends <span class="keyword">array</span><span class="operator">&lt;</span>string<span class="operator">&gt;</span>,</span><br><span class="line">children map<span class="operator">&lt;</span>string, <span class="type">int</span><span class="operator">&gt;</span>,</span><br><span class="line">address struct<span class="operator">&lt;</span>street:string, city:string<span class="operator">&gt;</span></span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line">collection items terminated <span class="keyword">by</span> <span class="string">&#x27;_&#x27;</span></span><br><span class="line">map keys terminated <span class="keyword">by</span> <span class="string">&#x27;:&#x27;</span></span><br><span class="line">lines terminated <span class="keyword">by</span> <span class="string">&#x27;\n&#x27;</span>;</span><br></pre></td></tr></table></figure>

<p>字段解释：</p>
<p>row format delimited fields terminated by ‘,’  – 列分隔符</p>
<p>collection items terminated by ‘_’  –MAP STRUCT 和 ARRAY 中元素之间的分隔符(数据分割符号)</p>
<p>map keys terminated by ‘:’            – MAP中的key与value的分隔符</p>
<p>lines terminated by ‘\n’;                – 行分隔符</p>
<p>4）导入文本数据到测试表</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ hadoop fs -put test.txt /user/hive/warehouse/test/</span><br></pre></td></tr></table></figure>

<p>5）访问三种集合列里的数据，以下分别是ARRAY，MAP，STRUCT的访问方式</p>
<p>访问STRUCT结构中元素    address.city    访问MAP中元素    children[‘xiao song’]    访问ARRAY元素    friends[1]</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select friends[1],children[&#x27;xiao song&#x27;],address.city from test</span><br><span class="line">where name=&quot;songsong&quot;;</span><br><span class="line">OK</span><br><span class="line">_c0     _c1     city</span><br><span class="line">lili    18      beijing</span><br><span class="line">Time taken: 0.076 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure>

<h3 id="类型转化"><a href="#类型转化" class="headerlink" title="类型转化"></a>类型转化</h3><p><strong>Hive的基本数据类型是可以进行隐式转换的，类似于Java的类型转换</strong>，例如某表达式使用INT类型，TINYINT会自动转换为INT类型，<strong>但是Hive不会进行反向转化</strong>，例如，某表达式使用TINYINT类型，INT不会自动转换为TINYINT类型，它会返回错误，除非使用CAST操作。</p>
<p>1）隐式类型转换规则如下</p>
<p>（1）任何整数类型都可以隐式地转换为一个范围更广的类型，如TINYINT可以转换成INT，INT可以转换成    BIGINT。</p>
<p>（2）所有整数类型、FLOAT和STRING类型都可以隐式地转换成DOUBLE。</p>
<p>（3）TINYINT、SMALLINT、INT都可以转换为FLOAT。</p>
<p>（4）BOOLEAN类型不可以转换为任何其它的类型。</p>
<p>2）可以使用CAST操作显示进行数据类型转换</p>
<p>例如CAST(‘1’ AS INT)将把字符串’1’ 转换成整数1；如果强制类型转换失败，如执行CAST(‘X’ AS INT)，表达式返回空值 NULL。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select &#x27;1&#x27; + 2, cast(&#x27;1&#x27; as int) + 2;</span><br></pre></td></tr></table></figure>

<figure class="highlight asciidoc"><table><tr><td class="code"><pre><span class="line"><span class="code">+------+</span>------+</span><br><span class="line"><span class="section">| _c0  | _c1  |</span></span><br><span class="line"><span class="section">+------+------+</span></span><br><span class="line"><span class="section">| 3.0  | 3    |</span></span><br><span class="line"><span class="section">+------+------+</span></span><br></pre></td></tr></table></figure>

<h2 id="DDL数据定义"><a href="#DDL数据定义" class="headerlink" title="DDL数据定义"></a>DDL数据定义</h2><h3 id="创建数据库"><a href="#创建数据库" class="headerlink" title="创建数据库"></a>创建数据库</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CREATE DATABASE [IF NOT EXISTS] database_name</span><br><span class="line">[COMMENT database_comment]</span><br><span class="line">[LOCATION hdfs_path]</span><br><span class="line">[WITH DBPROPERTIES (property_name=property_value, ...)];</span><br></pre></td></tr></table></figure>

<p>1）创建一个数据库，数据库在HDFS上的默认存储路径是/user/hive/warehouse/*.db</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create database db_hive;</span><br></pre></td></tr></table></figure>

<p>2）避免要创建的数据库已经存在错误，增加if not exists判断。（标准写法）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create database db_hive;</span><br><span class="line">FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Database db_hive already exists</span><br><span class="line">hive (default)&gt; create database if not exists db_hive;</span><br></pre></td></tr></table></figure>

<p>3）创建一个数据库，指定数据库在HDFS上存放的位置</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create database db_hive2 location &#x27;/db_hive2.db&#x27;;</span><br></pre></td></tr></table></figure>

<h3 id="查询数据库"><a href="#查询数据库" class="headerlink" title="查询数据库"></a>查询数据库</h3><h4 id="显示数据库"><a href="#显示数据库" class="headerlink" title="显示数据库"></a>显示数据库</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive&gt; show databases;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive&gt; show databases like &#x27;db_hive*&#x27;;</span><br><span class="line">OK</span><br><span class="line">db_hive</span><br><span class="line">db_hive_1</span><br></pre></td></tr></table></figure>

<h4 id="查看数据库详情"><a href="#查看数据库详情" class="headerlink" title="查看数据库详情"></a>查看数据库详情</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive&gt; desc database db_hive;</span><br></pre></td></tr></table></figure>

<p>更详细信息 extended</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive&gt; desc database extended db_hive;</span><br></pre></td></tr></table></figure>

<h4 id="切换当前数据库"><a href="#切换当前数据库" class="headerlink" title="切换当前数据库"></a>切换当前数据库</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; use db_hive;</span><br></pre></td></tr></table></figure>

<h3 id="修改数据库"><a href="#修改数据库" class="headerlink" title="修改数据库"></a>修改数据库</h3><p>用户可以使用ALTER DATABASE命令为某个数据库的DBPROPERTIES设置键-值对属性值，来描述这个数据库的属性信息。数据库的其他元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter database db_hive set dbproperties(&#x27;createtime&#x27;=&#x27;20170830&#x27;);</span><br></pre></td></tr></table></figure>

<p>在hive中查看修改结果</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive&gt; desc database extended db_hive;</span><br></pre></td></tr></table></figure>

<figure class="highlight asciidoc"><table><tr><td class="code"><pre><span class="line"><span class="code">+----------+</span>----------<span class="code">+----------------------------------------------------+</span>-------------<span class="code">+-------------+</span>------------------------+</span><br><span class="line"><span class="section">| db_name  | comment  |                      location                      | owner_name  | owner_type  |       parameters       |</span></span><br><span class="line"><span class="section">+----------+----------+----------------------------------------------------+-------------+-------------+------------------------+</span></span><br><span class="line"><span class="section">| db_hive  |          | hdfs://linux1:9820/user/hive/warehouse/db_hive.db  | vincent     | USER        | &#123;createtime=20170830&#125;  |</span></span><br><span class="line"><span class="section">+----------+----------+----------------------------------------------------+-------------+-------------+------------------------+</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="删除数据库"><a href="#删除数据库" class="headerlink" title="删除数据库"></a>删除数据库</h3><p>1）删除空数据库</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive&gt;drop database db_hive2;</span><br></pre></td></tr></table></figure>

<p>2）如果删除的数据库不存在，最好采用if exists判断数据库是否存在</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive&gt; drop database db_hive;</span><br><span class="line">FAILED: SemanticException [Error 10072]: Database does not exist: db_hive</span><br><span class="line">hive&gt; drop database if exists db_hive2;</span><br></pre></td></tr></table></figure>

<p>3）如果数据库不为空，可以采用cascade命令，强制删除</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive&gt; drop database db_hive;</span><br><span class="line">FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. InvalidOperationException(message:Database db_hive is not empty. One or more tables exist.)</span><br><span class="line">hive&gt; drop database db_hive cascade;</span><br></pre></td></tr></table></figure>

<h3 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h3><p>1）建表语法</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name </span><br><span class="line">[(col_name data_type [COMMENT col_comment], ...)] </span><br><span class="line">[COMMENT table_comment] </span><br><span class="line">[PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] </span><br><span class="line">[CLUSTERED BY (col_name, col_name, ...) </span><br><span class="line">[SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] </span><br><span class="line">[ROW FORMAT row_format] </span><br><span class="line">[STORED AS file_format] </span><br><span class="line">[LOCATION hdfs_path]</span><br><span class="line">[TBLPROPERTIES (property_name=property_value, ...)]</span><br><span class="line">[AS select_statement]</span><br></pre></td></tr></table></figure>

<p>2）字段解释说明</p>
<p>（1）CREATE TABLE 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。</p>
<p>（2）EXTERNAL关键字可以让用户创建一个外部表，在建表的同时可以指定一个指向实际数据的路径（LOCATION），在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。</p>
<p>（3）COMMENT：为表和列添加注释。</p>
<p><strong>（4）PARTITIONED BY创建分区表</strong></p>
<p><strong>（5）CLUSTERED BY创建分桶表</strong></p>
<p><strong>（6）SORTED BY不常用，对桶中的一个或多个列另外排序</strong></p>
<p>（7）ROW FORMAT </p>
<p>DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char]</p>
<p>​    [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] </p>
<p>  | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, …)]</p>
<p>用户在建表的时候可以自定义SerDe或者使用自带的SerDe。如果没有指定ROW FORMAT 或者ROW FORMAT DELIMITED，将会使用自带的SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的SerDe，Hive通过SerDe确定表的具体的列的数据。</p>
<p>SerDe是Serialize/Deserilize的简称， hive使用Serde进行行对象的序列与反序列化。</p>
<p>（8）STORED AS指定存储文件类型</p>
<p>常用的存储文件类型：SEQUENCEFILE（二进制序列文件）、TEXTFILE（文本）、RCFILE（列式存储格式文件）</p>
<p>如果文件数据是纯文本，可以使用STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCEFILE。</p>
<p>（9）LOCATION ：指定表在HDFS上的存储位置。</p>
<p>（10）AS：后跟查询语句，根据查询结果创建表。</p>
<p>（11）LIKE允许用户复制现有的表结构，但是不复制数据。</p>
<h4 id="管理表"><a href="#管理表" class="headerlink" title="管理表"></a>管理表</h4><p><strong>默认创建的表都是所谓的管理表，有时也被称为内部表。</strong>因为这种表，Hive会（或多或少地）控制着数据的生命周期。Hive默认情况下会将这些表的数据存储在由配置项hive.metastore.warehouse.dir(例如，/user/hive/warehouse)所定义的目录的子目录下。    <strong>当我们删除一个管理表时，Hive也会删除这个表中数据。管理表不适合和其他工具共享数据。</strong></p>
<p>案例实操</p>
<p>0）实际数据</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">1001</span>	ss<span class="number">1</span></span><br><span class="line"><span class="attribute">1002</span>	ss<span class="number">2</span></span><br><span class="line"><span class="attribute">1003</span>	ss<span class="number">3</span></span><br><span class="line"><span class="attribute">1004</span>	ss<span class="number">4</span></span><br><span class="line"><span class="attribute">1005</span>	ss<span class="number">5</span></span><br><span class="line"><span class="attribute">1006</span>	ss<span class="number">6</span></span><br><span class="line"><span class="attribute">1007</span>	ss<span class="number">7</span></span><br><span class="line"><span class="attribute">1008</span>	ss<span class="number">8</span></span><br><span class="line"><span class="attribute">1009</span>	ss<span class="number">9</span></span><br><span class="line"><span class="attribute">1010</span>	ss<span class="number">10</span></span><br><span class="line"><span class="attribute">1011</span>	ss<span class="number">11</span></span><br><span class="line"><span class="attribute">1012</span>	ss<span class="number">12</span></span><br><span class="line"><span class="attribute">1013</span>	ss<span class="number">13</span></span><br><span class="line"><span class="attribute">1014</span>	ss<span class="number">14</span></span><br><span class="line"><span class="attribute">1015</span>	ss<span class="number">15</span></span><br><span class="line"><span class="attribute">1016</span>	ss<span class="number">16</span></span><br></pre></td></tr></table></figure>

<p>1）创建内部表</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table if not exists student(</span><br><span class="line">id int, </span><br><span class="line">name string</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;</span><br><span class="line">stored as textfile</span><br><span class="line">location &#x27;/user/hive/warehouse/student&#x27;;</span><br></pre></td></tr></table></figure>

<p>2）根据查询结果创建表（查询的结果会添加到新创建的表中）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table if not exists student2 as select id, name from student;</span><br></pre></td></tr></table></figure>

<p>3）根据已经存在的表结构创建表（<strong>只复制表结构</strong>）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table if not exists student3 like student;</span><br></pre></td></tr></table></figure>

<p>4）查询表的类型</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student2;</span><br><span class="line">Table Type:             MANAGED_TABLE</span><br></pre></td></tr></table></figure>

<h4 id="外部表"><a href="#外部表" class="headerlink" title="外部表"></a>外部表</h4><p>因为表是外部表，所以Hive并非认为其完全拥有这份数据。<strong>删除该表并不会删除掉这份数据，不过描述表的元数据信息会被删除掉。</strong></p>
<p>管理表和外部表的使用场景</p>
<p>每天将收集到的网站日志定期流入HDFS文本文件。在外部表（原始日志表）的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过SELECT+INSERT进入内部表。</p>
<p>外部表删除后，hdfs中的数据还在，但是metadata中dept的元数据已被删除</p>
<h4 id="管理表与外部表的互相转换"><a href="#管理表与外部表的互相转换" class="headerlink" title="管理表与外部表的互相转换"></a>管理表与外部表的互相转换</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">alter table student set tblproperties(&#x27;EXTERNAL&#x27;=&#x27;TRUE&#x27;);</span><br><span class="line">alter table student set tblproperties(&#x27;EXTERNAL&#x27;=&#x27;FALSE&#x27;);</span><br></pre></td></tr></table></figure>

<p>注意：(‘EXTERNAL’=’TRUE’)和(‘EXTERNAL’=’FALSE’)为固定写法，区分大小写。</p>
<h3 id="修改表"><a href="#修改表" class="headerlink" title="修改表"></a>修改表</h3><h4 id="重命名表"><a href="#重命名表" class="headerlink" title="重命名表"></a>重命名表</h4><p>ALTER TABLE table_name RENAME TO new_table_name</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition2 rename to dept_partition3;</span><br></pre></td></tr></table></figure>

<h4 id="增加、修改和删除表分区"><a href="#增加、修改和删除表分区" class="headerlink" title="增加、修改和删除表分区"></a>增加、修改和删除表分区</h4><h4 id="增加-修改-替换列信息"><a href="#增加-修改-替换列信息" class="headerlink" title="增加/修改/替换列信息"></a>增加/修改/替换列信息</h4><p>更新列</p>
<p>ALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name]</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept change column deptdesc description string;</span><br></pre></td></tr></table></figure>

<p>增加和替换列</p>
<p>ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], …)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept add columns(deptdesc string);</span><br><span class="line">hive (default)&gt; alter table dept replace columns(deptno string, dname string, loc string);</span><br></pre></td></tr></table></figure>

<h3 id="删除表"><a href="#删除表" class="headerlink" title="删除表"></a>删除表</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; drop table dept;</span><br></pre></td></tr></table></figure>

<h2 id="分区表与分桶表"><a href="#分区表与分桶表" class="headerlink" title="分区表与分桶表"></a>分区表与分桶表</h2><h3 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h3><p>分区表实际上就是对应一个HDFS文件系统上的独立的文件夹，该文件夹下是该分区所有的数据文件。Hive中的分区就是分目录，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过WHERE子句中的表达式选择查询所需要的指定的分区，这样的查询效率会提高很多。</p>
<p>1）引入分区表</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><span class="line">dept_20200401.log		<span class="regexp">//</span><span class="number">10</span>	ACCOUNTING	<span class="number">1700</span></span><br><span class="line">										<span class="regexp">//</span><span class="number">20</span>	RESEARCH	<span class="number">1800</span></span><br><span class="line">dept_20200402.log		<span class="regexp">//</span><span class="number">30</span>	SALES	<span class="number">1900</span></span><br><span class="line">										<span class="regexp">//</span><span class="number">40</span>	OPERATIONS	<span class="number">1700</span></span><br><span class="line">dept_20200403.log		<span class="regexp">//</span><span class="number">50</span>	TEST	<span class="number">2000</span></span><br><span class="line">										<span class="regexp">//</span><span class="number">60</span>	DEV	<span class="number">1900</span></span><br></pre></td></tr></table></figure>

<p>2）创建分区表语法</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create table dept_partition(</span><br><span class="line">deptno int,</span><br><span class="line">dname string,</span><br><span class="line">loc string</span><br><span class="line">)</span><br><span class="line">partitioned by (day string)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure>

<p><strong>注意：分区字段不能是表中已经存在的数据，可以将分区字段看作表的伪列。</strong></p>
<p>3）加载数据到分区表中</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/hive/datas/dept_20200401.log&#x27; into table dept_partition partition(day=&#x27;20200401&#x27;);</span><br><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/hive/datas/dept_20200402.log&#x27; into table dept_partition partition(day=&#x27;20200402&#x27;);</span><br><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/hive/datas/dept_20200403.log&#x27; into table dept_partition partition(day=&#x27;20200403&#x27;);</span><br></pre></td></tr></table></figure>

<p>注意：分区表加载数据时，必须指定分区</p>
<p>4）查询分区表中数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition where day=&#x27;20200401&#x27;;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition where day=&#x27;20200401&#x27;</span><br><span class="line">              union</span><br><span class="line">              select * from dept_partition where day=&#x27;20200402&#x27;</span><br><span class="line">              union</span><br><span class="line">              select * from dept_partition where day=&#x27;20200403&#x27;;</span><br><span class="line">hive (default)&gt; select * from dept_partition where day=&#x27;20200401&#x27; or</span><br><span class="line">                day=&#x27;20200402&#x27; or day=&#x27;20200403&#x27; ;</span><br></pre></td></tr></table></figure>

<p>5）增加分区</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition add partition(day=&#x27;20200404&#x27;);</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition add partition(day=&#x27;20200405&#x27;) partition(day=&#x27;20200406&#x27;);</span><br></pre></td></tr></table></figure>

<p>6）删除分区</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition drop partition (day=&#x27;20200406&#x27;);</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition drop partition (day=&#x27;20200404&#x27;), partition(day=&#x27;20200405&#x27;);</span><br></pre></td></tr></table></figure>

<p>7）查看分区表有多少分区</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive&gt; show partitions dept_partition;</span><br></pre></td></tr></table></figure>

<p>8）查看分区表结构</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive&gt; desc formatted dept_partition; </span><br></pre></td></tr></table></figure>

<p>如果一天的的日志数据也很大，如何将一天的日志分区？</p>
<p>1）创建二级分区表</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table dept_partition2(</span><br><span class="line">deptno int,</span><br><span class="line">dname string,</span><br><span class="line">loc string</span><br><span class="line">)</span><br><span class="line">partitioned by (day string, hour string)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure>

<p>2）加载数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">load data local inpath &#x27;/opt/module/hive/datas/dept_20200401.log&#x27; into table</span><br><span class="line">dept_partition2 partition(day=&#x27;20200401&#x27;, hour=&#x27;12&#x27;);</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select * from dept_partition2 where day=&#x27;20200401&#x27; and hour=&#x27;12&#x27;;</span><br></pre></td></tr></table></figure>

<p>3）将数据直接上传到分区目录上，让分区表和数据产生关联的三种方式</p>
<p>1.上传数据后修复</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir -p /user/hive/warehouse/dept_partition2/day=20200401/hour=13;</span><br><span class="line"></span><br><span class="line">hive (default)&gt; dfs -put /opt/module/hive/datas/dept_20200401.log /user/hive/warehouse/dept_partition2/day=20200401/hour=13;</span><br><span class="line"></span><br><span class="line">hive (default)&gt; msck repair table dept_partition2;</span><br><span class="line"></span><br><span class="line">hive (default)&gt; select * from dept_partition2 where day=&#x27;20200401&#x27; and hour=&#x27;13&#x27;;</span><br></pre></td></tr></table></figure>

<p>2.创建文件夹后load数据到分区</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir -p /user/hive/warehouse/dept_partition2/day=20200401/hour=14;</span><br><span class="line"></span><br><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/hive/datas/dept_20200401.log&#x27; into table dept_partition2 partition(day=&#x27;20200401&#x27;,hour=&#x27;14&#x27;);</span><br><span class="line"></span><br><span class="line">hive (default)&gt; select * from dept_partition2 where day=&#x27;20200401&#x27; and hour=&#x27;14&#x27;;</span><br></pre></td></tr></table></figure>

<h3 id="动态分区调整"><a href="#动态分区调整" class="headerlink" title="动态分区调整"></a>动态分区调整</h3><p>关系型数据库中，对分区表Insert数据时候，数据库自动会根据分区字段的值，将数据插入到相应的分区中，Hive中也提供了类似的机制，即动态分区(Dynamic Partition)，只不过，使用Hive的动态分区，需要进行相应的配置。</p>
<p>（1）开启动态分区功能（默认true，开启）</p>
<figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="attr">hive.exec.dynamic.partition</span>=<span class="literal">true</span></span><br></pre></td></tr></table></figure>

<p>（2）设置为<strong>非严格模式</strong>（动态分区的模式，默认strict，表示必须指定至少一个分区为静态分区，nonstrict模式表示允许所有的分区字段都可以使用动态分区。）</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">hive<span class="selector-class">.exec</span><span class="selector-class">.dynamic</span><span class="selector-class">.partition</span>.mode=nonstrict</span><br></pre></td></tr></table></figure>

<p>（3）在<strong>所有执行MR的节点</strong>上，最大一共可以创建多少个动态分区。默认1000</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">hive<span class="selector-class">.exec</span><span class="selector-class">.max</span><span class="selector-class">.dynamic</span>.partitions=<span class="number">1000</span></span><br></pre></td></tr></table></figure>

<p>（4）在<strong>每个执行MR的节点上</strong>，最大可以创建多少个动态分区。该参数需要根据实际的数据来设定。比如：源数据中包含了一年的数据，即day字段有365个值，那么该参数就需要设置成大于365，如果使用默认值100，则会报错。</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">hive<span class="selector-class">.exec</span><span class="selector-class">.max</span><span class="selector-class">.dynamic</span><span class="selector-class">.partitions</span>.pernode=<span class="number">100</span></span><br></pre></td></tr></table></figure>

<p>（5）整个MR Job中，最大可以创建多少个HDFS文件。默认100000</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">hive<span class="selector-class">.exec</span><span class="selector-class">.max</span><span class="selector-class">.created</span>.files=<span class="number">100000</span></span><br></pre></td></tr></table></figure>

<p>（6）当有空分区生成时，是否抛出异常。一般不需要设置。默认false</p>
<figure class="highlight vbscript"><table><tr><td class="code"><pre><span class="line">hive.<span class="keyword">error</span>.<span class="keyword">on</span>.<span class="literal">empty</span>.partition=<span class="literal">false</span></span><br></pre></td></tr></table></figure>

<p>需求：将dept表中的数据按照地区（loc字段），插入到目标表dept_partition的相应分区中。</p>
<p>（1）创建目标分区表</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create table dept_partition_dy(id int, name string) partitioned by (loc int) row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure>

<p>（2）设置动态分区</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">set hive.exec.dynamic.partition.mode = nonstrict;</span><br><span class="line"></span><br><span class="line">hive (default)&gt; insert into table dept_partition_dy partition(loc) select deptno, dname, loc from dept_partition;</span><br></pre></td></tr></table></figure>

<p>（3）查看目标分区表的分区情况</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; show partitions dept_partition;</span><br><span class="line">hive (default)&gt; show partitions dept_partition_dy;</span><br></pre></td></tr></table></figure>

<h3 id="分桶表"><a href="#分桶表" class="headerlink" title="分桶表"></a>分桶表</h3><p>分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理的分区。<strong>对于一张表或者分区，Hive 可以进一步组织成桶，也就是更为细粒度的数据范围划分。</strong></p>
<p>分桶是将数据集分解成更容易管理的若干部分的另一个技术。</p>
<p><strong>分区针对的是数据的存储路径；分桶针对的是数据文件。</strong></p>
<p><strong>分桶表字段是表内字段。</strong></p>
<p>1）先创建分桶表，通过直接导入数据文件的方式</p>
<p>（1）数据准备</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">1001</span>	ss<span class="number">1</span></span><br><span class="line"><span class="attribute">1002</span>	ss<span class="number">2</span></span><br><span class="line"><span class="attribute">1003</span>	ss<span class="number">3</span></span><br><span class="line"><span class="attribute">1004</span>	ss<span class="number">4</span></span><br><span class="line"><span class="attribute">1005</span>	ss<span class="number">5</span></span><br><span class="line"><span class="attribute">1006</span>	ss<span class="number">6</span></span><br><span class="line"><span class="attribute">1007</span>	ss<span class="number">7</span></span><br><span class="line"><span class="attribute">1008</span>	ss<span class="number">8</span></span><br><span class="line"><span class="attribute">1009</span>	ss<span class="number">9</span></span><br><span class="line"><span class="attribute">1010</span>	ss<span class="number">10</span></span><br><span class="line"><span class="attribute">1011</span>	ss<span class="number">11</span></span><br><span class="line"><span class="attribute">1012</span>	ss<span class="number">12</span></span><br><span class="line"><span class="attribute">1013</span>	ss<span class="number">13</span></span><br><span class="line"><span class="attribute">1014</span>	ss<span class="number">14</span></span><br><span class="line"><span class="attribute">1015</span>	ss<span class="number">15</span></span><br><span class="line"><span class="attribute">1016</span>	ss<span class="number">16</span></span><br></pre></td></tr></table></figure>

<p>（2）创建分桶表</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table stu_buck(id int, name string)</span><br><span class="line">clustered by(id) </span><br><span class="line">into 4 buckets</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure>

<p>（3）查看表结构</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; desc formatted stu_buck;</span><br><span class="line">Num Buckets:            4   </span><br></pre></td></tr></table></figure>

<p>（4）导入数据到分桶表</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt;dfs -mkdir /user/hive/warehouse/stu_buck;</span><br><span class="line"></span><br><span class="line">hive (default)&gt;set hive.execution.engine = mr;			//只有mr引擎支持往分桶表load数据</span><br><span class="line"></span><br><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/hive/datas/student.txt&#x27; overwrite into table stu_buck;</span><br><span class="line"></span><br><span class="line">hive (default)&gt;msck repair table stu_buck;</span><br></pre></td></tr></table></figure>

<p>Hive的分桶采用对分桶字段的值进行哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中</p>
<h3 id="抽样查询"><a href="#抽样查询" class="headerlink" title="抽样查询"></a>抽样查询</h3><p>对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive可以通过对表进行抽样来满足这个需求。</p>
<p>查询表stu_buck中的数据：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from stu_buck tablesample(bucket 1 out of 4 on id);</span><br></pre></td></tr></table></figure>

<h2 id="DML数据操作"><a href="#DML数据操作" class="headerlink" title="DML数据操作"></a>DML数据操作</h2><h3 id="数据导入"><a href="#数据导入" class="headerlink" title="数据导入"></a>数据导入</h3><h4 id="向表中装载数据（Load）"><a href="#向表中装载数据（Load）" class="headerlink" title="向表中装载数据（Load）"></a>向表中装载数据（Load）</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive&gt; load data [local] inpath &#x27;数据的path&#x27; [overwrite] into table student [partition (partcol1=val1,…)];</span><br></pre></td></tr></table></figure>

<p>（1）load data:表示加载数据</p>
<p>（2）local:表示从本地加载数据到hive表；否则从HDFS加载数据到hive表</p>
<p>（3）inpath:表示加载数据的路径</p>
<p>（4）overwrite:表示覆盖表中已有数据，否则表示追加</p>
<p>（5）into table:表示加载到哪张表</p>
<p>（6）student:表示具体的表</p>
<p>（7）partition:表示上传到指定分区</p>
<p>实操案例</p>
<p>0）创建一张表</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create table student(id string, name string) row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure>

<p>1）加载本地文件到hive</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/hive/datas/student.txt&#x27; into table default.student;</span><br></pre></td></tr></table></figure>

<p>2）加载HDFS文件到hive中</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -put /opt/module/hive/datas/student.txt /user/vincent/hive;</span><br><span class="line">hive (default)&gt; load data inpath &#x27;/user/vincent/hive/student.txt&#x27; into table default.student;</span><br></pre></td></tr></table></figure>

<p>3）加载数据覆盖表中已有的数据（overwrite）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -put /opt/module/datas/student.txt /user/vincent/hive;</span><br><span class="line">hive (default)&gt; load data inpath &#x27;/user/vincent/hive/student.txt&#x27; overwrite into table default.student;</span><br></pre></td></tr></table></figure>

<h4 id="通过查询语句向表中插入数据（Insert）"><a href="#通过查询语句向表中插入数据（Insert）" class="headerlink" title="通过查询语句向表中插入数据（Insert）"></a>通过查询语句向表中插入数据（Insert）</h4><p>0）创建一张表</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create table student_par(id int, name string) row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure>

<p>1）基本插入数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; insert into table student_par values(1,&#x27;wangwu&#x27;),(2,&#x27;zhaoliu&#x27;);</span><br></pre></td></tr></table></figure>

<p>2）基本模式插入（根据单张表查询结果）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite table student select id, name from student where month=&#x27;201709&#x27;;</span><br></pre></td></tr></table></figure>

<p>insert into：以追加数据的方式插入到表或分区，原有数据不会删除</p>
<p>insert overwrite：会覆盖表中已存在的数据</p>
<p>注意：insert不支持插入部分字段</p>
<p>3）多表（多分区）插入模式（根据多张表查询结果）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; from student</span><br><span class="line">              insert overwrite table student partition(month=&#x27;201707&#x27;)</span><br><span class="line">              select id, name where month=&#x27;201709&#x27;</span><br><span class="line">              insert overwrite table student partition(month=&#x27;201706&#x27;)</span><br><span class="line">              select id, name where month=&#x27;201709&#x27;;</span><br></pre></td></tr></table></figure>

<h4 id="查询语句中创建表并加载数据（As-Select）"><a href="#查询语句中创建表并加载数据（As-Select）" class="headerlink" title="查询语句中创建表并加载数据（As Select）"></a>查询语句中创建表并加载数据（As Select）</h4><p>根据查询结果创建表（查询的结果会添加到新创建的表中）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create table if not exists student3 as select id, name from student;</span><br></pre></td></tr></table></figure>

<h4 id="创建表时通过Location指定加载数据路径"><a href="#创建表时通过Location指定加载数据路径" class="headerlink" title="创建表时通过Location指定加载数据路径"></a>创建表时通过Location指定加载数据路径</h4><p>1）上传数据到hdfs上</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir /student;</span><br><span class="line">hive (default)&gt; dfs -put /opt/module/datas/student.txt /student;</span><br></pre></td></tr></table></figure>

<p>2）创建表，并指定在hdfs上的位置</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create external table if not exists student5(</span><br><span class="line">              id int, name string</span><br><span class="line">              )</span><br><span class="line">              row format delimited fields terminated by &#x27;\t&#x27;</span><br><span class="line">              location &#x27;/student;</span><br></pre></td></tr></table></figure>

<p>3）查询数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from student5;</span><br></pre></td></tr></table></figure>

<h4 id="Import数据到指定Hive表中"><a href="#Import数据到指定Hive表中" class="headerlink" title="Import数据到指定Hive表中"></a>Import数据到指定Hive表中</h4><p>注意：先用export导出后，再将数据导入。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; import table student2 from &#x27;/user/hive/warehouse/export/student&#x27;;</span><br></pre></td></tr></table></figure>

<h3 id="数据导出"><a href="#数据导出" class="headerlink" title="数据导出"></a>数据导出</h3><h4 id="Insert导出"><a href="#Insert导出" class="headerlink" title="Insert导出"></a>Insert导出</h4><p>1）将查询的结果导出到本地</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite local directory &#x27;/opt/module/hive/datas/export/student&#x27; select * from student;</span><br></pre></td></tr></table></figure>

<p>2）将查询的结果格式化导出到本地</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive(default)&gt;insert overwrite local directory &#x27;/opt/module/hive/datas/export/student1&#x27; ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27; select * from student;</span><br></pre></td></tr></table></figure>

<p>3）将查询的结果导出到HDFS上(没有local)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite directory &#x27;/user/vincent/student2&#x27;</span><br><span class="line">             ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27; </span><br><span class="line">             select * from student;</span><br></pre></td></tr></table></figure>

<h4 id="Hadoop命令导出到本地"><a href="#Hadoop命令导出到本地" class="headerlink" title="Hadoop命令导出到本地"></a>Hadoop命令导出到本地</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -get /user/hive/warehouse/student/student.txt</span><br><span class="line">/opt/module/datas/export/student3.txt;</span><br></pre></td></tr></table></figure>

<h4 id="Hive-Shell-命令导出"><a href="#Hive-Shell-命令导出" class="headerlink" title="Hive Shell 命令导出"></a>Hive Shell 命令导出</h4><p>基本语法：（hive -f/-e 执行语句或者脚本 &gt; file）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hive]$ bin/hive -e &#x27;select * from default.student;&#x27; &gt;</span><br><span class="line"> /opt/module/hive/datas/export/student4.txt;</span><br></pre></td></tr></table></figure>

<h4 id="Export导出到HDFS上"><a href="#Export导出到HDFS上" class="headerlink" title="Export导出到HDFS上"></a>Export导出到HDFS上</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; export table default.student to</span><br><span class="line"> &#x27;/user/hive/warehouse/export/student&#x27;;</span><br></pre></td></tr></table></figure>

<p>export和import主要用于两个Hadoop平台集群之间Hive表迁移。</p>
<h4 id="Sqoop导出"><a href="#Sqoop导出" class="headerlink" title="Sqoop导出"></a>Sqoop导出</h4><h4 id="清除表中数据（Truncate）"><a href="#清除表中数据（Truncate）" class="headerlink" title="清除表中数据（Truncate）"></a>清除表中数据（Truncate）</h4><p>注意：Truncate只能删除管理表，不能删除外部表中数据。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; truncate table student;</span><br></pre></td></tr></table></figure>

<h2 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h2><figure class="highlight n1ql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> [<span class="keyword">ALL</span> | <span class="keyword">DISTINCT</span>] select_expr, select_expr, ...</span><br><span class="line">  <span class="keyword">FROM</span> table_reference</span><br><span class="line">  [<span class="keyword">WHERE</span> where_condition]</span><br><span class="line">  [<span class="keyword">GROUP</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">  [<span class="keyword">ORDER</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">  [<span class="keyword">CLUSTER</span> <span class="keyword">BY</span> col_list</span><br><span class="line">    | [DISTRIBUTE <span class="keyword">BY</span> col_list] [SORT <span class="keyword">BY</span> col_list]</span><br><span class="line">  ]</span><br><span class="line"> [<span class="keyword">LIMIT</span> <span class="keyword">number</span>]</span><br></pre></td></tr></table></figure>

<h3 id="基本查询（Select…From）"><a href="#基本查询（Select…From）" class="headerlink" title="基本查询（Select…From）"></a>基本查询（Select…From）</h3><p>案例操作</p>
<p>0）原始数据</p>
<p>dept:</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">10</span>	ACCOUNTING	<span class="number">1700</span></span><br><span class="line"><span class="attribute">20</span>	RESEARCH	<span class="number">1800</span></span><br><span class="line"><span class="attribute">30</span>	SALES	<span class="number">1900</span></span><br><span class="line"><span class="attribute">40</span>	OPERATIONS	<span class="number">1700</span></span><br></pre></td></tr></table></figure>

<p>emp：</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">7369</span>	SMITH	CLERK	<span class="number">7902</span>	<span class="number">1980</span>-<span class="number">12</span>-<span class="number">17</span>	<span class="number">800</span>.<span class="number">00</span>		<span class="number">20</span></span><br><span class="line"><span class="attribute">7499</span>	ALLEN	SALESMAN	<span class="number">7698</span>	<span class="number">1981</span>-<span class="number">2</span>-<span class="number">20</span>	<span class="number">1600</span>.<span class="number">00</span>	<span class="number">300</span>.<span class="number">00</span>	<span class="number">30</span></span><br><span class="line"><span class="attribute">7521</span>	WARD	SALESMAN	<span class="number">7698</span>	<span class="number">1981</span>-<span class="number">2</span>-<span class="number">22</span>	<span class="number">1250</span>.<span class="number">00</span>	<span class="number">500</span>.<span class="number">00</span>	<span class="number">30</span></span><br><span class="line"><span class="attribute">7566</span>	JONES	MANAGER	<span class="number">7839</span>	<span class="number">1981</span>-<span class="number">4</span>-<span class="number">2</span>	<span class="number">2975</span>.<span class="number">00</span>		<span class="number">20</span></span><br><span class="line"><span class="attribute">7654</span>	MARTIN	SALESMAN	<span class="number">7698</span>	<span class="number">1981</span>-<span class="number">9</span>-<span class="number">28</span>	<span class="number">1250</span>.<span class="number">00</span>	<span class="number">1400</span>.<span class="number">00</span>	<span class="number">30</span></span><br><span class="line"><span class="attribute">7698</span>	BLAKE	MANAGER	<span class="number">7839</span>	<span class="number">1981</span>-<span class="number">5</span>-<span class="number">1</span>	<span class="number">2850</span>.<span class="number">00</span>		<span class="number">30</span></span><br><span class="line"><span class="attribute">7782</span>	CLARK	MANAGER	<span class="number">7839</span>	<span class="number">1981</span>-<span class="number">6</span>-<span class="number">9</span>	<span class="number">2450</span>.<span class="number">00</span>		<span class="number">10</span></span><br><span class="line"><span class="attribute">7788</span>	SCOTT	ANALYST	<span class="number">7566</span>	<span class="number">1987</span>-<span class="number">4</span>-<span class="number">19</span>	<span class="number">3000</span>.<span class="number">00</span>		<span class="number">20</span></span><br><span class="line"><span class="attribute">7839</span>	KING	PRESIDENT		<span class="number">1981</span>-<span class="number">11</span>-<span class="number">17</span>	<span class="number">5000</span>.<span class="number">00</span>		<span class="number">10</span></span><br><span class="line"><span class="attribute">7844</span>	TURNER	SALESMAN	<span class="number">7698</span>	<span class="number">1981</span>-<span class="number">9</span>-<span class="number">8</span>	<span class="number">1500</span>.<span class="number">00</span>	<span class="number">0</span>.<span class="number">00</span>	<span class="number">30</span></span><br><span class="line"><span class="attribute">7876</span>	ADAMS	CLERK	<span class="number">7788</span>	<span class="number">1987</span>-<span class="number">5</span>-<span class="number">23</span>	<span class="number">1100</span>.<span class="number">00</span>		<span class="number">20</span></span><br><span class="line"><span class="attribute">7900</span>	JAMES	CLERK	<span class="number">7698</span>	<span class="number">1981</span>-<span class="number">12</span>-<span class="number">3</span>	<span class="number">950</span>.<span class="number">00</span>		<span class="number">30</span></span><br><span class="line"><span class="attribute">7902</span>	FORD	ANALYST	<span class="number">7566</span>	<span class="number">1981</span>-<span class="number">12</span>-<span class="number">3</span>	<span class="number">3000</span>.<span class="number">00</span>		<span class="number">20</span></span><br><span class="line"><span class="attribute">7934</span>	MILLER	CLERK	<span class="number">7782</span>	<span class="number">1982</span>-<span class="number">1</span>-<span class="number">23</span>	<span class="number">1300</span>.<span class="number">00</span>		<span class="number">10</span></span><br></pre></td></tr></table></figure>

<p>1）创建部门表</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create external table if not exists default.dept(</span><br><span class="line">deptno int,</span><br><span class="line">dname string,</span><br><span class="line">loc int</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure>

<p>2）创建员工表</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create external table if not exists default.emp(</span><br><span class="line">empno int,</span><br><span class="line">ename string,</span><br><span class="line">job string,</span><br><span class="line">mgr int,</span><br><span class="line">hiredate string, </span><br><span class="line">sal double, </span><br><span class="line">comm double,</span><br><span class="line">deptno int)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure>

<p>3）导入数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into table dept;</span><br><span class="line">load data local inpath &#x27;/opt/module/datas/emp.txt&#x27; into table emp;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select empno, ename from emp;</span><br><span class="line">select * from emp;</span><br></pre></td></tr></table></figure>

<p>注意：</p>
<p>（1）SQL 语言大小写不敏感。 </p>
<p>（2）SQL 可以写在一行或者多行</p>
<p>（3）关键字不能被缩写也不能分行</p>
<p>（4）各子句一般要分行写。</p>
<p>（5）使用缩进提高语句的可读性。</p>
<h4 id="列别名"><a href="#列别名" class="headerlink" title="列别名"></a>列别名</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select ename AS name, deptno dn from emp;</span><br></pre></td></tr></table></figure>

<h4 id="算术运算符"><a href="#算术运算符" class="headerlink" title="算术运算符"></a>算术运算符</h4><table>
<thead>
<tr>
<th>运算符</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>A+B</td>
<td>A和B 相加</td>
</tr>
<tr>
<td>A-B</td>
<td>A减去B</td>
</tr>
<tr>
<td>A*B</td>
<td>A和B 相乘</td>
</tr>
<tr>
<td>A/B</td>
<td>A除以B</td>
</tr>
<tr>
<td>A%B</td>
<td>A对B取余</td>
</tr>
<tr>
<td>A&amp;B</td>
<td>A和B按位取与</td>
</tr>
<tr>
<td>A|B</td>
<td>A和B按位取或</td>
</tr>
<tr>
<td>A^B</td>
<td>A和B按位取异或</td>
</tr>
<tr>
<td>~A</td>
<td>A按位取反</td>
</tr>
</tbody></table>
<h4 id="常用函数"><a href="#常用函数" class="headerlink" title="常用函数"></a>常用函数</h4><p>求总行数（count）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select count(*) cnt from emp;</span><br></pre></td></tr></table></figure>

<p>求工资的最大值（max）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select max(sal) max_sal from emp;</span><br></pre></td></tr></table></figure>

<p>求工资的最小值（min）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select min(sal) min_sal from emp;</span><br></pre></td></tr></table></figure>

<p>求工资的总和（sum）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select sum(sal) sum_sal from emp; </span><br></pre></td></tr></table></figure>

<p>求工资的平均值（avg）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select avg(sal) avg_sal from emp;</span><br></pre></td></tr></table></figure>

<h4 id="Limit语句"><a href="#Limit语句" class="headerlink" title="Limit语句"></a>Limit语句</h4><p>LIMIT子句用于限制返回的行数。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from emp limit 5;</span><br><span class="line">hive (default)&gt; select * from emp limit 2,3;</span><br></pre></td></tr></table></figure>

<h4 id="Where语句"><a href="#Where语句" class="headerlink" title="Where语句"></a>Where语句</h4><p>WHERE子句紧随FROM子句，where子句中不能使用字段别名。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where sal &gt; 1000;</span><br></pre></td></tr></table></figure>

<h4 id="比较运算符（Between-In-Is-Null）"><a href="#比较运算符（Between-In-Is-Null）" class="headerlink" title="比较运算符（Between/In/Is Null）"></a>比较运算符（Between/In/Is Null）</h4><table>
<thead>
<tr>
<th align="left">操作符</th>
<th>支持的数据类型</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td align="left">A=B</td>
<td>基本数据类型</td>
<td>如果A等于B则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td align="left">A&lt;=&gt;B</td>
<td>基本数据类型</td>
<td>如果A和B都为NULL，则返回TRUE，如果一边为NULL，返回False</td>
</tr>
<tr>
<td align="left">A&lt;&gt;B, A!=B</td>
<td>基本数据类型</td>
<td>A或者B为NULL则返回NULL；如果A不等于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td align="left">A&lt;B</td>
<td>基本数据类型</td>
<td>A或者B为NULL，则返回NULL；如果A小于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td align="left">A&lt;=B</td>
<td>基本数据类型</td>
<td>A或者B为NULL，则返回NULL；如果A小于等于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td align="left">A&gt;B</td>
<td>基本数据类型</td>
<td>A或者B为NULL，则返回NULL；如果A大于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td align="left">A&gt;=B</td>
<td>基本数据类型</td>
<td>A或者B为NULL，则返回NULL；如果A大于等于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td align="left">A [NOT] BETWEEN B AND C</td>
<td>基本数据类型</td>
<td>如果A，B或者C任一为NULL，则结果为NULL。如果A的值大于等于B而且小于或等于C，则结果为TRUE，反之为FALSE。如果使用NOT关键字则可达到相反的效果。</td>
</tr>
<tr>
<td align="left">A IS NULL</td>
<td>所有数据类型</td>
<td>如果A等于NULL，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td align="left">A IS NOT NULL</td>
<td>所有数据类型</td>
<td>如果A不等于NULL，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td align="left">IN(数值1, 数值2)</td>
<td>所有数据类型</td>
<td>使用 IN运算显示列表中的值</td>
</tr>
<tr>
<td align="left">A [NOT] LIKE B</td>
<td>STRING 类型</td>
<td>B是一个SQL下的简单正则表达式，也叫通配符模式，如果A与其匹配的话，则返回TRUE；反之返回FALSE。B的表达式说明如下：‘x%’表示A必须以字母‘x’开头，‘%x’表示A必须以字母’x’结尾，而‘%x%’表示A包含有字母’x’,可以位于开头，结尾或者字符串中间。如果使用NOT关键字则可达到相反的效果。</td>
</tr>
<tr>
<td align="left">A RLIKE B, A REGEXP B</td>
<td>STRING 类型</td>
<td>B是基于java的正则表达式，如果A与其匹配，则返回TRUE；反之返回FALSE。匹配使用的是JDK中的正则表达式接口实现的，因为正则也依据其中的规则。例如，正则表达式必须和整个字符串A相匹配，而不是只需与其字符串匹配。</td>
</tr>
</tbody></table>
<p>案例实操</p>
<p>（1）查询出薪水等于5000的所有员工</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where sal = 5000;</span><br></pre></td></tr></table></figure>

<p>（2）查询工资在500到1000的员工信息</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where sal between 500 and 1000;</span><br></pre></td></tr></table></figure>

<p>（3）查询comm为空的所有员工信息</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where comm is null;</span><br></pre></td></tr></table></figure>

<p>（4）查询工资是1500或5000的员工信息</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where sal IN (1500, 5000);</span><br></pre></td></tr></table></figure>

<h4 id="Like和RLike"><a href="#Like和RLike" class="headerlink" title="Like和RLike"></a>Like和RLike</h4><p>1）使用LIKE运算选择类似的值</p>
<p>2）选择条件可以包含字符或数字</p>
<p>% 代表零个或多个字符(任意个字符)。</p>
<p>_ 代表一个字符。</p>
<p>3）RLIKE子句</p>
<p>RLIKE子句是Hive中这个功能的一个扩展，其可以通过Java的正则表达式这个更强大的语言来指定匹配条件。</p>
<p>4）案例实操</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where ename LIKE &#x27;A%&#x27;;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where ename LIKE &#x27;_A%&#x27;;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where ename  RLIKE &#x27;[A]&#x27;;</span><br></pre></td></tr></table></figure>

<h4 id="逻辑运算符（And-Or-Not）"><a href="#逻辑运算符（And-Or-Not）" class="headerlink" title="逻辑运算符（And/Or/Not）"></a>逻辑运算符（And/Or/Not）</h4><table>
<thead>
<tr>
<th>操作符</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>AND</td>
<td>逻辑并</td>
</tr>
<tr>
<td>OR</td>
<td>逻辑或</td>
</tr>
<tr>
<td>NOT</td>
<td>逻辑否</td>
</tr>
</tbody></table>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where sal&gt;1000 and deptno=30;</span><br><span class="line">hive (default)&gt; select * from emp where sal&gt;1000 or deptno=30;</span><br><span class="line">hive (default)&gt; select * from emp where deptno not IN(30, 20);</span><br></pre></td></tr></table></figure>

<h3 id="分组"><a href="#分组" class="headerlink" title="分组"></a>分组</h3><h4 id="Group-By语句"><a href="#Group-By语句" class="headerlink" title="Group By语句"></a>Group By语句</h4><p>GROUP BY语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作。</p>
<p>计算emp表每个部门的平均工资</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select t.deptno, avg(t.sal) avg_sal from emp t group by t.deptno;</span><br></pre></td></tr></table></figure>

<p>计算emp每个部门中每个岗位的最高薪水</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select t.deptno, t.job, max(t.sal) max_sal from emp t group by t.deptno, t.job;</span><br></pre></td></tr></table></figure>

<h4 id="Having语句"><a href="#Having语句" class="headerlink" title="Having语句"></a>Having语句</h4><p>1）having与where不同点</p>
<p>（1）where后面不能写分组函数，而having后面可以使用分组函数。</p>
<p>（2）having只用于group by分组统计语句。</p>
<p>求平均薪水大于2000的部门</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select deptno, avg(sal) avg_sal from emp group by deptno having avg_sal &gt; 2000;</span><br></pre></td></tr></table></figure>

<h3 id="Join语句"><a href="#Join语句" class="headerlink" title="Join语句"></a>Join语句</h3><h4 id="等值Join"><a href="#等值Join" class="headerlink" title="等值Join"></a>等值Join</h4><p>Hive支持通常的SQL JOIN语句，但是只支持等值连接，不支持非等值连接。</p>
<p>根据员工表和部门表中的部门编号相等，查询员工编号、员工名称和部门名称：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; </span><br><span class="line">select e.empno, e.ename, d.deptno, d.dname </span><br><span class="line">from emp e </span><br><span class="line">join dept d </span><br><span class="line">on e.deptno = d.deptno;</span><br></pre></td></tr></table></figure>

<h4 id="表的别名"><a href="#表的别名" class="headerlink" title="表的别名"></a>表的别名</h4><p>1）好处</p>
<p>（1）使用别名可以简化查询。</p>
<p>（2）使用表名前缀可以提高执行效率。</p>
<p>2）案例实操</p>
<p>合并员工表和部门表</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select e.empno, e.ename, d.deptno from emp e join dept d on e.deptno = d.deptno;</span><br></pre></td></tr></table></figure>

<h4 id="内连接"><a href="#内连接" class="headerlink" title="内连接"></a>内连接</h4><p>只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select e.empno, e.ename, d.deptno from emp e join dept d on e.deptno</span><br><span class="line"> = d.deptno;</span><br></pre></td></tr></table></figure>

<h4 id="左外连接"><a href="#左外连接" class="headerlink" title="左外连接"></a>左外连接</h4><p>JOIN操作符左边表中符合WHERE子句的所有记录将会被返回。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select e.empno, e.ename, d.deptno from emp e left join dept d on e.deptno = d.deptno;</span><br></pre></td></tr></table></figure>

<h4 id="右外连接"><a href="#右外连接" class="headerlink" title="右外连接"></a>右外连接</h4><p>JOIN操作符右边表中符合WHERE子句的所有记录将会被返回。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select e.empno, e.ename, d.deptno from emp e right join dept d on e.deptno = d.deptno;</span><br></pre></td></tr></table></figure>

<h4 id="满外连接"><a href="#满外连接" class="headerlink" title="满外连接"></a>满外连接</h4><p>将会返回所有表中符合WHERE语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用NULL值替代。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select e.empno, e.ename, d.deptno from emp e full join dept d on e.deptno = d.deptno;</span><br></pre></td></tr></table></figure>

<h4 id="多表连接"><a href="#多表连接" class="headerlink" title="多表连接"></a>多表连接</h4><p>连接 n个表，至少需要n-1个连接条件。例如：连接三个表，至少需要两个连接条件。</p>
<p>location.txt</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="number">1700	</span><span class="string">Beijing</span></span><br><span class="line"><span class="number">1800	</span><span class="string">London</span></span><br><span class="line"><span class="number">1900	</span><span class="string">Tokyo</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table if not exists location(</span><br><span class="line">loc int,</span><br><span class="line">loc_name string</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/location.txt&#x27; into table location;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt;SELECT e.ename, d.dname, l.loc_name</span><br><span class="line">FROM   emp e </span><br><span class="line">JOIN   dept d</span><br><span class="line">ON     d.deptno = e.deptno </span><br><span class="line">JOIN   location l</span><br><span class="line">ON     d.loc = l.loc;</span><br></pre></td></tr></table></figure>

<p>大多数情况下，Hive会对每对JOIN连接对象启动一个MapReduce任务。本例中会首先启动一个MapReduce job对表e和表d进行连接操作，然后会再启动一个MapReduce job将第一个MapReduce job的输出和表l;进行连接操作。</p>
<h4 id="笛卡尔积"><a href="#笛卡尔积" class="headerlink" title="笛卡尔积"></a>笛卡尔积</h4><p>1）笛卡尔集会在下面条件下产生</p>
<p>（1）省略连接条件</p>
<p>（2）连接条件无效</p>
<p>（3）所有表中的所有行互相连接</p>
<p>2）案例实操</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select empno, dname from emp, dept;</span><br></pre></td></tr></table></figure>

<h3 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h3><h4 id="全局排序（Order-By）"><a href="#全局排序（Order-By）" class="headerlink" title="全局排序（Order By）"></a>全局排序（Order By）</h4><p><strong>Order By：全局排序，只有一个Reducer。</strong></p>
<p>1）使用ORDER BY子句排序</p>
<p>ASC（ascend）: 升序（默认）</p>
<p>DESC（descend）: 降序</p>
<p>2）ORDER BY子句在SELECT语句的结尾</p>
<p>3）案例实操</p>
<p>（1）查询员工信息按工资升序排列</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from emp order by sal;</span><br></pre></td></tr></table></figure>

<p>（2）查询员工信息按工资降序排列</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from emp order by sal desc;</span><br></pre></td></tr></table></figure>

<h4 id="按照别名排序"><a href="#按照别名排序" class="headerlink" title="按照别名排序"></a>按照别名排序</h4><p>按照员工薪水的2倍排序</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select ename, sal*2 twosal from emp order by twosal;</span><br></pre></td></tr></table></figure>

<h4 id="多个列排序"><a href="#多个列排序" class="headerlink" title="多个列排序"></a>多个列排序</h4><p>按照部门和工资升序排序</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select ename, deptno, sal from emp order by deptno, sal ;</span><br></pre></td></tr></table></figure>

<h4 id="每个MapReduce内部排序（Sort-By）"><a href="#每个MapReduce内部排序（Sort-By）" class="headerlink" title="每个MapReduce内部排序（Sort By）"></a>每个MapReduce内部排序（Sort By）</h4><p><strong>Sort By：对于大规模的数据集order by的效率非常低。在很多情况下，并不需要全局排序，此时可以使用sort by。</strong></p>
<p><strong>Sort by为每个reducer产生一个排序文件。每个Reducer内部进行排序，对全局结果集来说不是排序。</strong></p>
<p>1）设置reduce个数</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; set mapreduce.job.reduces=3;</span><br></pre></td></tr></table></figure>

<p>2）查看设置reduce个数</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; set mapreduce.job.reduces;</span><br></pre></td></tr></table></figure>

<p>3）根据部门编号降序查看员工信息</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from emp sort by deptno desc;</span><br></pre></td></tr></table></figure>

<p>4）将查询结果导入到文件中（按照部门编号降序排序）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite local directory &#x27;/opt/module/hive/datas/sortby-result&#x27; select * from emp sort by deptno desc;</span><br></pre></td></tr></table></figure>

<h4 id="分区排序（Distribute-By）"><a href="#分区排序（Distribute-By）" class="headerlink" title="分区排序（Distribute By）"></a>分区排序（Distribute By）</h4><p>Distribute By： 在有些情况下，我们需要<strong>控制某个特定行应该到哪个reducer</strong>，通常是为了进行后续的聚集操作。distribute by子句可以做这件事。distribute by类似MR中partition（自定义分区），进行分区，结合sort by使用。 <strong>对于distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果。</strong></p>
<p>案例实操</p>
<p>先按照部门编号分区，再按照员工编号降序排序。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; set mapreduce.job.reduces=3;</span><br><span class="line">hive (default)&gt; insert overwrite local directory &#x27;/opt/module/hive/datas/distribute-result&#x27; select * from emp distribute by deptno sort by empno desc;</span><br></pre></td></tr></table></figure>

<h4 id="Cluster-By"><a href="#Cluster-By" class="headerlink" title="Cluster By"></a>Cluster By</h4><p><strong>当distribute by和sort by字段相同时，可以使用cluster by方式。</strong></p>
<p>cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC。</p>
<p>以下两种写法等价：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from emp cluster by deptno;</span><br><span class="line">hive (default)&gt; select * from emp distribute by deptno sort by deptno;</span><br></pre></td></tr></table></figure>

<p>注意：按照部门编号分区，不一定就是固定死的数值，可以是20号和30号部门分到一个分区里面去。</p>
<h2 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h2><p>sql写顺序：select … from… where…. group by… having… order by.. limit</p>
<p>sql执行顺序：from… where…group by… having…. select … order by… limit</p>
<h3 id="系统内置函数"><a href="#系统内置函数" class="headerlink" title="系统内置函数"></a>系统内置函数</h3><p>1）查看系统自带的函数</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive&gt; show functions;</span><br></pre></td></tr></table></figure>

<p>2）显示自带的函数的用法</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive&gt; desc function upper;</span><br></pre></td></tr></table></figure>

<p>3）详细显示自带的函数的用法</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive&gt; desc function extended upper;</span><br></pre></td></tr></table></figure>

<h3 id="常用内置函数"><a href="#常用内置函数" class="headerlink" title="常用内置函数"></a>常用内置函数</h3><h4 id="空字段赋值（nvl）"><a href="#空字段赋值（nvl）" class="headerlink" title="空字段赋值（nvl）"></a>空字段赋值（nvl）</h4><p>NVL：给值为NULL的数据赋值，它的格式是NVL( value，default_value)。它的功能是如果value为NULL，则NVL函数返回default_value的值，否则返回value的值；如果两个参数都为NULL ，则返回NULL。</p>
<p>如果员工的comm为NULL，则用-1代替：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select comm, nvl(comm, -1) from emp;</span><br></pre></td></tr></table></figure>

<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">OK</span></span><br><span class="line"><span class="attribute">comm</span>    _c<span class="number">1</span></span><br><span class="line"><span class="attribute">NULL</span>    -<span class="number">1</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">300</span>.<span class="number">0</span>   <span class="number">300</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">500</span>.<span class="number">0</span>   <span class="number">500</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">NULL</span>    -<span class="number">1</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">1400</span>.<span class="number">0</span>  <span class="number">1400</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">NULL</span>    -<span class="number">1</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">NULL</span>    -<span class="number">1</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">NULL</span>    -<span class="number">1</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">NULL</span>    -<span class="number">1</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">0</span>.<span class="number">0</span>     <span class="number">0</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">NULL</span>    -<span class="number">1</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">NULL</span>    -<span class="number">1</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">NULL</span>    -<span class="number">1</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">NULL</span>    -<span class="number">1</span>.<span class="number">0</span></span><br></pre></td></tr></table></figure>

<p>如果员工的comm为NULL，则用领导id代替：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select comm, nvl(comm,mgr) from emp;</span><br></pre></td></tr></table></figure>

<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">OK</span></span><br><span class="line"><span class="attribute">comm</span>    _c<span class="number">1</span></span><br><span class="line"><span class="attribute">NULL</span>    <span class="number">7902</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">300</span>.<span class="number">0</span>   <span class="number">300</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">500</span>.<span class="number">0</span>   <span class="number">500</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">NULL</span>    <span class="number">7839</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">1400</span>.<span class="number">0</span>  <span class="number">1400</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">NULL</span>    <span class="number">7839</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">NULL</span>    <span class="number">7839</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">NULL</span>    <span class="number">7566</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">NULL</span>    NULL</span><br><span class="line"><span class="attribute">0</span>.<span class="number">0</span>     <span class="number">0</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">NULL</span>    <span class="number">7788</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">NULL</span>    <span class="number">7698</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">NULL</span>    <span class="number">7566</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">NULL</span>    <span class="number">7782</span>.<span class="number">0</span></span><br></pre></td></tr></table></figure>

<h4 id="CASE-WHEN"><a href="#CASE-WHEN" class="headerlink" title="CASE WHEN"></a>CASE WHEN</h4><p>1）数据准备</p>
<table>
<thead>
<tr>
<th>name</th>
<th>dept_id</th>
<th>sex</th>
</tr>
</thead>
<tbody><tr>
<td>悟空</td>
<td>A</td>
<td>男</td>
</tr>
<tr>
<td>大海</td>
<td>A</td>
<td>男</td>
</tr>
<tr>
<td>宋宋</td>
<td>B</td>
<td>男</td>
</tr>
<tr>
<td>凤姐</td>
<td>A</td>
<td>女</td>
</tr>
<tr>
<td>婷姐</td>
<td>B</td>
<td>女</td>
</tr>
<tr>
<td>婷婷</td>
<td>B</td>
<td>女</td>
</tr>
</tbody></table>
<p>2）需求</p>
<p>求出不同部门男女各多少人。结果如下：</p>
<p>A   2    1</p>
<p>B   1    2</p>
<p>3）创建本地emp_sex.txt，导入数据</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line">悟空	<span class="selector-tag">A</span>	男</span><br><span class="line">大海	<span class="selector-tag">A</span>	男</span><br><span class="line">宋宋	<span class="selector-tag">B</span>	男</span><br><span class="line">凤姐	<span class="selector-tag">A</span>	女</span><br><span class="line">婷姐	<span class="selector-tag">B</span>	女</span><br><span class="line">婷婷	<span class="selector-tag">B</span>	女</span><br></pre></td></tr></table></figure>

<p>4）创建hive表并导入数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table emp_sex(</span><br><span class="line">name string, </span><br><span class="line">dept_id string, </span><br><span class="line">sex string) </span><br><span class="line">row format delimited fields terminated by &quot;\t&quot;;</span><br><span class="line">load data local inpath &#x27;/opt/module/hive/datas/emp_sex.txt&#x27; into table emp_sex;</span><br></pre></td></tr></table></figure>

<p>5）按需求查询数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select </span><br><span class="line"> dept_id,</span><br><span class="line"> sum(case sex when &#x27;男&#x27; then 1 else 0 end) male_count,</span><br><span class="line"> sum(case sex when &#x27;女&#x27; then 1 else 0 end) female_count</span><br><span class="line">from emp_sex</span><br><span class="line">group by dept_id;</span><br></pre></td></tr></table></figure>

<h4 id="行转列"><a href="#行转列" class="headerlink" title="行转列"></a>行转列</h4><p>1）相关函数说明</p>
<p>CONCAT(string A/col, string B/col…)：返回输入字符串连接后的结果，支持任意个输入字符串;</p>
<p>CONCAT_WS(separator, str1, str2,…)：它是一个特殊形式的 CONCAT()。第一个参数剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是 NULL，返回值也将为 NULL。这个函数会跳过分隔符参数后的任何 NULL 和空字符串。分隔符将被加到被连接的字符串之间;</p>
<p>COLLECT_SET(col)：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生array类型字段。</p>
<p>2）数据准备</p>
<table>
<thead>
<tr>
<th>name</th>
<th>constellation</th>
<th>blood_type</th>
</tr>
</thead>
<tbody><tr>
<td>孙悟空</td>
<td>白羊座</td>
<td>A</td>
</tr>
<tr>
<td>大海</td>
<td>射手座</td>
<td>A</td>
</tr>
<tr>
<td>宋宋</td>
<td>白羊座</td>
<td>B</td>
</tr>
<tr>
<td>猪八戒</td>
<td>白羊座</td>
<td>A</td>
</tr>
<tr>
<td>凤姐</td>
<td>射手座</td>
<td>A</td>
</tr>
<tr>
<td>苍老师</td>
<td>白羊座</td>
<td>B</td>
</tr>
</tbody></table>
<p>3）需求</p>
<p>把星座和血型一样的人归类到一起。结果如下：</p>
<p>射手座,A       大海|凤姐</p>
<p>白羊座,A       孙悟空|猪八戒</p>
<p>白羊座,B       宋宋|苍老师</p>
<p>4）创建本地constellation.txt，导入数据</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line">孙悟空	白羊座	<span class="selector-tag">A</span></span><br><span class="line">大海	射手座	<span class="selector-tag">A</span></span><br><span class="line">宋宋	白羊座	<span class="selector-tag">B</span></span><br><span class="line">猪八戒	白羊座	<span class="selector-tag">A</span></span><br><span class="line">凤姐	射手座	<span class="selector-tag">A</span></span><br><span class="line">苍老师 白羊座 <span class="selector-tag">B</span></span><br></pre></td></tr></table></figure>

<p>5）创建hive表并导入数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table person_info(</span><br><span class="line">name string, </span><br><span class="line">constellation string, </span><br><span class="line">blood_type string) </span><br><span class="line">row format delimited fields terminated by &quot;\t&quot;;</span><br><span class="line">load data local inpath &quot;/opt/module/hive/datas/constellation.txt&quot; into table person_info;</span><br></pre></td></tr></table></figure>

<p>6）按需求查询数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select</span><br><span class="line">  t1.base,</span><br><span class="line">  concat_ws(&#x27;|&#x27;, collect_set(t1.name)) name</span><br><span class="line">from </span><br><span class="line">	(select</span><br><span class="line">    name,</span><br><span class="line">    concat(constellation, &quot;,&quot;, blood_type) base</span><br><span class="line">  from</span><br><span class="line">    person_info) t1</span><br><span class="line">group by t1.base;</span><br></pre></td></tr></table></figure>

<h4 id="列转行"><a href="#列转行" class="headerlink" title="列转行"></a>列转行</h4><p>1）函数说明</p>
<p><strong>EXPLODE(col)：将hive一列中复杂的array或者map结构拆分成多行。</strong></p>
<p>LATERAL VIEW</p>
<p>用法：LATERAL VIEW udtf(expression) tableAlias AS columnAlias</p>
<p>解释：用于和split, explode等UDTF一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。</p>
<p>2）数据准备</p>
<table>
<thead>
<tr>
<th>movie</th>
<th>category</th>
</tr>
</thead>
<tbody><tr>
<td>《疑犯追踪》</td>
<td>悬疑,动作,科幻,剧情</td>
</tr>
<tr>
<td>《Lie to me》</td>
<td>悬疑,警匪,动作,心理,剧情</td>
</tr>
<tr>
<td>《战狼2》</td>
<td>战争,动作,灾难</td>
</tr>
</tbody></table>
<p>3）需求</p>
<p>将电影分类中的数组数据展开。结果如下：</p>
<figure class="highlight applescript"><table><tr><td class="code"><pre><span class="line">《疑犯追踪》    悬疑</span><br><span class="line">《疑犯追踪》    动作</span><br><span class="line">《疑犯追踪》    科幻</span><br><span class="line">《疑犯追踪》    剧情</span><br><span class="line">《Lie <span class="keyword">to</span> <span class="keyword">me</span>》  悬疑</span><br><span class="line">《Lie <span class="keyword">to</span> <span class="keyword">me</span>》  警匪</span><br><span class="line">《Lie <span class="keyword">to</span> <span class="keyword">me</span>》  动作</span><br><span class="line">《Lie <span class="keyword">to</span> <span class="keyword">me</span>》  心理</span><br><span class="line">《Lie <span class="keyword">to</span> <span class="keyword">me</span>》  剧情</span><br><span class="line">《战狼<span class="number">2</span>》     战争</span><br><span class="line">《战狼<span class="number">2</span>》     动作</span><br><span class="line">《战狼<span class="number">2</span>》     灾难</span><br></pre></td></tr></table></figure>

<p>4）创建本地movie.txt，导入数据</p>
<figure class="highlight applescript"><table><tr><td class="code"><pre><span class="line">《疑犯追踪》	悬疑,动作,科幻,剧情</span><br><span class="line"></span><br><span class="line">《Lie <span class="keyword">to</span> <span class="keyword">me</span>》	悬疑,警匪,动作,心理,剧情</span><br><span class="line"></span><br><span class="line">《战狼<span class="number">2</span>》	战争,动作,灾难</span><br></pre></td></tr></table></figure>

<p>5）创建hive表并导入数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table movie_info(</span><br><span class="line">  movie string, </span><br><span class="line">  category string) </span><br><span class="line">row format delimited fields terminated by &quot;\t&quot;;</span><br><span class="line">load data local inpath &quot;/opt/module/hive/datas/movie_info.txt&quot; into table movie_info;</span><br></pre></td></tr></table></figure>

<p>6）按需求查询数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select </span><br><span class="line">	m.movie, </span><br><span class="line">	tbl.cate</span><br><span class="line">from </span><br><span class="line">	movie_info m</span><br><span class="line">lateral view </span><br><span class="line">	explode(split(category, &quot;,&quot;)) tbl as cate;</span><br></pre></td></tr></table></figure>

<h4 id="窗口函数（开窗函数）"><a href="#窗口函数（开窗函数）" class="headerlink" title="窗口函数（开窗函数）"></a>窗口函数（开窗函数）</h4><p>1）相关函数说明</p>
<p><strong>OVER()：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变化而变化。</strong></p>
<p>CURRENT ROW：当前行</p>
<p>n PRECEDING：往前n行数据</p>
<p>n FOLLOWING：往后n行数据</p>
<p>UNBOUNDED：起点，UNBOUNDED PRECEDING 表示从前面的起点开始， UNBOUNDED FOLLOWING表示到后面的终点</p>
<p>LAG(col,n,default_val)：往前第n行数据</p>
<p>LEAD(col,n, default_val)：往后第n行数据</p>
<p>first_value(col)：取分组内排序后，<strong>截止到当前行</strong>，第一个值</p>
<p>last_value(col)：取分组内排序后，<strong>截止到当前行</strong>，最后一个值</p>
<p>sum(col)：组内求和，<strong>截止到当前行</strong></p>
<p>NTILE(n)：把有序窗口的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号。注意：n必须为int类型</p>
<p>CUME_DIST()：返回小于等于当前值的行数/分组内总行数</p>
<p>percent_rank()：分组内当前行的RANK值-1/分组内总行数-1</p>
<p>2）数据准备：name，orderdate，cost</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">jack</span>,<span class="number">2017</span>-<span class="number">01</span>-<span class="number">01</span>,<span class="number">10</span></span><br><span class="line"><span class="attribute">tony</span>,<span class="number">2017</span>-<span class="number">01</span>-<span class="number">02</span>,<span class="number">15</span></span><br><span class="line"><span class="attribute">jack</span>,<span class="number">2017</span>-<span class="number">02</span>-<span class="number">03</span>,<span class="number">23</span></span><br><span class="line"><span class="attribute">tony</span>,<span class="number">2017</span>-<span class="number">01</span>-<span class="number">04</span>,<span class="number">29</span></span><br><span class="line"><span class="attribute">jack</span>,<span class="number">2017</span>-<span class="number">01</span>-<span class="number">05</span>,<span class="number">46</span></span><br><span class="line"><span class="attribute">jack</span>,<span class="number">2017</span>-<span class="number">04</span>-<span class="number">06</span>,<span class="number">42</span></span><br><span class="line"><span class="attribute">tony</span>,<span class="number">2017</span>-<span class="number">01</span>-<span class="number">07</span>,<span class="number">50</span></span><br><span class="line"><span class="attribute">jack</span>,<span class="number">2017</span>-<span class="number">01</span>-<span class="number">08</span>,<span class="number">55</span></span><br><span class="line"><span class="attribute">mart</span>,<span class="number">2017</span>-<span class="number">04</span>-<span class="number">08</span>,<span class="number">62</span></span><br><span class="line"><span class="attribute">mart</span>,<span class="number">2017</span>-<span class="number">04</span>-<span class="number">09</span>,<span class="number">68</span></span><br><span class="line"><span class="attribute">neil</span>,<span class="number">2017</span>-<span class="number">05</span>-<span class="number">10</span>,<span class="number">12</span></span><br><span class="line"><span class="attribute">mart</span>,<span class="number">2017</span>-<span class="number">04</span>-<span class="number">11</span>,<span class="number">75</span></span><br><span class="line"><span class="attribute">neil</span>,<span class="number">2017</span>-<span class="number">06</span>-<span class="number">12</span>,<span class="number">80</span></span><br><span class="line"><span class="attribute">mart</span>,<span class="number">2017</span>-<span class="number">04</span>-<span class="number">13</span>,<span class="number">94</span></span><br></pre></td></tr></table></figure>

<p>3）需求</p>
<p>（1）查询在2017年4月份购买过的顾客及总人数</p>
<p>（2）查询顾客的购买明细及月购买总额</p>
<p>（3）上述的场景, 将每个顾客的cost按照日期进行累加</p>
<p>（4）查询每个顾客上次的购买时间</p>
<p>（5）查询前20%时间的订单信息</p>
<p>4）创建本地business.txt，导入数据</p>
<p>5）创建hive表并导入数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table business(</span><br><span class="line">name string, </span><br><span class="line">orderdate string,</span><br><span class="line">cost int</span><br><span class="line">) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;,&#x27;;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">load data local inpath &quot;/opt/module/hive/datas/business.txt&quot; into table business;</span><br></pre></td></tr></table></figure>

<p>6）按需求查询数据</p>
<p>（1） 查询在2017年4月份购买过的顾客及总人数（统计窗口中有多少个组）：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select name,count(*) over () </span><br><span class="line">from business </span><br><span class="line">where substring(orderdate,1,7) = &#x27;2017-04&#x27; </span><br><span class="line">group by name;</span><br><span class="line"></span><br><span class="line">select name,collect_set(orderdate)</span><br><span class="line">from business </span><br><span class="line">where substring(orderdate,1,7) = &#x27;2017-04&#x27; </span><br><span class="line">group by name;</span><br></pre></td></tr></table></figure>

<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">name</span>    _c<span class="number">1</span></span><br><span class="line"><span class="attribute">jack</span>    <span class="number">2</span></span><br><span class="line"><span class="attribute">mart</span>    <span class="number">2</span></span><br></pre></td></tr></table></figure>

<p>此时窗口的大小为整个结果集，窗口（结果集）里边有两条数据。</p>
<p>如果不加over ()，则结果为：（统计组中数据条数）</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">name</span>    _c<span class="number">1</span></span><br><span class="line"><span class="attribute">jack</span>    <span class="number">1</span></span><br><span class="line"><span class="attribute">mart</span>    <span class="number">4</span></span><br></pre></td></tr></table></figure>

<p>（2） 查询顾客的购买明细及所有人月购买总额：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select name,orderdate,cost,sum(cost) over(partition by month(orderdate)) from business;</span><br></pre></td></tr></table></figure>

<p>查询顾客的购买明细及每个顾客的月购买总额：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select name,orderdate,cost,sum(cost) over(partition by name, month(orderdate)) from business;</span><br></pre></td></tr></table></figure>

<p>此时窗口的大小为每个分区。</p>
<p>partition by：在窗口函数中做分区</p>
<p>（3） 将所有顾客的cost按照日期进行累加：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select name, orderdate, cost, sum(cost) over(order by orderdate) sum_cost</span><br><span class="line">from business;</span><br></pre></td></tr></table></figure>

<p><strong>order by：在窗口中进行排序。窗口大小为从开始位置到当前数据位置（每次从第一行到当前行）。</strong></p>
<p>将每个顾客的cost按照日期进行累加：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select name, orderdate, cost, sum(cost) over(partition by name order by orderdate) sum_cost</span><br><span class="line">from business;</span><br></pre></td></tr></table></figure>

<p>求每个顾客的购买明细，及本次消费和上次消费的总和：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select name, orderdate, cost, </span><br><span class="line">sum(cost) over(partition by name order by orderdate rows between 1 preceding and current row) sum_cost</span><br><span class="line">from business;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select name,orderdate,cost, </span><br><span class="line">sum(cost) over() as sample1,--所有行相加 </span><br><span class="line">sum(cost) over(partition by name) as sample2,--按name分组，组内数据相加 </span><br><span class="line">sum(cost) over(partition by name order by orderdate) as sample3,--按name分组，组内数据累加 </span><br><span class="line">sum(cost) over(partition by name order by orderdate rows between UNBOUNDED PRECEDING and current row ) as sample4 ,--和sample3一样,由起点到当前行的聚合 </span><br><span class="line">sum(cost) over(partition by name order by orderdate rows between 1 PRECEDING and current row) as sample5, --当前行和前面一行做聚合 </span><br><span class="line">sum(cost) over(partition by name order by orderdate rows between 1 PRECEDING AND 1 FOLLOWING ) as sample6,--当前行和前边一行及后面一行 </span><br><span class="line">sum(cost) over(partition by name order by orderdate rows between current row and UNBOUNDED FOLLOWING ) as sample7 --当前行及后面所有行 </span><br><span class="line">from business;</span><br></pre></td></tr></table></figure>

<p>rows必须跟在Order by 子句之后，对排序的结果进行限制，使用固定的行数来限制分区中的数据行数量</p>
<p>（4） 查看顾客上次的购买时间</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select name,orderdate,cost, </span><br><span class="line">lag(orderdate,1,&#x27;1900-01-01&#x27;) over(partition by name order by orderdate ) as time1, lag(orderdate,2) over (partition by name order by orderdate) as time2 </span><br><span class="line">from business;</span><br></pre></td></tr></table></figure>

<p>（5） 查询前20%时间的订单信息</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select name,orderdate,cost from (</span><br><span class="line">  select name,orderdate,cost, ntile(5) over(order by orderdate) bucket_num</span><br><span class="line">  from business</span><br><span class="line">) t</span><br><span class="line">where bucket_num = 1;</span><br></pre></td></tr></table></figure>

<h4 id="易混淆关键字"><a href="#易混淆关键字" class="headerlink" title="易混淆关键字"></a>易混淆关键字</h4><p>建表：partitioned by（分区表）    clustered by（分桶表）    sorted by（对桶中的一个或多个列另外排序）</p>
<p>查询：order by（全局排序）    distribute by（查询做分区）    sort by（查询做排序）    cluster by（查询分区排序）</p>
<p>窗口函数：partition by（窗口函数中做分区）和 order by（窗口函数中做排序）    或者</p>
<p>​                 用 distribute by 和 sort by</p>
<h4 id="Rank"><a href="#Rank" class="headerlink" title="Rank"></a>Rank</h4><p>1）函数说明</p>
<p>RANK() 排序相同时会重复，总数不会变</p>
<p>DENSE_RANK() 排序相同时会重复，总数会减少</p>
<p>ROW_NUMBER() 会根据顺序计算</p>
<p>2）数据准备</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">孙悟空	语文	87</span><br><span class="line">孙悟空	数学	95</span><br><span class="line">孙悟空	英语	68</span><br><span class="line">大海	语文	94</span><br><span class="line">大海	数学	56</span><br><span class="line">大海	英语	84</span><br><span class="line">宋宋	语文	64</span><br><span class="line">宋宋	数学	86</span><br><span class="line">宋宋	英语	84</span><br><span class="line">婷婷	语文	65</span><br><span class="line">婷婷	数学	85</span><br><span class="line">婷婷	英语	78</span><br></pre></td></tr></table></figure>

<p>3）需求</p>
<p>计算每门学科成绩排名。</p>
<p>4）创建本地score.txt，导入数据</p>
<p>5）创建hive表并导入数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table score(</span><br><span class="line">name string,</span><br><span class="line">subject string, </span><br><span class="line">score int) </span><br><span class="line">row format delimited fields terminated by &quot;\t&quot;;</span><br><span class="line">load data local inpath &#x27;/opt/module/hive/datas/score.txt&#x27; into table score;</span><br></pre></td></tr></table></figure>

<p>6）按需求查询数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select name, subject, score,</span><br><span class="line">rank() over(partition by subject order by score desc) rk,</span><br><span class="line">dense_rank() over(partition by subject order by score desc) drk,</span><br><span class="line">row_number() over(partition by subject order by score desc) rn</span><br><span class="line">from score;</span><br></pre></td></tr></table></figure>

<figure class="highlight tap"><table><tr><td class="code"><pre><span class="line">name    subject score   rk      drk     rn</span><br><span class="line">孙悟空  数学   <span class="number"> 95 </span>    <span class="number"> 1 </span>     <span class="number"> 1 </span>      1</span><br><span class="line">宋宋    数学   <span class="number"> 86 </span>    <span class="number"> 2 </span>     <span class="number"> 2 </span>      2</span><br><span class="line">婷婷    数学   <span class="number"> 85 </span>    <span class="number"> 3 </span>     <span class="number"> 3 </span>      3</span><br><span class="line">大海    数学   <span class="number"> 56 </span>    <span class="number"> 4 </span>     <span class="number"> 4 </span>      4</span><br><span class="line">大海    英语   <span class="number"> 84 </span>    <span class="number"> 1 </span>     <span class="number"> 1 </span>      1</span><br><span class="line">宋宋    英语   <span class="number"> 84 </span>    <span class="number"> 1 </span>     <span class="number"> 1 </span>      2</span><br><span class="line">婷婷    英语   <span class="number"> 78 </span>    <span class="number"> 3 </span>     <span class="number"> 2 </span>      3</span><br><span class="line">孙悟空  英语   <span class="number"> 68 </span>    <span class="number"> 4 </span>     <span class="number"> 3 </span>      4</span><br><span class="line">大海    语文   <span class="number"> 94 </span>    <span class="number"> 1 </span>     <span class="number"> 1 </span>      1</span><br><span class="line">孙悟空  语文   <span class="number"> 87 </span>    <span class="number"> 2 </span>     <span class="number"> 2 </span>      2</span><br><span class="line">婷婷    语文   <span class="number"> 65 </span>    <span class="number"> 3 </span>     <span class="number"> 3 </span>      3</span><br><span class="line">宋宋    语文   <span class="number"> 64 </span>    <span class="number"> 4 </span>     <span class="number"> 4 </span>      4</span><br></pre></td></tr></table></figure>

<p>扩展：求出每门学科前三名的学生？</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select name, subject, score</span><br><span class="line">from (</span><br><span class="line">select name, subject, score, rank() over(partition by subject order by score desc) rk</span><br><span class="line">from score</span><br><span class="line">-- dense_rank() over(partition by subject order by score desc) drk,</span><br><span class="line">-- row_number() over(partition by subject order by score desc) rn</span><br><span class="line">)t</span><br><span class="line">where rk &lt; 4;</span><br></pre></td></tr></table></figure>

<h4 id="常用日期函数"><a href="#常用日期函数" class="headerlink" title="常用日期函数"></a>常用日期函数</h4><p>unix_timestamp:返回当前或指定时间的时间戳<br>from_unixtime：将时间戳转为日期格式<br>current_date：当前日期<br>current_timestamp：当前的日期加时间<br>to_date：抽取日期部分<br>year：获取年<br>month：获取月<br>day：获取日<br>hour：获取时<br>minute：获取分<br>second：获取秒<br>weekofyear：当前时间是一年中的第几周<br>dayofmonth：当前时间是一个月中的第几天<br>months_between： 两个日期间的月份<br>add_months：日期加减月<br>datediff：两个日期相差的天数<br>date_add：日期加天数<br>date_sub：日期减天数<br>last_day：日期的当月的最后一天</p>
<h4 id="常用取整函数"><a href="#常用取整函数" class="headerlink" title="常用取整函数"></a>常用取整函数</h4><p>round： 四舍五入<br>ceil：  向上取整<br>floor： 向下取整</p>
<h4 id="常用字符串操作函数"><a href="#常用字符串操作函数" class="headerlink" title="常用字符串操作函数"></a>常用字符串操作函数</h4><p>upper： 转大写<br>lower： 转小写<br>length： 长度<br>trim：  前后去空格<br>lpad： 向左补齐，到指定长度<br>rpad：  向右补齐，到指定长度<br>regexp_replace： SELECT regexp_replace(‘100-200’, ‘(\d+)’, ‘num’)<br>使用正则表达式匹配目标字符串，匹配成功后替换！</p>
<h4 id="集合操作"><a href="#集合操作" class="headerlink" title="集合操作"></a>集合操作</h4><p>size: 集合中元素的个数<br>map_keys: 返回map中的key<br>map_values: 返回map中的value<br>array_contains: 判断array中是否包含某个元素<br>sort_array: 将array中的元素排序</p>
<h4 id="自定义函数"><a href="#自定义函数" class="headerlink" title="自定义函数"></a>自定义函数</h4><p>官方文档：<a href="https://cwiki.apache.org/confluence/display/Hive/HivePlugins">https://cwiki.apache.org/confluence/display/Hive/HivePlugins</a></p>
<p>UDF（User-Defined-Function）</p>
<p>一进一出</p>
<p>UDAF（User-Defined Aggregation Function）</p>
<p>聚集函数，多进一出</p>
<p>如：count/max/min</p>
<p>UDTF（User-Defined Table-Generating Functions）</p>
<p>一进多出</p>
<p>如：lateral view explode()</p>
<h5 id="编程步骤"><a href="#编程步骤" class="headerlink" title="编程步骤"></a>编程步骤</h5><p>1）继承Hive提供的类</p>
<p>org.apache.hadoop.hive.ql.udf.generic.GenericUDF  </p>
<p>org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;</p>
<p>2）实现类中的抽象方法</p>
<p>3）在hive的命令行窗口创建函数</p>
<p>添加jar</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">add jar linux_jar_path</span><br></pre></td></tr></table></figure>

<p>创建function</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create [temporary] function [dbname.]function_name AS class_name;</span><br></pre></td></tr></table></figure>

<p>4）在hive的命令行窗口删除函数</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">drop [temporary] function [if exists] [dbname.]function_name;</span><br></pre></td></tr></table></figure>

<h5 id="案例实操（UDF）"><a href="#案例实操（UDF）" class="headerlink" title="案例实操（UDF）"></a>案例实操（UDF）</h5><p>0）需求</p>
<p>自定义一个UDF实现计算给定字符串的长度，例如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive(default)&gt; select my_len(&quot;abcd&quot;);</span><br></pre></td></tr></table></figure>

<figure class="highlight"><table><tr><td class="code"><pre><span class="line">4</span><br></pre></td></tr></table></figure>

<p>1）创建一个Maven工程Hive</p>
<p>2）导入依赖</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">	<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">	<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"> 		<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"> 		<span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">  </span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>3）创建一个类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyUDF</span> <span class="keyword">extends</span> <span class="title">GenericUDF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 对输入参数的判断处理和返回值类型的约定</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> objectInspectors</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> UDFArgumentException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ObjectInspector <span class="title">initialize</span><span class="params">(ObjectInspector[] objectInspectors)</span> <span class="keyword">throws</span> UDFArgumentException </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (objectInspectors.length != <span class="number">1</span>)&#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> UDFArgumentLengthException(<span class="string">&quot;Input Args Length Error&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (!objectInspectors[<span class="number">0</span>].getCategory().equals(ObjectInspector.Category.PRIMITIVE))&#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> UDFArgumentTypeException(<span class="number">0</span>, <span class="string">&quot;Input Args Type Error&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//约定函数的返回值为int</span></span><br><span class="line">        <span class="keyword">return</span> PrimitiveObjectInspectorFactory.javaIntObjectInspector;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 函数的逻辑处理</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> deferredObjects</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> HiveException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">evaluate</span><span class="params">(DeferredObject[] deferredObjects)</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line">        Object o = deferredObjects[<span class="number">0</span>].get();</span><br><span class="line">        <span class="keyword">if</span>(o == <span class="keyword">null</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> o.toString().length();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getDisplayString</span><span class="params">(String[] strings)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>4）打成jar包上传到服务器/opt/module/hive/datas/myudf.jar</p>
<p>5）将jar包添加到hive的classpath</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; add jar /opt/module/hive/datas/myudf.jar;</span><br></pre></td></tr></table></figure>

<p>6）创建临时函数与开发好的java class关联</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create temporary function my_len as &quot;com.vincent.hive.MyUDF&quot;;</span><br></pre></td></tr></table></figure>

<p>7）在hive中使用自定义的函数</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select my_len(&quot;122334&quot;);</span><br></pre></td></tr></table></figure>

<h5 id="案例实操（UDTF）"><a href="#案例实操（UDTF）" class="headerlink" title="案例实操（UDTF）"></a>案例实操（UDTF）</h5><p>0）需求</p>
<p>自定义一个UDTF实现将一个任意分割符的字符串切割成独立的单词，例如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive(default)&gt; select myudtf(&quot;hello,world,hadoop,hive&quot;, &quot;,&quot;);</span><br></pre></td></tr></table></figure>

<figure class="highlight ebnf"><table><tr><td class="code"><pre><span class="line"><span class="attribute">hello</span></span><br><span class="line"><span class="attribute">world</span></span><br><span class="line"><span class="attribute">hadoop</span></span><br><span class="line"><span class="attribute">hive</span> </span><br></pre></td></tr></table></figure>

<p>1）代码实现</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyUDTF</span> <span class="keyword">extends</span> <span class="title">GenericUDTF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> ArrayList&lt;String&gt; outList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> StructObjectInspector <span class="title">initialize</span><span class="params">(StructObjectInspector argOIs)</span> <span class="keyword">throws</span> UDFArgumentException </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">//1.定义输出数据的列名和类型</span></span><br><span class="line">        List&lt;String&gt; fieldNames = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        List&lt;ObjectInspector&gt; fieldOIs = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2.添加输出数据的列名和类型</span></span><br><span class="line">        fieldNames.add(<span class="string">&quot;lineToWord&quot;</span>);</span><br><span class="line">        fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Object[] args)</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//1.获取原始数据</span></span><br><span class="line">        String arg = args[<span class="number">0</span>].toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2.获取数据传入的第二个参数，此处为分隔符</span></span><br><span class="line">        String splitKey = args[<span class="number">1</span>].toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3.将原始数据按照传入的分隔符进行切分</span></span><br><span class="line">        String[] fields = arg.split(splitKey);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4.遍历切分后的结果，并写出</span></span><br><span class="line">        <span class="keyword">for</span> (String field : fields) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">//集合为复用的，首先清空集合</span></span><br><span class="line">            outList.clear();</span><br><span class="line"></span><br><span class="line">            <span class="comment">//将每一个单词添加至集合</span></span><br><span class="line">            outList.add(field);</span><br><span class="line"></span><br><span class="line">            <span class="comment">//将集合内容写出</span></span><br><span class="line">            forward(outList);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> HiveException </span>&#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>3）打成jar包上传到服务器/opt/module/hive/datas/myudtf.jar</p>
<p>4）将jar包添加到hive的classpath</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; add jar /opt/module/hive/datas/myudtf.jar;</span><br></pre></td></tr></table></figure>

<p>5）创建临时函数与开发好的java class关联</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create temporary function myudtf as &quot;com.vincent.hive.MyUDTF&quot;;</span><br></pre></td></tr></table></figure>

<p>6）在hive中使用自定义的函数</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select myudtf(&quot;hello,world,hadoop,hive&quot;,&quot;,&quot;);</span><br></pre></td></tr></table></figure>

<h2 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h2><h3 id="MR支持的压缩编码"><a href="#MR支持的压缩编码" class="headerlink" title="MR支持的压缩编码"></a>MR支持的压缩编码</h3><table>
<thead>
<tr>
<th>压缩格式</th>
<th>工具</th>
<th>算法</th>
<th>文件扩展名</th>
<th>是否可切分</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>无</td>
<td>DEFLATE</td>
<td>.deflate</td>
<td>否</td>
</tr>
<tr>
<td>Gzip</td>
<td>gzip</td>
<td>DEFLATE</td>
<td>.gz</td>
<td>否</td>
</tr>
<tr>
<td>bzip2</td>
<td>bzip2</td>
<td>bzip2</td>
<td>.bz2</td>
<td>是</td>
</tr>
<tr>
<td>LZO</td>
<td>lzop</td>
<td>LZO</td>
<td>.lzo</td>
<td>是</td>
</tr>
<tr>
<td>Snappy</td>
<td>无</td>
<td>Snappy</td>
<td>.snappy</td>
<td>否</td>
</tr>
</tbody></table>
<p>为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示：</p>
<table>
<thead>
<tr>
<th>压缩格式</th>
<th>对应的编码/解码器</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
</tr>
<tr>
<td>gzip</td>
<td>org.apache.hadoop.io.compress.GzipCodec</td>
</tr>
<tr>
<td>bzip2</td>
<td>org.apache.hadoop.io.compress.BZip2Codec</td>
</tr>
<tr>
<td>LZO</td>
<td>com.hadoop.compression.lzo.LzopCodec</td>
</tr>
<tr>
<td>Snappy</td>
<td>org.apache.hadoop.io.compress.SnappyCodec</td>
</tr>
</tbody></table>
<p>压缩性能的比较：</p>
<table>
<thead>
<tr>
<th>压缩算法</th>
<th>原始文件大小</th>
<th>压缩文件大小</th>
<th>压缩速度</th>
<th>解压速度</th>
</tr>
</thead>
<tbody><tr>
<td>gzip</td>
<td>8.3GB</td>
<td>1.8GB</td>
<td>17.5MB/s</td>
<td>58MB/s</td>
</tr>
<tr>
<td>bzip2</td>
<td>8.3GB</td>
<td>1.1GB</td>
<td>2.4MB/s</td>
<td>9.5MB/s</td>
</tr>
<tr>
<td>LZO</td>
<td>8.3GB</td>
<td>2.9GB</td>
<td>49.3MB/s</td>
<td>74.6MB/s</td>
</tr>
</tbody></table>
<h3 id="压缩参数配置"><a href="#压缩参数配置" class="headerlink" title="压缩参数配置"></a>压缩参数配置</h3><p>要在Hadoop中启用压缩，可以配置如下参数（mapred-site.xml文件中）：</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>默认值</th>
<th>阶段</th>
<th>建议</th>
</tr>
</thead>
<tbody><tr>
<td>io.compression.codecs  （在core-site.xml中配置）</td>
<td>org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec, org.apache.hadoop.io.compress.Lz4Codec</td>
<td>输入压缩</td>
<td>Hadoop使用文件扩展名判断是否支持某种编解码器</td>
</tr>
<tr>
<td>mapreduce.map.output.compress</td>
<td>false</td>
<td>mapper输出</td>
<td>这个参数设为true启用压缩</td>
</tr>
<tr>
<td>mapreduce.map.output.compress.codec</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
<td>mapper输出</td>
<td>使用LZO、LZ4或snappy编解码器在此阶段压缩数据</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress</td>
<td>false</td>
<td>reducer输出</td>
<td>这个参数设为true启用压缩</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress.codec</td>
<td>org.apache.hadoop.io.compress. DefaultCodec</td>
<td>reducer输出</td>
<td>使用标准工具或者编解码器，如gzip和bzip2</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress.type</td>
<td>RECORD</td>
<td>reducer输出</td>
<td>SequenceFile输出使用的压缩类型：NONE和BLOCK</td>
</tr>
</tbody></table>
<h3 id="开启Map输出阶段压缩"><a href="#开启Map输出阶段压缩" class="headerlink" title="开启Map输出阶段压缩"></a>开启Map输出阶段压缩</h3><p>1）开启hive中间传输数据压缩功能</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt;set hive.exec.compress.intermediate=true;</span><br></pre></td></tr></table></figure>

<p>2）开启mapreduce中map输出压缩功能</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt;set mapreduce.map.output.compress=true;</span><br></pre></td></tr></table></figure>

<p>3）设置mapreduce中map输出数据的压缩方式</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt;set mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec;</span><br></pre></td></tr></table></figure>

<p>4）执行查询语句</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select count(ename) name from emp;</span><br></pre></td></tr></table></figure>

<h3 id="开启Reduce输出阶段压缩"><a href="#开启Reduce输出阶段压缩" class="headerlink" title="开启Reduce输出阶段压缩"></a>开启Reduce输出阶段压缩</h3><p>1）开启hive最终输出数据压缩功能</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt;set hive.exec.compress.output=true;</span><br></pre></td></tr></table></figure>

<p>（2）开启mapreduce最终输出数据压缩</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt;set mapreduce.output.fileoutputformat.compress=true;</span><br></pre></td></tr></table></figure>

<p>（3）设置mapreduce最终数据输出压缩方式</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; set mapreduce.output.fileoutputformat.compress.codec = org.apache.hadoop.io.compress.SnappyCodec;</span><br></pre></td></tr></table></figure>

<p>（4）设置mapreduce最终数据输出压缩为块压缩</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; set mapreduce.output.fileoutputformat.compress.type=BLOCK;</span><br></pre></td></tr></table></figure>

<p>（5）测试一下输出结果是否是压缩文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite local directory</span><br><span class="line"> &#x27;/opt/module/datas/distribute-result&#x27; select * from emp distribute by deptno sort by empno desc;</span><br></pre></td></tr></table></figure>

<h2 id="文件存储格式"><a href="#文件存储格式" class="headerlink" title="文件存储格式"></a>文件存储格式</h2><p>Hive支持的存储数据的格式主要有：TEXTFILE 、SEQUENCEFILE、ORC、PARQUET。</p>
<p>如图所示左边为逻辑表，右边第一个为行式存储，第二个为列式存储。</p>
<p>1）行存储的特点</p>
<p>查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。</p>
<p>2）列存储的特点</p>
<p>因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。</p>
<p>TEXTFILE和SEQUENCEFILE的存储格式都是基于行存储的；</p>
<p>ORC和PARQUET是基于列式存储的。</p>
<h3 id="TextFile格式"><a href="#TextFile格式" class="headerlink" title="TextFile格式"></a>TextFile格式</h3><p>默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2使用，但使用Gzip这种方式，hive不会对数据进行切分，从而无法对数据进行并行操作。</p>
<h3 id="Orc格式"><a href="#Orc格式" class="headerlink" title="Orc格式"></a>Orc格式</h3><p>Orc (Optimized Row Columnar)是Hive 0.11版里引入的新的存储格式。</p>
<p>如下图所示可以看到每个Orc文件由1个或多个stripe组成，每个stripe一般为HDFS的块大小，每一个stripe包含多条记录，这些记录按照列进行独立存储，对应到Parquet中的row group的概念。每个Stripe里有三部分组成，分别是Index Data，Row Data，Stripe Footer：</p>
<p>1）Index Data：一个轻量级的index，默认是每隔1W行做一个索引。这里做的索引应该只是记录某行的各字段在Row Data中的offset。</p>
<p>2）Row Data：存的是具体的数据，先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个Stream来存储。</p>
<p>3）Stripe Footer：存的是各个Stream的类型，长度等信息。</p>
<p>每个文件有一个File Footer，这里面存的是每个Stripe的行数，每个Column的数据类型信息等；每个文件的尾部是一个PostScript，这里面记录了整个文件的压缩类型以及FileFooter的长度信息等。在读取文件时，会seek到文件尾部读PostScript，从里面解析到File Footer长度，再读FileFooter，从里面解析到各个Stripe信息，再读各个Stripe，即从后往前读。</p>
<h3 id="Parquet格式"><a href="#Parquet格式" class="headerlink" title="Parquet格式"></a>Parquet格式</h3><p>Parquet文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，因此Parquet格式文件是自解析的。</p>
<p>（1）行组(Row Group)：每一个行组包含一定的行数，在一个HDFS文件中至少存储一个行组，类似于orc的stripe的概念。</p>
<p>（2）列块(Column Chunk)：在一个行组中每一列保存在一个列块中，行组中的所有列连续的存储在这个列块文件中。一个列块中的值都是相同类型的，不同的列块可能使用不同的算法进行压缩。</p>
<p>（3）页(Page)：每一个列块划分为多个页，一个页是最小的编码的单位，在同一个列块的不同页可能使用不同的编码方式。</p>
<p>通常情况下，在存储Parquet数据的时候会按照Block大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，这样可以把每一个行组由一个Mapper任务处理，增大任务执行并行度。Parquet文件的格式。</p>
<p><img src="/Hive/37.png" alt="37"></p>
<p>上图展示了一个Parquet文件的内容，一个文件中可以存储多个行组，文件的首位都是该文件的Magic Code，用于校验它是否是一个Parquet文件，Footer length记录了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量，文件的元数据中包括每一个行组的元数据信息和该文件存储数据的Schema信息。除了文件中每一个行组的元数据，每一页的开始都会存储该页的元数据，在Parquet中，有三种类型的页：数据页、字典页和索引页。数据页用于存储当前行组中该列的值，字典页存储该列值的编码字典，每一个列块中最多包含一个字典页，索引页用来存储当前行组下该列的索引，目前Parquet中还不支持索引页。</p>
<h3 id="主流文件存储格式对比实验"><a href="#主流文件存储格式对比实验" class="headerlink" title="主流文件存储格式对比实验"></a>主流文件存储格式对比实验</h3><p>TextFile</p>
<p>1）创建表，存储数据格式为TEXTFILE</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table log_text (</span><br><span class="line">track_time string,</span><br><span class="line">url string,</span><br><span class="line">session_id string,</span><br><span class="line">referer string,</span><br><span class="line">ip string,</span><br><span class="line">end_user_id string,</span><br><span class="line">city_id string</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;</span><br><span class="line">stored as textfile;</span><br></pre></td></tr></table></figure>

<p>（2）向表中加载数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/log.data&#x27; into table log_text ;</span><br></pre></td></tr></table></figure>

<p>（3）查看表中数据大小</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -du -h /user/hive/warehouse/log_text;</span><br></pre></td></tr></table></figure>

<p>Orc</p>
<p>1）创建表，存储数据格式为ORC</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table log_orc(</span><br><span class="line">track_time string,</span><br><span class="line">url string,</span><br><span class="line">session_id string,</span><br><span class="line">referer string,</span><br><span class="line">ip string,</span><br><span class="line">end_user_id string,</span><br><span class="line">city_id string</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;</span><br><span class="line">stored as orc</span><br><span class="line">tblproperties(&quot;orc.compress&quot;=&quot;NONE&quot;);</span><br></pre></td></tr></table></figure>

<p>2）向表中加载数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; insert into table log_orc select * from log_text ;</span><br></pre></td></tr></table></figure>

<p>3）查看表中数据大小</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -du -h /user/hive/warehouse/log_orc/ ;</span><br></pre></td></tr></table></figure>

<p>Parquet</p>
<p>1）创建表，存储数据格式为parquet</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table log_parquet(</span><br><span class="line">track_time string,</span><br><span class="line">url string,</span><br><span class="line">session_id string,</span><br><span class="line">referer string,</span><br><span class="line">ip string,</span><br><span class="line">end_user_id string,</span><br><span class="line">city_id string</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;</span><br><span class="line">stored as parquet ;</span><br></pre></td></tr></table></figure>

<p>2）向表中加载数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; insert into table log_parquet select * from log_text ;</span><br></pre></td></tr></table></figure>

<p>（3）查看表中数据大小</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -du -h /user/hive/warehouse/log_parquet/ ;</span><br></pre></td></tr></table></figure>

<p>存储文件的压缩比总结：</p>
<p>ORC &gt;  Parquet &gt;  textFile</p>
<p>存储文件的查询速度总结：</p>
<p>查询速度相近</p>
<h2 id="存储和压缩结合"><a href="#存储和压缩结合" class="headerlink" title="存储和压缩结合"></a>存储和压缩结合</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC</a></p>
<p>ORC存储方式的压缩:</p>
<table>
<thead>
<tr>
<th>Key</th>
<th>Default</th>
<th>Notes</th>
</tr>
</thead>
<tbody><tr>
<td>orc.compress</td>
<td>ZLIB</td>
<td>high level compression (one of NONE, ZLIB, SNAPPY)</td>
</tr>
<tr>
<td>orc.compress.size</td>
<td>262,144</td>
<td>number of bytes in each compression chunk</td>
</tr>
<tr>
<td>orc.stripe.size</td>
<td>268,435,456</td>
<td>number of bytes in each stripe</td>
</tr>
<tr>
<td>orc.row.index.stride</td>
<td>10,000</td>
<td>number of rows between index entries (must be &gt;= 1000)</td>
</tr>
<tr>
<td>orc.create.index</td>
<td>true</td>
<td>whether to create row indexes</td>
</tr>
<tr>
<td>orc.bloom.filter.columns</td>
<td>“”</td>
<td>comma separated list of column names for which bloom filter should be created</td>
</tr>
<tr>
<td>orc.bloom.filter.fpp</td>
<td>0.05</td>
<td>false positive probability for bloom filter (must &gt;0.0 and &lt;1.0)</td>
</tr>
</tbody></table>
<h3 id="1）创建一个ZLIB压缩的的ORC存储方式"><a href="#1）创建一个ZLIB压缩的的ORC存储方式" class="headerlink" title="1）创建一个ZLIB压缩的的ORC存储方式"></a>1）创建一个ZLIB压缩的的ORC存储方式</h3><p>1）建表语句</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table log_orc_zlib(</span><br><span class="line">track_time string,</span><br><span class="line">url string,</span><br><span class="line">session_id string,</span><br><span class="line">referer string,</span><br><span class="line">ip string,</span><br><span class="line">end_user_id string,</span><br><span class="line">city_id string</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;</span><br><span class="line">stored as orc</span><br><span class="line">tblproperties(&quot;orc.compress&quot;=&quot;ZLIB&quot;);</span><br></pre></td></tr></table></figure>

<p>2）插入数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">insert into log_orc_zlib select * from log_text;</span><br></pre></td></tr></table></figure>

<p>3）查看插入后数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -du -h /user/hive/warehouse/log_orc_none/ ;</span><br></pre></td></tr></table></figure>

<h3 id="2）创建一个SNAPPY压缩的ORC存储方式"><a href="#2）创建一个SNAPPY压缩的ORC存储方式" class="headerlink" title="2）创建一个SNAPPY压缩的ORC存储方式"></a>2）创建一个SNAPPY压缩的ORC存储方式</h3><p>1）建表语句</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table log_orc_snappy(</span><br><span class="line">track_time string,</span><br><span class="line">url string,</span><br><span class="line">session_id string,</span><br><span class="line">referer string,</span><br><span class="line">ip string,</span><br><span class="line">end_user_id string,</span><br><span class="line">city_id string</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;</span><br><span class="line">stored as orc</span><br><span class="line">tblproperties(&quot;orc.compress&quot;=&quot;SNAPPY&quot;);</span><br></pre></td></tr></table></figure>

<p>2）插入数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">insert into log_orc_snappy select * from log_text;</span><br></pre></td></tr></table></figure>

<p>3）查看插入后数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -du -h /user/hive/warehouse/log_orc_snappy/ ;</span><br></pre></td></tr></table></figure>

<p>总结：ZLIB采用的是deflate压缩算法，比snappy压缩的小。在实际的项目开发当中，hive表的数据存储格式一般选择：orc或parquet。压缩方式一般选择snappy，lzo。</p>
<h2 id="Hive练习"><a href="#Hive练习" class="headerlink" title="Hive练习"></a>Hive练习</h2><h3 id="需求描述"><a href="#需求描述" class="headerlink" title="需求描述"></a>需求描述</h3><p>统计影音视频网站的常规指标，各种TopN指标：</p>
<figure class="highlight ada"><table><tr><td class="code"><pre><span class="line"><span class="comment">--统计视频观看数Top10</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--统计视频类别热度Top10</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--统计视频观看数Top20所属类别</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--统计视频观看数Top50所关联视频的所属类别Rank</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--统计每个类别中的视频热度Top10</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--统计每个类别视频观看数Top10</span></span><br></pre></td></tr></table></figure>



<h3 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h3><table>
<thead>
<tr>
<th>字段</th>
<th>备注</th>
<th>详细描述</th>
</tr>
</thead>
<tbody><tr>
<td>videoId</td>
<td>视频唯一id（String）</td>
<td>11位字符串</td>
</tr>
<tr>
<td>uploader</td>
<td>视频上传者（String）</td>
<td>上传视频的用户名String</td>
</tr>
<tr>
<td>age</td>
<td>视频年龄（int）</td>
<td>视频在平台上的整数天</td>
</tr>
<tr>
<td>category</td>
<td>视频类别（Array<String>）</String></td>
<td>上传视频指定的视频分类</td>
</tr>
<tr>
<td>length</td>
<td>视频长度（Int）</td>
<td>整形数字标识的视频长度</td>
</tr>
<tr>
<td>views</td>
<td>观看次数（Int）</td>
<td>视频被浏览的次数</td>
</tr>
<tr>
<td>rate</td>
<td>视频评分（Double）</td>
<td>满分5分</td>
</tr>
<tr>
<td>Ratings</td>
<td>流量（Int）</td>
<td>视频的流量，整型数字</td>
</tr>
<tr>
<td>conments</td>
<td>评论数（Int）</td>
<td>一个视频的整数评论数</td>
</tr>
<tr>
<td>relatedId</td>
<td>相关视频id（Array<String>）</String></td>
<td>相关视频的id，最多20个</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>字段</th>
<th>备注</th>
<th>字段类型</th>
</tr>
</thead>
<tbody><tr>
<td>uploader</td>
<td>上传者用户名</td>
<td>string</td>
</tr>
<tr>
<td>videos</td>
<td>上传视频数</td>
<td>int</td>
</tr>
<tr>
<td>friends</td>
<td>朋友数量</td>
<td>int</td>
</tr>
</tbody></table>
<h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><p>0）ETL</p>
<p>ETL之封装工具类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ETLUtil</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment"> 	* 数据清洗方法</span></span><br><span class="line"><span class="comment"> 	*/</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">etlData</span><span class="params">(String srcData)</span></span>&#123;</span><br><span class="line">        StringBuffer resultData = <span class="keyword">new</span> StringBuffer();</span><br><span class="line">        <span class="comment">//1. 先将数据通过\t 切割</span></span><br><span class="line">        String[] datas = srcData.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">        <span class="comment">//2. 判断长度是否小于9</span></span><br><span class="line">        <span class="keyword">if</span>(datas.length &lt;<span class="number">9</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span> ;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//3. 将数据中的视频类别的空格去掉</span></span><br><span class="line">        datas[<span class="number">3</span>]=datas[<span class="number">3</span>].replaceAll(<span class="string">&quot; &quot;</span>,<span class="string">&quot;&quot;</span>);</span><br><span class="line">        <span class="comment">//4. 将数据中的关联视频id通过&amp;拼接</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; datas.length; i++) &#123;</span><br><span class="line">            <span class="keyword">if</span>(i &lt; <span class="number">9</span>)&#123;</span><br><span class="line">                <span class="comment">//4.1 没有关联视频的情况</span></span><br><span class="line">                <span class="keyword">if</span>(i == datas.length-<span class="number">1</span>)&#123;</span><br><span class="line">                    resultData.append(datas[i]);</span><br><span class="line">                &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                    resultData.append(datas[i]).append(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                <span class="comment">//4.2 有关联视频的情况</span></span><br><span class="line">                <span class="keyword">if</span>(i == datas.length-<span class="number">1</span>)&#123;</span><br><span class="line">                    resultData.append(datas[i]);</span><br><span class="line">                &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                    resultData.append(datas[i]).append(<span class="string">&quot;&amp;&quot;</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> resultData.toString();</span><br><span class="line">    &#125;</span><br><span class="line">	&#125;  </span><br></pre></td></tr></table></figure>

<p>ETL之Mapper</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 清洗谷粒影音的原始数据</span></span><br><span class="line"><span class="comment"> * 清洗规则</span></span><br><span class="line"><span class="comment"> *  1. 将数据长度小于9的清洗掉</span></span><br><span class="line"><span class="comment"> *  2. 将数据中的视频类别中间的空格去掉   People &amp; Blogs</span></span><br><span class="line"><span class="comment"> *  3. 将数据中的关联视频id通过&amp;符号拼接</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EtlMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>,<span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Text k = <span class="keyword">new</span> Text();</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">       <span class="comment">//获取一行</span></span><br><span class="line">        String line = value.toString();</span><br><span class="line">        <span class="comment">//清洗</span></span><br><span class="line">        String resultData = ETLUtil.etlData(line);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(resultData != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="comment">//写出</span></span><br><span class="line">            k.set(resultData);</span><br><span class="line">            context.write(k,NullWritable.get());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>ETL之Driver</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EtlDriver</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        Job job  = Job.getInstance(conf);</span><br><span class="line">        job.setJarByClass(EtlDriver.class);</span><br><span class="line">        job.setMapperClass(EtlMapper.class);</span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(NullWritable.class);</span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(NullWritable.class);</span><br><span class="line">        job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line">        FileInputFormat.setInputPaths(job,<span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job,<span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>

<p>将ETL程序打包为etl.jar 并上传到Linux的 /opt/module/hive/datas 目录下。</p>
<p>上传原始数据到HDFS。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas] pwd</span><br><span class="line">/opt/module/hive/datas</span><br><span class="line">[vincent@linux1 datas] hadoop fs -mkdir -p  /gulivideo/video</span><br><span class="line">[vincent@linux1 datas] hadoop fs -mkdir -p  /gulivideo/user</span><br><span class="line">[vincent@linux1 datas] hadoop fs -put gulivideo/user/user.txt   /gulivideo/user</span><br><span class="line">[vincent@linux1 datas] hadoop fs -put gulivideo/video/*.txt   /gulivideo/video</span><br></pre></td></tr></table></figure>

<p>ETL数据</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas] hadoop jar etl.jar com.vincent.gulivideo.etl.EtlDriver /gulivideo/video /gulivideo/video/output</span><br></pre></td></tr></table></figure>



<p>创建原始数据表：gulivideo_ori，gulivideo_user_ori，</p>
<p>创建最终表：gulivideo_orc，gulivideo_user_orc</p>
<p>1）创建原始数据表：gulivideo_ori</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table gulivideo_ori(</span><br><span class="line">    videoId string, </span><br><span class="line">    uploader string, </span><br><span class="line">    age int, </span><br><span class="line">    category array&lt;string&gt;, </span><br><span class="line">    length int, </span><br><span class="line">    views int, </span><br><span class="line">    rate float, </span><br><span class="line">    ratings int, </span><br><span class="line">    comments int,</span><br><span class="line">    relatedId array&lt;string&gt;)</span><br><span class="line">row format delimited fields terminated by &quot;\t&quot;</span><br><span class="line">collection items terminated by &quot;&amp;&quot;</span><br><span class="line">stored as textfile;</span><br></pre></td></tr></table></figure>

<p>创建原始数据表: gulivideo_user_ori</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table gulivideo_user_ori(</span><br><span class="line">    uploader string,</span><br><span class="line">    videos int,</span><br><span class="line">    friends int)</span><br><span class="line">row format delimited </span><br><span class="line">fields terminated by &quot;\t&quot; </span><br><span class="line">stored as textfile;</span><br></pre></td></tr></table></figure>

<p>创建orc存储格式带snappy压缩的表：gulivideo_orc</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table gulivideo_orc(</span><br><span class="line">    videoId string, </span><br><span class="line">    uploader string, </span><br><span class="line">    age int, </span><br><span class="line">    category array&lt;string&gt;, </span><br><span class="line">    length int, </span><br><span class="line">    views int, </span><br><span class="line">    rate float, </span><br><span class="line">    ratings int, </span><br><span class="line">    comments int,</span><br><span class="line">    relatedId array&lt;string&gt;)</span><br><span class="line">stored as orc</span><br><span class="line">tblproperties(&quot;orc.compress&quot;=&quot;SNAPPY&quot;);</span><br></pre></td></tr></table></figure>

<p>创建orc存储格式带snappy压缩的表：gulivideo_user_orc</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table gulivideo_user_orc(</span><br><span class="line">    uploader string,</span><br><span class="line">    videos int,</span><br><span class="line">    friends int)</span><br><span class="line">row format delimited </span><br><span class="line">fields terminated by &quot;\t&quot; </span><br><span class="line">stored as orc</span><br><span class="line">tblproperties(&quot;orc.compress&quot;=&quot;SNAPPY&quot;);</span><br></pre></td></tr></table></figure>

<p>向ori表插入数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">load data inpath &quot;/gulivideo/video/output&quot; into table gulivideo_ori;</span><br><span class="line">load data inpath &quot;/gulivideo/user&quot; into table gulivideo_user_ori;</span><br></pre></td></tr></table></figure>

<p>向orc表插入数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">insert into table gulivideo_orc select * from gulivideo_ori;</span><br><span class="line">insert into table gulivideo_user_orc select * from gulivideo_user_ori;</span><br></pre></td></tr></table></figure>

<h3 id="业务分析"><a href="#业务分析" class="headerlink" title="业务分析"></a>业务分析</h3><h4 id="1-统计视频观看数Top10"><a href="#1-统计视频观看数Top10" class="headerlink" title="1.统计视频观看数Top10"></a>1.统计视频观看数Top10</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SELECT </span><br><span class="line">     videoId,</span><br><span class="line">     views </span><br><span class="line">FROM </span><br><span class="line">     gulivideo_orc</span><br><span class="line">ORDER BY </span><br><span class="line">     views DESC </span><br><span class="line">LIMIT 10;</span><br><span class="line">//或者</span><br><span class="line">select videoId, views from gulivideo_orc sort by views desc limit 10;</span><br></pre></td></tr></table></figure>

<h4 id="2-统计视频类别热度Top10"><a href="#2-统计视频类别热度Top10" class="headerlink" title="2.统计视频类别热度Top10"></a>2.统计视频类别热度Top10</h4><p>思路：</p>
<p>（1）即统计每个类别有多少个视频，显示出包含视频最多的前10个类别。</p>
<p>（2）我们需要按照类别group by聚合，然后count组内的videoId个数即可。</p>
<p>（3）因为当前表结构为：一个视频对应一个或多个类别。所以如果要group by类别，需要先将类别进行列转行(展开)，然后再进行count即可。</p>
<p>（4）最后按照热度排序，显示前10条。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select t1.category_name, count(t1.videoid) video_count</span><br><span class="line">from (</span><br><span class="line">    select videoid, category_name</span><br><span class="line">    from gulivideo_orc </span><br><span class="line">  	lateral view explode(category) gulivideo_orc_tmp as category_name</span><br><span class="line">) t1</span><br><span class="line">group by t1.category_name</span><br><span class="line">order by video_count desc</span><br><span class="line">limit 10</span><br></pre></td></tr></table></figure>

<h4 id="3-统计出视频观看数最高的20个视频的所属类别以及类别包含Top20视频的个数"><a href="#3-统计出视频观看数最高的20个视频的所属类别以及类别包含Top20视频的个数" class="headerlink" title="3.统计出视频观看数最高的20个视频的所属类别以及类别包含Top20视频的个数"></a>3.统计出视频观看数最高的20个视频的所属类别以及类别包含Top20视频的个数</h4><p>思路：</p>
<p>（1）先找到观看数最高的20个视频所属条目的所有信息，降序排列</p>
<p>（2）把这20条信息中的category分裂出来(列转行)</p>
<p>（3）最后查询视频分类名称和该分类下有多少个Top20的视频</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select t2.category_name, count(t2.videoid)</span><br><span class="line">from</span><br><span class="line">     (select t1.videoid, t1.`views`, category_name</span><br><span class="line">        from(</span><br><span class="line">            select videoid, `views`,category</span><br><span class="line">            from gulivideo_orc</span><br><span class="line">            order by `views` desc</span><br><span class="line">            limit 20) t1</span><br><span class="line">        lateral view explode(t1.category) tmp as category_name) t2</span><br><span class="line">group by t2.category_name</span><br></pre></td></tr></table></figure>

<h4 id="4-统计视频观看数Top50所关联视频的所属类别排序"><a href="#4-统计视频观看数Top50所关联视频的所属类别排序" class="headerlink" title="4.统计视频观看数Top50所关联视频的所属类别排序"></a>4.统计视频观看数Top50所关联视频的所属类别排序</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select t6.category_name, t6.category_count, rank() over (order by t6.category_count desc ) category_rk</span><br><span class="line">from (</span><br><span class="line">    select t5.category_name, count(t5.category_name) category_count</span><br><span class="line">    from (</span><br><span class="line">        select category_name</span><br><span class="line">        from(</span><br><span class="line">            select t2.relatedid_id, t3.category</span><br><span class="line">            from (</span><br><span class="line">                select relatedid_id</span><br><span class="line">                from(</span><br><span class="line">                    select videoid, relatedid, `views`</span><br><span class="line">                    from gulivideo_orc</span><br><span class="line">                    order by `views` desc</span><br><span class="line">                    limit 50) t1</span><br><span class="line">                lateral view explode(t1.relatedid) tmp as relatedid_id) t2 join gulivideo_orc t3</span><br><span class="line">                on t2.relatedid_id = t3.videoid) t4</span><br><span class="line">        lateral view explode(t4.category) t4_tmp as category_name) t5</span><br><span class="line">    group by (t5.category_name)) t6</span><br></pre></td></tr></table></figure>

<h4 id="5-统计每个类别中的视频热度Top10，以Music为例"><a href="#5-统计每个类别中的视频热度Top10，以Music为例" class="headerlink" title="5.统计每个类别中的视频热度Top10，以Music为例"></a>5.统计每个类别中的视频热度Top10，以Music为例</h4><p>思路：</p>
<p>（1）要想统计Music类别中的视频热度Top10，需要先找到Music类别，那么就需要将category展开，所以可以创建一张表用于存放categoryId展开的数据。</p>
<p>（2）向category展开的表中插入数据。</p>
<p>（3）统计对应类别（Music）中的视频热度。</p>
<p>统计Music类别的Top10（也可以统计其他）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select t1.videoid, t1.`views`, t1.category_name</span><br><span class="line">from (</span><br><span class="line">     select videoid, `views`, category_name</span><br><span class="line">     from gulivideo_orc</span><br><span class="line">     lateral view explode(category) tmp as category_name</span><br><span class="line">     ) t1</span><br><span class="line">where t1.category_name = &#x27;Music&#x27;</span><br><span class="line">order by t1.`views` desc</span><br><span class="line">limit 10</span><br></pre></td></tr></table></figure>

<h4 id="6-统计每个类别视频观看数Top10"><a href="#6-统计每个类别视频观看数Top10" class="headerlink" title="6.统计每个类别视频观看数Top10"></a>6.统计每个类别视频观看数Top10</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select t2.videoid, t2.`views`, t2.category_name, t2.rk</span><br><span class="line">from (</span><br><span class="line">      select t1.videoid, t1.`views`, t1.category_name, rank() over (partition by t1.category_name order by t1.`views` desc) rk</span><br><span class="line">      from (</span><br><span class="line">               select videoid, `views`, category_name</span><br><span class="line">               from gulivideo_orc</span><br><span class="line">               lateral view explode(category) tmp as category_name</span><br><span class="line">           ) t1</span><br><span class="line">     ) t2</span><br><span class="line">where t2.rk &lt;= 10</span><br></pre></td></tr></table></figure>

<h4 id="7-统计上传视频最多的用户Top10以及他们上传的视频观看次数在前20的视频"><a href="#7-统计上传视频最多的用户Top10以及他们上传的视频观看次数在前20的视频" class="headerlink" title="7.统计上传视频最多的用户Top10以及他们上传的视频观看次数在前20的视频"></a>7.统计上传视频最多的用户Top10以及他们上传的视频观看次数在前20的视频</h4><p>思路：</p>
<p>（1）求出上传视频最多的10个用户</p>
<p>（2）关联gulivideo_orc表，求出这10个用户上传的所有的视频，按照观看数取前20</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select t1.uploader, t2.videoid, t2.`views`</span><br><span class="line">from (</span><br><span class="line">         select uploader</span><br><span class="line">         from gulivideo_user_orc</span><br><span class="line">         order by videos desc</span><br><span class="line">         limit 10</span><br><span class="line">     ) t1 join gulivideo_orc t2 on t1.uploader = t2.uploader</span><br><span class="line">order by t2.`views` desc</span><br><span class="line">limit 20</span><br></pre></td></tr></table></figure>

<h2 id="Hive参数调优"><a href="#Hive参数调优" class="headerlink" title="Hive参数调优"></a>Hive参数调优</h2><p>1、启用数据压缩</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!--目的：减少存储和IO--&gt;</span></span><br><span class="line"><span class="comment">&lt;!--压缩Hive输出和中间结果--&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.exec.compress.output<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">      This controls whether the final outputs of a query (to a local/HDFS file or a Hive table) is compressed. </span><br><span class="line">      The compression codec and other options are determined from Hadoop config variables mapred.output.compress*</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.exec.compress.intermediate<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">      This controls whether intermediate files produced by Hive between multiple map-reduce jobs are compressed. </span><br><span class="line">      The compression codec and other options are determined from Hadoop config variables mapred.output.compress*</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--设置Hive中间表存储格式--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.query.result.fileformat<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>SequenceFile <span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>File format to use for a query&#x27;s intermediate results. Options are TextFile, SequenceFile, and RCfile. Default value is changed to SequenceFile since Hive 2.1.0<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>2、Job执行优化</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#并行执行多个job</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#hive的物理执行任务默认情况下是一个job执行完了之后再执行其它job，这种情况下如果想加快hive job的执行的话，可以采用并行的方式执行</span></span></span><br><span class="line">hive.exec.parallel=true (default false)</span><br><span class="line">hive.exec.parallel.thread.number=8 (default 8)</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#本地执行模式</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#在做测试的时候，如果任务量较小的时候可以使用本地执行模式，以上三个条件任意不满足的话就会提交的远程去执行</span></span></span><br><span class="line">hive.exec.mode.local.auto=true</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#Let Hive determine whether to run in local mode automatically</span></span></span><br><span class="line">hive.exec.mode.local.auto.inputbytes.max (default 128MB)</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#When hive.exec.mode.local.auto is true, input bytes should less than this for local mode</span></span></span><br><span class="line">hive.exec.mode.local.auto.input.files.max(default 4)</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#When hive.exec.mode.local.auto is true, the number of tasks should less than this for local mode</span></span></span><br></pre></td></tr></table></figure>

<p>3、择合适的引擎</p>
<figure class="highlight ebnf"><table><tr><td class="code"><pre><span class="line"><span class="attribute">mr</span></span><br><span class="line"><span class="attribute">tez</span></span><br><span class="line"><span class="attribute">spark</span></span><br></pre></td></tr></table></figure>

<p>4、map阶段优化</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!--列裁剪--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.optimize.cp<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether to enable column pruner.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--map端聚合--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.map.aggr<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether to use map-side aggregation in Hive Group By queries<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--Map端谓语下推--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.optimize.ppd<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether to enable predicate pushdown<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>5、reduce阶段优化</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!--reduce数量--&gt;</span>  </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.exec.reducers.max<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1009<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">      max number of reducers will be used. If the one specified in the configuration parameter mapred.reduce.tasks is</span><br><span class="line">      negative, Hive will use this one as the max number of reducers when automatically determine number of reducers.</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--reduce大小--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.exec.reducers.bytes.per.reducer<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1,000,000,000 <span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">    Size per reducer. The default in Hive 0.14.0 and earlier is 1 GB, that is, if the input size is 10 GB then 10 reducers will be used. In Hive 0.14.0 and later the default is 256 MB, that is, if the input size is 1 GB then 4 reducers will be used.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>6、shuffle阶段优化</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#压缩中间数据，从而减少磁盘操作以及减少网络传输数据量</span></span></span><br><span class="line">mapreduce.map.output.compress=true</span><br><span class="line">mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec</span><br></pre></td></tr></table></figure>

<p>7、join优化</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">##mapjoin</span><br><span class="line">##一个表的数据量很大而另一个表的数据量不大，这个时候可以把小表的数据拷贝到各个mapTask中，然后在map的内存中加载，这样再和大表的数据分片做<span class="keyword">join</span>，这样就可以避免将大表的数据在网络中进行shuffle，减少网络的开销，提升整个执行速度</span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join<span class="operator">=</span><span class="literal">true</span> (<span class="keyword">default</span> <span class="literal">false</span>)</span><br><span class="line"><span class="keyword">set</span> hive.mapjoin.smalltable.filesize<span class="operator">=</span> (<span class="keyword">default</span> <span class="number">25</span>M <span class="number">25000000</span>)</span><br><span class="line">##combine more map<span class="operator">-</span>side joins <span class="keyword">into</span> a single map<span class="operator">-</span>side <span class="keyword">join</span> if the size <span class="keyword">of</span> the n<span class="number">-1</span> <span class="keyword">table</span> <span class="keyword">is</span> less than <span class="number">10</span> MB <span class="keyword">using</span> hive.auto.convert.join.noconditionaltask</span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join.noconditionaltask <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join.noconditionaltask.size <span class="operator">=</span> <span class="number">20971520</span>(默认<span class="number">10000000</span>);</span><br><span class="line">##强制指定对a表做 map <span class="keyword">join</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">/</span><span class="operator">+</span>MAP <span class="keyword">JOIN</span>(a)<span class="operator">+</span><span class="operator">/</span>…a <span class="keyword">join</span> b</span><br><span class="line">##limitations</span><br><span class="line">##we can never <span class="keyword">convert</span> <span class="keyword">Full</span> <span class="keyword">outer</span> joins <span class="keyword">to</span> map<span class="operator">-</span>side joins</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">##Bucket Map <span class="keyword">Join</span></span><br><span class="line">##都是大表</span><br><span class="line">##map <span class="keyword">join</span>一起工作</span><br><span class="line">##所有要<span class="keyword">join</span>的表必须分桶，大表的桶的个数是小表的整数倍</span><br><span class="line">##做了bucket的列必须等于<span class="keyword">join</span>的列</span><br><span class="line">##所有表不排序</span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin<span class="operator">=</span><span class="literal">true</span></span><br><span class="line"><span class="keyword">set</span> hive.enforce.bucketing<span class="operator">=</span><span class="literal">true</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> b1(</span><br><span class="line">   col0 string</span><br><span class="line">  ,col1 string</span><br><span class="line">  ,col2 string</span><br><span class="line">  ,col3 string</span><br><span class="line">  ,col4 string</span><br><span class="line">  ,col5 string</span><br><span class="line">  ,col6 string</span><br><span class="line">)clustered <span class="keyword">by</span> (col0) <span class="keyword">into</span> <span class="number">32</span> buckets;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> b2(</span><br><span class="line">   col0 string</span><br><span class="line">  ,col1 string</span><br><span class="line">  ,col2 string</span><br><span class="line">  ,col3 string</span><br><span class="line">  ,col4 string</span><br><span class="line">  ,col5 string</span><br><span class="line">  ,col6 string)</span><br><span class="line">clustered <span class="keyword">by</span> (col0) <span class="keyword">into</span> <span class="number">8</span> buckets;</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> hive.enforce.bucketing <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">From</span> passwords <span class="keyword">insert</span> OVERWRITE  <span class="keyword">table</span> b1 <span class="keyword">select</span> <span class="operator">*</span> limit <span class="number">10000</span>;</span><br><span class="line"><span class="keyword">From</span> passwords <span class="keyword">insert</span> OVERWRITE  <span class="keyword">table</span> b2 <span class="keyword">select</span> <span class="operator">*</span> limit <span class="number">10000</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="keyword">select</span> <span class="comment">/*+ MAPJOIN(b2) */</span> b1.<span class="operator">*</span> <span class="keyword">from</span> b1,b2 <span class="keyword">where</span> b1.col0<span class="operator">=</span>b2.col0;</span><br><span class="line"></span><br><span class="line">##Skew <span class="keyword">join</span></span><br><span class="line">##数据倾斜</span><br><span class="line">##其中一个表中数据量某一类值特别多,分配到该值的reducer,耗时较长</span><br><span class="line">##功能：</span><br><span class="line">##<span class="number">1</span>、对于skewjoin.key，在执行job时，将它们存入临时的HDFS目录。其它数据正常执行</span><br><span class="line">##<span class="number">2</span>、对倾斜数据开启map <span class="keyword">join</span>操作，对非倾斜值采取普通<span class="keyword">join</span>操作</span><br><span class="line">##<span class="number">3</span>、将倾斜数据集和非倾斜数据集进行合并操作</span><br><span class="line"><span class="keyword">set</span> hive.optimize.skewjoin<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line">##表示当记录条数超过<span class="number">100000</span>时采用skewjoin操作</span><br><span class="line"><span class="keyword">set</span> hive.skewjoin.key<span class="operator">=</span><span class="number">100000</span>;</span><br><span class="line"></span><br><span class="line">##SMB <span class="keyword">Join</span>(Sort <span class="keyword">Merge</span> Bucket <span class="keyword">Join</span>)</span><br><span class="line">##都是大表</span><br><span class="line">##表已分桶，<span class="keyword">join</span>字段是分桶字段，<span class="keyword">join</span>字段已经排序</span><br><span class="line">##大表的桶的个数是小表的整数倍</span><br></pre></td></tr></table></figure>

<p>8、group by优化</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">##如果在不确定倾斜值的情况下，可以设置hive.groupby.skewindata参数</span><br><span class="line">##功能：是先对key值进行均匀分布，然后开启新一轮reducer求值</span><br><span class="line"><span class="keyword">set</span> hive.groupby.skewindata<span class="operator">=</span><span class="literal">true</span>;</span><br></pre></td></tr></table></figure>







<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">#设置任务名</span><br><span class="line"><span class="keyword">set</span> mapreduce.job.name<span class="operator">=</span>xxxx($&#123;statis_date&#125;)  # 方便定位具体任务</span><br><span class="line"></span><br><span class="line">#输入合并参数设置</span><br><span class="line">#文件切分splitSize <span class="operator">=</span>  Math.<span class="built_in">max</span>(minSize, Math.<span class="built_in">min</span>(maxSize, blockSize));</span><br><span class="line">#maxSize，默认<span class="number">0</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.input.fileinputformat.split.maxsize<span class="operator">=</span><span class="number">2048000000</span>;</span><br><span class="line">#minSize，默认<span class="number">256</span>M</span><br><span class="line"><span class="keyword">set</span> mapreduce.input.fileinputformat.split.minsize<span class="operator">=</span><span class="number">1024000000</span>;</span><br><span class="line">#同一节点的数据块形成切片时，切片大小的最小值</span><br><span class="line"><span class="keyword">set</span> mapreduce.input.fileinputformat.split.minsize.per.node<span class="operator">=</span><span class="number">100000000</span>;</span><br><span class="line">#同一机架的数据块形成切片时，切片大小的最小值</span><br><span class="line"><span class="keyword">set</span> mapreduce.input.fileinputformat.split.minsize.per.rack<span class="operator">=</span><span class="number">100000000</span>;</span><br><span class="line">#小文件有可能是直接来自于数据源的小文件，也可能是Reduce产生的小文件</span><br><span class="line">#执行Map前进行小文件合并</span><br><span class="line"><span class="keyword">set</span> hive.input.format<span class="operator">=</span>org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</span><br><span class="line"><span class="keyword">set</span> mapred.max.split.size <span class="operator">=</span> <span class="number">256000000</span> #每一个mapper最大的输入大小</span><br><span class="line">#低于<span class="number">128</span>M就算小文件，数据在一个节点会合并，在多个不同的节点会把数据抓过来进行合并。</span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.node <span class="operator">=</span> <span class="number">128000000</span></span><br><span class="line">#一个机架下split的至少的大小(这个值决定了该机架下的文件是否需要合并)</span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.rack <span class="operator">=</span> <span class="number">100000000</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> hive.merge.mapFiles<span class="operator">=</span><span class="literal">true</span>;     #Map端小文件聚合，默认<span class="literal">true</span></span><br><span class="line"><span class="keyword">set</span> hive.merge.mapredFiles<span class="operator">=</span><span class="literal">true</span>;  #Reduce端小文件聚合，默认<span class="literal">false</span></span><br><span class="line"><span class="keyword">set</span> hive.merge.size.per.task<span class="operator">=</span><span class="number">256000000</span>; #合并文件的大小，默认<span class="number">256000000</span></span><br><span class="line">#当输出文件的平均大小小于该值时，启动一个独立的map<span class="operator">-</span>reduce任务进行文件<span class="keyword">merge</span></span><br><span class="line">hive.merge.smallfiles.avgsize<span class="operator">=</span><span class="number">16000000</span></span><br><span class="line">#不进行小文件合并</span><br><span class="line"><span class="keyword">set</span> hive.input.format<span class="operator">=</span>org.apache.hadoop.hive.ql.io.HiveInputFormat; </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.sortmerge.join<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin.sortedmerge <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.sortmerge.join.bigtable.selection.policy </span><br><span class="line">    <span class="operator">=</span> org.apache.hadoop.hive.ql.optimizer.TableSizeBasedBigTableSelectorForAutoSMJ;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>离线计算</category>
      </categories>
  </entry>
  <entry>
    <title>Kafka</title>
    <url>/Kafka/</url>
    <content><![CDATA[<p>Kafka是一个分布式的基于发布/订阅模式的消息队列，主要应用于大数据实时处理领域。</p>
<p>官网：<a href="https://kafka.apache.org/">https://kafka.apache.org</a></p>
<h2 id="Kafka基础架构"><a href="#Kafka基础架构" class="headerlink" title="Kafka基础架构"></a>Kafka基础架构</h2><p><img src="/Kafka/18.png" alt="18"></p>
<p>（1）Producer ：消息生产者，就是向kafka broker发消息的客户端；</p>
<p>（2）Consumer ：消息消费者，向kafka broker取消息的客户端；</p>
<p>（3）Consumer Group （CG）：消费者组，由多个consumer组成。<strong>消费者组内每个消费者负责消费不同分区的数据</strong>，一个分区只能由一个消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。</p>
<p>（4）Broker ：一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic。</p>
<p>（5）Topic ：可以理解为一个队列，生产者和消费者面向的都是一个topic；</p>
<p>（6）Partition：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列；</p>
<p>（7）Replica：副本，为保证集群中的某个节点发生故障时，该节点上的partition数据不丢失，且kafka仍然能够继续工作，kafka提供了副本机制，一个topic的每个分区都有若干个副本，一个leader和若干个follower。</p>
<p>（8）leader：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是leader。</p>
<p>（9）follower：每个分区多个副本中的“从”，实时从leader中同步数据，保持和leader数据的同步。leader发生故障时，某个follower会成为新的leader。</p>
<h2 id="Kafka架构深入"><a href="#Kafka架构深入" class="headerlink" title="Kafka架构深入"></a>Kafka架构深入</h2><h3 id="Kafka工作流程及文件存储机制"><a href="#Kafka工作流程及文件存储机制" class="headerlink" title="Kafka工作流程及文件存储机制"></a>Kafka工作流程及文件存储机制</h3><p><img src="/Kafka/19.png" alt="19"></p>
<p>Kafka中消息是以topic进行分类的，生产者生产消息，消费者消费消息，都是面向topic的。</p>
<p><strong>topic是逻辑上的概念，而partition是物理上的概念，每个partition对应于一个log文件，该log文件中存储的就是producer生产的数据</strong>。Producer生产的数据会被不断追加到该log文件末端，且每条数据都有自己的offset。消费者组中的每个消费者，都会实时记录自己消费到了哪个offset，以便出错恢复时，从上次的位置继续消费。</p>
<p><img src="/Kafka/20.png" alt="20"></p>
<p>由于生产者生产的消息会不断追加到log文件末尾，为防止log文件过大导致数据定位效率低下，Kafka采取了<strong>分片和索引</strong>机制，将<strong>每个partition分为多个segment</strong>。每个<strong>segment对应两个文件——.index文件和.log文件</strong>。这些文件位于一个文件夹下，该文件夹的命名规则为：<strong>topic名称+分区序号</strong>。例如，first这个topic有三个分区，则其对应的文件夹为first-0,first-1,first-2。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--first-0</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#第一个segment</span></span></span><br><span class="line">00000000000000000000.index</span><br><span class="line">00000000000000000000.log</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#第二个segment</span></span></span><br><span class="line">00000000000000170410.index</span><br><span class="line">00000000000000170410.log</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#第三个segment</span></span></span><br><span class="line">00000000000000239430.index</span><br><span class="line">00000000000000239430.log</span><br></pre></td></tr></table></figure>

<p><strong>index和log文件以当前segment的第一条消息的offset命名</strong>。下图为index文件和log文件的结构示意图。</p>
<p><img src="/Kafka/21.png" alt="21"></p>
<p>.index文件存储大量的索引信息，.log文件存储大量的数据，索引文件中的元数据指向对应数据文件中message的物理偏移地址。</p>
<h3 id="Kafka生产者"><a href="#Kafka生产者" class="headerlink" title="Kafka生产者"></a>Kafka生产者</h3><h4 id="分区策略"><a href="#分区策略" class="headerlink" title="分区策略"></a>分区策略</h4><p><strong>1）分区的原因</strong></p>
<p>（1）方便在集群中<strong>扩展</strong>，每个Partition可以通过调整以适应它所在的机器，而一个topic又可以有多个Partition组成，因此整个集群就可以适应任意大小的数据了；</p>
<p>（2）可以提高<strong>并发</strong>，因为可以<strong>以Partition为单位读写</strong>了。</p>
<p><strong>2）分区的原则</strong></p>
<p>我们需要将producer发送的数据封装成一个<strong>ProducerRecord</strong>对象。</p>
<p>（1）指明partition的情况下，直接将指明的值作为 partiton 值；</p>
<p>（2）没有指明partition值但有key的情况下，将key的hash值与topic的partition数进行取余得到partition值；</p>
<p>（3）既没有partition值又没有 key 的情况下，第一次调用时随机生成一个整数（后面每次调用在这个整数上自增），将这个值与topic可用的partition总数取余得到partition值，也就是常说的round-robin算法。</p>
<h4 id="数据可靠性保证"><a href="#数据可靠性保证" class="headerlink" title="数据可靠性保证"></a>数据可靠性保证</h4><p>为保证producer发送的数据，能可靠的发送到指定的topic，topic的每个partition收到producer发送的数据后，都需要向producer发送ack（acknowledgement确认收到），如果producer收到ack，就会进行下一轮的发送，否则重新发送数据。</p>
<h5 id="1）副本数据同步策略"><a href="#1）副本数据同步策略" class="headerlink" title="1）副本数据同步策略"></a>1）副本数据同步策略</h5><table>
<thead>
<tr>
<th>方案</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody><tr>
<td>半数以上完成同步，就发送ack</td>
<td>延迟低</td>
<td>选举新的leader时，容忍n台节点的故障，需要2n+1个副本</td>
</tr>
<tr>
<td>全部完成同步，才发送ack</td>
<td>选举新的leader时，容忍n台节点的故障，需要n+1个副本</td>
<td>延迟高</td>
</tr>
</tbody></table>
<p>Kafka选择了第二种方案，原因如下：</p>
<p>（1）同样为了容忍n台节点的故障，第一种方案需要2n+1个副本，而第二种方案只需要n+1个副本，而Kafka的每个分区都有大量的数据，第一种方案会造成大量数据的冗余。</p>
<p>（2）虽然第二种方案的网络延迟会比较高，但网络延迟对Kafka的影响较小。</p>
<h5 id="2）ISR"><a href="#2）ISR" class="headerlink" title="2）ISR"></a>2）ISR</h5><p>采用第二种方案之后，设想以下情景：leader收到数据，所有follower都开始同步数据，但有一个follower，因为某种故障，迟迟不能与leader进行同步，那leader就要一直等下去，直到它完成同步，才能发送ack。这个问题怎么解决呢？</p>
<p>Leader维护了一个动态的in-sync replica set (ISR)，意为和leader保持同步的follower集合。当ISR中的follower完成数据的同步之后，leader就会给producer发送ack。如果follower长时间未向leader同步数据，则该follower将被踢出ISR，该时间阈值由<strong>replica.lag.time.max.ms</strong>参数设定。Leader发生故障之后，就会从ISR中选举新的leader。</p>
<h5 id="3）ack应答机制"><a href="#3）ack应答机制" class="headerlink" title="3）ack应答机制"></a>3）ack应答机制</h5><p>对于某些不太重要的数据，对数据的可靠性要求不是很高，能够容忍数据的少量丢失，所以没必要等ISR中的follower全部接收成功。</p>
<p>所以Kafka为用户提供了三种可靠性级别，用户根据对可靠性和延迟的要求进行权衡，选择以下的配置。</p>
<p>acks参数配置：</p>
<p>acks：</p>
<p>0：producer不等待broker的ack，这一操作提供了一个最低的延迟，broker一接收到还没有写入磁盘就已经返回，当broker故障时有可能<strong>丢失数据</strong>；</p>
<p>1：producer等待broker的ack，partition的leader落盘成功后返回ack，如果在follower同步成功之前leader故障，那么将会<strong>丢失数据</strong>；</p>
<p><img src="/Kafka/159.png" alt="159"></p>
<p>-1（all）：producer等待broker的ack，partition的leader和follower全部落盘成功后才返回ack。但是如果在follower同步完成后，broker发送ack之前，leader发生故障，那么会造成<strong>数据重复</strong>。</p>
<p><img src="/Kafka/160.png" alt="160"></p>
<h5 id="4）故障处理细节"><a href="#4）故障处理细节" class="headerlink" title="4）故障处理细节"></a>4）故障处理细节</h5><p><img src="/Kafka/161.png" alt="161"></p>
<p>（1）follower故障</p>
<p>follower发生故障后会被临时踢出ISR，待该follower恢复后，follower会读取本地磁盘记录的上次的HW，并将log文件高于HW的部分截取掉，从HW开始向leader进行同步。等该follower的LEO大于等于该Partition的HW，即follower追上leader之后，就可以重新加入ISR了。</p>
<p>（2）leader故障</p>
<p>leader发生故障之后，会从ISR中选出一个新的leader，之后，为保证多个副本之间的数据一致性，其余的follower会先将各自的log文件高于HW的部分截掉，然后从新的leader同步数据。</p>
<p>注意：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。</p>
<h4 id="Exactly-Once语义"><a href="#Exactly-Once语义" class="headerlink" title="Exactly Once语义"></a>Exactly Once语义</h4><p><strong>将服务器的ACK级别设置为-1，可以保证Producer到Server之间不会丢失数据，即At Least Once语义。相对的，将服务器ACK级别设置为0，可以保证生产者每条消息只会被发送一次，即At Most Once语义。</strong></p>
<p>At Least Once可以保证数据不丢失，但是不能保证数据不重复；相对的，At Most Once可以保证数据不重复，但是不能保证数据不丢失。但是，对于一些非常重要的信息，比如说交易数据，下游数据消费者要求数据既不重复也不丢失，即Exactly Once语义。在0.11版本以前的Kafka，对此是无能为力的，只能保证数据不丢失，再在下游消费者对数据做全局去重。对于多个下游应用的情况，每个都需要单独做全局去重，这就对性能造成了很大影响。</p>
<p>0.11版本的Kafka，引入了一项重大特性：幂等性。<strong>所谓的幂等性就是指Producer不论向Server发送多少次重复数据，Server端都只会持久化一条</strong>。幂等性结合At Least Once语义，就构成了Kafka的Exactly Once语义。即：</p>
<p>At Least Once + 幂等性 = Exactly Once</p>
<p><strong>要启用幂等性，只需要将Producer的参数中enable.idompotence设置为true即可</strong>。Kafka的幂等性实现其实就是将原来下游需要做的去重放在了数据上游。开启幂等性的Producer在初始化的时候会被分配一个PID，发往同一Partition的消息会附带Sequence Number。而Broker端会对&lt;PID, Partition, SeqNumber&gt;做缓存，当具有相同主键的消息提交时，Broker只会持久化一条。</p>
<p>但是PID重启就会变化，同时不同的Partition也具有不同主键，所以幂等性无法保证跨分区跨会话的Exactly Once。</p>
<h3 id="Kafka消费者"><a href="#Kafka消费者" class="headerlink" title="Kafka消费者"></a>Kafka消费者</h3><h4 id="消费方式"><a href="#消费方式" class="headerlink" title="消费方式"></a>消费方式</h4><p>consumer采用pull（拉）模式从broker中读取数据。</p>
<p>push（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而pull模式则可以根据consumer的消费能力以适当的速率消费消息。</p>
<p>pull模式不足之处是，如果kafka没有数据，消费者可能会陷入循环中，一直返回空数据。针对这一点，Kafka的消费者在消费数据时会传入一个时长参数timeout，<strong>如果当前没有数据可供消费，consumer会等待一段时间之后再返回，这段时长即为timeout</strong>。</p>
<h4 id="分区分配策略"><a href="#分区分配策略" class="headerlink" title="分区分配策略"></a>分区分配策略</h4><p>一个consumer group中有多个consumer，一个 topic有多个partition，所以必然会涉及到partition的分配问题，即确定那个partition由哪个consumer来消费。</p>
<p><strong>Kafka有3种分配策略，RangeAssignor，RoundRobinAssignor，StickyAssignor。</strong></p>
<h5 id="RangeAssignor"><a href="#RangeAssignor" class="headerlink" title="RangeAssignor"></a><strong>RangeAssignor</strong></h5><p>RangeAssignor对每个Topic进行独立的分区分配。对于每一个Topic，首先对分区按照分区ID进行排序，然后订阅这个Topic的消费组的消费者再进行排序，之后尽量均衡的将分区分配给消费者。这里只能是尽量均衡，因为分区数可能无法被消费者数量整除，那么有一些消费者就会多分配到一些分区。</p>
<p><img src="/Kafka/%E6%88%AA%E5%B1%8F2021-12-28%20%E4%B8%8B%E5%8D%882.34.50.png" alt="截屏2021-12-28 下午2.34.50"></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">assign(topic, consumers) &#123;</span><br><span class="line">  <span class="comment">// 对分区和Consumer进行排序</span></span><br><span class="line">  List&lt;Partition&gt; partitions = topic.getPartitions();</span><br><span class="line">  sort(partitions);</span><br><span class="line">  sort(consumers);</span><br><span class="line">  <span class="comment">// 计算每个Consumer分配的分区数</span></span><br><span class="line">  <span class="keyword">int</span> numPartitionsPerConsumer = partition.size() / consumers.size();</span><br><span class="line">  <span class="comment">// 额外有一些Consumer会多分配到分区</span></span><br><span class="line">  <span class="keyword">int</span> consumersWithExtraPartition = partition.size() % consumers.size();</span><br><span class="line">  <span class="comment">// 计算分配结果</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>, n = consumers.size(); i &lt; n; i++) &#123;</span><br><span class="line">    <span class="comment">// 第i个Consumer分配到的分区的index</span></span><br><span class="line">        <span class="keyword">int</span> start = numPartitionsPerConsumer * i + Math.min(i, consumersWithExtraPartition);</span><br><span class="line">        <span class="comment">// 第i个Consumer分配到的分区数</span></span><br><span class="line">        <span class="keyword">int</span> length = numPartitionsPerConsumer + (i + <span class="number">1</span> &gt; consumersWithExtraPartition ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">        <span class="comment">// 分装分配结果</span></span><br><span class="line">        assignment.get(consumersForTopic.get(i)).addAll(partitions.subList(start, start + length));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>RangeAssignor策略的原理是按照消费者总数和分区总数进行整除运算来获得一个跨度，然后将分区按照跨度进行平均分配，以保证分区尽可能均匀地分配给所有的消费者。对于每一个Topic，RangeAssignor策略会将消费组内所有订阅这个Topic的消费者按照名称的字典序排序，然后为每个消费者划分固定的分区范围，如果<strong>不够平均分配，那么字典序靠前的消费者会被多分配一个分区</strong>。</p>
<p>这种分配方式明显的一个问题是随着消费者订阅的Topic的数量的增加，不均衡的问题会越来越严重，比如上图中4个分区3个消费者的场景，C0会多分配一个分区。如果此时再订阅一个分区数为4的Topic，那么C0又会比C1、C2多分配一个分区，这样C0总共就比C1、C2多分配两个分区了，而且随着Topic的增加，这个情况会越来越严重。</p>
<p>分配结果：</p>
<p>订阅2个Topic，每个Topic4个分区，共3个Consumer</p>
<p>C0：[T0P0，T0P1，T1P0，T1P1]</p>
<p>C1：[T0P2，T1P2]</p>
<p>C2：[T0P3，T1P3]</p>
<h5 id="RoundRobinAssignor"><a href="#RoundRobinAssignor" class="headerlink" title="RoundRobinAssignor"></a>RoundRobinAssignor</h5><p>RoundRobinAssignor的分配策略是将消费组内订阅的所有Topic的分区及所有消费者进行排序后尽量均衡的分配（RangeAssignor是针对单个Topic的分区进行排序分配的）。如果消费组内，消费者订阅的Topic列表是相同的（每个消费者都订阅了相同的Topic），那么分配结果是尽量均衡的（消费者之间分配到的分区数的差值不会超过1）。如果订阅的Topic列表是不同的，那么分配结果是不保证“尽量均衡”的，因为某些消费者不参与一些Topic的分配。</p>
<p>RangeAssignor和RoundRobinAssignor(消费相同的topic)对比：</p>
<p><img src="/Kafka/%E6%88%AA%E5%B1%8F2021-12-28%20%E4%B8%8B%E5%8D%882.39.32.png" alt="截屏2021-12-28 下午2.39.32"></p>
<p>相对于RangeAssignor，在订阅多个Topic的情况下，RoundRobinAssignor的方式能使消费者之间尽量均衡的分配到分区（分配到的分区数的差值不会超过1——RangeAssignor的分配策略可能随着订阅的Topic越来越多，差值越来越大）。</p>
<p>对于组内消费者订阅Topic不一致的情况：假设有三个消费者分别为C0、C1、C2，有3个Topic T0、T1、T2，分别拥有1、2、3个分区，并且C0订阅T0，C1订阅T0和T1，C2订阅T0、T1、T0，那么RoundRobinAssignor的分配结果如下：</p>
<p><img src="/Kafka/%E6%88%AA%E5%B1%8F2021-12-28%20%E4%B8%8B%E5%8D%882.53.13.png" alt="截屏2021-12-28 下午2.53.13"></p>
<h5 id="StickyAssignor"><a href="#StickyAssignor" class="headerlink" title="StickyAssignor"></a>StickyAssignor</h5><p>尽管RoundRobinAssignor已经在RangeAssignor上做了一些优化来更均衡的分配分区，但是在一些情况下依旧会产生严重的分配偏差，<strong>比如消费组中订阅的Topic列表不相同的情况下</strong>（这个情况可能更多的发生在发布阶段，但是这真的是一个问题吗？——可以参照Kafka官方的说明：KIP-49 Fair Partition Assignment Strategy）。更核心的问题是<strong>无论是RangeAssignor，还是RoundRobinAssignor，当前的分区分配算法都没有考虑上一次的分配结果</strong>。显然，在执行一次新的分配之前，如果能考虑到上一次分配的结果，尽量少的调整分区分配的变动，显然是能节省很多开销的。</p>
<p>从字面意义上看，Sticky是“粘性的”，可以理解为分配结果是带“粘性的”——每一次分配变更相对上一次分配做最少的变动（上一次的结果是有粘性的），其目标有两点：</p>
<ol>
<li><p><strong>分区的分配尽量的均衡</strong></p>
</li>
<li><p><strong>每一次重分配的结果尽量与上一次分配结果保持一致</strong></p>
</li>
</ol>
<p>当这两个目标发生冲突时，优先保证第一个目标。第一个目标是每个分配算法都尽量尝试去完成的，而第二个目标才真正体现出StickyAssignor特性的。 </p>
<p>我们先来看预期分配的结构，后续再具体分析StickyAssignor的算法实现。</p>
<p>例如：</p>
<ul>
<li>有3个Consumer：C0、C1、C2</li>
<li>有4个Topic：T0、T1、T2、T3，每个Topic有2个分区</li>
<li>所有Consumer都订阅了这4个分区</li>
</ul>
<p>StickyAssignor的分配结果如下图所示（增加RoundRobinAssignor分配作为对比）：</p>
<p><img src="/Kafka/%E6%88%AA%E5%B1%8F2021-12-28%20%E4%B8%8B%E5%8D%882.58.53.png" alt="截屏2021-12-28 下午2.58.53"></p>
<p>上面的例子中，Sticky模式原来分配给C0、C2的分区都没有发生变动，且最终C0、C1达到的均衡的目的。</p>
<p>再举一个例子：</p>
<ul>
<li>有3个Consumer：C0、C1、C2</li>
<li>3个Topic：T0、T1、T2，它们分别有1、2、3个分区</li>
<li>C0订阅T0；C1订阅T0、T1；C2订阅T0、T1、T2</li>
</ul>
<p>分配结果如下图所示：</p>
<p><img src="/Kafka/%E6%88%AA%E5%B1%8F2021-12-28%20%E4%B8%8B%E5%8D%883.01.35.png" alt="截屏2021-12-28 下午3.01.35"></p>
<h4 id="offset的维护"><a href="#offset的维护" class="headerlink" title="offset的维护"></a>offset的维护</h4><p>由于consumer在消费过程中可能会出现断电宕机等故障，consumer恢复后，需要从故障前的位置的继续消费，所以<strong>consumer需要实时记录自己消费到了哪个offset，以便故障恢复后继续消费</strong>。</p>
<p>Kafka 0.9版本之前，consumer默认将offset保存在Zookeeper中，从0.9版本开始，consumer默认将offset保存在Kafka一个内置的topic中，该topic为<strong>consumer_offsets</strong>。</p>
<h3 id="Kafka高效读写数据"><a href="#Kafka高效读写数据" class="headerlink" title="Kafka高效读写数据"></a>Kafka高效读写数据</h3><p><strong>1）顺序写磁盘</strong></p>
<p>Kafka的producer生产数据，要写入到log文件中，写的过程是一直追加到文件末端，为顺序写。官网有数据表明，同样的磁盘，顺序写能到到600M/s，而随机写只有100k/s。这与磁盘的机械机构有关，顺序写之所以快，是因为其省去了大量磁头寻址的时间。</p>
<p><strong>2）应用Pagecache</strong></p>
<p>Kafka数据持久化是直接持久化到Pagecache中，这样会产生以下几个好处： </p>
<p>I/O Scheduler 会将连续的小块写组装成大块的物理写从而提高性能</p>
<p>I/O Scheduler 会尝试将一些写操作重新按顺序排好，从而减少磁盘头的移动时间</p>
<p>充分利用所有空闲内存（非 JVM 内存）。如果使用应用层 Cache（即 JVM 堆内存），会增加 GC 负担</p>
<p>读操作可直接在 Page Cache 内进行。如果消费和生产速度相当，甚至不需要通过物理磁盘（直接通过 Page Cache）交换数据</p>
<p>如果进程重启，JVM 内的 Cache 会失效，但 Page Cache 仍然可用</p>
<p>尽管持久化到Pagecache上可能会造成宕机丢失数据的情况，但这可以被Kafka的Replication机制解决。如果为了保证这种情况下数据不丢失而强制将 Page Cache 中的数据 Flush 到磁盘，反而会降低性能。</p>
<p><strong>3）零复制技术</strong></p>
<p><img src="/Kafka/162.png" alt="162"></p>
<h3 id="Zookeeper在Kafka中的作用"><a href="#Zookeeper在Kafka中的作用" class="headerlink" title="Zookeeper在Kafka中的作用"></a>Zookeeper在Kafka中的作用</h3><p>Kafka集群中有一个broker会被选举为Controller，负责管理集群broker的上下线，所有topic的分区副本分配和leader选举等工作。</p>
<p>Controller的管理工作都是依赖于Zookeeper的。</p>
<p>以下为partition的leader选举过程：</p>
<p><img src="/Kafka/163.png" alt="163"></p>
<h3 id="Kafka事务"><a href="#Kafka事务" class="headerlink" title="Kafka事务"></a>Kafka事务</h3><p>Kafka从0.11版本开始引入了事务支持。事务可以保证Kafka在Exactly Once语义的基础上，生产和消费可以跨分区和会话，要么全部成功，要么全部失败。</p>
<h4 id="Producer事务"><a href="#Producer事务" class="headerlink" title="Producer事务"></a>Producer事务</h4><p>为了实现跨分区跨会话的事务，需要引入一个<strong>全局唯一的Transaction ID</strong>，并ack。这样当Producer重启后就可以通过正在进行的Transaction ID获得原来的PID。</p>
<p>为了管理Transaction，Kafka引入了一个新的组件<strong>Transaction Coordinator</strong>。Producer就是通过和Transaction Coordinator交互获得Transaction ID对应的任务状态。Transaction Coordinator还负责将所有事务写入Kafka的一个内部Topic，这样即使整个服务重启，由于事务状态得到保存，进行中的事务状态可以得到恢复，从而继续进行。</p>
<h4 id="Consumer事务（精准一次性消费）"><a href="#Consumer事务（精准一次性消费）" class="headerlink" title="Consumer事务（精准一次性消费）"></a>Consumer事务（精准一次性消费）</h4><p>上述事务机制主要是从Producer方面考虑，对于Consumer而言，事务的保证就会相对较弱，尤其是无法保证Commit的信息被精确消费。这是由于Consumer可以通过offset访问任意信息，而且不同的Segment File生命周期不同，同一事务的消息可能会出现重启后被删除的情况。</p>
<p>如果想完成Consumer端的精准一次性消费，那么需要<strong>kafka消费端将消费过程和提交offset过程做原子绑定</strong>。此时我们需<strong>要将kafka的offset保存到支持事务的自定义介质中（比如mysql）</strong>。这部分知识会在后续项目部分涉及。</p>
<h2 id="Kafka-API"><a href="#Kafka-API" class="headerlink" title="Kafka API"></a>Kafka API</h2><h3 id="Producer-API"><a href="#Producer-API" class="headerlink" title="Producer API"></a>Producer API</h3><h4 id="消息发送流程"><a href="#消息发送流程" class="headerlink" title="消息发送流程"></a>消息发送流程</h4><p>Kafka的Producer发送消息采用的是<strong>异步发送</strong>的方式。在消息发送的过程中，涉及到了两个线程——main线程和Sender线程，以及一个线程共享变量——RecordAccumulator。main线程将消息发送给RecordAccumulator，Sender线程不断从RecordAccumulator中拉取消息发送到Kafka broker。</p>
<p><img src="/Kafka/164.png" alt="164"></p>
<p>相关参数：</p>
<p><strong>batch.size</strong>: 只有数据积累到batch.size之后，sender才会发送数据。</p>
<p><strong>linger.ms</strong>: 如果数据迟迟未达到batch.size，sender等待linger.time之后就会发送数据。</p>
<h4 id="异步发送API"><a href="#异步发送API" class="headerlink" title="异步发送API"></a>异步发送API</h4><p>1）导入依赖</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>2）编写代码</p>
<p>需要用到的类：</p>
<p><strong>KafkaProducer</strong>：需要创建一个生产者对象，用来发送数据</p>
<p><strong>ProducerConfig</strong>：获取所需的一系列配置参数</p>
<p><strong>ProducerRecord</strong>：每条数据都要封装成一个ProducerRecord对象</p>
<p>（1）不带回调函数的API</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.*;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ExecutionException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducer</span> </span>&#123;    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ExecutionException, InterruptedException </span>&#123;  </span><br><span class="line"></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();     </span><br><span class="line"></span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;linux1:9092&quot;</span>);    <span class="comment">//kafka集群，broker-list        </span></span><br><span class="line">        props.put(<span class="string">&quot;acks&quot;</span>, <span class="string">&quot;all&quot;</span>);                         <span class="comment">//-1        </span></span><br><span class="line">        props.put(<span class="string">&quot;retries&quot;</span>, <span class="number">1</span>);                          <span class="comment">//重试次数        </span></span><br><span class="line">        props.put(<span class="string">&quot;batch.size&quot;</span>, <span class="number">16384</span>);                   <span class="comment">//批次大小 16Kb        </span></span><br><span class="line">        props.put(<span class="string">&quot;linger.ms&quot;</span>, <span class="number">1</span>);                        <span class="comment">//等待时间        </span></span><br><span class="line">        props.put(<span class="string">&quot;buffer.memory&quot;</span>, <span class="number">33554432</span>);             <span class="comment">//RecordAccumulator缓冲区大小,缓冲区存放RecordBatch    </span></span><br><span class="line">        props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        Producer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;            </span><br><span class="line">            producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(<span class="string">&quot;first&quot;</span>, Integer.toString(i), Integer.toString(i)));            </span><br><span class="line">            <span class="comment">// topic, Partition, k, v                </span></span><br><span class="line">            <span class="comment">// Partition 和 k 可以省略        </span></span><br><span class="line">        &#125;      </span><br><span class="line"></span><br><span class="line">        producer.close();    </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（2）带回调函数的API</p>
<p>回调函数会在producer收到ack时调用，为异步调用，该方法有两个参数，分别是RecordMetadata和Exception，如果Exception为null，说明消息发送成功，如果Exception不为null，说明消息发送失败。</p>
<p>注意：消息发送失败会自动重试，不需要我们在回调函数中手动重试。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.*;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ExecutionException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducer</span> </span>&#123;    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ExecutionException, InterruptedException </span>&#123;        </span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();        </span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;linux1:9092&quot;</span>);<span class="comment">//kafka集群，broker-list        </span></span><br><span class="line">        props.put(<span class="string">&quot;acks&quot;</span>, <span class="string">&quot;all&quot;</span>);        </span><br><span class="line">        props.put(<span class="string">&quot;retries&quot;</span>, <span class="number">1</span>);<span class="comment">//重试次数        </span></span><br><span class="line">        props.put(<span class="string">&quot;batch.size&quot;</span>, <span class="number">16384</span>);<span class="comment">//批次大小        </span></span><br><span class="line">        props.put(<span class="string">&quot;linger.ms&quot;</span>, <span class="number">1</span>);<span class="comment">//等待时间        </span></span><br><span class="line">        props.put(<span class="string">&quot;buffer.memory&quot;</span>, <span class="number">33554432</span>);<span class="comment">//RecordAccumulator缓冲区大小        </span></span><br><span class="line">        props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);        </span><br><span class="line">        props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);        </span><br><span class="line"></span><br><span class="line">        Producer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);        </span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;            </span><br><span class="line">            producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(<span class="string">&quot;first&quot;</span>, Integer.toString(i), Integer.toString(i)), <span class="keyword">new</span> Callback() &#123;                </span><br><span class="line">                <span class="comment">//回调函数，该方法会在Producer收到ack时调用，为异步调用                </span></span><br><span class="line">                <span class="meta">@Override</span>                </span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCompletion</span><span class="params">(RecordMetadata metadata, Exception exception)</span> </span>&#123;                    </span><br><span class="line">                    <span class="keyword">if</span> (exception == <span class="keyword">null</span>) &#123;                        </span><br><span class="line">                        System.out.println(<span class="string">&quot;success-&gt;&quot;</span> + metadata.offset());                    </span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;                        </span><br><span class="line">                        exception.printStackTrace();                    </span><br><span class="line">                    &#125;                </span><br><span class="line">                &#125;            </span><br><span class="line">            &#125;);        </span><br><span class="line">        &#125;        </span><br><span class="line"></span><br><span class="line">        producer.close();    </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="同步发送API"><a href="#同步发送API" class="headerlink" title="同步发送API"></a>同步发送API</h4><p><strong>同步发送的意思就是，一条消息发送之后，会阻塞当前线程，直至返回ack。</strong></p>
<p>由于send方法返回的是一个Future对象，根据Futrue对象的特点，我们也可以实现同步发送的效果，<strong>只需调用Future对象的get方法即可</strong>。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.Producer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties; </span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ExecutionException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducer</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ExecutionException, InterruptedException </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();        </span><br><span class="line"></span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;linux1:9092&quot;</span>);                  <span class="comment">//kafka集群，broker-list        </span></span><br><span class="line">        props.put(<span class="string">&quot;acks&quot;</span>, <span class="string">&quot;all&quot;</span>);        props.put(<span class="string">&quot;retries&quot;</span>, <span class="number">1</span>);       <span class="comment">//重试次数        </span></span><br><span class="line">        props.put(<span class="string">&quot;batch.size&quot;</span>, <span class="number">16384</span>);                                 <span class="comment">//批次大小        </span></span><br><span class="line">        props.put(<span class="string">&quot;linger.ms&quot;</span>, <span class="number">1</span>);                                      <span class="comment">//等待时间        </span></span><br><span class="line">        props.put(<span class="string">&quot;buffer.memory&quot;</span>, <span class="number">33554432</span>);                           <span class="comment">//RecordAccumulator缓冲区大小        </span></span><br><span class="line">        props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);        </span><br><span class="line">        props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);        </span><br><span class="line">        Producer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);        </span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;            </span><br><span class="line">            producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(<span class="string">&quot;first&quot;</span>, Integer.toString(i), Integer.toString(i))).get();        </span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        producer.close();    </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Consumer-API"><a href="#Consumer-API" class="headerlink" title="Consumer API"></a>Consumer API</h3><p>Consumer消费数据时的可靠性是很容易保证的，因为数据在Kafka中是持久化的，故不用担心数据丢失问题。</p>
<p>由于consumer在消费过程中可能会出现断电宕机等故障，consumer恢复后，需要从故障前的位置继续消费，<strong>所以consumer需要实时记录自己消费到了哪个offset</strong>，以便故障恢复后继续消费。</p>
<p>所以offset的维护是Consumer消费数据是必须考虑的问题。</p>
<h4 id="自动提交offset"><a href="#自动提交offset" class="headerlink" title="自动提交offset"></a>自动提交offset</h4><p><strong>1）导入依赖</strong></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka&lt;/groupId &gt;</span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><strong>2）编写代码</strong></p>
<p>需要用到的类：</p>
<p><strong>KafkaConsumer</strong>：需要创建一个消费者对象，用来消费数据</p>
<p><strong>ConsumerConfig</strong>：获取所需的一系列配置参数</p>
<p><strong>ConsuemrRecord</strong>：每条数据都要封装成一个ConsumerRecord对象</p>
<p>为了使我们能够专注于自己的业务逻辑，Kafka提供了自动提交offset的功能。 </p>
<p><strong>自动提交offset的相关参数</strong>：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">enable.auto.commit：是否开启自动提交offset功能</span><br><span class="line">auto.commit.interval.ms：自动提交offset的时间间隔</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;<span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomConsumer</span> </span>&#123;    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;        </span><br><span class="line"></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();        </span><br><span class="line"></span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;linux1:9092&quot;</span>);        </span><br><span class="line">        props.put(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;test&quot;</span>);        </span><br><span class="line">        props.put(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;true&quot;</span>);        </span><br><span class="line">        props.put(<span class="string">&quot;auto.commit.interval.ms&quot;</span>, <span class="string">&quot;1000&quot;</span>);        </span><br><span class="line">        props.put(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);        </span><br><span class="line">        props.put(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);              </span><br><span class="line"></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);        </span><br><span class="line"></span><br><span class="line">        consumer.subscribe(Arrays.asList(<span class="string">&quot;first&quot;</span>));        </span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;            </span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);            </span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records)                </span><br><span class="line">                System.out.printf(<span class="string">&quot;offset = %d, key = %s, value = %s%n&quot;</span>, record.offset(), record.key(), record.value());        </span><br><span class="line">        &#125;    </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="手动提交offset"><a href="#手动提交offset" class="headerlink" title="手动提交offset"></a>手动提交offset</h4><p>虽然自动提交offset十分简介便利，但由于其是基于时间提交的，开发人员难以把握offset提交的时机。因此Kafka还提供了手动提交offset的API。</p>
<p>手动提交offset的方法有两种：分别是<strong>commitSync（同步提交）</strong>和<strong>commitAsync（异步提交）</strong>。两者的相同点是，都会将本次poll的一批数据最高的偏移量提交；不同点是，commitSync阻塞当前线程，一直到提交成功，并且会自动失败重试（由不可控因素导致，也会出现提交失败）；而commitAsync则没有失败重试机制，故有可能提交失败。</p>
<h5 id="1）同步提交offset"><a href="#1）同步提交offset" class="headerlink" title="1）同步提交offset"></a>1）同步提交offset</h5><p>由于同步提交offset有<strong>失败重试机制</strong>，故更加可靠，以下为同步提交offset的示例。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomComsumer</span> </span>&#123;    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;        </span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();        </span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;linux1:9092&quot;</span>);<span class="comment">//Kafka集群        </span></span><br><span class="line">        props.put(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;test&quot;</span>);<span class="comment">//消费者组，只要group.id相同，就属于同一个消费者组        </span></span><br><span class="line">        props.put(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;false&quot;</span>);<span class="comment">//关闭自动提交offset        </span></span><br><span class="line">        props.put(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);        </span><br><span class="line">        props.put(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);        </span><br><span class="line">        </span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);  </span><br><span class="line"></span><br><span class="line">        consumer.subscribe(Arrays.asList(<span class="string">&quot;first&quot;</span>));<span class="comment">//消费者订阅主题        </span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;      </span><br><span class="line"></span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);<span class="comment">//消费者拉取数据            </span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;                </span><br><span class="line">                System.out.printf(<span class="string">&quot;offset = %d, key = %s, value = %s%n&quot;</span>, record.offset(), record.key(), record.value());            </span><br><span class="line">            &#125;            </span><br><span class="line"></span><br><span class="line">            consumer.commitSync();<span class="comment">//同步提交，当前线程会阻塞直到offset提交成功        </span></span><br><span class="line">        &#125;    </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="2）异步提交offset"><a href="#2）异步提交offset" class="headerlink" title="2）异步提交offset"></a>2）异步提交offset</h5><p>虽然同步提交offset更可靠一些，但是由于其会阻塞当前线程，直到提交成功。因此吞吐量会收到很大的影响。因此更多的情况下，会选用异步提交offset的方式。</p>
<p>以下为异步提交offset的示例：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.TopicPartition;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays; </span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomConsumer</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();        </span><br><span class="line"></span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;linux1:9092&quot;</span>);<span class="comment">//Kafka集群        </span></span><br><span class="line">        props.put(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;test&quot;</span>);<span class="comment">//消费者组，只要group.id相同，就属于同一个消费者组        </span></span><br><span class="line">        props.put(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;false&quot;</span>);<span class="comment">//关闭自动提交offset        </span></span><br><span class="line">        props.put(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);        </span><br><span class="line">        props.put(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);        </span><br><span class="line"></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);        </span><br><span class="line"></span><br><span class="line">        consumer.subscribe(Arrays.asList(<span class="string">&quot;first&quot;</span>));<span class="comment">//消费者订阅主题        </span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;            </span><br><span class="line"></span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);<span class="comment">//消费者拉取数据            </span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;  </span><br><span class="line">                System.out.printf(<span class="string">&quot;offset = %d, key = %s, value = %s%n&quot;</span>, record.offset(), record.key(), record.value());            </span><br><span class="line">            &#125;            </span><br><span class="line"></span><br><span class="line">            consumer.commitAsync(<span class="keyword">new</span> OffsetCommitCallback() &#123;                </span><br><span class="line">                <span class="meta">@Override</span>                </span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onComplete</span><span class="params">(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, Exception exception)</span> </span>&#123;                    </span><br><span class="line">                    <span class="keyword">if</span> (exception != <span class="keyword">null</span>) &#123;                        </span><br><span class="line">                        System.err.println(<span class="string">&quot;Commit failed for&quot;</span> + offsets);                    </span><br><span class="line">                    &#125;                </span><br><span class="line">                &#125;            </span><br><span class="line">            &#125;);<span class="comment">//异步提交        </span></span><br><span class="line">        &#125;    </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="数据漏消费和重复消费"><a href="#数据漏消费和重复消费" class="headerlink" title="数据漏消费和重复消费"></a>数据漏消费和重复消费</h5><p>无论是同步提交还是异步提交offset，都有可能会造成数据的漏消费或者重复消费。<strong>先提交offset后消费，有可能造成数据的漏消费；而先消费后提交offset，有可能会造成数据的重复消费。</strong></p>
<h2 id="安装部署"><a href="#安装部署" class="headerlink" title="安装部署"></a>安装部署</h2><h3 id="jar包下载"><a href="#jar包下载" class="headerlink" title="jar包下载"></a>jar包下载</h3><p><a href="http://kafka.apache.org/downloads">http://kafka.apache.org/downloads</a></p>
<h3 id="集群部署"><a href="#集群部署" class="headerlink" title="集群部署"></a>集群部署</h3><p>1）解压安装包</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 software]$ tar -zxvf kafka_2.11-2.4.1.tgz -C /opt/module/</span><br></pre></td></tr></table></figure>

<p>2）修改解压后的文件名称</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 module]$ mv kafka_2.11-2.4.1 kafka</span><br></pre></td></tr></table></figure>

<p>3）在/opt/module/kafka目录下创建logs文件夹</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 kafka]$ mkdir logs</span><br></pre></td></tr></table></figure>

<p>4）修改配置文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 kafka]$ cd config</span><br><span class="line">[vincent@linux1 config]$ vim server.properties</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">broker的全局唯一编号，不能重复</span></span><br><span class="line">broker.id=0</span><br><span class="line"><span class="meta">#</span><span class="bash">删除topic功能使能</span></span><br><span class="line">delete.topic.enable=true</span><br><span class="line"><span class="meta">#</span><span class="bash">处理网络请求的线程数量</span></span><br><span class="line">num.network.threads=3</span><br><span class="line"><span class="meta">#</span><span class="bash">用来处理磁盘IO的线程数量</span></span><br><span class="line">num.io.threads=8</span><br><span class="line"><span class="meta">#</span><span class="bash">发送套接字的缓冲区大小</span></span><br><span class="line">socket.send.buffer.bytes=102400</span><br><span class="line"><span class="meta">#</span><span class="bash">接收套接字的缓冲区大小</span></span><br><span class="line">socket.receive.buffer.bytes=102400</span><br><span class="line"><span class="meta">#</span><span class="bash">请求套接字的缓冲区大小</span></span><br><span class="line">socket.request.max.bytes=104857600</span><br><span class="line"><span class="meta">#</span><span class="bash">kafka运行日志存放的路径</span></span><br><span class="line">log.dirs=/opt/module/kafka/logs</span><br><span class="line"><span class="meta">#</span><span class="bash">topic在当前broker上的分区个数</span></span><br><span class="line">num.partitions=1</span><br><span class="line"><span class="meta">#</span><span class="bash">用来恢复和清理data下数据的线程数量</span></span><br><span class="line">num.recovery.threads.per.data.dir=1</span><br><span class="line"><span class="meta">#</span><span class="bash">segment文件保留的最长时间，超时将被删除</span></span><br><span class="line">log.retention.hours=168</span><br><span class="line"><span class="meta">#</span><span class="bash">配置连接Zookeeper集群地址</span></span><br><span class="line">zookeeper.connect=linux1:2181,linux2:2181,linux3:2181/kafka</span><br></pre></td></tr></table></figure>

<p>5）配置环境变量</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 module]$ sudo vim /etc/profile.d/my_env.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">KAFKA_HOME</span></span><br><span class="line">export KAFKA_HOME=/opt/module/kafka</span><br><span class="line">export PATH=$PATH:$KAFKA_HOME/bin</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 module]$ source /etc/profile</span><br></pre></td></tr></table></figure>

<p>6）分发安装包</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 module]$ xsync kafka</span><br></pre></td></tr></table></figure>

<p>注意：分发之后记得配置其他机器的环境变量</p>
<p>7）分别在linux2和linux3上修改配置文件/opt/module/kafka/config/server.properties中的broker.id=1、broker.id=2</p>
<p>注：broker.id不得重复</p>
<p>8）启动集群</p>
<p>依次在linux1、linux2、linux3节点上启动kafka</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 kafka]$ kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties</span><br><span class="line">[vincent@linux2 kafka]$ kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties</span><br><span class="line">[vincent@linux3 kafka]$ kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties</span><br></pre></td></tr></table></figure>

<p>9）关闭集群</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 kafka]$ bin/kafka-server-stop.sh</span><br><span class="line">[vincent@linux2 kafka]$ bin/kafka-server-stop.sh</span><br><span class="line">[vincent@linux3 kafka]$ bin/kafka-server-stop.sh</span><br></pre></td></tr></table></figure>

<p>10）kafka群起脚本</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">if [ $# -lt 1 ] then        </span><br><span class="line">    echo &quot;No Args Input!!!!&quot;        </span><br><span class="line">    exit;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">for i in linux1 linux2 linux3</span><br><span class="line">do</span><br><span class="line">    case $1 in</span><br><span class="line">    &quot;start&quot;)        </span><br><span class="line">        echo &quot;=============== start $i kafka ==============&quot;        </span><br><span class="line">        ssh $i /opt/module/kafka/bin/kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties</span><br><span class="line">        ;;</span><br><span class="line">    &quot;stop&quot;)        </span><br><span class="line">        echo &quot;=============== stop $i kafka ===============&quot;        </span><br><span class="line">        ssh $i /opt/module/kafka/bin/kafka-server-stop.sh</span><br><span class="line">        ;;</span><br><span class="line">    *)        </span><br><span class="line">        echo &quot;Input Args Error!!!!&quot;</span><br><span class="line">        ;;</span><br><span class="line">    esac</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<h3 id="kafka命令行操作"><a href="#kafka命令行操作" class="headerlink" title="kafka命令行操作"></a>kafka命令行操作</h3><p>1）查看当前服务器中的所有topic</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 kafka]$ bin/kafka-topics.sh --zookeeper linux1:2181/kafka --list</span><br></pre></td></tr></table></figure>

<p>2）创建topic</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 kafka]$ bin/kafka-topics.sh --zookeeper linux1:2181/kafka --create --topic second --replication-factor 2 --partitions 2</span><br></pre></td></tr></table></figure>

<p>选项说明：</p>
<p>–topic 定义topic名</p>
<p>–replication-factor  定义副本数</p>
<p>–partitions  定义分区数</p>
<p>3）删除topic</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 kafka]$ bin/kafka-topics.sh --zookeeper linux1:2181/kafka --delete --topic first</span><br></pre></td></tr></table></figure>

<p>需要server.properties中设置delete.topic.enable=true否则只是标记删除。</p>
<p>4）发送消息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 kafka]$ bin/kafka-console-producer.sh --broker-list linux1:9092 --topic first</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">hello world</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">vincent  vincent</span></span><br></pre></td></tr></table></figure>

<p>5）消费消息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 kafka]$ bin/kafka-console-consumer.sh --bootstrap-server linux1:9092 --from-beginning --topic first</span><br></pre></td></tr></table></figure>

<p>–from-beginning：会把主题中以往所有的数据都读取出来。</p>
<p>6）查看某个Topic的详情</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 kafka]$ bin/kafka-topics.sh --zookeeper linux1:2181/kafka --describe --topic first</span><br></pre></td></tr></table></figure>

<p>7）修改分区数</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 kafka]$bin/kafka-topics.sh --zookeeper linux1:2181/kafka --alter --topic first --partitions 6</span><br></pre></td></tr></table></figure>

<h2 id="Kafka监控（Kafka-Eagle）"><a href="#Kafka监控（Kafka-Eagle）" class="headerlink" title="Kafka监控（Kafka Eagle）"></a>Kafka监控（Kafka Eagle）</h2><p>1）修改kafka启动命令</p>
<p>修改kafka-server-start.sh命令中</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">if [ &quot;x$KAFKA_HEAP_OPTS&quot; = &quot;x&quot; ]; </span><br><span class="line">    then export KAFKA_HEAP_OPTS=&quot;-Xmx1G -Xms1G&quot;</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<p>为</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">if [ &quot;x$KAFKA_HEAP_OPTS&quot; = &quot;x&quot; ]; then</span><br><span class="line">  export KAFKA_HEAP_OPTS=&quot;-server -Xms2G -Xmx2G -XX:PermSize=128m -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:ParallelGCThreads=8 -XX:ConcGCThreads=5 -XX:InitiatingHeapOccupancyPercent=70&quot;</span><br><span class="line">  export JMX_PORT=&quot;9999&quot;</span><br><span class="line"><span class="meta">  #</span><span class="bash"><span class="built_in">export</span> KAFKA_HEAP_OPTS=<span class="string">&quot;-Xmx1G -Xms1G&quot;</span></span></span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<p>注意：修改之后在启动Kafka之前要分发至其他节点</p>
<p>2）上传压缩包kafka-eagle-bin-1.4.5.tar.gz到集群/opt/software目录</p>
<p>3）解压到本地</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 software]$ tar -zxvf kafka-eagle-bin-1.4.5.tar.gz</span><br></pre></td></tr></table></figure>

<p>4）进入刚才解压的目录</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 kafka-eagle-bin-1.4.5]$ ll</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">总用量 82932</span><br><span class="line">-rw-rw-r--. 1 vincent vincent 84920710 8月  13 23:00 kafka-eagle-web-1.4.5-bin.tar.gz</span><br></pre></td></tr></table></figure>

<p>5）将kafka-eagle-web-1.3.7-bin.tar.gz解压至/opt/module</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 kafka-eagle-bin-1.3.7]$ tar -zxvf kafka-eagle-web-1.4.5-bin.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>

<p>6）修改名称</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 module]$ mv kafka-eagle-web-1.4.5 eagle</span><br></pre></td></tr></table></figure>

<p>7）给启动文件执行权限</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 eagle]$ cd bin</span><br><span class="line">[vincent@linux1 bin]$ ll</span><br><span class="line"></span><br><span class="line">总用量 12</span><br><span class="line">-rw-r--r--. 1 vincent vincent 1848 8月  22 2017 ke.bat</span><br><span class="line">-rw-r--r--. 1 vincent vincent 7190 7月  30 20:12 ke.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 bin]$ chmod 777 ke.sh</span><br></pre></td></tr></table></figure>

<p>8）修改配置文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#####################################</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> multi zookeeper&amp;kafka cluster list</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#####################################</span></span></span><br><span class="line"></span><br><span class="line">kafka.eagle.zk.cluster.alias=cluster1</span><br><span class="line">cluster1.zk.list=linux1:2181,linux2:2181,linux3:2181</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#####################################</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> kafka offset storage</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#####################################</span></span></span><br><span class="line"></span><br><span class="line">cluster1.kafka.eagle.offset.storage=kafka</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#####################################</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">enable</span> kafka metrics</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#####################################</span></span></span><br><span class="line"></span><br><span class="line">kafka.eagle.metrics.charts=true</span><br><span class="line">kafka.eagle.sql.fix.error=false</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#####################################</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> kafka jdbc driver address</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#####################################</span></span></span><br><span class="line"></span><br><span class="line">kafka.eagle.driver=com.mysql.jdbc.Driver</span><br><span class="line">kafka.eagle.url=jdbc:mysql://linux1:3306/ke?useUnicode=true&amp;characterEncoding=UTF-8&amp;zeroDateTimeBehavior=convertToNull</span><br><span class="line">kafka.eagle.username=root</span><br><span class="line">kafka.eagle.password=211819</span><br></pre></td></tr></table></figure>

<p>9）添加环境变量</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export KE_HOME=/opt/module/eagle</span><br><span class="line">export PATH=$PATH:$KE_HOME/bin</span><br></pre></td></tr></table></figure>

<p>注意：source /etc/profile</p>
<p>10）启动</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 eagle]$ bin/ke.sh start</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">... ...</span><br><span class="line">... ...</span><br><span class="line"></span><br><span class="line">*******************************************************************</span><br><span class="line"></span><br><span class="line">* Kafka Eagle Service has started success.</span><br><span class="line">* Welcome, Now you can visit &#x27;http://10.211.55.10:8048/ke&#x27;</span><br><span class="line">* Account:admin ,Password:211819</span><br><span class="line">*******************************************************************</span><br><span class="line">* &lt;Usage&gt; ke.sh [start|status|stop|restart|stats] &lt;/Usage&gt;</span><br><span class="line">* &lt;Usage&gt; https://www.kafka-eagle.org/ &lt;/Usage&gt;</span><br><span class="line">*******************************************************************</span><br><span class="line"></span><br><span class="line">[vincent@linux1 eagle]$</span><br></pre></td></tr></table></figure>

<p>注意：启动之前需要先启动ZK以及KAFKA</p>
<p>11）登录页面查看监控数据</p>
<p><a href="http://10.211.55.10:8048/ke">http://10.211.55.10:8048/ke</a></p>
]]></content>
      <categories>
        <category>实时计算</category>
      </categories>
  </entry>
  <entry>
    <title>Linux</title>
    <url>/Linux/</url>
    <content><![CDATA[<h2 id="VI-VIM编辑器"><a href="#VI-VIM编辑器" class="headerlink" title="VI/VIM编辑器"></a>VI/VIM编辑器</h2><p>一般模式：</p>
<table>
<thead>
<tr>
<th>语法</th>
<th>功能描述</th>
</tr>
</thead>
<tbody><tr>
<td>yy</td>
<td>复制光标当前一行</td>
</tr>
<tr>
<td>y数字y</td>
<td>复制一段（从第几行到第几行）</td>
</tr>
<tr>
<td>p</td>
<td>箭头移动到目的行粘贴</td>
</tr>
<tr>
<td>u</td>
<td>撤销上一步</td>
</tr>
<tr>
<td>dd</td>
<td>删除光标当前行</td>
</tr>
<tr>
<td>d数字d</td>
<td>删除光标（含）后多少行</td>
</tr>
<tr>
<td>x</td>
<td>删除一个字母，相当于del</td>
</tr>
<tr>
<td>X</td>
<td>删除一个字母，相当于Backspace</td>
</tr>
<tr>
<td>yw</td>
<td>复制一个词</td>
</tr>
<tr>
<td>dw</td>
<td>删除一个词</td>
</tr>
<tr>
<td>shift+^</td>
<td>移动到行头</td>
</tr>
<tr>
<td>shift+$</td>
<td>移动到<strong>行尾</strong></td>
</tr>
<tr>
<td>1+shift+g</td>
<td>移动到页头，数字</td>
</tr>
<tr>
<td>shift+g</td>
<td>移动到<strong>页尾</strong></td>
</tr>
<tr>
<td>数字N+shift+g</td>
<td>移动到<strong>目标行</strong></td>
</tr>
</tbody></table>
<p>编辑模式：</p>
<table>
<thead>
<tr>
<th>按键</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>i</td>
<td>当前光标前</td>
</tr>
<tr>
<td>a</td>
<td>当前光标后</td>
</tr>
<tr>
<td>o</td>
<td>当前光标行的下一行</td>
</tr>
<tr>
<td>I</td>
<td>光标所在行最前</td>
</tr>
<tr>
<td>A</td>
<td>光标所在行最后</td>
</tr>
<tr>
<td>O</td>
<td>当前光标行的上一行</td>
</tr>
</tbody></table>
<p>指令模式：</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>:w</td>
<td>保存</td>
</tr>
<tr>
<td>:q</td>
<td>退出</td>
</tr>
<tr>
<td>:!</td>
<td>强制执行</td>
</tr>
<tr>
<td>/要查找的词</td>
<td>n 查找下一个，N 往上查找</td>
</tr>
<tr>
<td>? 要查找的词</td>
<td>n是查找上一个，shift+n是往下查找</td>
</tr>
<tr>
<td>:set nu</td>
<td><strong>显示行号</strong></td>
</tr>
<tr>
<td>:set nonu</td>
<td><strong>关闭行号</strong></td>
</tr>
<tr>
<td>:%s/old/new/g</td>
<td>替换内容</td>
</tr>
</tbody></table>
<h2 id="网络配置和系统管理操作"><a href="#网络配置和系统管理操作" class="headerlink" title="网络配置和系统管理操作"></a>网络配置和系统管理操作</h2><h3 id="查看当前网络ip"><a href="#查看当前网络ip" class="headerlink" title="查看当前网络ip"></a>查看当前网络ip</h3><p>ifconfig</p>
<h3 id="测试主机之间网络连通性"><a href="#测试主机之间网络连通性" class="headerlink" title="测试主机之间网络连通性"></a>测试主机之间网络连通性</h3><p>ping 目的主机</p>
<p>ping <a href="http://www.baidu.com/">www.baidu.com</a></p>
<h3 id="修改IP地址"><a href="#修改IP地址" class="headerlink" title="修改IP地址"></a>修改IP地址</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim /etc/sysconfig/network-scripts/ifcfg-ens33</span><br><span class="line">vim /etc/sysconfig/network-scripts/ifcfg-eth0</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">TYPE=&quot;Ethernet&quot;</span><br><span class="line">PROXY_METHOD=&quot;none&quot;</span><br><span class="line">BROWSER_ONLY=&quot;no&quot;</span><br><span class="line">BOOTPROTO=static</span><br><span class="line">DEFROUTE=&quot;yes&quot;</span><br><span class="line">IPV4_FAILURE_FATAL=&quot;no&quot;</span><br><span class="line">IPV6INIT=&quot;yes&quot;</span><br><span class="line">IPV6_AUTOCONF=&quot;yes&quot;</span><br><span class="line">IPV6_DEFROUTE=&quot;yes&quot;</span><br><span class="line">IPV6_FAILURE_FATAL=&quot;no&quot;</span><br><span class="line">IPV6_ADDR_GEN_MODE=&quot;stable-privacy&quot;</span><br><span class="line">NAME=&quot;eth0&quot;</span><br><span class="line">UUID=&quot;19c440dc-9a12-4fd7-bc46-0a121ef6af96&quot;</span><br><span class="line">DEVICE=&quot;eth0&quot;</span><br><span class="line">ONBOOT=&quot;yes&quot;</span><br><span class="line">IPADDR=10.211.55.10</span><br><span class="line">GATEWAY=10.211.55.1</span><br><span class="line">DNS1=10.211.55.1</span><br></pre></td></tr></table></figure>



<h3 id="重启网络"><a href="#重启网络" class="headerlink" title="重启网络"></a>重启网络</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">service network restart</span><br></pre></td></tr></table></figure>



<h2 id="配置主机名"><a href="#配置主机名" class="headerlink" title="配置主机名"></a>配置主机名</h2><h3 id="修改主机名称"><a href="#修改主机名称" class="headerlink" title="修改主机名称"></a>修改主机名称</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo vim /etc/hostname</span><br></pre></td></tr></table></figure>

<h3 id="修改hosts映射文件"><a href="#修改hosts映射文件" class="headerlink" title="修改hosts映射文件"></a>修改hosts映射文件</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo vim /etc/hosts</span><br></pre></td></tr></table></figure>

<figure class="highlight accesslog"><table><tr><td class="code"><pre><span class="line"><span class="number">10.211.55.10</span>	linux1</span><br><span class="line"><span class="number">10.211.55.11</span>	linux2</span><br><span class="line"><span class="number">10.211.55.12</span>	linux3</span><br></pre></td></tr></table></figure>

<p>Windows 主机映射文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">C:\Windows\System32\drivers\etc</span><br></pre></td></tr></table></figure>

<p>Mac 主机映射文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/etc/hosts</span><br></pre></td></tr></table></figure>



<h2 id="关闭防火墙"><a href="#关闭防火墙" class="headerlink" title="关闭防火墙"></a>关闭防火墙</h2><h3 id="service"><a href="#service" class="headerlink" title="service"></a>service</h3><p>service  服务名 start | stop | restart | status</p>
<p>service network status</p>
<h3 id="chkconfig"><a href="#chkconfig" class="headerlink" title="chkconfig"></a>chkconfig</h3><p>chkconfig 设置后台服务的自启配置</p>
<p>chkconfig               （功能描述：查看所有服务器自启配置）</p>
<p>chkconfig 服务名 off  （功能描述：关掉指定服务的自动启动）</p>
<p>chkconfig 服务名 on  （功能描述：开启指定服务的自动启动）</p>
<p>chkconfig 服务名 –list     （功能描述：查看服务开机启动状态）</p>
<h4 id="启-关闭iptables-防火墙-服务的自动启动"><a href="#启-关闭iptables-防火墙-服务的自动启动" class="headerlink" title="启/关闭iptables(防火墙)服务的自动启动"></a>启/关闭iptables(防火墙)服务的自动启动</h4><p>chkconfig iptables on</p>
<p>chkconfig iptables off</p>
<h4 id="开启-关闭-iptables服务指定级别的自动启动"><a href="#开启-关闭-iptables服务指定级别的自动启动" class="headerlink" title="开启/关闭 iptables服务指定级别的自动启动"></a>开启/关闭 iptables服务指定级别的自动启动</h4><p>chkconfig –level 指定级别 iptables on</p>
<p>chkconfig –level 指定级别 iptables off</p>
<h3 id="systemctl"><a href="#systemctl" class="headerlink" title="systemctl"></a>systemctl</h3><p>systemctl  start | stop | restart | status     服务名</p>
<p>systemctl status firewalld</p>
<h4 id="systemctl-设置后台服务的自启配置"><a href="#systemctl-设置后台服务的自启配置" class="headerlink" title="systemctl 设置后台服务的自启配置"></a>systemctl 设置后台服务的自启配置</h4><p>systemctl list-unit-files     （功能描述：查看服务开机启动状态）</p>
<p>systemctl disable service_name  （功能描述：关掉指定服务的自动启动）</p>
<p>systemctl enable service_name  （功能描述：开启指定服务的自动启动）</p>
<h4 id="开启-关闭iptables-防火墙-服务的自动启动"><a href="#开启-关闭iptables-防火墙-服务的自动启动" class="headerlink" title="开启/关闭iptables(防火墙)服务的自动启动"></a>开启/关闭iptables(防火墙)服务的自动启动</h4><p>systemctl enable firewalld.service</p>
<p>systemctl disable firewalld.service</p>
<h4 id="临时关闭防火墙"><a href="#临时关闭防火墙" class="headerlink" title="临时关闭防火墙"></a>临时关闭防火墙</h4><p>systemctl stop firewalld</p>
<h4 id="开机启动时关闭防火墙"><a href="#开机启动时关闭防火墙" class="headerlink" title="开机启动时关闭防火墙"></a>开机启动时关闭防火墙</h4><p>systemctl disable firewalld.service</p>
<h2 id="关机重启命令"><a href="#关机重启命令" class="headerlink" title="关机重启命令"></a>关机重启命令</h2><p>sync &gt; shutdown &gt; reboot &gt; halt</p>
<p>sync          （功能描述：将数据由内存同步到硬盘中）</p>
<p>halt             （功能描述：关闭系统，等同于shutdown -h now 和 poweroff）</p>
<p>reboot         （功能描述：就是重启，等同于 shutdown -r now）</p>
<p>shutdown [选项] 时间    </p>
<p>powerpff</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-h</td>
<td>-h=halt关机</td>
</tr>
<tr>
<td>-r</td>
<td>-r=reboot重启</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>参数</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>now</td>
<td>立刻关机</td>
</tr>
<tr>
<td>时间</td>
<td>等待多久后关机（时间单位是分钟）。</td>
</tr>
</tbody></table>
<h2 id="帮助命令"><a href="#帮助命令" class="headerlink" title="帮助命令"></a>帮助命令</h2><p>man [命令或配置文件]        （功能描述：获得帮助信息）</p>
<p>man ls</p>
<p>help 命令    （功能描述：获得shell内置命令的帮助信息）</p>
<p>help cd</p>
<h2 id="文件目录类命令"><a href="#文件目录类命令" class="headerlink" title="文件目录类命令"></a>文件目录类命令</h2><h3 id="pwd"><a href="#pwd" class="headerlink" title="pwd"></a>pwd</h3><p>功能描述：显示当前工作目录的绝对路径</p>
<h3 id="ls"><a href="#ls" class="headerlink" title="ls"></a>ls</h3><p>ls [选项] [目录或是文件]</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-a</td>
<td>全部的文件，连同隐藏档( 开头为 . 的文件) 一起列出来(常用)</td>
</tr>
<tr>
<td>-l</td>
<td>长数据串列出，包含文件的属性与权限等等数据；(常用)</td>
</tr>
</tbody></table>
<h3 id="cd"><a href="#cd" class="headerlink" title="cd"></a>cd</h3><p>切换路径</p>
<h3 id="mkdir"><a href="#mkdir" class="headerlink" title="mkdir"></a>mkdir</h3><p>创建新目录</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-p</td>
<td>创建多层目录</td>
</tr>
</tbody></table>
<p>mkdir -p xiyou/dssz/meihouwang</p>
<h3 id="rmdir"><a href="#rmdir" class="headerlink" title="rmdir"></a>rmdir</h3><p>删除空目录</p>
<h3 id="touch"><a href="#touch" class="headerlink" title="touch"></a>touch</h3><p>创建空文件</p>
<h3 id="cp"><a href="#cp" class="headerlink" title="cp"></a>cp</h3><p>复制文件或者目录</p>
<p>cp [选项] source dest</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-r</td>
<td>递归复制整个文件夹</td>
</tr>
</tbody></table>
<p>cp -r xiyou/dssz/ ./</p>
<h3 id="rm"><a href="#rm" class="headerlink" title="rm"></a>rm</h3><p>移除文件或目录</p>
<p>rm [选项] deleteFile</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-r</td>
<td>递归删除目录中所有内容</td>
</tr>
<tr>
<td>-f</td>
<td>强制执行删除操作，而不提示用于进行确认。</td>
</tr>
<tr>
<td>-v</td>
<td>显示指令的详细执行过程</td>
</tr>
</tbody></table>
<p>rm -rf dssz/</p>
<h3 id="mv"><a href="#mv" class="headerlink" title="mv"></a>mv</h3><p>移动文件与目录或重命名</p>
<p>mv oldNameFile newNameFile    （功能描述：重命名）</p>
<p>mv /temp/movefile /targetFolder    （功能描述：移动文件）</p>
<h3 id="cat"><a href="#cat" class="headerlink" title="cat"></a>cat</h3><p>cat  [选项] 要查看的文件</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能描述</th>
</tr>
</thead>
<tbody><tr>
<td>-n</td>
<td>显示所有行的行号，包括空行。</td>
</tr>
</tbody></table>
<p>cat -n houge.txt</p>
<h3 id="more"><a href="#more" class="headerlink" title="more"></a>more</h3><p>文件内容分屏查看器</p>
<p>more 要查看的文件</p>
<table>
<thead>
<tr>
<th>操作</th>
<th>功能说明</th>
</tr>
</thead>
<tbody><tr>
<td>空白键 (space)</td>
<td>代表向下翻一页；</td>
</tr>
<tr>
<td>Enter</td>
<td>代表向下翻『一行』；</td>
</tr>
<tr>
<td>q</td>
<td>代表立刻离开 more ，不再显示该文件内容。</td>
</tr>
<tr>
<td>Ctrl+F</td>
<td>向下滚动一屏</td>
</tr>
<tr>
<td>Ctrl+B</td>
<td>返回上一屏</td>
</tr>
<tr>
<td>=</td>
<td>输出当前行的行号</td>
</tr>
<tr>
<td>:f</td>
<td>输出文件名和当前行的行号</td>
</tr>
</tbody></table>
<h3 id="less"><a href="#less" class="headerlink" title="less"></a>less</h3><p>分屏显示文件内容</p>
<table>
<thead>
<tr>
<th>操作</th>
<th>功能说明</th>
</tr>
</thead>
<tbody><tr>
<td>空白键</td>
<td>向下翻动一页；</td>
</tr>
<tr>
<td>[pagedown]</td>
<td>向下翻动一页</td>
</tr>
<tr>
<td>[pageup]</td>
<td>向上翻动一页；</td>
</tr>
<tr>
<td>/字串</td>
<td>向下搜寻『字串』的功能；n：向下查找；N：向上查找；</td>
</tr>
<tr>
<td>?字串</td>
<td>向上搜寻『字串』的功能；n：向上查找；N：向下查找；</td>
</tr>
<tr>
<td>q</td>
<td>离开 less 这个程序；</td>
</tr>
</tbody></table>
<h3 id="echo"><a href="#echo" class="headerlink" title="echo"></a>echo</h3><p>输出内容到控制台</p>
<p>echo [选项] [输出内容]</p>
<p>选项： </p>
<p> -e：  支持反斜线控制的字符转换</p>
<table>
<thead>
<tr>
<th>控制字符</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>\</td>
<td>输出\本身</td>
</tr>
<tr>
<td>\n</td>
<td>换行符</td>
</tr>
<tr>
<td>\t</td>
<td>制表符，也就是Tab键</td>
</tr>
</tbody></table>
<h3 id="head"><a href="#head" class="headerlink" title="head"></a>head</h3><p>head用于显示文件的开头部分内容，默认情况下head指令显示文件的前10行内容。</p>
<p>head 文件       （功能描述：查看文件头10行内容）</p>
<p>head -n 5 文件    （功能描述：查看文件头5行内容，5可以是任意行数）</p>
<p>head -n 2 smartd.conf</p>
<h3 id="tail"><a href="#tail" class="headerlink" title="tail"></a>tail</h3><p>tail用于输出文件中尾部的内容，默认情况下tail指令显示文件的后10行内容。</p>
<p>tail  文件             （功能描述：查看文件尾部10行内容）</p>
<p>tail  -n  5 文件     （功能描述：查看文件尾部5行内容，5可以是任意行数）</p>
<p>tail  -f  文件        （功能描述：实时追踪该文档的所有更新）</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-n&lt;行数&gt;</td>
<td>输出文件尾部n行内容</td>
</tr>
<tr>
<td>-f</td>
<td>显示文件最新追加的内容，监视文件变化</td>
</tr>
</tbody></table>
<h3 id="gt-输出重定向和-gt-gt-追加"><a href="#gt-输出重定向和-gt-gt-追加" class="headerlink" title="&gt; 输出重定向和 &gt;&gt; 追加"></a>&gt; 输出重定向和 &gt;&gt; 追加</h3><p>ls -l &gt; 文件                 （功能描述：列表的内容写入文件中（****覆盖写****））</p>
<p>ls -al  &gt;&gt; 文件            （功能描述：列表的内容****追加****到文件的末尾）</p>
<p>cat 文件1 &gt; 文件2      （功能描述：将文件1的内容覆盖到文件2）</p>
<p>echo “内容” &gt;&gt; 文件</p>
<p>将ls查看信息写入到文件中</p>
<p>ls -l&gt;houge.txt</p>
<p>将ls查看信息追加到文件中</p>
<p>ls -l&gt;&gt;houge.txt</p>
<p>采用echo将hello单词追加到文件中</p>
<p>echo hello&gt;&gt;houge.txt</p>
<h3 id="ln"><a href="#ln" class="headerlink" title="ln"></a>ln</h3><p>软链接</p>
<p>ln -s [原文件或目录] [软链接名]        （功能描述：给原文件创建一个软链接）</p>
<p>创建软连接</p>
<p>ln -s xiyou/dssz/houge.txt ./houzi</p>
<p>删除软连接</p>
<p>rm -rf houzi</p>
<p>进入软连接实际物理路径</p>
<p>ln -s xiyou/dssz/ ./dssz</p>
<p>cd -P dssz/</p>
<h3 id="history"><a href="#history" class="headerlink" title="history"></a>history</h3><p>history                        （功能描述：查看已经执行过历史命令）</p>
<h2 id="时间日期类命令"><a href="#时间日期类命令" class="headerlink" title="时间日期类命令"></a>时间日期类命令</h2><p>date [OPTION]… [+FORMAT]</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-d&lt;时间字符串&gt;</td>
<td>显示指定的“时间字符串”表示的时间，而非当前时间</td>
</tr>
<tr>
<td>-s&lt;日期时间&gt;</td>
<td>设置系统日期时间</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>参数</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>&lt;+日期时间格式&gt;</td>
<td>指定显示时使用的日期时间格式</td>
</tr>
</tbody></table>
<h3 id="date-显示当前时间"><a href="#date-显示当前时间" class="headerlink" title="date 显示当前时间"></a>date 显示当前时间</h3><p>date                                    （功能描述：显示当前时间）</p>
<p>date +%Y                            （功能描述：显示当前年份）</p>
<p>date +%m                            （功能描述：显示当前月份）</p>
<p>date +%d                            （功能描述：显示当前是哪一天）</p>
<p>date “+%Y-%m-%d %H:%M:%S”        （功能描述：显示年月日时分秒）</p>
<h3 id="date-显示非当前时间"><a href="#date-显示非当前时间" class="headerlink" title="date 显示非当前时间"></a>date 显示非当前时间</h3><p>date -d ‘1 days ago’            （功能描述：显示前一天时间）</p>
<p>date -d ‘-1 days ago’            （功能描述：显示明天时间）</p>
<h3 id="date-设置系统时间"><a href="#date-设置系统时间" class="headerlink" title="date 设置系统时间"></a>date 设置系统时间</h3><p>date -s 字符串时间</p>
<p>date -s “2017-06-19 20:52:18”</p>
<h3 id="cal-查看日历"><a href="#cal-查看日历" class="headerlink" title="cal 查看日历"></a>cal 查看日历</h3><p>cal [选项]            （功能描述：不加选项，显示本月日历）</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>具体某一年</td>
<td>显示这一年的日历</td>
</tr>
</tbody></table>
<p>cal 2020</p>
<h2 id="用户管理命令"><a href="#用户管理命令" class="headerlink" title="用户管理命令"></a>用户管理命令</h2><h3 id="useradd"><a href="#useradd" class="headerlink" title="useradd"></a>useradd</h3><p>useradd 用户名            （功能描述：添加新用户）</p>
<p>useradd -g 组名 用户名    （功能描述：添加新用户到某个组）</p>
<h3 id="passwd"><a href="#passwd" class="headerlink" title="passwd"></a>passwd</h3><p>设置用户密码</p>
<p>passwd 用户名    （功能描述：设置用户密码）</p>
<h3 id="id"><a href="#id" class="headerlink" title="id"></a>id</h3><p>查看用户是否存在</p>
<p>id 用户名</p>
<h3 id="cat-etc-passwd"><a href="#cat-etc-passwd" class="headerlink" title="cat  /etc/passwd"></a>cat  /etc/passwd</h3><p>查看创建了哪些用户</p>
<h3 id="su"><a href="#su" class="headerlink" title="su"></a>su</h3><p>切换用户</p>
<p>su 用户名称  （功能描述：切换用户，只能获得用户的执行权限，不能获得环境变量）</p>
<p>su - 用户名称（功能描述：切换到用户并获得该用户的环境变量及执行权限）</p>
<h3 id="userdel"><a href="#userdel" class="headerlink" title="userdel"></a>userdel</h3><p>删除用户</p>
<p>userdel  用户名        （功能描述：删除用户但保存用户主目录）</p>
<p>userdel -r 用户名      （功能描述：用户和用户主目录，都删除）</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-r</td>
<td>删除用户的同时，删除与用户相关的所有文件。</td>
</tr>
</tbody></table>
<h3 id="who"><a href="#who" class="headerlink" title="who"></a>who</h3><p>查看登录用户信息</p>
<p>whoami            （功能描述：显示自身用户名称）</p>
<p>who am i          （功能描述：显示登录用户的用户名)</p>
<h3 id="sudo"><a href="#sudo" class="headerlink" title="sudo"></a>sudo</h3><p>设置普通用户具有root权限</p>
<p>先修改配置文件</p>
<p>vim /etc/sudoers</p>
<p>修改 /etc/sudoers 文件，找到下面一行(91行)，在root下面添加一行，如下所示：</p>
<p>## Allow root to run any commands anywhere</p>
<p>root   ALL=(ALL)   ALL</p>
<p>vincent  ALL=(ALL)   ALL</p>
<h3 id="usermod"><a href="#usermod" class="headerlink" title="usermod"></a>usermod</h3><p>修改用户</p>
<p>usermod -g 用户组 用户名</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-g</td>
<td>修改用户的初始登录组，给定的组必须存在。默认组id是1。</td>
</tr>
</tbody></table>
<h3 id="groupadd"><a href="#groupadd" class="headerlink" title="groupadd"></a>groupadd</h3><p>groupadd 组名</p>
<h3 id="groupdel"><a href="#groupdel" class="headerlink" title="groupdel"></a>groupdel</h3><p>groupdel 组名</p>
<h3 id="groupmod"><a href="#groupmod" class="headerlink" title="groupmod"></a>groupmod</h3><p>groupmod -n 新组名 老组名</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能描述</th>
</tr>
</thead>
<tbody><tr>
<td>-n&lt;新组名&gt;</td>
<td>指定工作组的新组名</td>
</tr>
</tbody></table>
<h3 id="cat-etc-group"><a href="#cat-etc-group" class="headerlink" title="cat  /etc/group"></a>cat  /etc/group</h3><p>查看创建了哪些组</p>
<h2 id="文件权限类命令"><a href="#文件权限类命令" class="headerlink" title="文件权限类命令"></a>文件权限类命令</h2><h3 id="文件属性"><a href="#文件属性" class="headerlink" title="文件属性"></a>文件属性</h3><p>从左到右的10个字符</p>
<p>0首位表示类型</p>
<p>在Linux中第一个字符代表这个文件是目录、文件或链接文件等等</p>
<p>- 代表文件</p>
<p> d 代表目录</p>
<p> l 链接文档(link file)</p>
<p>第1-3位确定属主（该文件的所有者）拥有该文件的权限。—User</p>
<p>第4-6位确定属组（所有者的同组用户）拥有该文件的权限，—Group</p>
<p>第7-9位确定其他用户拥有该文件的权限 —Other</p>
<h3 id="rxw作用文件和目录的不同解释"><a href="#rxw作用文件和目录的不同解释" class="headerlink" title="rxw作用文件和目录的不同解释"></a>rxw作用文件和目录的不同解释</h3><p>作用到文件：</p>
<p>[ r ]代表可读(read): 可以读取，查看</p>
<p>[ w ]代表可写(write): 可以修改，但是不代表可以删除该文件，删除一个文件的前提条件是对该文件所在的目录有写权限，才能删除该文件.</p>
<p>[ x ]代表可执行(execute):可以被系统执行</p>
<p>作用到目录：</p>
<p>[ r ]代表可读(read): 可以读取，ls查看目录内容</p>
<p>[ w ]代表可写(write): 可以修改，目录内创建+删除+重命名目录</p>
<p>[ x ]代表可执行(execute):可以进入该目录</p>
<h3 id="chmod"><a href="#chmod" class="headerlink" title="chmod"></a>chmod</h3><p>改变权限</p>
<p>第一种方式变更权限</p>
<p>chmod  [{ugoa}{+-=}{rwx}] 文件或目录</p>
<p>chmod u-x,o+x houge.txt</p>
<p>第二种方式变更权限</p>
<p>chmod  [mode=421 ]  [文件或目录]</p>
<p>chmod 777 houge.txt</p>
<p>chmod -R 777 xiyou/</p>
<p>u:所有者  g:所有组  o:其他人  a:所有人(u、g、o的总和)</p>
<p>r=4 w=2 x=1     rwx=4+2+1=7</p>
<h3 id="chown"><a href="#chown" class="headerlink" title="chown"></a>chown</h3><p>改变所有者</p>
<p>chown [选项] [最终用户] [文件或目录]        （功能描述：改变文件或者目录的所有者）</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-R</td>
<td>递归操作</td>
</tr>
</tbody></table>
<p>chown -R vincent:vincent xiyou/</p>
<h3 id="chgrp"><a href="#chgrp" class="headerlink" title="chgrp"></a>chgrp</h3><p>chgrp [最终用户组] [文件或目录]    （功能描述：改变文件或者目录的所属组）</p>
<p>chgrp root houge.txt</p>
<h2 id="搜索查找类命令"><a href="#搜索查找类命令" class="headerlink" title="搜索查找类命令"></a>搜索查找类命令</h2><h3 id="find"><a href="#find" class="headerlink" title="find"></a>find</h3><p>ind指令将从指定目录向下递归地遍历其各个子目录，将满足条件的文件显示在终端。</p>
<p>find [搜索范围] [选项]</p>
<table>
<thead>
<tr>
<th>选项</th>
<th align="left">功能</th>
</tr>
</thead>
<tbody><tr>
<td>-name&lt;查询方式&gt;</td>
<td align="left">按照指定的文件名查找模式查找文件</td>
</tr>
<tr>
<td>-user&lt;用户名&gt;</td>
<td align="left">查找属于指定用户名所有文件</td>
</tr>
<tr>
<td>-size&lt;文件大小&gt;</td>
<td align="left">按照指定的文件大小查找文件,单位为: <em><strong>*b*</strong></em> —— 块（512字节）<em><strong>*c*</strong></em> —— 字节<em><strong>*w*</strong></em> —— 字（2字节）<em><strong>*k*</strong></em> —— 千字节<em><strong>*M*</strong></em> —— 兆字节<em><strong>*G*</strong></em> —— 吉字节</td>
</tr>
</tbody></table>
<p>find xiyou/ -name *.txt</p>
<p>find xiyou/ -user vincent</p>
<p>find /home -size +204800</p>
<h3 id="locate"><a href="#locate" class="headerlink" title="locate"></a>locate</h3><p>快速定位文件路径</p>
<p>locate 搜索文件</p>
<h3 id="grep"><a href="#grep" class="headerlink" title="grep"></a>grep</h3><p>过滤查找及“|”管道符</p>
<p>管道符，“|”，表示<strong>将前一个命令的处理结果输出传递给后面的命令处理</strong></p>
<p>grep 选项 查找内容 源文件</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-n</td>
<td>显示匹配行及行号。</td>
</tr>
</tbody></table>
<p>找test在第几行</p>
<p>ls | grep -n test</p>
<h2 id="压缩和解压类命令"><a href="#压缩和解压类命令" class="headerlink" title="压缩和解压类命令"></a>压缩和解压类命令</h2><h3 id="gzip-gunzip"><a href="#gzip-gunzip" class="headerlink" title="gzip/gunzip"></a>gzip/gunzip</h3><p>gzip 文件        （功能描述：压缩文件，只能将文件压缩为*.gz文件）</p>
<p>gunzip 文件.gz    （功能描述：解压缩文件命令）</p>
<p>只能压缩文件不能压缩目录。</p>
<p>不保留原来的文件。</p>
<h3 id="zip-unzip"><a href="#zip-unzip" class="headerlink" title="zip/unzip"></a>zip/unzip</h3><p>zip  [选项] XXX.zip  将要压缩的内容         （功能描述：压缩文件和目录的命令）</p>
<p>unzip [选项] XXX.zip                        （功能描述：解压缩文件）</p>
<table>
<thead>
<tr>
<th>zip选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-r</td>
<td>压缩目录</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>unzip选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-d&lt;目录&gt;</td>
<td>指定解压后文件的存放目录</td>
</tr>
</tbody></table>
<p>zip 压缩命令在window/linux都通用，可以压缩目录且保留源文件。</p>
<h3 id="tar"><a href="#tar" class="headerlink" title="tar"></a>tar</h3><p>tar  [选项]  XXX.tar.gz  将要打包进去的内容        （功能描述：打包目录，压缩后的文件格式.tar.gz）</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-c</td>
<td>产生.tar打包文件</td>
</tr>
<tr>
<td>-v</td>
<td>显示详细信息</td>
</tr>
<tr>
<td>-f</td>
<td>指定压缩后的文件名</td>
</tr>
<tr>
<td>-z</td>
<td>打包同时压缩</td>
</tr>
<tr>
<td>-x</td>
<td>解包.tar文件</td>
</tr>
</tbody></table>
<p>压缩多个文件</p>
<p>tar -zcvf houma.tar.gz houge.txt bailongma.txt</p>
<p>压缩目录</p>
<p>tar -zcvf xiyou.tar.gz xiyou/</p>
<p>解压到当前目录</p>
<p>tar -zxvf houma.tar.gz</p>
<p>解压到指定目录</p>
<p>tar -zxvf xiyou.tar.gz -C /opt</p>
<h2 id="磁盘分区类命令"><a href="#磁盘分区类命令" class="headerlink" title="磁盘分区类命令"></a>磁盘分区类命令</h2><h3 id="df"><a href="#df" class="headerlink" title="df"></a>df</h3><p>df: disk free 空余硬盘</p>
<p>查看磁盘空间使用情况</p>
<p>df  选项    （功能描述：列出文件系统的整体磁盘使用量，检查文件系统的磁盘空间占用情况）</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-h</td>
<td>以人们较易阅读的 GBytes, MBytes, KBytes 等格式自行显示；</td>
</tr>
</tbody></table>
<p>df -h</p>
<h3 id="fdisk"><a href="#fdisk" class="headerlink" title="fdisk"></a>fdisk</h3><p>查看分区</p>
<p>fdisk -l            （功能描述：查看磁盘分区详情）</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-l</td>
<td>显示所有硬盘的分区列表</td>
</tr>
</tbody></table>
<p>该命令必须在root用户下才能使用。</p>
<h3 id="lsblk"><a href="#lsblk" class="headerlink" title="lsblk"></a>lsblk</h3><p>查看设备挂载情况</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-f</td>
<td>查看详细的设备挂载情况，显示文件系统信息</td>
</tr>
</tbody></table>
<h3 id="mount-umount"><a href="#mount-umount" class="headerlink" title="mount/umount"></a>mount/umount</h3><p>mount [-t vfstype] [-o options] device dir    （功能描述：挂载设备）</p>
<p>umount 设备文件名或挂载点            （功能描述：卸载设备）</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-t vfstype</td>
<td>指定文件系统的类型，通常不必指定。mount 会自动选择正确的类型。常用类型有：光盘或光盘镜像：iso9660DOS fat16文件系统：msdos<a href="http://blog.csdn.net/hancunai0017/article/details/6995284">Windows</a> 9x fat32文件系统：vfatWindows NT ntfs文件系统：ntfsMount Windows文件<a href="http://blog.csdn.net/hancunai0017/article/details/6995284">网络</a>共享：smbfs<a href="http://blog.csdn.net/hancunai0017/article/details/6995284">UNIX</a>(LINUX) 文件网络共享：nfs</td>
</tr>
<tr>
<td>-o options</td>
<td>主要用来描述设备或档案的挂接方式。常用的参数有：loop：用来把一个文件当成硬盘分区挂接上系统ro：采用只读方式挂接设备rw：采用读写方式挂接设备　  iocharset：指定访问文件系统所用字符集</td>
</tr>
<tr>
<td>device</td>
<td>要挂接(mount)的设备</td>
</tr>
<tr>
<td>dir</td>
<td>设备在系统上的挂接点(mount point)</td>
</tr>
</tbody></table>
<h2 id="进程线程类命令"><a href="#进程线程类命令" class="headerlink" title="进程线程类命令"></a>进程线程类命令</h2><h3 id="ps"><a href="#ps" class="headerlink" title="ps"></a>ps</h3><p>查看当前系统进程状态</p>
<p>ps -aux | grep xxx        （功能描述：查看系统中所有进程）</p>
<p>ps -ef | grep xxx        （功能描述：可以查看子父进程之间的关系）</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-a</td>
<td>选择所有进程</td>
</tr>
<tr>
<td>-u</td>
<td>显示所有用户的所有进程</td>
</tr>
<tr>
<td>-x</td>
<td>显示没有终端的进程</td>
</tr>
</tbody></table>
<h4 id="功能说明"><a href="#功能说明" class="headerlink" title="功能说明"></a>功能说明</h4><p>（1）ps -aux显示信息说明</p>
<p>​    USER：该进程是由哪个用户产生的</p>
<p>​    PID：进程的ID号</p>
<p>%CPU：该进程占用CPU资源的百分比，占用越高，进程越耗费资源；</p>
<p>%MEM：该进程占用物理内存的百分比，占用越高，进程越耗费资源；</p>
<p>VSZ：该进程占用虚拟内存的大小，单位KB；</p>
<p>RSS：该进程占用实际物理内存的大小，单位KB；</p>
<p>TTY：该进程是在哪个终端中运行的。其中tty1-tty7代表本地控制台终端，tty1-tty6是本地的字符界面终端，tty7是图形终端。pts/0-255代表虚拟终端。</p>
<p>STAT：进程状态。常见的状态有：R：运行、S：睡眠、T：停止状态、s：包含子进程、+：位于后台</p>
<p>START：该进程的启动时间</p>
<p>TIME：该进程占用CPU的运算时间，注意不是系统时间</p>
<p>COMMAND：产生此进程的命令名</p>
<p>（2）ps -ef显示信息说明</p>
<p>UID：用户ID </p>
<p>PID：进程ID </p>
<p>PPID：父进程ID </p>
<p>C：CPU用于计算执行优先级的因子。数值越大，表明进程是CPU密集型运算，执行优先级会降低；数值越小，表明进程是I/O密集型运算，执行优先级会提高 </p>
<p>STIME：进程启动的时间 </p>
<p>TTY：完整的终端名称 </p>
<p>TIME：CPU时间 </p>
<p>CMD：启动进程所用的命令和参数</p>
<h4 id="经验技巧"><a href="#经验技巧" class="headerlink" title="经验技巧"></a>经验技巧</h4><p>如果想查看进程的CPU占用率和内存占用率，可以使用aux;</p>
<p>如果想查看进程的父进程ID可以使用ef;</p>
<h3 id="kill"><a href="#kill" class="headerlink" title="kill"></a>kill</h3><p>终止进程</p>
<p>kill  [选项] 进程号        （功能描述：通过进程号杀死进程）</p>
<p>killall 进程名称            （功能描述：通过进程名称杀死进程，也支持通配符，这在系统因负载过大而变得很慢时很有用）</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-9</td>
<td>表示强迫进程立即停止</td>
</tr>
</tbody></table>
<p>kill -9 5102</p>
<p>killall firefox</p>
<h3 id="pstree"><a href="#pstree" class="headerlink" title="pstree"></a>pstree</h3><p>查看进程树</p>
<p>pstree [选项]</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-p</td>
<td>显示进程的PID</td>
</tr>
<tr>
<td>-u</td>
<td>显示进程的所属用户</td>
</tr>
</tbody></table>
<h3 id="top"><a href="#top" class="headerlink" title="top"></a>top</h3><p>查看系统健康状态</p>
<p>top [选项]</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-d 秒数</td>
<td>指定top命令每隔几秒更新。默认是3秒在top命令的交互模式当中可以执行的命令：</td>
</tr>
<tr>
<td>-i</td>
<td>使top不显示任何闲置或者僵死进程。</td>
</tr>
<tr>
<td>-p</td>
<td>通过指定监控进程ID来仅仅监控某个进程的状态。</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>操作</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>P</td>
<td>以CPU使用率排序，默认就是此项</td>
</tr>
<tr>
<td>M</td>
<td>以内存的使用率排序</td>
</tr>
<tr>
<td>N</td>
<td>以PID排序</td>
</tr>
<tr>
<td>q</td>
<td>退出top</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>内容</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>12:26:46</td>
<td>系统当前时间</td>
</tr>
<tr>
<td>up 1 day, 13:32</td>
<td>系统的运行时间，本机已经运行1天13小时32分钟</td>
</tr>
<tr>
<td>2 users</td>
<td>当前登录了两个用户</td>
</tr>
<tr>
<td>load  average:  0.00, 0.00, 0.00</td>
<td>系统在之前1分钟，5分钟，15分钟的平均负载。一般认为小于1时，负载较小。如果大于1，系统已经超出负荷。</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>Tasks:  95 total</th>
<th>系统中的进程总数</th>
</tr>
</thead>
<tbody><tr>
<td>1 running</td>
<td>正在运行的进程数</td>
</tr>
<tr>
<td>94 sleeping</td>
<td>睡眠的进程</td>
</tr>
<tr>
<td>0 stopped</td>
<td>正在停止的进程</td>
</tr>
<tr>
<td>0 zombie</td>
<td>僵尸进程。如果不是0，需要手工检查僵尸进程</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>Cpu(s):  0.1%us</th>
<th>用户模式占用的CPU百分比</th>
</tr>
</thead>
<tbody><tr>
<td>0.1%sy</td>
<td>系统模式占用的CPU百分比</td>
</tr>
<tr>
<td>0.0%ni</td>
<td>改变过优先级的用户进程占用的CPU百分比</td>
</tr>
<tr>
<td>99.7%id</td>
<td>空闲CPU的CPU百分比</td>
</tr>
<tr>
<td>0.1%wa</td>
<td>等待输入/输出的进程的占用CPU百分比</td>
</tr>
<tr>
<td>0.0%hi</td>
<td>硬中断请求服务占用的CPU百分比</td>
</tr>
<tr>
<td>0.1%si</td>
<td>软中断请求服务占用的CPU百分比</td>
</tr>
<tr>
<td>0.0%st</td>
<td>st（Steal  time）虚拟时间百分比。就是当有虚拟机时，虚拟CPU等待实际CPU的时间百分比。</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>Mem:   625344k total</th>
<th>物理内存的总量，单位KB</th>
</tr>
</thead>
<tbody><tr>
<td>571504k used</td>
<td>已经使用的物理内存数量</td>
</tr>
<tr>
<td>53840k free</td>
<td>空闲的物理内存数量，我们使用的是虚拟机，总共只分配了628MB内存，所以只有53MB的空闲内存了</td>
</tr>
<tr>
<td>65800k buffers</td>
<td>作为缓冲的内存数量</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>Swap:  524280k total</th>
<th>交换分区（虚拟内存）的总大小</th>
</tr>
</thead>
<tbody><tr>
<td>0k used</td>
<td>已经使用的交互分区的大小</td>
</tr>
<tr>
<td>524280k free</td>
<td>空闲交换分区的大小</td>
</tr>
<tr>
<td>409280k cached</td>
<td>作为缓存的交互分区的大小</td>
</tr>
</tbody></table>
<h3 id="netstat"><a href="#netstat" class="headerlink" title="netstat"></a>netstat</h3><p>显示网络统计信息和端口占用情况</p>
<p>netstat -anp | grep 进程号    （功能描述：查看该进程网络信息）</p>
<p>netstat -nlp | grep 端口号    （功能描述：查看网络端口号占用情况）</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-n</td>
<td>拒绝显示别名，能显示数字的全部转化成数字</td>
</tr>
<tr>
<td>-l</td>
<td>仅列出有在listen（监听）的服务状态</td>
</tr>
<tr>
<td>-p</td>
<td>表示显示哪个进程在调用</td>
</tr>
</tbody></table>
<h3 id="lsof"><a href="#lsof" class="headerlink" title="lsof"></a>lsof</h3><p>查看端口被哪个进程占用</p>
<p>lsof -i:44444</p>
<p>COMMAND  PID    USER   FD   TYPE DEVICE SIZE/OFF NODE NAME<br>java    7073 vincent  670u  IPv6  73392      0t0  TCP *:cognex-dataman (LISTEN)<br>java    7073 vincent  672u  IPv6  73395      0t0  TCP localhost:cognex-dataman-&gt;localhost:45638 (CLOSE_WAIT)<br>java    7073 vincent  729u  IPv6  68482      0t0  TCP linux1:cognex-dataman-&gt;linux1:56552 (CLOSE_WAIT)</p>
<h3 id="crontab"><a href="#crontab" class="headerlink" title="crontab"></a>crontab</h3><p>系统定时任务</p>
<p>crontab [选项]</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-e</td>
<td>编辑crontab定时任务</td>
</tr>
<tr>
<td>-l</td>
<td>查询crontab任务</td>
</tr>
<tr>
<td>-r</td>
<td>删除当前用户所有的crontab任务</td>
</tr>
</tbody></table>
<p>重新启动crond服务</p>
<p>service crond restart</p>
<p>crontab -e </p>
<table>
<thead>
<tr>
<th>项目</th>
<th>含义</th>
<th>范围</th>
</tr>
</thead>
<tbody><tr>
<td>第一个“*”</td>
<td>一小时当中的第几分钟</td>
<td>0-59</td>
</tr>
<tr>
<td>第二个“*”</td>
<td>一天当中的第几小时</td>
<td>0-23</td>
</tr>
<tr>
<td>第三个“*”</td>
<td>一个月当中的第几天</td>
<td>1-31</td>
</tr>
<tr>
<td>第四个“*”</td>
<td>一年当中的第几月</td>
<td>1-12</td>
</tr>
<tr>
<td>第五个“*”</td>
<td>一周当中的星期几</td>
<td>0-7（0和7都代表星期日）</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>特殊符号</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>*</td>
<td>代表任何时间。比如第一个“*”就代表一小时中每分钟都执行一次的意思。</td>
</tr>
<tr>
<td>，</td>
<td>代表不连续的时间。比如“0 8,12,16 * * * 命令”，就代表在每天的8点0分，12点0分，16点0分都执行一次命令</td>
</tr>
<tr>
<td>-</td>
<td>代表连续的时间范围。比如“0 5  *  *  1-6命令”，代表在周一到周六的凌晨5点0分执行命令</td>
</tr>
<tr>
<td>*/n</td>
<td>代表每隔多久执行一次。比如“*/10  *  *  *  *  命令”，代表每隔10分钟就执行一遍命令</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>时间</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>45 22 * * * 命令</td>
<td>在22点45分执行命令</td>
</tr>
<tr>
<td>0 17 * * 1 命令</td>
<td>每周1 的17点0分执行命令</td>
</tr>
<tr>
<td>0 5 1,15 * * 命令</td>
<td>每月1号和15号的凌晨5点0分执行命令</td>
</tr>
<tr>
<td>40 4 * * 1-5 命令</td>
<td>每周一到周五的凌晨4点40分执行命令</td>
</tr>
<tr>
<td>*/10 4 * * * 命令</td>
<td>每天的凌晨4点，每隔10分钟执行一次命令</td>
</tr>
<tr>
<td>0 0 1,15 * 1 命令</td>
<td>每月1号和15号，每周1的0点0分都会执行命令。注意：星期几和几号最好不要同时出现，因为他们定义的都是天。非常容易让管理员混乱。</td>
</tr>
</tbody></table>
<p>每隔1分钟，向/root/bailongma.txt文件中添加一个11的数字</p>
<p>*/1 * * * * /bin/echo ”11” &gt;&gt; /root/bailongma.txt</p>
<h2 id="软件包管理"><a href="#软件包管理" class="headerlink" title="软件包管理"></a>软件包管理</h2><h3 id="rpm"><a href="#rpm" class="headerlink" title="rpm"></a>rpm</h3><p>RedHat Package Manager</p>
<h4 id="rpm查询命令"><a href="#rpm查询命令" class="headerlink" title="rpm查询命令"></a>rpm查询命令</h4><p>rpm -qa                （功能描述：查询所安装的所有rpm软件包）</p>
<p>rpm -qa | grep rpm软件包</p>
<p>rpm -qa | grep firefox</p>
<h4 id="rpm卸载命令"><a href="#rpm卸载命令" class="headerlink" title="rpm卸载命令"></a>rpm卸载命令</h4><p>rpm -e RPM软件包</p>
<p>rpm -e –nodeps 软件包</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-e</td>
<td>卸载软件包</td>
</tr>
<tr>
<td>–nodeps</td>
<td>卸载软件时，不检查依赖。这样的话，那些使用该软件包的软件在此之后可能就不能正常工作了。</td>
</tr>
</tbody></table>
<h4 id="rpm安装命令"><a href="#rpm安装命令" class="headerlink" title="rpm安装命令"></a>rpm安装命令</h4><p>rpm -ivh RPM包全名</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-i</td>
<td>-i=install，安装</td>
</tr>
<tr>
<td>-v</td>
<td>-v=verbose，显示详细信息</td>
</tr>
<tr>
<td>-h</td>
<td>-h=hash，进度条</td>
</tr>
<tr>
<td>–nodeps</td>
<td>–nodeps，不检测依赖进度</td>
</tr>
</tbody></table>
<h3 id="yum"><a href="#yum" class="headerlink" title="yum"></a>yum</h3><p>Yellow dog Updater, Modified。是一个在Fedora和RedHat以及CentOS中的Shell前端软件包管理器。基于RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软件包，无须繁琐地一次次下载、安装。</p>
<p>yum [选项] [参数]</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-y</td>
<td>对所有提问都回答“yes”</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>参数</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>install</td>
<td>安装rpm软件包</td>
</tr>
<tr>
<td>update</td>
<td>更新rpm软件包</td>
</tr>
<tr>
<td>check-update</td>
<td>检查是否有可用的更新rpm软件包</td>
</tr>
<tr>
<td>remove</td>
<td>删除指定的rpm软件包</td>
</tr>
<tr>
<td>list</td>
<td>显示软件包信息</td>
</tr>
<tr>
<td>clean</td>
<td>清理yum过期的缓存</td>
</tr>
<tr>
<td>deplist</td>
<td>显示yum软件包的所有依赖关系</td>
</tr>
</tbody></table>
<p>yum -y install firefox.x86_64</p>
<h4 id="修改网络yum源"><a href="#修改网络yum源" class="headerlink" title="修改网络yum源"></a>修改网络yum源</h4><p>默认的系统YUM源，需要连接国外apache网站，网速比较慢，可以修改关联的网络YUM源为国内镜像的网站，比如网易163,aliyun等。</p>
<p>1）安装wget, wget用来从指定的URL下载文件</p>
<p>yum install wget</p>
<p>2）在/etc/yum.repos.d/目录下，备份默认的repos文件</p>
<p>cp CentOS-Base.repo  CentOS-Base.repo.backup</p>
<p>3）下载网易163或者是aliyun的repos文件,任选其一</p>
<p>wget <a href="http://mirrors.aliyun.com/repo/Centos-7.repo">http://mirrors.aliyun.com/repo/Centos-7.repo</a>  //阿里云</p>
<p>wget <a href="http://mirrors.163.com/.help/CentOS7-Base-163.repo">http://mirrors.163.com/.help/CentOS7-Base-163.repo</a> //网易163</p>
<p>4）使用下载好的repos文件替换默认的repos文件，例如:用CentOS7-Base-163.repo替换CentOS-Base.repo</p>
<p>mv CentOS7-Base-163.repo  CentOS-Base.repo</p>
<p>5）清理旧缓存数据，缓存新数据</p>
<p>yum clean all</p>
<p>yum makecache</p>
<p>6）测试</p>
<p>yum -y install firefox.x86_64</p>
<h2 id="安全拷贝和远程同步"><a href="#安全拷贝和远程同步" class="headerlink" title="安全拷贝和远程同步"></a>安全拷贝和远程同步</h2><h3 id="scp（secure-copy）安全拷贝"><a href="#scp（secure-copy）安全拷贝" class="headerlink" title="scp（secure copy）安全拷贝"></a>scp（secure copy）安全拷贝</h3><p>scp可以实现服务器与服务器之间的数据拷贝。</p>
<p>scp    -r        $pdir/$fname                   $user@linux$host:$pdir/$fname</p>
<p>命令  递归    要拷贝的文件路径/名称   目的用户@主机:目的路径/名称</p>
<p>在hadoop101上，将hadoop101中/opt/module目录下的软件拷贝到hadoop102上。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@hadoop101 /]$ scp -r /opt/module/applog/application.properties  vincent@linux2:/opt/module/applog/application.properties</span><br></pre></td></tr></table></figure>

<h3 id="rsync-远程同步工具"><a href="#rsync-远程同步工具" class="headerlink" title="rsync 远程同步工具"></a>rsync 远程同步工具</h3><p>rsync主要用于备份和镜像。具有速度快、避免复制相同内容和支持符号链接的优点。</p>
<p>rsync和scp区别：用rsync做文件的复制要比scp的速度快，rsync只对差异文件做更新。scp是把所有文件都复制过去。</p>
<p>rsync   -av              $pdir/$fname                     $user@hadoop$host:$pdir/$fname</p>
<p>命令     选项参数    要拷贝的文件路径/名称     目的用户@主机:目的路径/名称</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-a</td>
<td>归档拷贝</td>
</tr>
<tr>
<td>-v</td>
<td>显示复制过程</td>
</tr>
</tbody></table>
<p>把hadoop101机器上的/opt/software目录同步到hadoop102服务器的/opt/software目录下。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@hadoop101 opt]$ rsync -av /opt/software vincent@hadoop102:/opt /software</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>系统</category>
      </categories>
  </entry>
  <entry>
    <title>Shell</title>
    <url>/Shell/</url>
    <content><![CDATA[<h1 id="Shell解析器"><a href="#Shell解析器" class="headerlink" title="Shell解析器"></a>Shell解析器</h1><p>（1）Linux提供的Shell解析器有：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 ~]$ cat /etc/shells </span><br><span class="line">/bin/sh</span><br><span class="line">/bin/bash</span><br><span class="line">/sbin/nologin</span><br><span class="line">/usr/bin/sh</span><br><span class="line">/usr/bin/bash</span><br><span class="line">/usr/sbin/nologin</span><br><span class="line">/bin/tcsh</span><br><span class="line">/bin/csh</span><br></pre></td></tr></table></figure>

<p>（2）bash和sh的关系</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 bin]$ ll | grep bash</span><br><span class="line">-rwxr-xr-x. 1 root root 941880  5月  11 2016 bash</span><br><span class="line">lrwxrwxrwx. 1 root root       4  5月  27 2017 sh -&gt; bash</span><br></pre></td></tr></table></figure>

<p>sh是bash的一种特殊的模式，也就是 /bin/sh 相当于 /bin/bash –posix。说白了sh就是 开启了POSIX(可移植操作系统接口)标准的bash 。sh一般设成bash的软链。</p>
<p>（3）Centos默认的解析器是bash</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 bin]$ echo $SHELL</span><br><span class="line">/bin/bash</span><br></pre></td></tr></table></figure>

<h1 id="Shell脚本入门"><a href="#Shell脚本入门" class="headerlink" title="Shell脚本入门"></a>Shell脚本入门</h1><p>1．脚本格式</p>
<p>脚本以#!/bin/bash开头（指定解析器）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br></pre></td></tr></table></figure>

<p>2．脚本执行方式</p>
<p>第一种：采用 bash+脚本 或  sh+脚本 的相对路径或绝对路径（不用赋予脚本+x权限）</p>
<p>第二种：采用输入脚本的绝对路径或相对路径执行脚本（必须具有可执行权限+x）</p>
<p>注意：第一种执行方法，本质是bash解析器帮你执行脚本，所以脚本本身不需要执行权限。第二种执行方法，本质是脚本需要自己执行，所以需要执行权限。</p>
<p>第三种：在脚本的路径前加上“.”</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">(a) 有以下脚本</span><br><span class="line">[root@0725pc shells]# cat test1.sh </span><br><span class="line"><span class="meta">#</span><span class="bash">! /bin/bash</span></span><br><span class="line">A=&quot;hello&quot;</span><br><span class="line"></span><br><span class="line">(b) 分别使用sh,bash,./ 和 . 的方式来执行，结果如下：</span><br><span class="line">[root@0725pc shells]#  bash test1.sh </span><br><span class="line">[root@0725pc shells]# echo $A</span><br><span class="line"></span><br><span class="line">[root@0725pc shells]# sh test1.sh </span><br><span class="line">[root@0725pc shells]# echo $A</span><br><span class="line"></span><br><span class="line">[root@0725pc shells]# ./test1.sh </span><br><span class="line">[root@0725pc shells]# echo $A</span><br><span class="line"></span><br><span class="line">[root@0725pc shells]# . test1.sh </span><br><span class="line">[root@0725pc shells]# echo $A</span><br><span class="line">hello</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>原因：</p>
<p>前三种方式都是在当前shell中打开一个子shell来执行脚本内容，当脚本内容结束，则子shell关闭，回到父shell中。</p>
<p>第四种，也就是使用在脚本路径前加.的方式，可以使脚本内容在当前shell里执行，而无需打开子shell！</p>
<p>开子shell与不开子shell的区别就在于，环境变量的继承关系，如<strong>在子shell中设置的当前变量，父shell是不可见的</strong>。</p>
<h1 id="Shell中的变量"><a href="#Shell中的变量" class="headerlink" title="Shell中的变量"></a>Shell中的变量</h1><h2 id="系统变量"><a href="#系统变量" class="headerlink" title="系统变量"></a>系统变量</h2><ol>
<li>常用系统变量</li>
</ol>
<p>$HOME、$PWD、$SHELL等</p>
<ol start="2">
<li>案例实操</li>
</ol>
<p>1）查看系统变量的值</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ echo $HOME</span><br><span class="line">/home/vincent</span><br></pre></td></tr></table></figure>

<p>（2）显示当前Shell中所有变量：set</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ set</span><br><span class="line">BASH=/bin/bash</span><br><span class="line">BASH_ALIASES=()</span><br><span class="line">BASH_ARGC=()</span><br><span class="line">BASH_ARGV=()</span><br></pre></td></tr></table></figure>

<h2 id="自定义变量"><a href="#自定义变量" class="headerlink" title="自定义变量"></a>自定义变量</h2><p>1．基本语法</p>
<p>（1）定义变量：变量=值 </p>
<p>（2）撤销变量：unset 变量</p>
<p>（3）声明静态变量：readonly变量，注意：不能unset</p>
<p>2．变量定义规则</p>
<p>​    （1）变量名称可以由字母、数字和下划线组成，但是不能以数字开头，环境变量名建议大写。</p>
<p>​    （2）<strong>等号两侧不能有空格</strong></p>
<p>​    （3）在bash中，变量默认类型都是字符串类型，无法直接进行数值运算。</p>
<p>​    （4）变量的值如果有空格，需要使用双引号或单引号括起来。</p>
<p>3．案例实操</p>
<p>（1）定义变量A</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ A=5</span><br><span class="line">[vincent@linux1 datas]$ echo $A</span><br><span class="line">5</span><br></pre></td></tr></table></figure>

<p>（2）给变量A重新赋值</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ A=6</span><br><span class="line">[vincent@linux1 datas]$ echo $A</span><br><span class="line">6</span><br></pre></td></tr></table></figure>

<p>（3）撤销变量A</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ unset A</span><br><span class="line">[vincent@linux1 datas]$ echo $A</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>（4）声明静态的变量B=2，不能unset</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ readonly B=2</span><br><span class="line">[vincent@linux1 datas]$ echo $B</span><br><span class="line">2</span><br><span class="line">[vincent@linux1 datas]$ B=9</span><br><span class="line">-bash: B: readonly variable</span><br></pre></td></tr></table></figure>

<p>（5）在bash中，<strong>变量默认类型都是字符串类型</strong>，无法直接进行数值运算</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 ~]$ C=1+2</span><br><span class="line">[vincent@linux1 ~]$ echo $C</span><br><span class="line">1+2</span><br></pre></td></tr></table></figure>

<p>（6）变量的值如果有空格，需要使用双引号或单引号括起来</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 ~]$ D=I love banzhang</span><br><span class="line">-bash: world: command not found</span><br><span class="line">[vincent@linux1 ~]$ D=&quot;I love banzhang&quot;</span><br><span class="line">[vincent@linux1 ~]$ echo $A</span><br><span class="line">I love banzhang</span><br></pre></td></tr></table></figure>

<p>（7）可把变量提升为全局环境变量，可供其他Shell程序使用</p>
<p>export 变量名</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ vim helloworld.sh </span><br><span class="line"></span><br><span class="line">在helloworld.sh文件中增加echo $B</span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">echo &quot;helloworld&quot;</span><br><span class="line">echo $B</span><br><span class="line"></span><br><span class="line">[vincent@linux1 datas]$ ./helloworld.sh </span><br><span class="line">Helloworld</span><br></pre></td></tr></table></figure>

<p>发现并没有打印输出变量B的值。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ export B</span><br><span class="line">[vincent@linux1 datas]$ ./helloworld.sh </span><br><span class="line">helloworld</span><br><span class="line">2</span><br></pre></td></tr></table></figure>

<h2 id="特殊变量：-n"><a href="#特殊变量：-n" class="headerlink" title="特殊变量：$n"></a>特殊变量：$n</h2><p>1．基本语法</p>
<p>​    $n  （功能描述：n为数字，**$0代表该脚本名称**，$1-$9代表第一到第九个参数，十以上的参数，十以上的参数需要用大括号包含，如${10}）</p>
<p>2．案例实操</p>
<p>（1）输出该脚本文件名称、输入参数1和输入参数2的值</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ touch parameter.sh </span><br><span class="line">[vincent@linux1 datas]$ vim parameter.sh</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">echo &quot;$0 $1 $2&quot;</span><br><span class="line"></span><br><span class="line">[vincent@linux1 datas]$ chmod 777 parameter.sh</span><br><span class="line"></span><br><span class="line">[vincent@linux1 datas]$ ./parameter.sh cls  xz</span><br><span class="line">./parameter.sh  cls   xz</span><br></pre></td></tr></table></figure>

<h2 id="特殊变量："><a href="#特殊变量：" class="headerlink" title="特殊变量：$#"></a>特殊变量：$#</h2><p>1．基本语法</p>
<p>​    $#  （功能描述：获取所有<strong>输入参数个数</strong>，常用于循环）。</p>
<p>2．案例实操</p>
<p>（1）获取输入参数的个数</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ vim parameter.sh</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">echo &quot;$0 $1 $2&quot;</span><br><span class="line">echo $#</span><br><span class="line"></span><br><span class="line">[vincent@linux1 datas]$ chmod 777 parameter.sh</span><br><span class="line"></span><br><span class="line">[vincent@linux1 datas]$ ./parameter.sh cls  xz</span><br><span class="line">parameter.sh cls xz </span><br><span class="line">2</span><br></pre></td></tr></table></figure>

<h2 id="特殊变量：-、"><a href="#特殊变量：-、" class="headerlink" title="特殊变量：$*、$@"></a>特殊变量：$*、$@</h2><p>1．基本语法</p>
<p>​    $*  （功能描述：这个变量代表命令行中所有的参数，$*把所有的参数看成<strong>一个整体</strong>）</p>
<p>​    $@ （功能描述：这个变量也代表命令行中所有的参数，不过$@把每个参数<strong>区分对待</strong>）</p>
<p>2．案例实操</p>
<p>（1）打印输入的所有参数</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ vim parameter.sh</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">echo &quot;$0 $1 $2&quot;</span><br><span class="line">echo $#</span><br><span class="line">echo $*</span><br><span class="line">echo $@</span><br><span class="line"></span><br><span class="line">[vincent@linux1 datas]$ bash parameter.sh 1 2 3</span><br><span class="line">parameter.sh  1   2</span><br><span class="line">3</span><br><span class="line">1 2 3</span><br><span class="line">1 2 3</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="特殊变量：-？"><a href="#特殊变量：-？" class="headerlink" title="特殊变量：$？"></a>特殊变量：$？</h2><p>1．基本语法</p>
<p>$？ （功能描述：<strong>最后一次执行的命令的返回状态</strong>。如果这个<strong>变量的值为0，证明上一个命令正确执行</strong>；如果这个变量的值为非0（具体是哪个数，由命令自己来决定），则证明上一个命令执行不正确了。）</p>
<p>2．案例实操</p>
<p>​    （1）判断helloworld.sh脚本是否正确执行</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ ./helloworld.sh </span><br><span class="line">hello world</span><br><span class="line">[vincent@linux1 datas]$ echo $?</span><br><span class="line">0</span><br></pre></td></tr></table></figure>

<p>（2）删除目录下文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd mytmp</span><br><span class="line">if (( $? == 0 )); then rm * ; fi</span><br><span class="line">或者</span><br><span class="line">if cd mytmp; then rm * ; fi</span><br></pre></td></tr></table></figure>

<h1 id="运算符"><a href="#运算符" class="headerlink" title="运算符"></a>运算符</h1><p>1．基本语法</p>
<p>（1）“**$((运算式))<strong>”或</strong>“$[运算式]”**</p>
<p>（2）expr  + , - , *, /, %  加，减，乘，除，取余</p>
<p>   (3)用 <code>$(( ))</code> 或 <code>let</code> 进行整数运算</p>
<p>注意：expr运算符间要有<strong>空格</strong></p>
<p>2．案例实操：</p>
<p>（1）计算3+2的值</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ expr 2 + 3</span><br><span class="line">5</span><br></pre></td></tr></table></figure>

<p>（2）计算 (2+3)*4 的值</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">expr一步完成计算</span><br><span class="line">[vincent@linux1 datas]$ expr `expr 2 + 3` \* 4</span><br><span class="line">20</span><br><span class="line"></span><br><span class="line">采用$[运算式]方式</span><br><span class="line">[vincent@linux1 datas]$ S=$[(2+3)*4]</span><br><span class="line">[vincent@linux1 datas]$ echo $S</span><br><span class="line">[vincent@linux1 datas]$ COUNT=$((COUNT + 5 + MAX * 2))</span><br><span class="line">[vincent@linux1 datas]$ let COUNT+=&#x27;5+MAX*2&#x27;</span><br></pre></td></tr></table></figure>

<h1 id="条件判断"><a href="#条件判断" class="headerlink" title="条件判断"></a>条件判断</h1><ol>
<li>基本语法</li>
</ol>
<p>[ condition ]<strong>（注意condition前后要有空格）</strong></p>
<p>或者</p>
<p>((condition ))</p>
<p><strong>注意：条件非空即为true，[ vincent ]返回true，[] 返回false。</strong></p>
<ol start="2">
<li>常用判断条件</li>
</ol>
<p>（1）两个整数之间比较</p>
<figure class="highlight asciidoc"><table><tr><td class="code"><pre><span class="line"><span class="section">= 字符串比较</span></span><br><span class="line"></span><br><span class="line">-lt 小于（less than）         -le 小于等于（less equal）</span><br><span class="line"></span><br><span class="line">-eq 等于（equal）           -gt 大于（greater than）</span><br><span class="line"></span><br><span class="line">-ge 大于等于（greater equal）  -ne 不等于（Not equal）</span><br></pre></td></tr></table></figure>

<p>（2）按照文件权限进行判断</p>
<figure class="highlight lua"><table><tr><td class="code"><pre><span class="line">-r 有读的权限（<span class="built_in">read</span>）       -w 有写的权限（<span class="built_in">write</span>）</span><br><span class="line"></span><br><span class="line">-x 有执行的权限（<span class="built_in">execute</span>）</span><br></pre></td></tr></table></figure>

<p>（3）按照文件类型进行判断</p>
<figure class="highlight diff"><table><tr><td class="code"><pre><span class="line"><span class="deletion">-f 文件存在并且是一个常规的文件（file）</span></span><br><span class="line"></span><br><span class="line"><span class="deletion">-e 文件存在（existence）      -d 文件存在并是一个目录（directory）</span></span><br></pre></td></tr></table></figure>

<ol start="3">
<li>案例实操</li>
</ol>
<p>（1）23是否大于等于22</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ [ 23 -ge 22 ]</span><br><span class="line">[vincent@linux1 datas]$ echo $?</span><br><span class="line">0</span><br></pre></td></tr></table></figure>

<p>（2）helloworld.sh是否具有写权限</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ [ -w helloworld.sh ]</span><br><span class="line">[vincent@linux1 datas]$ echo $?</span><br><span class="line">0</span><br></pre></td></tr></table></figure>

<p>（3）/home/vincent/cls.txt目录中的文件是否存在</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ [ -e /home/vincent/cls.txt ]</span><br><span class="line">[vincent@linux1 datas]$ echo $?</span><br><span class="line">1</span><br></pre></td></tr></table></figure>

<p>（4）多条件判断（**&amp;&amp; 表示前一条命令执行成功时，才执行后一条命令，|| 表示上一条命令执行失败后，才执行下一条命令**）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 ~]$ [ condition ] &amp;&amp; echo OK || echo notok</span><br><span class="line">OK</span><br><span class="line">[vincent@linux1 datas]$ [ condition ] &amp;&amp; [ ] || echo notok</span><br><span class="line">notok</span><br></pre></td></tr></table></figure>

<h1 id="流程控制"><a href="#流程控制" class="headerlink" title="流程控制"></a>流程控制</h1><h2 id="if-判断"><a href="#if-判断" class="headerlink" title="if 判断"></a>if 判断</h2><p>1．基本语法</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">if condition</span><br><span class="line">then</span><br><span class="line">    command1 </span><br><span class="line">    command2</span><br><span class="line">    ...</span><br><span class="line">    commandN </span><br><span class="line">fi</span><br><span class="line">一行:	if [ $(ps -ef | grep -c &quot;ssh&quot;) -gt 1 ]; then echo &quot;true&quot;; fi</span><br><span class="line"></span><br><span class="line">if condition</span><br><span class="line">then</span><br><span class="line">    command1 </span><br><span class="line">    command2</span><br><span class="line">    ...</span><br><span class="line">    commandN</span><br><span class="line">else</span><br><span class="line">    command</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if condition1</span><br><span class="line">then</span><br><span class="line">    command1</span><br><span class="line">elif condition2 </span><br><span class="line">then </span><br><span class="line">    command2</span><br><span class="line">else</span><br><span class="line">    commandN</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<p>​    注意事项：</p>
<p>（1）**[ 条件判断式 ]<strong>，中括号和条件判断式之间必须有</strong>空格**</p>
<p>（2）<strong>if后要有空格</strong></p>
<p>2．案例实操</p>
<p>（1）输入一个数字，如果是1，则输出banzhang zhen shuai，如果是2，则输出 cls zhen mei，如果是其它，什么也不输出。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ touch if.sh</span><br><span class="line">[vincent@linux1 datas]$ vim if.sh</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">if [ $1 -eq &quot;1&quot; ]</span><br><span class="line">then</span><br><span class="line">        echo &quot;banzhang zhen shuai&quot;</span><br><span class="line">elif [ $1 -eq &quot;2&quot; ]</span><br><span class="line">then</span><br><span class="line">        echo &quot;cls zhen mei&quot;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">[vincent@linux1 datas]$ chmod 777 if.sh </span><br><span class="line">[vincent@linux1 datas]$ ./if.sh 1</span><br><span class="line">banzhang zhen shuai</span><br></pre></td></tr></table></figure>

<h2 id="case-语句"><a href="#case-语句" class="headerlink" title="case 语句"></a>case 语句</h2><p>1．基本语法</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">case $变量名 in </span><br><span class="line">  &quot;值1&quot;)</span><br><span class="line">    如果变量的值等于值1，则执行程序1 </span><br><span class="line">    ;; </span><br><span class="line">  &quot;值2&quot;)</span><br><span class="line">    如果变量的值等于值2，则执行程序2 </span><br><span class="line">    ;; </span><br><span class="line">  …省略其他分支… </span><br><span class="line">  *)</span><br><span class="line">    如果变量的值都不是以上的值，则执行此程序 </span><br><span class="line">    ;; </span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<p>注意事项：</p>
<ol>
<li><p>  case行尾必须为单词in，每一个模式匹配必须以右括号)结束。</p>
</li>
<li><p>  双分号;;表示命令序列结束，相当于java中的break。</p>
</li>
<li><p>  最后的*)表示默认模式，相当于java中的default。</p>
</li>
</ol>
<p>2．案例实操</p>
<p>（1）输入一个数字，如果是1，则输出banzhang，如果是2，则输出cls，如果是其它，输出renyao。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;1&quot;)</span><br><span class="line">	echo &quot;banzhang&quot;</span><br><span class="line">	;;</span><br><span class="line">&quot;2&quot;)</span><br><span class="line">	echo &quot;cls&quot;</span><br><span class="line">	;;</span><br><span class="line">*)</span><br><span class="line">	echo &quot;renyao&quot;</span><br><span class="line">	;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<h2 id="for-循环"><a href="#for-循环" class="headerlink" title="for 循环"></a>for 循环</h2><p>1．基本语法1</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for ((初始值;循环控制条件;变量变化)) </span><br><span class="line">  do </span><br><span class="line">    程序 </span><br><span class="line">  done</span><br></pre></td></tr></table></figure>

<p>2．案例实操</p>
<p>（1）从1加到100</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">s=0</span><br><span class="line">for ((i=1;i&lt;=100;i=i+1))</span><br><span class="line">	do</span><br><span class="line">		s=$[$i+$s]</span><br><span class="line">	done</span><br><span class="line">echo $s</span><br></pre></td></tr></table></figure>

<p>3．基本语法2</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for 变量 in 值1 值2 值3… </span><br><span class="line">  do </span><br><span class="line">    程序 </span><br><span class="line">  done</span><br></pre></td></tr></table></figure>

<p>4．案例实操</p>
<p>​    （1）打印所有输入参数</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ vim for.sh</span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span> </span><br><span class="line"></span><br><span class="line">for i in $*</span><br><span class="line">do</span><br><span class="line">      echo &quot;ban zhang love $i &quot;</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">for j in $@</span><br><span class="line">do      </span><br><span class="line">        echo &quot;ban zhang love $j&quot;</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">[vincent@linux1 datas]$ bash for.sh cls xz bd</span><br><span class="line">ban zhang love cls </span><br><span class="line">ban zhang love xz </span><br><span class="line">ban zhang love bd </span><br><span class="line">ban zhang love cls</span><br><span class="line">ban zhang love xz</span><br><span class="line">ban zhang love bd</span><br></pre></td></tr></table></figure>

<p>（2）比较$*和$@区别</p>
<p>（a）$*和$@都表示传递给函数或脚本的所有参数，不被双引号“”包含时，都以$1 $2 …$n的形式输出所有参数。</p>
<p>（b）当它们被双引号“”包含时，“$*”会将所有的参数作为一个整体，以“$1 $2 …$n”的形式输出所有参数；“$@”会将各个参数分开，以“$1” “$2”…”$n”的形式输出所有参数。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ vim for.sh</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span> </span><br><span class="line"></span><br><span class="line">for i in &quot;$*&quot; </span><br><span class="line"><span class="meta">#</span><span class="bash">$*中的所有参数看成是一个整体，所以这个<span class="keyword">for</span>循环只会循环一次</span> </span><br><span class="line">        do </span><br><span class="line">                echo &quot;ban zhang love $i&quot;</span><br><span class="line">        done </span><br><span class="line"></span><br><span class="line">for j in &quot;$@&quot; </span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="variable">$@</span>中的每个参数都看成是独立的，所以“<span class="variable">$@</span>”中有几个参数，就会循环几次</span> </span><br><span class="line">        do </span><br><span class="line">                echo &quot;ban zhang love $j&quot; </span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">[vincent@linux1 datas]$ chmod 777 for.sh</span><br><span class="line">[vincent@linux1 datas]$ bash for.sh cls xz bd</span><br><span class="line">ban zhang love cls xz bd</span><br><span class="line">ban zhang love cls</span><br><span class="line">ban zhang love xz</span><br><span class="line">ban zhang love bd</span><br></pre></td></tr></table></figure>

<h2 id="while-循环"><a href="#while-循环" class="headerlink" title="while 循环"></a>while 循环</h2><p>1．基本语法</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">while [ 条件判断式 ] 或者 ((条件判断式))</span><br><span class="line">  do </span><br><span class="line">    程序</span><br><span class="line">  done</span><br><span class="line">注意: while 后面需要有空格</span><br></pre></td></tr></table></figure>

<p>2．案例实操</p>
<p>​    （1）从1加到100</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">s=0</span><br><span class="line">i=1</span><br><span class="line">while [ i -le 100 ]</span><br><span class="line">	do</span><br><span class="line">   s=$[$s+$i]</span><br><span class="line">   i=$[$i+1]</span><br><span class="line">  done</span><br><span class="line">echo $s</span><br></pre></td></tr></table></figure>

<h1 id="read读取控制台输入"><a href="#read读取控制台输入" class="headerlink" title="read读取控制台输入"></a>read读取控制台输入</h1><p>1．基本语法</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">read(选项)(参数)   </span><br><span class="line">	选项：</span><br><span class="line">-p：指定读取值时的提示符；</span><br><span class="line">-t：指定读取值时等待的时间（秒）。</span><br><span class="line">参数</span><br><span class="line">	变量：指定读取值的变量名</span><br></pre></td></tr></table></figure>

<p>2．案例实操</p>
<p>​    （1）提示7秒内，读取控制台输入的名称</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ vim read.sh</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">read -t 7 -p &quot;Enter your name in 7 seconds &quot; NAME</span><br><span class="line">echo $NAME</span><br><span class="line"></span><br><span class="line">[vincent@linux1 datas]$ ./read.sh </span><br><span class="line">Enter your name in 7 seconds xiaoze</span><br><span class="line">xiaoze</span><br></pre></td></tr></table></figure>

<h1 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h1><h2 id="系统函数"><a href="#系统函数" class="headerlink" title="系统函数"></a>系统函数</h2><ol>
<li>basename基本语法</li>
</ol>
<figure class="highlight mel"><table><tr><td class="code"><pre><span class="line"><span class="keyword">basename</span> [<span class="keyword">string</span> / pathname] [suffix]  	（功能描述：<span class="keyword">basename</span>命令会删掉所有的前缀包括最后一个（‘/’）字符，然后将字符串显示出来。</span><br><span class="line">选项：</span><br><span class="line">suffix为后缀，如果suffix被指定了，<span class="keyword">basename</span>会将pathname或<span class="keyword">string</span>中的suffix去掉。</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>案例实操</li>
</ol>
<p>（1）截取该/home/vincent/banzhang.txt路径的文件名称</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ basename /home/vincent/banzhang.txt </span><br><span class="line">banzhang.txt</span><br><span class="line">[vincent@linux1 datas]$ basename /home/vincent/banzhang.txt .txt</span><br><span class="line">banzhang</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>dirname基本语法</li>
</ol>
<figure class="highlight mel"><table><tr><td class="code"><pre><span class="line"><span class="keyword">dirname</span> 文件绝对路径		（功能描述：从给定的包含绝对路径的文件名中去除文件名（非目录的部分），然后返回剩下的路径（目录的部分））</span><br></pre></td></tr></table></figure>

<p>4．案例实操</p>
<p>（1）获取banzhang.txt文件的路径</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 ~]$ dirname /home/vincent/banzhang.txt </span><br><span class="line">/home/vincent</span><br></pre></td></tr></table></figure>

<h2 id="自定义函数"><a href="#自定义函数" class="headerlink" title="自定义函数"></a>自定义函数</h2><p>1．基本语法</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[ function ] funname[()]</span><br><span class="line">&#123;</span><br><span class="line">	Action;</span><br><span class="line">	[return int;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>2．经验技巧</p>
<p>​    （1）必须在调用函数地方之前，<strong>先声明函数</strong>，shell脚本是<strong>逐行运行</strong>。不会像其它语言一样先编译。</p>
<p>​    （2）函数返回值，只能通过$?系统变量获得，可以显示加：return返回，如果不加，将以最后一条命令运行结果，作为返回值。</p>
<p>3．案例实操</p>
<p>​    （1）计算两个输入参数的和</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ touch fun.sh</span><br><span class="line">[vincent@linux1 datas]$ vim fun.sh</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">function sum()</span><br><span class="line">&#123;</span><br><span class="line">    s=0</span><br><span class="line">    s=$[$1+$2]</span><br><span class="line">    echo &quot;$s&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">read -p &quot;Please input the number1: &quot; n1;</span><br><span class="line">read -p &quot;Please input the number2: &quot; n2;</span><br><span class="line">sum $n1 $n2;</span><br><span class="line"></span><br><span class="line">[vincent@linux1 datas]$ chmod 777 fun.sh</span><br><span class="line">[vincent@linux1 datas]$ ./fun.sh </span><br><span class="line">Please input the number1: 2</span><br><span class="line">Please input the number2: 5</span><br><span class="line">7</span><br><span class="line"></span><br><span class="line">funWithReturn()&#123;</span><br><span class="line">    echo &quot;这个函数会对输入的两个数字进行相加运算...&quot;</span><br><span class="line">    echo &quot;输入第一个数字: &quot;</span><br><span class="line">    read aNum</span><br><span class="line">    echo &quot;输入第二个数字: &quot;</span><br><span class="line">    read anotherNum</span><br><span class="line">    echo &quot;两个数字分别为 $aNum 和 $anotherNum !&quot;</span><br><span class="line">    return $(($aNum+$anotherNum))</span><br><span class="line">&#125;</span><br><span class="line">funWithReturn</span><br><span class="line">echo &quot;输入的两个数字之和为 $? !&quot;</span><br><span class="line"></span><br><span class="line">这个函数会对输入的两个数字进行相加运算...</span><br><span class="line">输入第一个数字: </span><br><span class="line">1</span><br><span class="line">输入第二个数字: </span><br><span class="line">2</span><br><span class="line">两个数字分别为 1 和 2 !</span><br><span class="line">输入的两个数字之和为 3 !</span><br></pre></td></tr></table></figure>

<h1 id="Shell-输入-输出重定向"><a href="#Shell-输入-输出重定向" class="headerlink" title="Shell 输入/输出重定向"></a>Shell 输入/输出重定向</h1><table>
<thead>
<tr>
<th align="left">命令</th>
<th align="left">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="left">command &gt; file</td>
<td align="left">将输出重定向到 file。</td>
</tr>
<tr>
<td align="left">command &lt; file</td>
<td align="left">将输入重定向到 file。</td>
</tr>
<tr>
<td align="left">command &gt;&gt; file</td>
<td align="left">将输出以追加的方式重定向到 file。</td>
</tr>
<tr>
<td align="left">n &gt; file</td>
<td align="left">将文件描述符为 n 的文件重定向到 file。</td>
</tr>
<tr>
<td align="left">n &gt;&gt; file</td>
<td align="left">将文件描述符为 n 的文件以追加的方式重定向到 file。</td>
</tr>
<tr>
<td align="left">n &gt;&amp; m</td>
<td align="left">将输出文件 m 和 n 合并。</td>
</tr>
<tr>
<td align="left">n &lt;&amp; m</td>
<td align="left">将输入文件 m 和 n 合并。</td>
</tr>
<tr>
<td align="left">&lt;&lt; tag</td>
<td align="left">将开始标记 tag 和结束标记 tag 之间的内容作为输入。</td>
</tr>
</tbody></table>
<p><strong>需要注意的是文件描述符 0 通常是标准输入（STDIN），1 是标准输出（STDOUT），2 是标准错误输出（STDERR）</strong></p>
<p>一般情况下，每个 Unix/Linux 命令运行时都会打开三个文件：</p>
<ul>
<li>标准输入文件(stdin)：stdin的文件描述符为0，Unix程序默认从stdin读取数据。</li>
<li>标准输出文件(stdout)：stdout 的文件描述符为1，Unix程序默认向stdout输出数据。</li>
<li>标准错误文件(stderr)：stderr的文件描述符为2，Unix程序会向stderr流中写入错误信息。</li>
</ul>
<p>默认情况下，command &gt; file 将 stdout 重定向到 file，command &lt; file 将stdin 重定向到 file。</p>
<p>如果希望 stderr 重定向到 file，可以这样写：</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line">$ <span class="keyword">command</span> <span class="number">2</span>&gt;<span class="keyword">file</span></span><br></pre></td></tr></table></figure>

<p>如果希望 stderr 追加到 file 文件末尾，可以这样写：</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line">$ <span class="keyword">command</span> <span class="number">2</span>&gt;&gt;<span class="keyword">file</span></span><br></pre></td></tr></table></figure>

<p>如果希望将 stdout 和 stderr 合并后重定向到 file，可以这样写：</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line">$ <span class="keyword">command</span> &gt; <span class="keyword">file</span> <span class="number">2</span>&gt;&amp;<span class="number">1</span></span><br><span class="line"></span><br><span class="line">或者</span><br><span class="line"></span><br><span class="line">$ <span class="keyword">command</span> &gt;&gt; <span class="keyword">file</span> <span class="number">2</span>&gt;&amp;<span class="number">1</span></span><br></pre></td></tr></table></figure>

<p>如果希望对 stdin 和 stdout 都重定向，可以这样写：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">command</span> &lt; file1 &gt;file2</span></span><br></pre></td></tr></table></figure>

<p><strong>/dev/null 文件</strong></p>
<p>如果希望执行某个命令，但又不希望在屏幕上显示输出结果，那么可以将输出重定向到 /dev/null：</p>
<figure class="highlight arcade"><table><tr><td class="code"><pre><span class="line">$ command &gt; <span class="regexp">/dev/</span><span class="literal">null</span></span><br></pre></td></tr></table></figure>

<p>/dev/null 是一个特殊的文件，写入到它的内容都会被丢弃；如果尝试从该文件读取内容，那么什么也读不到。但是 /dev/null 文件非常有用，将命令的输出重定向到它，会起到”禁止输出”的效果。</p>
<p>如果希望屏蔽 stdout 和 stderr，可以这样写：</p>
<figure class="highlight arcade"><table><tr><td class="code"><pre><span class="line">$ command &gt; <span class="regexp">/dev/</span><span class="literal">null</span> <span class="number">2</span>&gt;&amp;<span class="number">1</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p><strong>注意：</strong>0 是标准输入（STDIN），1 是标准输出（STDOUT），2 是标准错误输出（STDERR）。</p>
<p>这里的 <strong>2</strong> 和 <strong>&gt;</strong> 之间不可以有空格，<strong>2&gt;</strong> 是一体的时候才表示错误输出。</p>
</blockquote>
<p><strong>Here Document</strong></p>
<p>Here Document 是 Shell 中的一种特殊的重定向方式，用来将输入重定向到一个交互式 Shell 脚本或程序。</p>
<p>它的基本的形式如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">command</span> &lt;&lt; <span class="string">delimiter</span></span><br><span class="line"><span class="string">    document</span></span><br><span class="line"><span class="string">delimiter</span></span><br></pre></td></tr></table></figure>

<p>它的作用是将两个 delimiter 之间的内容(document) 作为输入传递给 command。</p>
<p>注意：</p>
<p>结尾的delimiter 一定要顶格写，前面不能有任何字符，后面也不能有任何字符，包括空格和 tab 缩进。</p>
<p>开始的delimiter前后的空格会被忽略掉。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> wc -l &lt;&lt; <span class="string">EOF</span></span></span><br><span class="line">    欢迎来到</span><br><span class="line">    菜鸟教程</span><br><span class="line">    www.runoob.com</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">3          # 输出结果为 3 行</span><br></pre></td></tr></table></figure>

<h1 id="Shell工具"><a href="#Shell工具" class="headerlink" title="Shell工具"></a>Shell工具</h1><h2 id="cut"><a href="#cut" class="headerlink" title="cut"></a>cut</h2><p>cut的工作就是“剪”，具体的说就是在文件中负责剪切数据用的。cut 命令从文件的每一行剪切字节、字符和字段并将这些字节、字符和字段输出。</p>
<p>1.基本用法</p>
<figure class="highlight coq"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cut</span> [选项参数]  filename</span><br><span class="line">说明：默认分隔符是制表符</span><br></pre></td></tr></table></figure>

<p>2.选项参数说明</p>
<table>
<thead>
<tr>
<th>选项参数</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-f</td>
<td>列号，提取第几列  （-n：1到n列，n-：n到最后一列）</td>
</tr>
<tr>
<td>-d</td>
<td>分隔符，按照指定分隔符分割列</td>
</tr>
</tbody></table>
<p>3.案例实操</p>
<p>（0）数据准备</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ vim cut.txt</span><br><span class="line">dong shen</span><br><span class="line">guan zhen</span><br><span class="line">wo  wo</span><br><span class="line">lai  lai</span><br><span class="line">le  le</span><br></pre></td></tr></table></figure>

<p>（1）切割cut.txt第一列</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ cut -d &quot; &quot; -f 1 cut.txt </span><br><span class="line">dong</span><br><span class="line">guan</span><br><span class="line">wo</span><br><span class="line">lai</span><br><span class="line">le</span><br></pre></td></tr></table></figure>

<p>（2）切割cut.txt第二、三列</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ cut -d &quot; &quot; -f 2,3 cut.txt </span><br><span class="line">shen</span><br><span class="line">zhen</span><br><span class="line"> wo</span><br><span class="line"> lai</span><br><span class="line"> le</span><br></pre></td></tr></table></figure>

<p>（3）在cut.txt文件中切割出guan</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ cat cut.txt | grep &quot;guan&quot; | cut -d &quot; &quot; -f 1</span><br><span class="line">guan</span><br></pre></td></tr></table></figure>

<p>（4）选取系统PATH变量值，第2个“：”开始后的所有路径：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ echo $PATH</span><br><span class="line">/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/vincent/bin</span><br><span class="line"></span><br><span class="line">[vincent@linux1 datas]$ echo $PATH | cut -d : -f 2-</span><br><span class="line">/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/vincent/bin</span><br></pre></td></tr></table></figure>

<h2 id="sed"><a href="#sed" class="headerlink" title="sed"></a>sed</h2><p>sed是一种<strong>流编辑器，它一次处理一行内容</strong>。处理时，把当前处理的行存储在临时缓冲区中，称为“模式空间”，接着用sed命令处理缓冲区中的内容，处理完成后，把缓冲区的内容送往屏幕。接着处理下一行，这样不断重复，直到文件末尾。文件内容并没有改变，除非你使用重定向存储输出。</p>
<ol>
<li>基本用法</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sed [选项参数]  ‘<span class="built_in">command</span>’ filename</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>选项参数说明</li>
</ol>
<figure class="highlight diff"><table><tr><td class="code"><pre><span class="line"><span class="deletion">-e : 直接在指令列模式上进行sed的动作编辑。</span></span><br><span class="line"><span class="deletion">-n ：使用安静(silent)模式。在一般 sed 的用法中，所有来自 STDIN 的数据一般都会被列出到终端上。但如果加上 -n 参数后，则只有经过sed 特殊处理的那一行(或者动作)才会被列出来。</span></span><br><span class="line"><span class="deletion">-f ：直接将 sed 的动作写在一个文件内， -f filename 则可以运行 filename 内的 sed 动作。</span></span><br><span class="line"><span class="deletion">-r ：sed 的动作支持的是延伸型正规表示法的语法。(默认是基础正规表示法语法)</span></span><br><span class="line"><span class="deletion">-i ：直接修改读取的文件内容，而不是输出到终端。</span></span><br></pre></td></tr></table></figure>

<ol start="3">
<li>命令功能描述</li>
</ol>
<p>表10-3</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>功能描述</th>
</tr>
</thead>
<tbody><tr>
<td><em>a</em></td>
<td>新增，a的后面可以接字串，在下一行出现</td>
</tr>
<tr>
<td>d</td>
<td>删除</td>
</tr>
<tr>
<td>s</td>
<td>查找并替换</td>
</tr>
</tbody></table>
<ol start="4">
<li>案例实操</li>
</ol>
<p>（0）数据准备</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ vim sed.txt</span><br><span class="line">dong shen</span><br><span class="line">guan zhen</span><br><span class="line">wo  wo</span><br><span class="line">lai  lai</span><br><span class="line"></span><br><span class="line">le  le</span><br></pre></td></tr></table></figure>

<p>（1）将“mei nv”这个单词插入到sed.txt第二行下，打印</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ sed &#x27;2a mei nv&#x27; sed.txt </span><br><span class="line">dong shen</span><br><span class="line">guan zhen</span><br><span class="line">mei nv</span><br><span class="line">wo  wo</span><br><span class="line">lai  lai</span><br><span class="line"></span><br><span class="line">le  le</span><br><span class="line">[vincent@linux1 datas]$ cat sed.txt </span><br><span class="line">dong shen</span><br><span class="line">guan zhen</span><br><span class="line">wo  wo</span><br><span class="line">lai  lai</span><br><span class="line"></span><br><span class="line">le  le</span><br></pre></td></tr></table></figure>

<p>注意：文件并没有改变</p>
<p>（2）删除sed.txt文件所有包含wo的行</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ sed &#x27;/wo/d&#x27; sed.txt</span><br><span class="line">dong shen</span><br><span class="line">guan zhen</span><br><span class="line">lai  lai</span><br><span class="line"></span><br><span class="line">le  le</span><br></pre></td></tr></table></figure>

<p>（3）将sed.txt文件中wo替换为ni</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ sed &#x27;s/wo/ni/g&#x27; sed.txt </span><br><span class="line">dong shen</span><br><span class="line">guan zhen</span><br><span class="line">ni  ni</span><br><span class="line">lai  lai</span><br><span class="line"></span><br><span class="line">le  le</span><br></pre></td></tr></table></figure>

<p><strong>注意：‘g’表示global，全部替换</strong></p>
<p>（4）将sed.txt文件中的第二行删除并将wo替换为ni</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ sed -e &#x27;2d&#x27; -e &#x27;s/wo/ni/g&#x27; sed.txt </span><br><span class="line">dong shen</span><br><span class="line">ni  ni </span><br><span class="line">lai  lai</span><br><span class="line"></span><br><span class="line">le  le</span><br></pre></td></tr></table></figure>

<h2 id="awk"><a href="#awk" class="headerlink" title="awk"></a>awk</h2><p><a href="https://www.w3cschool.cn/awk/">https://www.w3cschool.cn/awk/</a></p>
<p>一个强大的文本分析工具，<strong>把文件逐行的读入，以空格为默认分隔符将每行切片</strong>，切开的部分再进行分析处理。</p>
<h3 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">命令行</span><br><span class="line">awk [options] file ...</span><br><span class="line"></span><br><span class="line">awk程序文件</span><br><span class="line">awk [option] -f file ....</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk --help</span><br><span class="line"></span><br><span class="line">Usage: awk [POSIX or GNU style options] -f progfile [--] file ...</span><br><span class="line">Usage: awk [POSIX or GNU style options] [--] &#x27;program&#x27; file ...</span><br><span class="line">POSIX options:          GNU long options: (standard)</span><br><span class="line">        -f progfile             --file=progfile</span><br><span class="line">        -F fs                   --field-separator=fs</span><br><span class="line">        -v var=val              --assign=var=val</span><br><span class="line">Short options:          GNU long options: (extensions)</span><br><span class="line">        -b                      --characters-as-bytes</span><br><span class="line">        -c                      --traditional</span><br><span class="line">        -C                      --copyright</span><br><span class="line">        -d[file]                --dump-variables[=file]</span><br><span class="line">        -e &#x27;program-text&#x27;       --source=&#x27;program-text&#x27;</span><br><span class="line">        -E file                 --exec=file</span><br><span class="line">        -g                      --gen-pot</span><br><span class="line">        -h                      --help</span><br><span class="line">        -L [fatal]              --lint[=fatal]</span><br><span class="line">        -n                      --non-decimal-data</span><br><span class="line">        -N                      --use-lc-numeric</span><br><span class="line">        -O                      --optimize</span><br><span class="line">        -p[file]                --profile[=file]</span><br><span class="line">        -P                      --posix</span><br><span class="line">        -r                      --re-interval</span><br><span class="line">        -S                      --sandbox</span><br><span class="line">        -t                      --lint-old</span><br><span class="line">        -V                      --version</span><br><span class="line"></span><br><span class="line">To report bugs, see node `Bugs&#x27; in `gawk.info&#x27;, which is</span><br><span class="line">section `Reporting Problems and Bugs&#x27; in the printed version.</span><br><span class="line"></span><br><span class="line">gawk is a pattern scanning and processing language.</span><br><span class="line">By default it reads standard input and writes standard output.</span><br><span class="line"></span><br><span class="line">Examples:</span><br><span class="line">        gawk &#x27;&#123; sum += $1 &#125;; END &#123; print sum &#125;&#x27; file</span><br><span class="line">        gawk -F: &#x27;&#123; print $1 &#125;&#x27; /etc/passwd</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">-v 选项</span><br><span class="line">在程序执行之前为变量赋值</span><br><span class="line"></span><br><span class="line">awk -v name=Jerry &#x27;BEGIN&#123;printf &quot;Name = %s\n&quot;, name&#125;&#x27;</span><br><span class="line">Name = Jerry</span><br></pre></td></tr></table></figure>

<h3 id="内置变量"><a href="#内置变量" class="headerlink" title="内置变量"></a>内置变量</h3><table>
<thead>
<tr>
<th>变量</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>ARGC</td>
<td>命令行参数个数</td>
</tr>
<tr>
<td>ARGV</td>
<td>命令行参数排列</td>
</tr>
<tr>
<td>CONVFMT</td>
<td>数据转换为字符串的格式，其默认值为 %.6g</td>
</tr>
<tr>
<td>OFMT</td>
<td>表示数值输出的格式，它的默认值为 %.6g</td>
</tr>
<tr>
<td>ENVIRON</td>
<td>支持队列中系统环境变量的使用</td>
</tr>
<tr>
<td>FILENAME</td>
<td>awk浏览的文件名</td>
</tr>
<tr>
<td>FNR</td>
<td>浏览文件的记录数</td>
</tr>
<tr>
<td>RLENGTH</td>
<td>表示 match 函数匹配的字符串长度</td>
</tr>
<tr>
<td>RSTART</td>
<td>表示由 match 函数匹配的字符串的第一个字符的位置</td>
</tr>
<tr>
<td>FS</td>
<td>FS command contains the field separator character which is used to divide fields on the input line. The default is “white space”, meaning space and tab characters. FS can be reassigned to another character (typically in BEGIN) to change the field separator.</td>
</tr>
<tr>
<td>NF</td>
<td>NF command keeps a count of the number of fields within the current input record.</td>
</tr>
<tr>
<td>NR</td>
<td>NR command keeps a current count of the number of input records. Remember that records are usually lines. Awk command performs the pattern/action statements once for each record in a file.</td>
</tr>
<tr>
<td>OFS</td>
<td>OFS command stores the output field separator, which separates the fields when Awk prints them. The default is a blank space. Whenever print has several parameters separated with commas, it will print the value of OFS in between each parameter.</td>
</tr>
<tr>
<td>ORS</td>
<td>ORS command stores the output record separator, which separates the output lines when Awk prints them. The default is a newline character. print automatically outputs the contents of ORS at the end of whatever it is given to print.</td>
</tr>
<tr>
<td>RS</td>
<td>RS command stores the current record separator character. Since, by default, an input line is the input record, the default record separator character is a newline.</td>
</tr>
</tbody></table>
<h4 id="ARGC"><a href="#ARGC" class="headerlink" title="ARGC"></a>ARGC</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;BEGIN &#123;print &quot;Arguments =&quot;, ARGC&#125;&#x27; One Two Three Four</span><br><span class="line">Arguments = 5</span><br></pre></td></tr></table></figure>

<h4 id="ARGV"><a href="#ARGV" class="headerlink" title="ARGV"></a>ARGV</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;BEGIN &#123; for (i = 0; i &lt; ARGC; ++i)</span><br><span class="line">      &#123; printf &quot;ARGV[%d] = %s\n&quot;, i, ARGV[i] &#125; </span><br><span class="line">                    &#125;&#x27; one two three four</span><br><span class="line">ARGV[0] = awk</span><br><span class="line">ARGV[1] = one</span><br><span class="line">ARGV[2] = two</span><br><span class="line">ARGV[3] = three</span><br><span class="line">ARGV[4] = four</span><br></pre></td></tr></table></figure>

<h4 id="CONVFMT"><a href="#CONVFMT" class="headerlink" title="CONVFMT"></a>CONVFMT</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;BEGIN &#123; print &quot;Conversion Format =&quot;, CONVFMT &#125;&#x27;</span><br><span class="line">Conversion Format = %.6g</span><br></pre></td></tr></table></figure>

<h4 id="ENVIRON"><a href="#ENVIRON" class="headerlink" title="ENVIRON"></a>ENVIRON</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;BEGIN &#123; print ENVIRON[&quot;USER&quot;] &#125;&#x27;</span><br><span class="line">vincent</span><br></pre></td></tr></table></figure>

<h4 id="FILENAME"><a href="#FILENAME" class="headerlink" title="FILENAME"></a>FILENAME</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;END &#123;print FILENAME&#125;&#x27; marks.txt</span><br><span class="line">marks.txt</span><br></pre></td></tr></table></figure>

<h4 id="FS"><a href="#FS" class="headerlink" title="FS"></a>FS</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;BEGIN &#123;print &quot;FS = &quot; FS&#125;&#x27; | cat -vte</span><br><span class="line">FS =  $</span><br></pre></td></tr></table></figure>

<h4 id="NF"><a href="#NF" class="headerlink" title="NF"></a>NF</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo -e &quot;One Two\nOne Two Three\nOne Two Three Four&quot; | awk &#x27;NF &gt; 2&#x27;</span><br><span class="line">One Two Three</span><br><span class="line">One Two Three Four</span><br></pre></td></tr></table></figure>

<h4 id="NR"><a href="#NR" class="headerlink" title="NR"></a>NR</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo -e &quot;One Two\nOne Two Three\nOne Two Three Four&quot; | awk &#x27;NR &lt; 3&#x27;</span><br><span class="line">One Two</span><br><span class="line">One Two Three</span><br></pre></td></tr></table></figure>

<h4 id="RLENGTH"><a href="#RLENGTH" class="headerlink" title="RLENGTH"></a>RLENGTH</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;BEGIN &#123; if (match(&quot;One Two Three&quot;, &quot;ree&quot;)) &#123; print RLENGTH &#125; &#125;&#x27;</span><br></pre></td></tr></table></figure>

<h4 id="RSTART"><a href="#RSTART" class="headerlink" title="RSTART"></a>RSTART</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;BEGIN &#123; if (match(&quot;One Two Three&quot;, &quot;Thre&quot;)) &#123; print RSTART &#125; &#125;&#x27;</span><br><span class="line">9</span><br></pre></td></tr></table></figure>

<h3 id="操作符"><a href="#操作符" class="headerlink" title="操作符"></a>操作符</h3><h4 id="算术运算符"><a href="#算术运算符" class="headerlink" title="算术运算符"></a>算术运算符</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;BEGIN &#123; a = 50; b = 20; print &quot;(a + b) = &quot; (a + b) &#125;&#x27;</span><br><span class="line">(a + b) = 70</span><br><span class="line"></span><br><span class="line">awk &#x27;BEGIN &#123; a = 50; b = 20; print &quot;(a / b) = &quot;, (a / b) &#125;&#x27;</span><br><span class="line">(a / b) =  2.5</span><br><span class="line"></span><br><span class="line">awk &#x27;BEGIN &#123; a = 10; a = a ^ 2; print &quot;a =&quot;, a &#125;&#x27;</span><br><span class="line">a = 100</span><br><span class="line"></span><br><span class="line">awk &#x27;BEGIN &#123; a = 10; a = a ** 2; print &quot;a =&quot;, a &#125;&#x27;</span><br><span class="line">a = 100</span><br></pre></td></tr></table></figure>

<h4 id="递增运算符与递减运算符"><a href="#递增运算符与递减运算符" class="headerlink" title="递增运算符与递减运算符"></a>递增运算符与递减运算符</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;BEGIN &#123; a = 10; b = ++a; printf &quot;a = %d, b = %d\n&quot;, a, b &#125;&#x27;</span><br><span class="line">a = 11, b = 11</span><br><span class="line"></span><br><span class="line">awk &#x27;BEGIN &#123; a = 10; b = a++; printf &quot;a = %d, b = %d\n&quot;, a, b &#125;&#x27;</span><br><span class="line">a = 11, b = 10</span><br></pre></td></tr></table></figure>

<h4 id="赋值操作符"><a href="#赋值操作符" class="headerlink" title="赋值操作符"></a>赋值操作符</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;BEGIN &#123; name = &quot;Jerry&quot;; print &quot;My name is&quot;, name &#125;&#x27;</span><br><span class="line">My name is Jerry</span><br><span class="line"></span><br><span class="line">awk &#x27;BEGIN &#123; cnt=10; cnt += 10; print &quot;Counter =&quot;, cnt &#125;&#x27;</span><br><span class="line">Counter = 20</span><br></pre></td></tr></table></figure>

<h4 id="关系运算符"><a href="#关系运算符" class="headerlink" title="关系运算符"></a>关系运算符</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;BEGIN &#123; a = 10; b = 10; if (a == b) print &quot;a == b&quot; &#125;&#x27;</span><br></pre></td></tr></table></figure>

<h4 id="逻辑运算符"><a href="#逻辑运算符" class="headerlink" title="逻辑运算符"></a>逻辑运算符</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;BEGIN &#123;num = 5; if (num &gt;= 0 &amp;&amp; num &lt;= 7) printf &quot;%d is in octal format\n&quot;, num &#125;&#x27;</span><br><span class="line">awk &#x27;BEGIN &#123;ch = &quot;\n&quot;; if (ch == &quot; &quot; || ch == &quot;\t&quot; || ch == &quot;\n&quot;) print &quot;Current character is whitespace.&quot; &#125;&#x27;</span><br><span class="line">awk &#x27;BEGIN &#123; name = &quot;&quot;; if (! length(name)) print &quot;name is empty string.&quot; &#125;&#x27;</span><br><span class="line">awk &#x27;BEGIN &#123; a = 10; b = 20; (a &gt; b) ? max = a : max = b; print &quot;Max =&quot;, max&#125;&#x27;</span><br></pre></td></tr></table></figure>

<h4 id="一元运算符"><a href="#一元运算符" class="headerlink" title="一元运算符"></a>一元运算符</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;BEGIN &#123; a = -10; a = +a; print &quot;a =&quot;, a &#125;&#x27;</span><br><span class="line">a = -10</span><br><span class="line">awk &#x27;BEGIN &#123; a = -10; a = -a; print &quot;a =&quot;, a &#125;&#x27;</span><br><span class="line">a = 10</span><br></pre></td></tr></table></figure>

<h4 id="字符串连接操作符"><a href="#字符串连接操作符" class="headerlink" title="字符串连接操作符"></a>字符串连接操作符</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;BEGIN &#123; str1=&quot;Hello, &quot;; str2=&quot;World&quot;; str3 = str1 str2; print str3 &#125;&#x27;</span><br><span class="line">Hello, World</span><br></pre></td></tr></table></figure>

<h4 id="数组成员操作符"><a href="#数组成员操作符" class="headerlink" title="数组成员操作符"></a>数组成员操作符</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;BEGIN &#123; arr[0] = 1; arr[1] = 2; arr[2] = 3; for (i in arr) printf &quot;arr[%d] = %d\n&quot;, i, arr[i] &#125;&#x27;</span><br><span class="line">arr[2] = 3</span><br><span class="line">arr[0] = 1</span><br><span class="line">arr[1] = 2</span><br></pre></td></tr></table></figure>

<h4 id="正则表达式操作符"><a href="#正则表达式操作符" class="headerlink" title="正则表达式操作符"></a>正则表达式操作符</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;$0 ~ 9&#x27; marks.txt</span><br><span class="line">2)    Rahul    Maths      90</span><br><span class="line">5)    Hari     History    89</span><br><span class="line"></span><br><span class="line">awk &#x27;$0 !~ 9&#x27; marks.txt</span><br><span class="line">1)  Amit    Physics 80</span><br><span class="line">3)  Shyam   Biology 87</span><br><span class="line">4)  Kedar   English 85</span><br></pre></td></tr></table></figure>

<h3 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h3><p>（1）统计passwd文件名，每行的行号，每行的列数</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ awk -F: &#x27;&#123;print &quot;filename:&quot;  FILENAME &quot;, linenumber:&quot; NR  &quot;,columns:&quot; NF&#125;&#x27; passwd </span><br><span class="line">filename:passwd, linenumber:1,columns:7</span><br><span class="line">filename:passwd, linenumber:2,columns:7</span><br><span class="line">filename:passwd, linenumber:3,columns:7</span><br></pre></td></tr></table></figure>

<p>（2）切割IP</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ ifconfig eth0 | grep &quot;netmask&quot; | awk -F &#x27; &#x27; &#x27;&#123;print $2&#125;&#x27; </span><br><span class="line">10.211.55.10</span><br></pre></td></tr></table></figure>

<p>（3）查询sed.txt中空行所在的行号</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ awk &#x27;/^$/&#123;print NR&#125;&#x27; sed.txt </span><br><span class="line">5</span><br></pre></td></tr></table></figure>

<p>（4）其他实例</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">who am i | awk &#x27;&#123;print $0&#125;&#x27;</span><br><span class="line">vincent  ttys000  Jan  4 11:18</span><br><span class="line"></span><br><span class="line">who am i | awk &#x27;&#123;print $1&#125;&#x27;</span><br><span class="line">vincent</span><br><span class="line"></span><br><span class="line">who am i | awk &#x27;&#123;print $1,$5&#125;&#x27;</span><br><span class="line">vincent 11:18</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">dennis_ritchie.txt</span><br><span class="line">UNIX is basically a simple operating system, but you hve to be genius to understand the simplicity.</span><br><span class="line"></span><br><span class="line">awk &#x27;&#123;print $1,$2,$NF&#125;&#x27; dennis_ritchie.txt</span><br><span class="line">UNIX is simplicity.</span><br><span class="line"></span><br><span class="line">awk &#x27;BEGIN &#123;print &quot;Dennis Ritchie&quot;&#125; &#123;print $0&#125;&#x27; dennis_ritchie.txt</span><br><span class="line">Dennis Ritchie</span><br><span class="line">UNIX is basically a simple operating system, but you hve to be genius to understand the simplicity.</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">date | awk &#x27;&#123;print $2,$3,$6&#125;&#x27;</span><br><span class="line">Jan 4 2022</span><br><span class="line"></span><br><span class="line">date | awk &#x27;OFS=&quot;/&quot; &#123;print$2,$3,$6&#125;&#x27;</span><br><span class="line">Jan/4/2022</span><br><span class="line"></span><br><span class="line">date | awk &#x27;OFS=&quot;-&quot; &#123;print$2,$3,$6&#125;&#x27;</span><br><span class="line">Jan-4-2022</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">password.txt</span><br><span class="line">root:x:0:0:root:/root:/bin/bash</span><br><span class="line">bin:x:1:1:bin:/bin:/sbin/nologin</span><br><span class="line">vincent:x:1000:1000:vincent:/home/vincent:/bin/bash</span><br><span class="line">mysql:x:27:27:MySQL Server:/var/lib/mysql:/bin/false</span><br><span class="line"></span><br><span class="line">awk -F: &#x27;&#123;print $1,$6&#125;&#x27; ./password.txt</span><br><span class="line">root /root</span><br><span class="line">bin /bin</span><br><span class="line">vincent /home/vincent</span><br><span class="line">mysql /var/lib/mysql</span><br><span class="line"></span><br><span class="line">awk -F: &#x27;$3 &gt;= 1000 &#123;print $1,$6&#125;&#x27; ./password.txt</span><br><span class="line">vincent /home/vincent</span><br><span class="line"></span><br><span class="line">awk -F: &#x27;BEGIN &#123;print &quot;User Accounts\n-------------&quot;&#125; $3 &gt;= 1000 &#123;print $1,$6&#125;&#x27; ./password.txt</span><br><span class="line">User Accounts</span><br><span class="line">-------------</span><br><span class="line">vincent /home/vincent</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">fstab.txt</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="bash"><span class="comment"># /etc/fstab</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Created by anaconda on Thu Jun 18 08:00:40 2020</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="bash"><span class="comment"># Accessible filesystems, by reference, are maintained under &#x27;/dev/disk&#x27;</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) <span class="keyword">for</span> more info</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="bash">UUID=a707eb5e-c386-49c1-ae34-3bbfa02a5550 /                       ext4    defaults        1 1</span></span><br><span class="line">UUID=f7665600-7d3b-442e-a2cc-93ed374db403 /boot                   ext4    defaults        1 2</span><br><span class="line">UUID=4b94cc3e-3bf3-4654-9772-e1e7d290b8de swap                    swap    defaults        0 0</span><br><span class="line"></span><br><span class="line">awk &#x27;/^UUID/ &#123;print $0&#125;&#x27; ./fstab.txt</span><br><span class="line">UUID=a707eb5e-c386-49c1-ae34-3bbfa02a5550 /                       ext4    defaults        1 1</span><br><span class="line">UUID=f7665600-7d3b-442e-a2cc-93ed374db403 /boot                   ext4    defaults        1 2</span><br><span class="line">UUID=4b94cc3e-3bf3-4654-9772-e1e7d290b8de swap                    swap    defaults        0 0</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;BEGIN &#123; print sqrt((2+3)*5)&#125;&#x27;</span><br><span class="line">25</span><br><span class="line"></span><br><span class="line">awk &#x27;BEGIN &#123;print atan2(0, -1)&#125;&#x27;</span><br><span class="line">3.14159</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">employee.txt</span><br><span class="line">ajay manager account 45000</span><br><span class="line">sunil clerk account 25000</span><br><span class="line">varun manager sales 50000</span><br><span class="line">amit manager account 47000</span><br><span class="line">tarun peon sales 15000</span><br><span class="line">deepak clerk sales 23000</span><br><span class="line">sunil peon sales 13000</span><br><span class="line">satvik director purchase 80000</span><br><span class="line"></span><br><span class="line">awk &#x27;&#123;print NR,$0&#125;&#x27; employee.txt </span><br><span class="line">1 ajay manager account 45000</span><br><span class="line">2 sunil clerk account 25000</span><br><span class="line">3 varun manager sales 50000</span><br><span class="line">4 amit manager account 47000</span><br><span class="line">5 tarun peon sales 15000</span><br><span class="line">6 deepak clerk sales 23000</span><br><span class="line">7 sunil peon sales 13000</span><br><span class="line">8 satvik director purchase 80000 </span><br><span class="line"></span><br><span class="line">awk &#x27;&#123;print $1,$NF&#125;&#x27; employee.txt </span><br><span class="line">ajay 45000</span><br><span class="line">sunil 25000</span><br><span class="line">varun 50000</span><br><span class="line">amit 47000</span><br><span class="line">tarun 15000</span><br><span class="line">deepak 23000</span><br><span class="line">sunil 13000</span><br><span class="line">satvik 80000</span><br><span class="line"></span><br><span class="line">awk &#x27;&#123;print $1 &quot;\t&quot; $NF&#125;&#x27; employee.txt</span><br><span class="line">ajay    45000</span><br><span class="line">sunil   25000</span><br><span class="line">varun   50000</span><br><span class="line">amit    47000</span><br><span class="line">tarun   15000</span><br><span class="line">deepak  23000</span><br><span class="line">sunil   13000</span><br><span class="line">satvik  80000</span><br><span class="line"></span><br><span class="line">Display Line From 3 to 6</span><br><span class="line">awk &#x27;NR==3, NR==6 &#123;print NR,$0&#125;&#x27; employee.txt </span><br><span class="line">3 varun manager sales 50000</span><br><span class="line">4 amit manager account 47000</span><br><span class="line">5 tarun peon sales 15000</span><br><span class="line">6 deepak clerk sales 23000</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">geeksforgeeks.txt</span><br><span class="line"></span><br><span class="line">A    B    C</span><br><span class="line">Tarun    A12    1</span><br><span class="line">Man    B6    2</span><br><span class="line">Praveen    M42    3</span><br><span class="line"></span><br><span class="line">awk &#x27;&#123;print NR &quot; - &quot; $1 &#125;&#x27; geeksforgeeks.txt </span><br><span class="line">1 -</span><br><span class="line">2 - A</span><br><span class="line">3 - Tarun</span><br><span class="line">4 - Man</span><br><span class="line">5 - Praveen</span><br><span class="line"></span><br><span class="line">awk &#x27;NF &gt; 0 &#123;print NR &quot; - &quot; $1 &#125;&#x27; geeksforgeeks.txt </span><br><span class="line">2 - A</span><br><span class="line">3 - Tarun</span><br><span class="line">4 - Man</span><br><span class="line">5 - Praveen</span><br><span class="line"></span><br><span class="line">awk &#x27;NF &lt;= 0 &#123;print NR&#125;&#x27;  geeksforgeeks.txt</span><br><span class="line">1</span><br><span class="line"></span><br><span class="line">To find the length of the longest line present in the file:</span><br><span class="line">awk &#x27;BEGIN&#123;max=0&#125; &#123; if (length($0) &gt; max) max = length($0) &#125; END &#123; print max &#125;&#x27; geeksforgeeks.txt</span><br><span class="line">19</span><br><span class="line"></span><br><span class="line">Printing lines with more than 15 characters:  </span><br><span class="line">awk &#x27;length($0) &gt; 15&#x27; geeksforgeeks.txt</span><br><span class="line">Tarun    A12    1</span><br><span class="line">Praveen    M42    3</span><br><span class="line"></span><br><span class="line">To find/check for any string in any specific column:  </span><br><span class="line">awk &#x27;&#123; if($2 == &quot;B6&quot;) print $0;&#125;&#x27; geeksforgeeks.txt</span><br><span class="line">Man    B6    2</span><br><span class="line"></span><br><span class="line">To print the squares of first numbers from 1 to n say 6:  </span><br><span class="line">awk &#x27;BEGIN &#123; for(i=1;i&lt;=6;i++) print &quot;square of&quot;, i, &quot;is&quot;,i*i&#125;&#x27;</span><br><span class="line">square of 1 is 1</span><br><span class="line">square of 2 is 4</span><br><span class="line">square of 3 is 9</span><br><span class="line">square of 4 is 16</span><br><span class="line">square of 5 is 25</span><br><span class="line">square of 6 is 36</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">information.txt</span><br><span class="line">fristName       lastName        age     city       ID</span><br><span class="line"></span><br><span class="line">Thomas          Shelby          30      Rio        400</span><br><span class="line">Omega           Night           45      Ontario    600</span><br><span class="line">Wood            Tinker          54      Lisbon     N/A</span><br><span class="line">Giorgos         Georgiou        35      London     300</span><br><span class="line">Timmy           Turner          32      Berlin     N/A</span><br><span class="line"></span><br><span class="line">awk &#x27;&#123;print $1&#125;&#x27; information.txt | head -3</span><br><span class="line">fristName</span><br><span class="line"></span><br><span class="line">awk &#x27;!/0$/&#x27; information.txt  </span><br><span class="line">fristName       lastName        age     city       ID</span><br><span class="line"></span><br><span class="line">Wood            Tinker          54      Lisbon     N/A</span><br><span class="line">Timmy           Turner          32      Berlin     N/A</span><br><span class="line"></span><br><span class="line">awk &#x27; /io/ &#123;print $0&#125;&#x27; information.txt </span><br><span class="line">Thomas          Shelby          30      Rio        400</span><br><span class="line">Omega           Night           45      Ontario    600</span><br><span class="line">Giorgos         Georgiou        35      London     300</span><br><span class="line"></span><br><span class="line">awk &#x27;/N\/A$/&#x27; information.txt </span><br><span class="line">Wood            Tinker          54      Lisbon     N/A</span><br><span class="line">Timmy           Turner          32      Berlin     N/A</span><br><span class="line"></span><br><span class="line">awk &#x27;NF &gt; 0 &amp;&amp; $3 &lt; 40  &#123; print $0 &#125;&#x27; information.txt</span><br><span class="line">Thomas          Shelby          30      Rio        400</span><br><span class="line">Giorgos         Georgiou        35      London     300</span><br><span class="line">Timmy           Turner          32      Berlin     N/A</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">marks.txt</span><br><span class="line">1)    Amit     Physics    80</span><br><span class="line">2)    Rahul    Maths      90</span><br><span class="line">3)    Shyam    Biology    87</span><br><span class="line">4)    Kedar    English    85</span><br><span class="line">5)    Hari     History    89</span><br><span class="line"></span><br><span class="line">awk &#x27;/a/&#123;++cnt&#125; END &#123;print &quot;Count = &quot; &quot;&quot; cnt&#125;&#x27; marks.txt</span><br><span class="line">Count = 4</span><br><span class="line">AWK 在使用一个变量前不需要特意地声明这个变量</span><br><span class="line"></span><br><span class="line">awk &#x27;length($0) &gt; 18&#x27; marks.txt</span><br></pre></td></tr></table></figure>

<h2 id="sort"><a href="#sort" class="headerlink" title="sort"></a>sort</h2><p>sort命令是在Linux里非常有用，它将文件进行排序，并将排序结果标准输出。</p>
<ol>
<li>基本语法</li>
</ol>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="title">sort</span><span class="params">(选项)</span><span class="params">(参数)</span></span></span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>选项</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>-n</td>
<td>依照数值的大小排序</td>
</tr>
<tr>
<td>-r</td>
<td>以相反的顺序来排序</td>
</tr>
<tr>
<td>-t</td>
<td>设置排序时所用的分隔字符</td>
</tr>
<tr>
<td>-k</td>
<td>指定需要排序的列</td>
</tr>
</tbody></table>
<p>参数：指定待排序的文件列表</p>
<ol start="2">
<li>案例实操</li>
</ol>
<p>（0）数据准备</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ vim sort.sh </span><br><span class="line">bb:40:5.4</span><br><span class="line">bd:20:4.2</span><br><span class="line">xz:50:2.3</span><br><span class="line">cls:10:3.5</span><br><span class="line">ss:30:1.6</span><br></pre></td></tr></table></figure>

<p>（1）按照“：”分割后的第三列倒序排序。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ sort -t : -nrk 3 sort.sh </span><br><span class="line">bb:40:5.4</span><br><span class="line">bd:20:4.2</span><br><span class="line">cls:10:3.5</span><br><span class="line">xz:50:2.3</span><br><span class="line">ss:30:1.6</span><br></pre></td></tr></table></figure>

<h2 id="wc"><a href="#wc" class="headerlink" title="wc"></a>wc</h2><p>wc命令用来计算数字。利用wc指令我们可以计算文件的<strong>Byte数、字数或是列数</strong>，若不指定文件名称，或是所给予的文件名为“-”，则wc指令会从标准输入设备读取数据。</p>
<ol>
<li>基本用法</li>
</ol>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line">wc <span class="selector-attr">[选项参数]</span> filename</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>参数说明</li>
</ol>
<table>
<thead>
<tr>
<th>选项参数</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-l</td>
<td>统计文件行数</td>
</tr>
<tr>
<td>-w</td>
<td>统计文件的单词数</td>
</tr>
<tr>
<td>-m</td>
<td>统计文件的字符数</td>
</tr>
<tr>
<td>-c</td>
<td>统计文件的字节数</td>
</tr>
</tbody></table>
<ol start="3">
<li>案例实操</li>
</ol>
<p> 统计redis_6379.conf文件的行数、单词数、字节数。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@0725pc myredis]$ ll | grep redis_6379.conf</span><br><span class="line">-rw-r--r--. 1 root root      188 10月  9 15:33 redis_6379.conf</span><br><span class="line">[root@0725pc myredis]$ cat -n redis_6379.conf </span><br><span class="line">     1	include /root/myredis/redis.conf</span><br><span class="line">     2	pidfile &quot;/var/run/redis_6379.pid&quot;</span><br><span class="line">     3	port 6379</span><br><span class="line">     4	dbfilename &quot;dump_6379.rdb&quot;</span><br><span class="line">     5	cluster-enabled yes</span><br><span class="line">     6	cluster-config-file nodes-6379.conf</span><br><span class="line">     7	cluster-node-timeout 15000</span><br><span class="line">     8	</span><br><span class="line">[root@0725pc myredis]$ wc -w redis_6379.conf </span><br><span class="line">14 redis_6379.conf</span><br><span class="line">[root@0725pc myredis]$ wc -l redis_6379.conf </span><br><span class="line">8 redis_6379.conf</span><br><span class="line">[root@0725pc myredis]$ wc -m redis_6379.conf </span><br><span class="line">188 redis_6379.conf</span><br></pre></td></tr></table></figure>

<h2 id="zcat"><a href="#zcat" class="headerlink" title="zcat"></a>zcat</h2><p>显示压缩包中文件的内容</p>
<p><strong>zcat命令</strong> 用于不真正解压缩文件，就能显示压缩包中文件的内容的场合。</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="title">zcat</span><span class="params">(选项)</span><span class="params">(参数)</span></span></span><br></pre></td></tr></table></figure>

<figure class="highlight diff"><table><tr><td class="code"><pre><span class="line"><span class="deletion">-S：指定gzip格式的压缩包的后缀。当后缀不是标准压缩包后缀时使用此选项；</span></span><br><span class="line"><span class="deletion">-c：将文件内容写到标注输出；</span></span><br><span class="line"><span class="deletion">-d：执行解压缩操作；</span></span><br><span class="line"><span class="deletion">-l：显示压缩包中文件的列表；</span></span><br><span class="line"><span class="deletion">-L：显示软件许可信息；</span></span><br><span class="line"><span class="deletion">-q：禁用警告信息；</span></span><br><span class="line"><span class="deletion">-r：在目录上执行递归操作；</span></span><br><span class="line"><span class="deletion">-t：测试压缩文件的完整性；</span></span><br><span class="line"><span class="deletion">-V：显示指令的版本信息；</span></span><br><span class="line"><span class="deletion">-l：更快的压缩速度；</span></span><br><span class="line"><span class="deletion">-9：更高的压缩比。</span></span><br></pre></td></tr></table></figure>

<h1 id="案例-1"><a href="#案例-1" class="headerlink" title="案例"></a>案例</h1><p>1.使用Linux命令查询file1中空行所在的行号</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;/^$/&#123;print NR&#125;&#x27; file1</span><br></pre></td></tr></table></figure>

<p>2.有文件chengji.txt内容如下:</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">张三 40</span><br><span class="line">李四 50</span><br><span class="line">王五 60</span><br></pre></td></tr></table></figure>

<p>使用Linux命令计算第二列的和并输出</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cat chengji.txt | awk -F &quot; &quot; -v sum=0 &#x27;&#123;sum+=$2&#125; END&#123;print sum&#125;&#x27;</span><br></pre></td></tr></table></figure>

<p>3.Shell脚本里如何检查一个文件是否存在？如果不存在该如何处理？</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">if [ -f file.txt ]; then</span><br><span class="line">   echo &quot;文件存在!&quot;</span><br><span class="line">else</span><br><span class="line">   echo &quot;文件不存在!&quot;</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<p>4.用shell写一个脚本，对文本中无序的一列数字排序</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sort -n test.txt | awk &#x27;&#123;a+=$1; print $1&#125; END&#123;print &quot;SUM=&quot;a&#125;&#x27;</span><br></pre></td></tr></table></figure>

<p>5.请用shell脚本写出查找当前文件夹（/home）下所有的文本文件内容中包含有字符”shen”的文件名称</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">grep -r &quot;shen&quot; /home | cut -d &quot;:&quot; -f 1</span><br></pre></td></tr></table></figure>





<figure class="highlight dos"><table><tr><td class="code"><pre><span class="line">ARGC               命令行参数个数</span><br><span class="line">ARGV               命令行参数排列</span><br><span class="line">ENVIRON            支持队列中系统环境变量的使用</span><br><span class="line">FILENAME           awk浏览的文件名</span><br><span class="line">FNR                浏览文件的记录数</span><br><span class="line"><span class="built_in">FS</span>                 设置输入域分隔符，等价于命令行 -F选项</span><br><span class="line">NF                 浏览记录的域的个数</span><br><span class="line">NR                 已读的记录数</span><br><span class="line">OFS                输出域分隔符</span><br><span class="line">ORS                输出记录分隔符</span><br><span class="line">RS                 控制记录分隔符</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>语言</category>
      </categories>
  </entry>
  <entry>
    <title>Yarn</title>
    <url>/Yarn/</url>
    <content><![CDATA[<p>Yarn是一个资源调度平台，<strong>负责为运算程序提供服务器运算资源</strong>，相当于一个分布式的操作系统平台，而MapReduce等运算程序则相当于运行于操作系统之上的应用程序。</p>
<h1 id="Yarn基本架构"><a href="#Yarn基本架构" class="headerlink" title="Yarn基本架构"></a>Yarn基本架构</h1><p>YARN主要由ResourceManager、NodeManager、ApplicationMaster和Container等组件构成。</p>
<p><img src="/Yarn/121.png" alt="121"></p>
<h1 id="Yarn工作机制"><a href="#Yarn工作机制" class="headerlink" title="Yarn工作机制"></a>Yarn工作机制</h1><p><img src="/Yarn/122.png" alt="122"></p>
<p>工作机制详解</p>
<p>​    （1）MR程序提交到<strong>客户端所在的节点</strong>。</p>
<p>​    （2）YarnRunner向ResourceManager申请一个Application。</p>
<p>​    （3）RM将该应用程序的资源路径返回给YarnRunner。</p>
<p>​    （4）该程序将运行所需资源提交到HDFS上。</p>
<p>​    （5）程序资源提交完毕后，申请运行mrAppMaster。</p>
<p>​    （6）RM将用户的请求初始化成一个Task。</p>
<p>​    （7）其中一个NodeManager领取到Task任务。</p>
<p>​    （8）该NodeManager创建容器Container，并产生MRAppmaster。</p>
<p>​    （9）Container从HDFS上拷贝资源到本地。</p>
<p>​    （10）MRAppmaster向RM 申请运行MapTask资源。</p>
<p>​    （11）RM将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。</p>
<p>​    （12）MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序。</p>
<p>​    （13）<strong>MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。</strong></p>
<p>​    （14）ReduceTask向MapTask获取相应分区的数据。</p>
<p>​    （15）程序运行完毕后，MR会向RM申请注销自己。</p>
<h1 id="作业提交全过程"><a href="#作业提交全过程" class="headerlink" title="作业提交全过程"></a>作业提交全过程</h1><h2 id="作业提交过程之YARN"><a href="#作业提交过程之YARN" class="headerlink" title="作业提交过程之YARN"></a>作业提交过程之YARN</h2><p><img src="/Yarn/123.png" alt="123"></p>
<p>（1）作业提交</p>
<p>第1步：Client调用job.waitForCompletion方法，向整个集群提交MapReduce作业。</p>
<p>第2步：Client向RM申请一个作业id。</p>
<p>第3步：<strong>RM给Client返回该job资源的提交路径和作业id</strong>。</p>
<p>第4步：<strong>Client提交jar包、切片信息和配置文件到指定的资源提交路径</strong>。</p>
<p>第5步：Client提交完资源后，向RM申请运行MrAppMaster。</p>
<p>（2）作业初始化</p>
<p>第6步：当RM收到Client的请求后，将该job添加到<strong>容量调度器</strong>中。</p>
<p>第7步：某一个空闲的NM领取到该Job。</p>
<p>第8步：<strong>该NM创建Container，并产生MRAppmaster</strong>。</p>
<p>第9步：<strong>下载Client提交的资源到本地</strong>。</p>
<p>（3）任务分配</p>
<p>第10步：MrAppMaster向RM申请运行多个MapTask任务资源。</p>
<p>第11步：RM将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。</p>
<p>（4）任务运行</p>
<p>第12步：MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序。</p>
<p>第13步：MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。</p>
<p>第14步：ReduceTask向MapTask获取相应分区的数据。</p>
<p>第15步：程序运行完毕后，MR会向RM申请注销自己。</p>
<p>（5）进度和状态更新</p>
<p>YARN中的任务将其进度和状态(包括counter)返回给应用管理器, 客户端每秒(通过mapreduce.client.progressmonitor.pollinterval设置)向应用管理器请求进度更新, 展示给用户。</p>
<p>（6）作业完成</p>
<p>除了向应用管理器请求作业进度外, 客户端每5秒都会通过调用waitForCompletion()来检查作业是否完成。时间间隔可以通过mapreduce.client.completion.pollinterval来设置。作业完成之后, 应用管理器和Container会清理工作状态。作业的信息会被作业历史服务器存储以备之后用户核查。</p>
<h2 id="作业提交过程之MapReduce"><a href="#作业提交过程之MapReduce" class="headerlink" title="作业提交过程之MapReduce"></a>作业提交过程之MapReduce</h2><p><img src="/Yarn/124.png" alt="124"></p>
<h1 id="资源调度器"><a href="#资源调度器" class="headerlink" title="资源调度器"></a>资源调度器</h1><p>Hadoop作业调度器主要有三种：FIFO、Capacity Scheduler和Fair Scheduler。Hadoop3.1.3默认的资源调度器是Capacity Scheduler。</p>
<p>具体设置详见：yarn-default.xml文件</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>The class to use as the resource scheduler.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>FIFO：</p>
<p><img src="/Yarn/125.png" alt="125"></p>
<p><strong>FIFO调度器以集群资源独占的方式来运行作业，这样的好处是一个作业可以充分利用所有的集群资源，但是对于运行时间短，重要性高或者交互式查询类的MR作业就要等待排在序列前的作业完成才能被执行，这也就导致了如果有一个非常大的Job在运行，那么后面的作业将会被阻塞。</strong></p>
<p>Capacity Scheduler：</p>
<p><img src="/Yarn/126.png" alt="126"></p>
<p>可以将它理解成一个个的资源队列。这个资源队列是用户自己去分配的。例如因为工作所需要把整个集群分成了AB两个队列，A队列下面还可以继续分，比如将A队列再分为1和2两个子队列。那么队列的分配就可以参考下面的树形结构：</p>
<p>—A[60%]</p>
<p>|—A.1[40%]</p>
<p>|—A.2[60%]</p>
<p>—B[40%]</p>
<p>上述的树形结构可以理解为A队列占用整个资源的60%，B队列占用整个资源的40%。A队列里面又分了两个子队列，A.1占据40%，A.2占据60%，也就是说此时A.1和A.2分别占用A队列的40%和60%的资源。虽然此时已经具体分配了集群的资源，但是并不是说A提交了任务之后只能使用它被分配到的60%的资源，而B队列的40%的资源就处于空闲。只要是其它队列中的资源处于空闲状态，那么有任务提交的队列可以使用空闲队列所分配到的资源，使用的多少是依据配来决定。</p>
<p>Capacity调度器具有以下的几个特性：</p>
<ol>
<li><p><strong>层次化的队列设计</strong>，这种层次化的队列设计保证了<strong>子队列可以使用父队列设置的全部资源</strong>。这样通过层次化的管理，更容易合理分配和限制资源的使用。</p>
</li>
<li><p>容量保证，队列上都会设置一个资源的占比，这样可以<strong>保证每个队列都不会占用整个集群的资源</strong>。</p>
</li>
<li><p>安全，<strong>每个队列又严格的访问控制</strong>。用户只能向自己的队列里面提交任务，而且不能修改或者访问其他队列的任务。</p>
</li>
<li><p>弹性分配，空闲的资源可以被分配给任何队列。当多个队列出现争用的时候，则会按照比例进行平衡。</p>
</li>
<li><p>多租户租用，通过队列的容量限制，多个用户就可以共享同一个集群，同事保证每个队列分配到自己的容量，提高利用率。</p>
</li>
<li><p>操作性，Yarn支持动态修改调整容量、权限等的分配，可以在运行时直接修改。还提供给管理员界面，来显示当前的队列状况。管理员可以在运行时，添加一个队列；但是不能删除一个队列。管理员还可以在运行时暂停某个队列，这样可以保证当前的队列在执行过程中，集群不会接收其他的任务。如果一个队列被设置成了stopped，那么就不能向他或者子队列上提交任务了。</p>
</li>
<li><p>基于资源的调度，协调不同资源需求的应用程序，比如内存、CPU、磁盘等等。</p>
</li>
</ol>
<p>相关参数的配置：</p>
<p>（1）capacity：队列的资源容量（百分比）。 当系统非常繁忙时，应保证每个队列的容量得到满足，而如果每个队列应用程序较少，可将剩余资源共享给其他队列。注意，所有队列的容量之和应小于100。</p>
<p>（2）maximum-capacity：队列的资源使用上限（百分比）。由于存在资源共享，因此一个队列使用的资源量可能超过其容量，而最多使用资源量可通过该参数限制。（这也是前文提到的关于有任务运行的队列可以占用的资源的最大百分比）</p>
<p>（3）user-limit-factor：每个用户最多可使用的资源量（百分比）。比如，假设该值为30，则任何时刻，每个用户使用的资源量不能超过该队列容量的30%。</p>
<p>（4）maximum-applications ：集群或者队列中同时处于等待和运行状态的应用程序数目上限，这是一个强限制，一旦集群中应用程序数目超过该上限，后续提交的应用程序将被拒绝，默认值为 10000。所有队列的数目上限可通过参数yarn.scheduler.capacity.maximum-applications设置（可看做默认值），而单个队列可通过参数yarn.scheduler.capacity.<queue-path>.maximum- applications设置适合自己的值。</queue-path></p>
<p>（5）maximum-am-resource-percent：集群中用于运行应用程序<br>ApplicationMaster的资源比例上限，该参数通常用于限制处于活动状态的应用程序数目。该参数类型为浮点型，默认是0.1，表示10%。所有队列的ApplicationMaster资源比例上限可通过参数yarn.scheduler.capacity. maximum-am-resource-percent设置（可看做默认值），而单个队列可通过参数<br>yarn.scheduler.capacity.<queue-path>.<br>maximum-am-resource-percent设置适合自己的值。</queue-path></p>
<p>（6）state ：队列状态可以为STOPPED或者 RUNNING，如果一个队列处于STOPPED状态，用户不可以将应用程序提交到该队列或者它的子队列中，类似的，如果ROOT队列处于STOPPED 状态，用户不可以向集群中提交应用程序，但正在运行的应用程序仍可以正常运行结束，以便队列可以优雅地退出。</p>
<p>（7）acl_submit_applications：限定哪些Linux用户/用户组可向给定队列中提交应用程序。需要注意的是，该属性具有继承性，即如果一个用户可以向某个队列中提交应用程序，则它可以向它的所有子队列中提交应用程序。配置该属性时，用户之间或用户组之间用“，”分割，用户和用户组之间用空格分割，比如“user1, user2 group1,group2”。</p>
<p>（8）acl_administer_queue：为队列指定一个管理员，该管理员可控制该队列的所有应用程序，比如杀死任意一个应用程序等。同样，该属性具有继承性，如果一个用户可以向某个队列中提交应用程序，则它可以向它的所有子队列中提交应用程序。</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><span class="line">如果用idea，可以在用</span><br><span class="line">conf.set(<span class="string">&quot;mapred.job.queue.name&quot;</span>, <span class="string">&quot;a&quot;</span>);	<span class="regexp">//</span>a 是队列名称</span><br><span class="line"></span><br><span class="line">如果在Linux上运行jar包，则可以用</span><br><span class="line">hadoop jar hadoop-mapreduce-examples-<span class="number">2.7</span>.<span class="number">2</span>.jar  wordcount -D mapreduce.job.queuename=a <span class="regexp">/mapjoin /</span>output3</span><br></pre></td></tr></table></figure>

<p>Fair Scheduler：</p>
<p><img src="/Yarn/127.png" alt="127"></p>
<p><img src="/Yarn/128.png" alt="128"></p>
<p>Fair调度器是一个队列资源分配方式，在整个时间线上，所有的Job平均的获取资源。默认情况下，Fair调度器只是对内存资源做公平的调度和分配。当集群中只有一个任务在运行时，那么此任务会占用整个集群的资源。当其他的任务提交后，那些释放的资源将会被分配给新的Job，所以每个任务最终都能获取几乎一样多的资源。</p>
<p>例如有两个用户A和B，他们分别拥有一个队列。当A启动一个Job而B没有任务提交时，A会获得全部集群资源；当B启动一个Job后，A的任务会继续运行，不过队列A会慢慢释放它的一些资源，一会儿之后两个任务会各自获得一半的集群资源。如果此时B再启动第二个Job并且其它任务也还在运行时，那么它将会和B队列中的的第一个Job共享队列B的资源，也就是队列B的两个Job会分别使用集群四分之一的资源，而队列A的Job仍然会使用集群一半的资源，结果就是<strong>集群的资源最终在两个用户之间平等的共享</strong>。</p>
<p>相关参数的配置：</p>
<p>（1）yarn.scheduler.fair.allocation.file： “allocation”文件的位置，“allocation”文件是一个用来描述queue以及它们的属性的配置文件。这个文件必须为格式严格的xml文件。如果为相对路径，那么将会在classpath下查找此文件(conf目录下)。默认值为“fair-scheduler.xml”。</p>
<p>（2）yarn.scheduler.fair.user-as-default-queue：是否将与allocation有关的username作为默认的queue name，当queue name没有指定的时候。如果设置成false(且没有指定queue name) 或者没有设定，所有的jobs将共享“default”<br>queue。默认值为true。</p>
<p>（3）yarn.scheduler.fair.preemption：是否使用“preemption”(优先权，抢占)，默认为fasle，在此版本中此功能为测试性的。</p>
<p>（4）yarn.scheduler.fair.assignmultiple：是在允许在一个心跳中，发送多个container分配信息。默认值为false。</p>
<p>（5）yarn.scheduler.fair.max.assign：如果assignmultuple为true，那么在一次心跳中，最多发送分配container的个数。默认为-1，无限制。</p>
<p>（6）yarn.scheduler.fair.locality.threshold.node：一个float值，在0~1之间，表示在等待获取满足node-local条件的containers时，最多放弃不满足node-local的container的机会次数，放弃的nodes个数为集群的大小的比例。默认值为-1.0表示不放弃任何调度的机会。</p>
<p>（7）yarn.scheduler.fair.locality.threashod.rack：同上，满足rack-local。</p>
<p>（8）yarn.scheduler.fair.sizebaseweight：是否根据application的大小(Job的个数)作为权重。默认为false，如果为true，那么复杂的application将获取更多的资源。</p>
<p><strong>如果业务逻辑比较简单或者刚接触Hadoop的时候建议使用FIFO调度器；如果需要控制部分应用的优先级同时又想要充分利用集群资源的情况下，建议使用Capacity调度器；如果想要多用户或者多队列公平的共享集群资源，那么就选用Fair调度器。希望大家能够根据业务所需选择合适的调度器。</strong></p>
<h1 id="容量调度器多队列提交案例"><a href="#容量调度器多队列提交案例" class="headerlink" title="容量调度器多队列提交案例"></a>容量调度器多队列提交案例</h1><h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><p><strong>Yarn默认的容量调度器是一条单队列的调度器</strong>，在实际使用中会出现单个任务阻塞整个队列的情况。同时，随着业务的增长，公司需要分业务限制集群使用率。这就<strong>需要我们按照业务种类配置多条任务队列</strong>。</p>
<h2 id="配置多队列的容量调度器"><a href="#配置多队列的容量调度器" class="headerlink" title="配置多队列的容量调度器"></a>配置多队列的容量调度器</h2><p>默认Yarn的配置下，容量调度器只有一条Default队列。在capacity-scheduler.xml中可以配置多条队列，并降低default队列资源占比：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.queues<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>default,hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">      The queues at the this level (root is the root queue).</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.default.capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>40<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--队列目标资源百分比，所有队列相加必须等于100--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--队列最大资源百分比--&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.maximum-capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>100<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--单用户可用队列资源占比--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.user-limit-factor<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--队列状态（RUNNING或STOPPING）--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.state<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>RUNNING<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--队列允许哪些用户提交--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.acl_submit_applications<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--队列允许哪些用户管理--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.acl_administer_queue<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.maximum-application-lifetime<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>-1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>hive队列中任务的最大生命时长<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>在配置完成后，重启Yarn，就可以看到两条队列。</p>
<h2 id="向Hive队列提交任务"><a href="#向Hive队列提交任务" class="headerlink" title="向Hive队列提交任务"></a>向Hive队列提交任务</h2><p>默认的任务提交都是提交到default队列的。如果希望向其他队列提交任务，需要在Driver中声明：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WcDrvier</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line"></span><br><span class="line">        configuration.set(<span class="string">&quot;mapred.job.queue.name&quot;</span>, <span class="string">&quot;hive&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//1. 获取一个Job实例</span></span><br><span class="line">        Job job = Job.getInstance(configuration);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2. 设置类路径</span></span><br><span class="line">        job.setJarByClass(WcDrvier.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3. 设置Mapper和Reducer</span></span><br><span class="line">        job.setMapperClass(WcMapper.class);</span><br><span class="line">        job.setReducerClass(WcReducer.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4. 设置Mapper和Reducer的输出类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        job.setCombinerClass(WcReducer.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//5. 设置输入输出文件</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//6. 提交Job</span></span><br><span class="line">        <span class="keyword">boolean</span> b = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="任务的推测执行"><a href="#任务的推测执行" class="headerlink" title="任务的推测执行"></a>任务的推测执行</h1><p>1．作业完成时间取决于最慢的任务完成时间</p>
<p>一个作业由若干个Map任务和Reduce任务构成。因硬件老化、软件Bug等，某些任务可能运行非常慢。</p>
<p>思考：系统中有99%的Map任务都完成了，只有少数几个Map老是进度很慢，完不成，怎么办？</p>
<p>2．推测执行机制</p>
<p>发现拖后腿的任务，比如某个任务运行速度远慢于任务平均速度。<strong>为拖后腿任务启动一个备份任务</strong>，同时运行。谁先运行完，则采用谁的结果。</p>
<p>3．执行推测任务的前提条件</p>
<p>（1）每个Task只能有一个备份任务</p>
<p>（2）当前Job已完成的Task必须不小于0.05（5%）</p>
<p>（3）开启推测执行参数设置。mapred-site.xml文件中默认是打开的。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, then multiple instances of some map tasks may be executed in parallel.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, then multiple instances of some reduce tasks may be executed in parallel.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>4．不能启用推测执行机制情况</p>
<p>  （1）任务间存在严重的负载倾斜；</p>
<p>  （2）特殊任务，比如任务向数据库中写数据。</p>
<h1 id="查看YARN任务日志的几种方式"><a href="#查看YARN任务日志的几种方式" class="headerlink" title="查看YARN任务日志的几种方式"></a>查看YARN任务日志的几种方式</h1><h2 id="1、通过history-server"><a href="#1、通过history-server" class="headerlink" title="1、通过history server"></a>1、通过history server</h2><h2 id="2、通过yarn命令-用户要和提交任务的用户一致）"><a href="#2、通过yarn命令-用户要和提交任务的用户一致）" class="headerlink" title="2、通过yarn命令(用户要和提交任务的用户一致）"></a>2、通过yarn命令(用户要和提交任务的用户一致）</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yarn application -list -appStates ALL（这个不显示时间信息）</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yarn logs -applicationId application_1493700892407_0007</span><br></pre></td></tr></table></figure>

<h2 id="3、直接查看hdfs路径的log"><a href="#3、直接查看hdfs路径的log" class="headerlink" title="3、直接查看hdfs路径的log"></a>3、直接查看hdfs路径的log</h2><p>1）查看yarn-site.xml，确定log配置目录</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.remote-app-log-dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>/app-logs<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>2）查看日志文件信息（注意日期和时间）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hdfs@node1 root]$ hdfs dfs -ls /app-logs/hdfs/logs</span><br><span class="line">Found 1 items</span><br><span class="line">drwxrwx---   - hdfs hadoop          0 2017-05-02 04:18 /app-logs/hdfs/logs/application_1493700892407_0007</span><br></pre></td></tr></table></figure>

<p>3）查看日志详情（注意查看节点重启前的几个敏感app）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yarn logs -applicationId application_1493700892407_0007</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title>Flume</title>
    <url>/Flume/</url>
    <content><![CDATA[<h2 id="Flume定义"><a href="#Flume定义" class="headerlink" title="Flume定义"></a>Flume定义</h2><p>Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。Flume基于流式架构，灵活简单。</p>
<p><img src="/Flume/3.png" alt="3"></p>
<h2 id="Flume基础架构"><a href="#Flume基础架构" class="headerlink" title="Flume基础架构"></a>Flume基础架构</h2><p><img src="/Flume/4.png" alt="4"></p>
<h3 id="Agent"><a href="#Agent" class="headerlink" title="Agent"></a>Agent</h3><p>Agent是一个JVM进程，它以事件的形式将数据从源头送至目的。</p>
<p>Agent主要有3个部分组成，Source、Channel、Sink。</p>
<h3 id="Source"><a href="#Source" class="headerlink" title="Source"></a>Source</h3><p>Source是负责接收数据到Flume Agent的组件。Source组件可以处理各种类型、各种格式的日志数据，包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy。</p>
<h3 id="Sink"><a href="#Sink" class="headerlink" title="Sink"></a>Sink</h3><p><strong>Sink不断地轮询Channel中的事件且批量地移除它们</strong>，并将这些事件批量写入到存储或索引系统、或者被发送到另一个Flume Agent。</p>
<p>Sink组件目的地包括<strong>hdfs</strong>、logger、avro、thrift、ipc、file、<strong>HBase</strong>、solr、自定义。</p>
<h3 id="Channel"><a href="#Channel" class="headerlink" title="Channel"></a>Channel</h3><p>channels是一个Agent上存储events的仓库，Source向其中添加events，而Sink从中取走移除events。</p>
<p>Channel是位于Source和Sink之间的缓冲区。因此，<strong>Channel允许Source和Sink运作在不同的速率上</strong>。<strong>Channel是线程安全的，可以同时处理几个Source的写入操作和几个Sink的读取操作。</strong></p>
<p>Flume自带两种Channel：Memory Channel和File Channel。</p>
<p>Memory Channel是内存中的队列。Memory Channel在不需要关心数据丢失的情景下适用。如果需要关心数据丢失，那么Memory Channel就不应该使用，因为程序死亡、机器宕机或者重启都会导致数据丢失。</p>
<p>File Channel将所有事件写到磁盘。因此在程序关闭或机器宕机的情况下不会丢失数据。</p>
<p>kafka channel的parseAsFlumeEvent解析event</p>
<p>Kafka Channel的parseAsFlumeEvent的默认值为true，即会为对source来的数据进行解析，解析完会对数据前加前缀，前缀为topic名，因此这种情况，下游会需要做额外的截取工作，所以，当不需要前缀名时，将该属性设置为false.</p>
<p>实际中使用kafkachannel, 使用header中时间戳滚动, 会有问题, 建议值使用本地时间来滚动, 并且parseAsFlumeEvent设置为false</p>
<h3 id="Event"><a href="#Event" class="headerlink" title="Event"></a>Event</h3><p>传输单元，Flume数据传输的基本单元，以Event的形式将数据从源头送至目的地。<strong>Event由Header和Body两部分组成</strong>，Header用来存放该event的一些属性，为<strong>K-V结构</strong>，Body用来存放该条数据，形式为<strong>字节数组</strong>。</p>
<h2 id="Flume入门"><a href="#Flume入门" class="headerlink" title="Flume入门"></a>Flume入门</h2><h3 id="Flume安装部署"><a href="#Flume安装部署" class="headerlink" title="Flume安装部署"></a>Flume安装部署</h3><p>Flume官网地址：<a href="http://flume.apache.org/">http://flume.apache.org/</a></p>
<p>文档查看地址：<a href="http://flume.apache.org/FlumeUserGuide.html">http://flume.apache.org/FlumeUserGuide.html</a></p>
<p>下载地址：<a href="http://archive.apache.org/dist/flume/">http://archive.apache.org/dist/flume/</a></p>
<p>将apache-flume-1.9.0-bin.tar.gz上传到linux的/opt/software目录下</p>
<p>解压apache-flume-1.9.0-bin.tar.gz到/opt/module/目录下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -zxf /opt/software/apache-flume-1.9.0-bin.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>

<p>修改apache-flume-1.9.0-bin的名称为flume</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mv /opt/module/apache-flume-1.9.0-bin /opt/module/flume</span><br></pre></td></tr></table></figure>

<p>将lib文件夹下的guava-11.0.2.jar删除以兼容Hadoop 3.1.3</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">rm /opt/module/flume/lib/guava-11.0.2.jar</span><br></pre></td></tr></table></figure>

<h3 id="监控端口数据官方案例"><a href="#监控端口数据官方案例" class="headerlink" title="监控端口数据官方案例"></a>监控端口数据官方案例</h3><p>使用Flume监听一个端口，收集该端口数据，并打印到控制台。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">netcat</span><br><span class="line"></span><br><span class="line">client/server模式</span><br><span class="line"><span class="meta">#</span><span class="bash"> linux1作为服务器监听4444端口</span></span><br><span class="line">[vincent@linux1 ~]$ nc -l 4444</span><br><span class="line"><span class="meta">#</span><span class="bash"> linux2作为客户端向服务器4444端口发送数据</span></span><br><span class="line">[vincent@linux2 ~]$ nc -linux1 4444</span><br><span class="line"></span><br><span class="line">data transfer模式</span><br><span class="line"><span class="meta">#</span><span class="bash"> filename.in的数据写入filename.out</span></span><br><span class="line">[vincent@linux1 ~]$ nc -l 1234 &gt; filename.out</span><br><span class="line">[vincent@linux1 ~]$ nc host.example 1234 &lt; filename.in</span><br></pre></td></tr></table></figure>

<p>（1）安装netcat工具</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 software]$ sudo yum install -y nc</span><br></pre></td></tr></table></figure>

<p>（2）判断6666端口是否被占用</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume-telnet]$ sudo netstat -tunlp | grep 6666</span><br></pre></td></tr></table></figure>

<p>（3）创建Flume Agent配置文件flume-netcat-logger.conf</p>
<p>在flume目录下创建job文件夹并进入job文件夹。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ mkdir job</span><br><span class="line"></span><br><span class="line">[vincent@linux1 flume]$ cd job/</span><br></pre></td></tr></table></figure>

<p>在job文件夹下创建Flume Agent配置文件netcat-logger.conf。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 job]$ vim netcat-logger.conf</span><br></pre></td></tr></table></figure>

<p>在flume-netcat-logger.conf文件中添加如下内容。</p>
<p>添加内容如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinks = k1</span><br><span class="line"></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = 0.0.0.0</span><br><span class="line">a1.sources.r1.port = 6666</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line"></span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line"></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line"><span class="meta">#</span><span class="bash"> the maximum number of events stored <span class="keyword">in</span> the channel</span></span><br><span class="line">a1.channels.c1.capacity = 10000</span><br><span class="line"><span class="meta">#</span><span class="bash"> the maximum number of events the channel will take from</span> </span><br><span class="line"><span class="meta">#</span><span class="bash"> a <span class="built_in">source</span> or give to a sink per transaction</span> </span><br><span class="line">a1.channels.c1.transactionCapacity = 10000</span><br><span class="line">a1.channels.c1.byteCapacityBufferPercentage = 20</span><br><span class="line">a1.channels.c1.byteCapacity = 800000</span><br></pre></td></tr></table></figure>

<p>注：配置文件来源于官方手册<a href="http://flume.apache.org/FlumeUserGuide.html">http://flume.apache.org/FlumeUserGuide.html</a></p>
<p>4）修改log4j.properties，将数据输出到控制台</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim /opt/module/hive/conf/log4j.properties</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">flume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<p>5）先开启flume监听端口</p>
<p>第一种写法：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/netcat-logger.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<p>第二种写法：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ bin/flume-ng agent -c conf/ -n a1 -f job/netcat-logger.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<p>参数说明：</p>
<p>​    –conf/-c：表示配置文件存储在conf/目录</p>
<p>​    –name/-n：表示给agent起名为a1</p>
<p>​    –conf-file/-f：flume本次启动读取的配置文件是在job文件夹下的flume-telnet.conf文件。</p>
<p>​    -Dflume.root.logger=INFO,console ：-D表示flume运行时动态修改flume.root.logger参数属性值，并将控制台日志打印级别设置为INFO级别。日志级别包括:log、info、warn、error。</p>
<p>（6）使用netcat工具向本机的6666端口发送内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 ~]$ nc localhost 6666</span><br></pre></td></tr></table></figure>

<p>（7）在Flume监听页面观察接收数据情况</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="number">2020</span>-<span class="number">06</span>-<span class="number">16</span> <span class="number">10</span>:<span class="number">18</span>:<span class="number">22</span>,<span class="number">393</span> (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.<span class="built_in">process</span>(LoggerSink.java:<span class="number">95</span>)] Event: &#123; headers:&#123;&#125; <span class="selector-tag">body</span>: <span class="number">61</span> <span class="number">61</span> <span class="number">61</span> <span class="number">61</span>                                     aaaa &#125;</span><br><span class="line"><span class="number">2020</span>-<span class="number">06</span>-<span class="number">16</span> <span class="number">10</span>:<span class="number">18</span>:<span class="number">32</span>,<span class="number">305</span> (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.<span class="built_in">process</span>(LoggerSink.java:<span class="number">95</span>)] Event: &#123; headers:&#123;&#125; <span class="selector-tag">body</span>: <span class="number">73</span> <span class="number">73</span> <span class="number">73</span> <span class="number">73</span>                                     ssss &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="实时监控单个追加文件"><a href="#实时监控单个追加文件" class="headerlink" title="实时监控单个追加文件"></a>实时监控单个追加文件</h3><p>实时监控Hive日志，并上传到HDFS中</p>
<p><img src="/Flume/5.png" alt="5"></p>
<p>（1）Flume要想将数据输出到HDFS，依赖Hadoop相关jar包</p>
<p>检查/etc/profile.d/my_env.sh文件，确认Hadoop和Java环境变量配置正确</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">JAVA_HOME</span></span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_212</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"> </span></span><br><span class="line"><span class="meta">#</span><span class="bash">HADOOP_HOME</span></span><br><span class="line">export HADOOP_HOME=/opt/module/hadoop-3.1.3</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure>

<p>（2）创建exec-hdfs.conf文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 job]$ vim exec-hdfs.conf</span><br></pre></td></tr></table></figure>

<p>注：要想读取Linux系统中的文件，就得按照Linux命令的规则执行命令。由于Hive日志在Linux系统中所以读取文件的类型选择：exec即execute执行的意思，表示执行Linux命令来读取文件。</p>
<p>添加如下内容:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a2.sources = r2</span><br><span class="line">a2.sinks = k2</span><br><span class="line">a2.channels = c2</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a2.sources.r2.type = exec</span><br><span class="line"><span class="meta">#</span><span class="bash"> 不能实现断点续传，可能会在flume挂掉的时候丢失数据</span></span><br><span class="line">a2.sources.r2.command = tail -F /opt/module/hive/logs/hive.log</span><br><span class="line"><span class="meta">#</span><span class="bash"> 下面一行配置可以使上面一行执行更复杂的命令，如增加管道符运算等</span></span><br><span class="line">a2.sources.r2.shell = /bin/bash -c</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a2.sinks.k2.type = hdfs</span><br><span class="line">a2.sinks.k2.hdfs.path = hdfs://linux1:9820/flume/%Y%m%d/%H</span><br><span class="line"><span class="meta">#</span><span class="bash">上传文件的前缀</span></span><br><span class="line">a2.sinks.k2.hdfs.filePrefix = logs-</span><br><span class="line"><span class="meta">#</span><span class="bash">是否对时间戳取整</span></span><br><span class="line">a2.sinks.k2.hdfs.round = true</span><br><span class="line"><span class="meta">#</span><span class="bash">多少时间单位创建一个新的文件夹</span></span><br><span class="line">a2.sinks.k2.hdfs.roundValue = 2</span><br><span class="line"><span class="meta">#</span><span class="bash">重新定义时间单位</span></span><br><span class="line">a2.sinks.k2.hdfs.roundUnit = hour</span><br><span class="line"><span class="meta">#</span><span class="bash">是否使用本地时间戳</span></span><br><span class="line">a2.sinks.k2.hdfs.useLocalTimeStamp = true</span><br><span class="line"><span class="meta">#</span><span class="bash">积攒多少个Event才flush到HDFS一次，小于等于transactionCapacity</span></span><br><span class="line">a2.sinks.k2.hdfs.batchSize = 100</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">设置文件类型，可支持压缩</span></span><br><span class="line">a2.sinks.k2.hdfs.fileType = DataStream</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">多久生成一个新的目录（60秒钟滚动一次）</span></span><br><span class="line">a2.sinks.k2.hdfs.rollInterval = 60</span><br><span class="line"><span class="meta">#</span><span class="bash">设置每个目录的滚动大小（128M滚动一次）</span></span><br><span class="line">a2.sinks.k2.hdfs.rollSize = 134217700</span><br><span class="line"><span class="meta">#</span><span class="bash">目录的滚动与Event数量无关(多少个event写到一个文件)</span></span><br><span class="line">a2.sinks.k2.hdfs.rollCount = 0</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a2.channels.c2.type = memory</span><br><span class="line">a2.channels.c2.capacity = 1000</span><br><span class="line">a2.channels.c2.transactionCapacity = 100</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a2.sources.r2.channels = c2</span><br><span class="line">a2.sinks.k2.channel = c2</span><br></pre></td></tr></table></figure>

<p>注意：</p>
<p>对于所有与时间相关的转义序列，Event Header中必须存在以 “timestamp”的key（除非hdfs.useLocalTimeStamp设置为true，此方法会使用TimestampInterceptor自动添加timestamp）。</p>
<p>a3.sinks.k3.hdfs.useLocalTimeStamp = true</p>
<p>（3）启动HDFS并等待安全模式退出</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ mycluster start[vincent@linux1 flume]$ hdfs dfsadmin -safemode wait</span><br></pre></td></tr></table></figure>

<p>（4）运行Flume</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/exec-hdfs.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<p>（5）开启Hadoop和Hive并操作Hive产生日志</p>
<figure class="highlight elixir"><table><tr><td class="code"><pre><span class="line">[vincent<span class="variable">@linux1</span> hadoop<span class="number">-3.1</span>.<span class="number">3</span>]<span class="variable">$ </span>sbin/start-dfs.sh[vincent<span class="variable">@linux1</span> hadoop<span class="number">-3.1</span>.<span class="number">3</span>]<span class="variable">$ </span>sbin/start-yarn.sh[vincent<span class="variable">@linux1</span> hive]<span class="variable">$ </span>bin/hivehive (default)&gt;</span><br></pre></td></tr></table></figure>

<p>（6）在HDFS上查看文件</p>
<p>linux1:9820</p>
<h3 id="实时监控目录下多个新文件"><a href="#实时监控目录下多个新文件" class="headerlink" title="实时监控目录下多个新文件"></a>实时监控目录下多个新文件</h3><p>使用Flume监听整个目录的文件，并上传至HDFS。</p>
<p><img src="/Flume/6.png" alt="6"></p>
<p>（1）创建配置文件spooldir-hdfs.conf</p>
<p>创建一个文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 job]$ vim spooldir-hdfs.conf</span><br></pre></td></tr></table></figure>

<p>添加如下内容：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a3.sources = r3</span><br><span class="line">a3.sinks = k3</span><br><span class="line">a3.channels = c3</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 监控一个目录下的多个文件</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 断点续传</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 延迟高，不能进行实时采集</span></span><br><span class="line">a3.sources.r3.type = spooldir</span><br><span class="line">a3.sources.r3.spoolDir = /opt/module/flume/upload</span><br><span class="line">a3.sources.r3.fileSuffix = .COMPLETED</span><br><span class="line">a3.sources.r3.fileHeader = true</span><br><span class="line"><span class="meta">#</span><span class="bash">忽略所有以.tmp结尾的文件，不上传</span></span><br><span class="line">a3.sources.r3.ignorePattern = \\S*\\.tmp</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a3.sinks.k3.type = hdfs</span><br><span class="line">a3.sinks.k3.hdfs.path = hdfs://linux1:9820/flume/upload/%Y%m%d/%H</span><br><span class="line"><span class="meta">#</span><span class="bash">上传文件的前缀</span></span><br><span class="line">a3.sinks.k3.hdfs.filePrefix = upload-</span><br><span class="line"><span class="meta">#</span><span class="bash">是否按照时间滚动文件夹</span></span><br><span class="line">a3.sinks.k3.hdfs.round = true</span><br><span class="line"><span class="meta">#</span><span class="bash">多少时间单位创建一个新的文件夹</span></span><br><span class="line">a3.sinks.k3.hdfs.roundValue = 1</span><br><span class="line"><span class="meta">#</span><span class="bash">重新定义时间单位</span></span><br><span class="line">a3.sinks.k3.hdfs.roundUnit = hour</span><br><span class="line"><span class="meta">#</span><span class="bash">是否使用本地时间戳</span></span><br><span class="line">a3.sinks.k3.hdfs.useLocalTimeStamp = true</span><br><span class="line"><span class="meta">#</span><span class="bash">积攒多少个Event才flush到HDFS一次</span></span><br><span class="line">a3.sinks.k3.hdfs.batchSize = 100</span><br><span class="line"><span class="meta">#</span><span class="bash">设置文件类型，可支持压缩</span></span><br><span class="line">a3.sinks.k3.hdfs.fileType = DataStream</span><br><span class="line"><span class="meta">#</span><span class="bash">多久生成一个新的文件</span></span><br><span class="line">a3.sinks.k3.hdfs.rollInterval = 60</span><br><span class="line"><span class="meta">#</span><span class="bash">设置每个文件的滚动大小大概是128M</span></span><br><span class="line">a3.sinks.k3.hdfs.rollSize = 134217700</span><br><span class="line"><span class="meta">#</span><span class="bash">文件的滚动与Event数量无关</span></span><br><span class="line">a3.sinks.k3.hdfs.rollCount = 0</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a3.channels.c3.type = memory</span><br><span class="line">a3.channels.c3.capacity = 1000</span><br><span class="line">a3.channels.c3.transactionCapacity = 100</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a3.sources.r3.channels = c3</span><br><span class="line">a3.sinks.k3.channel = c3</span><br></pre></td></tr></table></figure>

<p>（2）启动监控文件夹命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/spooldir-hdfs.conf</span><br></pre></td></tr></table></figure>

<p>说明：在使用Spooling Directory Source时，不要在监控目录中创建并持续修改文件；上传完成的文件会以.COMPLETED结尾；被监控文件夹每500毫秒扫描一次文件变动。</p>
<p>（3）向upload文件夹中添加文件</p>
<p>在/opt/module/flume目录下创建upload文件夹</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ mkdir upload</span><br></pre></td></tr></table></figure>

<p>向upload文件夹中添加文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 upload]$ touch vincent.txt</span><br><span class="line">[vincent@linux1 upload]$ touch vincent.tmp</span><br><span class="line">[vincent@linux1 upload]$ touch vincent.log</span><br></pre></td></tr></table></figure>

<p>（4）查看HDFS上的数据</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><span class="line"><span class="number">2020</span>-<span class="number">06</span>-<span class="number">17</span> <span class="number">11</span>:<span class="number">05</span>:<span class="number">23</span>,<span class="number">957</span> (hdfs-k3-roll-timer-<span class="number">0</span>) [INFO - org.apache.flume.sink.hdfs.HDFSEventSink<span class="variable">$1</span>.run(HDFSEventSink.java:<span class="number">393</span>)] Writer callback called.</span><br><span class="line"><span class="number">2020</span>-<span class="number">06</span>-<span class="number">17</span> <span class="number">11</span>:<span class="number">05</span>:<span class="number">23</span>,<span class="number">958</span> (hdfs-k3-roll-timer-<span class="number">0</span>) [INFO - org.apache.flume.sink.hdfs.BucketWriter.doClose(BucketWriter.java:<span class="number">438</span>)] Closing hdfs:<span class="regexp">//</span>linux1:<span class="number">9820</span><span class="regexp">/flume/u</span>pload<span class="regexp">/20200617/</span><span class="number">11</span>/upload-.<span class="number">1592362973087</span>.tmp</span><br><span class="line"><span class="number">2020</span>-<span class="number">06</span>-<span class="number">17</span> <span class="number">11</span>:<span class="number">05</span>:<span class="number">23</span>,<span class="number">967</span> (hdfs-k3-call-runner-<span class="number">5</span>) [INFO - org.apache.flume.sink.hdfs.BucketWriter<span class="variable">$7</span>.call(BucketWriter.java:<span class="number">681</span>)] Renaming hdfs:<span class="regexp">//</span>linux1:<span class="number">9820</span><span class="regexp">/flume/u</span>pload<span class="regexp">/20200617/</span><span class="number">11</span><span class="regexp">/upload-.1592362973087.tmp to hdfs:/</span><span class="regexp">/linux1:9820/</span>flume<span class="regexp">/upload/</span><span class="number">20200617</span><span class="regexp">/11/u</span>pload-.<span class="number">1592362973087</span></span><br><span class="line"><span class="number">2020</span>-<span class="number">06</span>-<span class="number">17</span> <span class="number">11</span>:<span class="number">05</span>:<span class="number">27</span>,<span class="number">909</span> (pool-<span class="number">5</span>-thread-<span class="number">1</span>) [INFO - org.apache.flume.client.avro.ReliableSpoolingFileEventReader.readEvents(ReliableSpoolingFileEventReader.java:<span class="number">384</span>)] Last read took us just up to a file boundary. Rolling to the <span class="keyword">next</span> file, <span class="keyword">if</span> there is one.</span><br><span class="line"><span class="number">2020</span>-<span class="number">06</span>-<span class="number">17</span> <span class="number">11</span>:<span class="number">05</span>:<span class="number">27</span>,<span class="number">909</span> (pool-<span class="number">5</span>-thread-<span class="number">1</span>) [INFO - org.apache.flume.client.avro.ReliableSpoolingFileEventReader.rollCurrentFile(ReliableSpoolingFileEventReader.java:<span class="number">497</span>)] Preparing to move file <span class="regexp">/opt/m</span>odule<span class="regexp">/flume/u</span>pload<span class="regexp">/d.txt to /</span>opt<span class="regexp">/module/</span>flume<span class="regexp">/upload/</span>d.txt.COMPLETED</span><br><span class="line"><span class="number">2020</span>-<span class="number">06</span>-<span class="number">17</span> <span class="number">11</span>:<span class="number">05</span>:<span class="number">28</span>,<span class="number">982</span> (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.hdfs.HDFSDataStream.configure(HDFSDataStream.java:<span class="number">57</span>)] Serializer = TEXT, UseRawLocalFileSystem = false</span><br><span class="line"><span class="number">2020</span>-<span class="number">06</span>-<span class="number">17</span> <span class="number">11</span>:<span class="number">05</span>:<span class="number">28</span>,<span class="number">992</span> (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.hdfs.BucketWriter.open(BucketWriter.java:<span class="number">246</span>)] Creating hdfs:<span class="regexp">//</span>linux1:<span class="number">9820</span><span class="regexp">/flume/u</span>pload<span class="regexp">/20200617/</span><span class="number">11</span>/upload-.<span class="number">1592363128983</span>.tmp</span><br><span class="line"><span class="number">2020</span>-<span class="number">06</span>-<span class="number">17</span> <span class="number">11</span>:<span class="number">05</span>:<span class="number">32</span>,<span class="number">011</span> (Thread-<span class="number">20</span>) [INFO - org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:<span class="number">239</span>)] SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false</span><br></pre></td></tr></table></figure>

<h3 id="实时监控目录下的多个追加文件"><a href="#实时监控目录下的多个追加文件" class="headerlink" title="实时监控目录下的多个追加文件"></a>实时监控目录下的多个追加文件</h3><p><strong>Exec source适用于监控一个实时追加的文件，不能实现断点续传；Spooldir Source适合用于同步新文件，但不适合对实时追加日志的文件进行监听并同步；而Taildir Source适合用于监听多个实时追加的文件，并且能够实现断点续传。</strong></p>
<p>使用Flume监听整个目录的实时追加文件，并上传至HDFS。</p>
<p><img src="/Flume/7.png" alt="7"></p>
<p>（1）创建配置文件taildir-hdfs.conf</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 job]$ vim taildir-hdfs.conf</span><br></pre></td></tr></table></figure>

<p>添加如下内容：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a3.sources = r3</span><br><span class="line">a3.sinks = k3</span><br><span class="line">a3.channels = c3</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a3.sources.r3.type = TAILDIR</span><br><span class="line">a3.sources.r3.positionFile = /opt/module/flume/tail_dir.json</span><br><span class="line">a3.sources.r3.filegroups = f1 f2</span><br><span class="line">a3.sources.r3.filegroups.f1 = /opt/module/flume/files/.*file.*</span><br><span class="line">a3.sources.r3.filegroups.f2 = /opt/module/flume/files2/.*</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a3.sinks.k3.type = hdfs</span><br><span class="line">a3.sinks.k3.hdfs.path = hdfs://linux1:9820/flume/upload2/%Y%m%d/%H</span><br><span class="line"><span class="meta">#</span><span class="bash">上传文件的前缀</span></span><br><span class="line">a3.sinks.k3.hdfs.filePrefix = upload-</span><br><span class="line"><span class="meta">#</span><span class="bash">是否按照时间滚动文件夹</span></span><br><span class="line">a3.sinks.k3.hdfs.round = true</span><br><span class="line"><span class="meta">#</span><span class="bash">多少时间单位创建一个新的文件夹</span></span><br><span class="line">a3.sinks.k3.hdfs.roundValue = 1</span><br><span class="line"><span class="meta">#</span><span class="bash">重新定义时间单位</span></span><br><span class="line">a3.sinks.k3.hdfs.roundUnit = hour</span><br><span class="line"><span class="meta">#</span><span class="bash">是否使用本地时间戳</span></span><br><span class="line">a3.sinks.k3.hdfs.useLocalTimeStamp = true</span><br><span class="line"><span class="meta">#</span><span class="bash">积攒多少个Event才flush到HDFS一次</span></span><br><span class="line">a3.sinks.k3.hdfs.batchSize = 100</span><br><span class="line"><span class="meta">#</span><span class="bash">设置文件类型，可支持压缩</span></span><br><span class="line">a3.sinks.k3.hdfs.fileType = DataStream</span><br><span class="line"><span class="meta">#</span><span class="bash">多久生成一个新的文件</span></span><br><span class="line">a3.sinks.k3.hdfs.rollInterval = 60</span><br><span class="line"><span class="meta">#</span><span class="bash">设置每个文件的滚动大小大概是128M</span></span><br><span class="line">a3.sinks.k3.hdfs.rollSize = 134217700</span><br><span class="line"><span class="meta">#</span><span class="bash">文件的滚动与Event数量无关</span></span><br><span class="line">a3.sinks.k3.hdfs.rollCount = 0</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a3.channels.c3.type = memory</span><br><span class="line">a3.channels.c3.capacity = 1000</span><br><span class="line">a3.channels.c3.transactionCapacity = 100</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a3.sources.r3.channels = c3</span><br><span class="line">a3.sinks.k3.channel = c3</span><br></pre></td></tr></table></figure>

<p>（2）创建目录files和files2</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ mkdir files files2</span><br></pre></td></tr></table></figure>

<p>（2）启动监控文件夹命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file jobs/taildir-hdfs.conf</span><br></pre></td></tr></table></figure>

<p>（3）向files文件夹中追加内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 files]$ echo hello &gt;&gt; file1.txt</span><br><span class="line">[vincent@linux1 files]$ echo world &gt;&gt; file2.txt</span><br></pre></td></tr></table></figure>

<p>（4）查看HDFS上的数据</p>
<p>Taildir说明：</p>
<p>Taildir Source维护了一个json格式的position File，其会定期的往position File中更新每个文件读取到的最新的位置，因此能够实现断点续传。Position File的格式如下：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;<span class="attr">&quot;inode&quot;</span>:<span class="number">2496272</span>,<span class="attr">&quot;pos&quot;</span>:<span class="number">12</span>,<span class="attr">&quot;file&quot;</span>:<span class="string">&quot;/opt/module/flume/files/file1.txt&quot;</span>&#125;&#123;<span class="attr">&quot;inode&quot;</span>:<span class="number">2496275</span>,<span class="attr">&quot;pos&quot;</span>:<span class="number">12</span>,<span class="attr">&quot;file&quot;</span>:<span class="string">&quot;/opt/module/flume/files/file2.txt&quot;</span>&#125;</span><br></pre></td></tr></table></figure>

<p>注：Linux中储存文件元数据的区域就叫做inode，每个inode都有一个号码，操作系统用inode号码来识别不同的文件，Unix/Linux系统内部不使用文件名，而使用inode号码来识别文件。</p>
<h2 id="Flume事务"><a href="#Flume事务" class="headerlink" title="Flume事务"></a>Flume事务</h2><p><img src="/Flume/142.png" alt="142"></p>
<h2 id="Flume-Agent内部原理"><a href="#Flume-Agent内部原理" class="headerlink" title="Flume Agent内部原理"></a>Flume Agent内部原理</h2><p><img src="/Flume/143.png" alt="143"></p>
<h3 id="1）ChannelSelector"><a href="#1）ChannelSelector" class="headerlink" title="1）ChannelSelector"></a>1）ChannelSelector</h3><p>ChannelSelector的作用就是选出Event将要被发往哪个Channel。其共有两种类型，分别是<strong>Replicating（复制）</strong>和<strong>Multiplexing（多路复用）</strong>。</p>
<p><strong>ReplicatingSelector会将同一个Event发往所有的Channel，Multiplexing会根据相应的原则，将不同的Event发往不同的Channel。</strong></p>
<h3 id="2）SinkProcessor"><a href="#2）SinkProcessor" class="headerlink" title="2）SinkProcessor"></a>2）SinkProcessor</h3><p>SinkProcessor共有三种类型，分别是<strong>DefaultSinkProcessor、LoadBalancingSinkProcessor和FailoverSinkProcessor</strong></p>
<p><strong>DefaultSinkProcessor对应的是单个的Sink，LoadBalancingSinkProcessor和FailoverSinkProcessor对应的是Sink Group，LoadBalancingSinkProcessor可以实现负载均衡的功能，FailoverSinkProcessor具有错误恢复的功能。</strong></p>
<h2 id="Flume拓扑结构"><a href="#Flume拓扑结构" class="headerlink" title="Flume拓扑结构"></a>Flume拓扑结构</h2><h3 id="简单串联"><a href="#简单串联" class="headerlink" title="简单串联"></a>简单串联</h3><p><img src="/Flume/144.png" alt="144"></p>
<p>这种模式是将多个flume顺序连接起来了，从最初的source开始到最终sink传送的目的存储系统。此模式不建议桥接过多的flume数量， flume数量过多不仅会影响传输速率，而且一旦传输过程中某个节点flume宕机，会影响整个传输系统。</p>
<h3 id="复制和多路复用"><a href="#复制和多路复用" class="headerlink" title="复制和多路复用"></a>复制和多路复用</h3><p><img src="/Flume/145.png" alt="145"></p>
<p>Flume支持将事件流向一个或者多个目的地。这种模式可以将相同数据复制到多个channel中，或者将不同数据分发到不同的channel中，sink可以选择传送到不同的目的地。</p>
<h3 id="负载均衡和故障转移"><a href="#负载均衡和故障转移" class="headerlink" title="负载均衡和故障转移"></a>负载均衡和故障转移</h3><p><img src="/Flume/146.png" alt="146"></p>
<p>Flume支持使用将多个sink逻辑上分到一个sink组，sink组配合不同的SinkProcessor可以实现负载均衡和错误恢复的功能。</p>
<h3 id="聚合"><a href="#聚合" class="headerlink" title="聚合"></a>聚合</h3><p><img src="/Flume/147.png" alt="147"></p>
<p>这种模式是我们最常见的，也非常实用，日常web应用通常分布在上百个服务器，大者甚至上千个、上万个服务器。产生的日志，处理起来也非常麻烦。用flume的这种组合方式能很好的解决这一问题，每台服务器部署一个flume采集日志，传送到一个集中收集日志的flume，再由此flume上传到hdfs、hive、hbase等，进行日志分析。</p>
<h2 id="Flume企业开发案例"><a href="#Flume企业开发案例" class="headerlink" title="Flume企业开发案例"></a>Flume企业开发案例</h2><h3 id="复制和多路复用-1"><a href="#复制和多路复用-1" class="headerlink" title="复制和多路复用"></a>复制和多路复用</h3><p>使用Flume-1监控文件变动，Flume-1将变动内容传递给Flume-2，Flume-2负责存储到HDFS。同时Flume-1将变动内容传递给Flume-3，Flume-3负责输出到Local FileSystem。</p>
<p>注：先启动服务端flume（后面的）</p>
<p><img src="/Flume/8.png" alt="8"></p>
<p>（1）准备工作</p>
<p>配置f1在linux1，配置f2在linux2，配置f3在linux3</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 job]$ mkdir replicating</span><br><span class="line">[vincent@linux2 job]$ mkdir replicating</span><br><span class="line">[vincent@linux3 job]$ mkdir replicating</span><br></pre></td></tr></table></figure>

<p>（2）创建a1.conf</p>
<p>配置1个接收日志文件的source和两个channel、两个sink，分别输送给flume-hdfs和flume-dir。</p>
<p>编辑配置文件：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 replicating]$ vim taildir-avro.conf</span><br></pre></td></tr></table></figure>

<p>添加如下内容：</p>
<p>注：channel selector没配置默认使用复制模式</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line">a1.channels = c1 c2</span><br><span class="line"><span class="meta">#</span><span class="bash"> 将数据流复制给所有channel</span></span><br><span class="line">a1.sources.r1.selector.type = replicating</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = TAILDIR</span><br><span class="line">a1.sources.r1.positionFile = /opt/module/flume/t1.json</span><br><span class="line">a1.sources.r1.filegroups = f1</span><br><span class="line">a1.sources.r1.filegroups.f1 = /opt/module/hive/logs/hive.log</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> sink端的avro是一个数据发送者</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line"><span class="meta">#</span><span class="bash"> 服务端ip</span></span><br><span class="line">a1.sinks.k1.hostname = linux2</span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line"></span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line"><span class="meta">#</span><span class="bash"> 服务端ip</span></span><br><span class="line">a1.sinks.k2.hostname = linux3</span><br><span class="line">a1.sinks.k2.port = 4141</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line">a1.channels.c2.type = memory</span><br><span class="line">a1.channels.c2.capacity = 1000</span><br><span class="line">a1.channels.c2.transactionCapacity = 100</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1 c2</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c2</span><br></pre></td></tr></table></figure>

<p>（3）创建a2.conf</p>
<p>配置上级Flume输出的Source，输出是到HDFS的Sink。</p>
<p>编辑配置文件：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux2 replicating]$ vim avro-hdfs.conf</span><br></pre></td></tr></table></figure>

<p>添加如下内容：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.sinks = k1</span><br><span class="line">a2.channels = c1</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">source</span>端的avro是一个数据接收服务</span></span><br><span class="line">a2.sources.r1.type = avro</span><br><span class="line">a2.sources.r1.bind = linux2</span><br><span class="line">a2.sources.r1.port = 4141</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a2.sinks.k1.type = hdfs</span><br><span class="line">a2.sinks.k1.hdfs.path = hdfs://linux1:9820/flume2/%Y%m%d/%H</span><br><span class="line"><span class="meta">#</span><span class="bash">上传文件的前缀</span></span><br><span class="line">a2.sinks.k1.hdfs.filePrefix = flume2-</span><br><span class="line"><span class="meta">#</span><span class="bash">是否按照时间滚动文件夹</span></span><br><span class="line">a2.sinks.k1.hdfs.round = true</span><br><span class="line"><span class="meta">#</span><span class="bash">多少时间单位创建一个新的文件夹</span></span><br><span class="line">a2.sinks.k1.hdfs.roundValue = 1</span><br><span class="line"><span class="meta">#</span><span class="bash">重新定义时间单位</span></span><br><span class="line">a2.sinks.k1.hdfs.roundUnit = hour</span><br><span class="line"><span class="meta">#</span><span class="bash">是否使用本地时间戳</span></span><br><span class="line">a2.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line"><span class="meta">#</span><span class="bash">积攒多少个Event才flush到HDFS一次</span></span><br><span class="line">a2.sinks.k1.hdfs.batchSize = 100</span><br><span class="line"><span class="meta">#</span><span class="bash">设置文件类型，可支持压缩</span></span><br><span class="line">a2.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line"><span class="meta">#</span><span class="bash">多久生成一个新的文件</span></span><br><span class="line">a2.sinks.k1.hdfs.rollInterval = 600</span><br><span class="line"><span class="meta">#</span><span class="bash">设置每个文件的滚动大小大概是128M</span></span><br><span class="line">a2.sinks.k1.hdfs.rollSize = 134217700</span><br><span class="line"><span class="meta">#</span><span class="bash">文件的滚动与Event数量无关</span></span><br><span class="line">a2.sinks.k1.hdfs.rollCount = 0</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p>（4）创建a3.conf</p>
<p>配置上级Flume输出的Source，输出是到本地目录的Sink。</p>
<p>编辑配置文件：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux3 replicating]$ vim avro-file.conf</span><br></pre></td></tr></table></figure>

<p>添加如下内容：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.sinks = k1</span><br><span class="line">a3.channels = c2</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line">a3.sources.r1.bind = linux3</span><br><span class="line">a3.sources.r1.port = 4141</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a3.sinks.k1.type = file_roll</span><br><span class="line"><span class="meta">#</span><span class="bash"> 需要手动创建下面本地路径</span></span><br><span class="line">a3.sinks.k1.sink.directory = /opt/module/datas/flume3</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a3.channels.c2.type = memory</span><br><span class="line">a3.channels.c2.capacity = 1000</span><br><span class="line">a3.channels.c2.transactionCapacity = 100</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a3.sources.r1.channels = c2</span><br><span class="line">a3.sinks.k1.channel = c2</span><br></pre></td></tr></table></figure>

<p>提示：输出的本地目录必须是已经存在的目录，如果该目录不存在，并不会创建新的目录。</p>
<p>（6）启动Hadoop和Hive</p>
<p>（7）先后启动后两个flume，最后启动第一个flume（a3、a2、a1）</p>
<p>（8）检查HDFS上数据</p>
<p>（9）检查/opt/module/datas/flume3目录中数据</p>
<h3 id="负载均衡和故障转移-1"><a href="#负载均衡和故障转移-1" class="headerlink" title="负载均衡和故障转移"></a>负载均衡和故障转移</h3><p>使用Flume1监控一个端口，其sink组中的sink分别对接Flume2和Flume3，采用FailoverSinkProcessor，实现故障转移的功能。</p>
<p>注：一个channel对应一个sink group</p>
<p><img src="/Flume/9.png" alt="9"></p>
<p>（1）准备工作</p>
<p>在/opt/module/flume/job目录下创建group2文件夹</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 job]$ mkdir failover</span><br></pre></td></tr></table></figure>

<p>（2）创建a1.conf.conf</p>
<p>配置1个netcat source和1个channel、1个sink group（2个sink），分别输送给a2.conf和a3.conf。</p>
<p>编辑配置文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 failover]$ vim a1.conf</span><br></pre></td></tr></table></figure>

<p>添加如下内容:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line"></span><br><span class="line">a1.sinkgroups = g1</span><br><span class="line">a1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line">a1.sinkgroups.g1.processor.type = failover</span><br><span class="line"><span class="meta">#</span><span class="bash"> 优先级高的工作，断了之后低的工作，连上之后高的继续工作</span></span><br><span class="line">a1.sinkgroups.g1.processor.priority.k1 = 5</span><br><span class="line">a1.sinkgroups.g1.processor.priority.k2 = 10</span><br><span class="line">a1.sinkgroups.g1.processor.maxpenalty = 10000</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = 0.0.0.0</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = linux2</span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line">a1.sinks.k2.hostname = linux3</span><br><span class="line">a1.sinks.k2.port = 4141</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c1</span><br></pre></td></tr></table></figure>

<p>（3）创建a2.conf</p>
<p>配置上级Flume输出的Source，输出是到本地控制台。</p>
<p>编辑配置文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux2 failover]$ vim a2.conf</span><br></pre></td></tr></table></figure>

<p>添加如下内容:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.sinks = k1</span><br><span class="line">a2.channels = c1</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a2.sources.r1.type = avro</span><br><span class="line">a2.sources.r1.bind = linux2</span><br><span class="line">a2.sources.r1.port = 4141</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a2.sinks.k1.type = logger</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="meta"> </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p>（4）创建a3.conf</p>
<p>配置上级Flume输出的Source，输出是到本地控制台。</p>
<p>编辑配置文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux3 failover]$ vim a3.conf</span><br></pre></td></tr></table></figure>

<p>添加如下内容:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.sinks = k1</span><br><span class="line">a3.channels = c2</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line">a3.sources.r1.bind = linux3</span><br><span class="line">a3.sources.r1.port = 4141</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a3.sinks.k1.type = logger</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a3.channels.c2.type = memory</span><br><span class="line">a3.channels.c2.capacity = 1000</span><br><span class="line">a3.channels.c2.transactionCapacity = 100</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a3.sources.r1.channels = c2</span><br><span class="line">a3.sinks.k1.channel = c2</span><br></pre></td></tr></table></figure>

<p>（5）执行配置文件</p>
<p>分别开启对应配置文件：a3.conf，a2.conf，a1.conf。</p>
<p>（6）使用netcat工具向本机的44444端口发送内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> nc localhost 44444</span></span><br></pre></td></tr></table></figure>

<p>（7）查看Flume2及Flume3的控制台打印日志</p>
<p>（8）将Flume2 kill，观察Flume3的控制台打印情况。</p>
<p>注：使用jps -ml查看Flume进程。</p>
<h3 id="聚合-1"><a href="#聚合-1" class="headerlink" title="聚合"></a>聚合</h3><p>linux1上的Flume-1监控文件/opt/module/group.log，</p>
<p>linux2上的Flume-2监控某一个端口的数据流，</p>
<p>Flume-1与Flume-2将数据发送给linux3上的Flume-3，Flume-3将最终数据打印到控制台。</p>
<p><img src="/Flume/10.png" alt="10"></p>
<p>（1）准备工作</p>
<p>分发Flume</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 module]$ xsync flume</span><br></pre></td></tr></table></figure>

<p>在linux1、linux2以及linux3的/opt/module/flume/job目录下创建一个consolidation文件夹。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 job]$ mkdir consolidation</span><br><span class="line">[vincent@linux2 job]$ mkdir consolidation</span><br><span class="line">[vincent@linux3 job]$ mkdir consolidation</span><br></pre></td></tr></table></figure>

<p>（2）创建a1.conf </p>
<p>配置Source用于监控hive.log文件，配置Sink输出数据到下一级Flume。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 consolidation]$ vim exec-avro.conf </span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">a1.sources.r1.command = tail -F /opt/module/hive/logs/hive.log</span><br><span class="line">a1.sources.r1.shell = /bin/bash -c</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = linux3</span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p>（3）创建a2.conf</p>
<p>配置Source监控端口44444数据流，配置Sink数据到下一级Flume：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux2 consolidation]$ vim netcat-avro.conf</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.sinks = k1</span><br><span class="line">a2.channels = c1</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a2.sources.r1.type = netcat</span><br><span class="line">a2.sources.r1.bind = linux2</span><br><span class="line">a2.sources.r1.port = 4444</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a2.sinks.k1.type = avro</span><br><span class="line">a2.sinks.k1.hostname = linux3</span><br><span class="line">a2.sinks.k1.port = 4141</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p>（4）创建a3.conf</p>
<p>配置source用于接收flume1与flume2发送过来的数据流，最终合并后sink到控制台。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux3 consolidation]$ vim avro-logger.conf</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.sinks = k1</span><br><span class="line">a3.channels = c1</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line">a3.sources.r1.bind = linux3</span><br><span class="line">a3.sources.r1.port = 4141</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a3.sinks.k1.type = logger</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a3.channels.c1.type = memory</span><br><span class="line">a3.channels.c1.capacity = 1000</span><br><span class="line">a3.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a3.sources.r1.channels = c1</span><br><span class="line">a3.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p>（5）分发配置文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume] xsync ../flume</span><br></pre></td></tr></table></figure>

<p>（6）执行配置文件</p>
<p>分别开启对应配置文件：avro-logger.conf，netcat-avro.conf，exec-avro.conf 。</p>
<p>（7）在linux1上向/opt/module目录下的hive.log追加内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 module]$ echo &#x27;hello&#x27; &gt; hive.log</span><br></pre></td></tr></table></figure>

<p>（7）向linux2的4444端口发送数据</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux2 flume]$ nc linux2 4444</span><br></pre></td></tr></table></figure>

<p>（8）检查linux3上数据</p>
<h3 id="自定义Interceptor"><a href="#自定义Interceptor" class="headerlink" title="自定义Interceptor"></a>自定义Interceptor</h3><p>1）案例需求</p>
<p>使用Flume采集服务器本地日志，需要按照日志类型的不同，将不同种类的日志发往不同的分析系统。</p>
<p>2）需求分析</p>
<p>在实际的开发中，一台服务器产生的日志类型可能有很多种，不同类型的日志可能需要发送到不同的分析系统。此时会用到Flume拓扑结构中的Multiplexing结构，Multiplexing的原理是，根据event中Header的某个key的值，将不同的event发送到不同的Channel中，所以我们需要自定义一个Interceptor，为不同类型的event的Header中的key赋予不同的值。</p>
<p>在该案例中，我们以端口数据模拟日志，以数字（单个）和字母（单个）模拟不同类型的日志，我们需要自定义interceptor区分数字和字母，将其分别发往不同的分析系统（Channel）。</p>
<p><img src="/Flume/11.png" alt="11"></p>
<p>3）实现步骤</p>
<p>（1）创建一个maven项目，并引入以下依赖。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flume<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flume-ng-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.9.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>（2）定义MyInterceptor类并实现Interceptor接口。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.vincent.flume.interceptor;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Context;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Event;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.interceptor.Interceptor;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 根据输入数据的首字符不同，添加不同的header来让ChannelSelector处理</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> Vincent</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span> 2020-06-17</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyInterceptor</span> <span class="keyword">implements</span> <span class="title">Interceptor</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 初始化方法，新建Interceptor时候用</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 更改方法 对event进行处理</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> event 传入数据</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span>  处理好的数据</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Event <span class="title">intercept</span><span class="params">(Event event)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">byte</span>[] body = event.getBody();</span><br><span class="line">        <span class="keyword">if</span> (body[<span class="number">0</span>] &lt; <span class="string">&#x27;z&#x27;</span> &amp;&amp; body[<span class="number">0</span>] &gt; <span class="string">&#x27;a&#x27;</span>) &#123;</span><br><span class="line">            event.getHeaders().put(<span class="string">&quot;type&quot;</span>, <span class="string">&quot;letter&quot;</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (body[<span class="number">0</span>] &gt; <span class="string">&#x27;0&#x27;</span> &amp;&amp; body[<span class="number">0</span>] &lt; <span class="string">&#x27;9&#x27;</span>) &#123;</span><br><span class="line">            event.getHeaders().put(<span class="string">&quot;type&quot;</span>, <span class="string">&quot;number&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> event;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 批处理方法，对传入的一批数据进行处理，source设置了batchSize</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> events</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span>  处理好的数据</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;Event&gt; <span class="title">intercept</span><span class="params">(List&lt;Event&gt; events)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (Event event : events) &#123;</span><br><span class="line">            intercept(event);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> events;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 如果有需要关闭的资源，在这个方法中关闭</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 框架会调用Builder来创建Interceptor实例</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyBuilder</span> <span class="keyword">implements</span> <span class="title">Interceptor</span>.<span class="title">Builder</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 创建实例的方法</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@return</span>  新的Interceptor</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Interceptor <span class="title">build</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> MyInterceptor();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 读取配置文件的方法</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> context   配置文件</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（3）编辑flume配置文件</p>
<p>为linux1上的Flume-1配置1个netcat source，1个sink group（2个avro sink），并配置相应的ChannelSelector和interceptor。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line">a1.channels = c1 c2</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = linux1</span><br><span class="line">a1.sources.r1.port = 4444</span><br><span class="line"></span><br><span class="line">a1.sources.r1.interceptors = i1</span><br><span class="line">a1.sources.r1.interceptors.i1.type = com.vincent.flume.interceptor.MyInterceptor$MyBuilder</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 多路复用模式</span></span><br><span class="line">a1.sources.r1.selector.type = multiplexing</span><br><span class="line">a1.sources.r1.selector.header = type</span><br><span class="line">a1.sources.r1.selector.mapping.letter = c1</span><br><span class="line">a1.sources.r1.selector.mapping.number = c2</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = linux2</span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line"></span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line">a1.sinks.k2.hostname = linux3</span><br><span class="line">a1.sinks.k2.port = 4141</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c2.type = memory</span><br><span class="line">a1.channels.c2.capacity = 1000</span><br><span class="line">a1.channels.c2.transactionCapacity = 100</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1 c2</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c2</span><br></pre></td></tr></table></figure>

<p>为linux2上的Flume-2配置一个avro source和一个logger sink。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">a2.sources = r1</span><br><span class="line">a2.sinks = k1</span><br><span class="line">a2.channels = c1</span><br><span class="line"></span><br><span class="line">a2.sources.r1.type = avro</span><br><span class="line">a2.sources.r1.bind = linux2</span><br><span class="line">a2.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line">a2.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line">a2.sinks.k1.channel = c1</span><br><span class="line">a2.sources.r1.channels = c1</span><br></pre></td></tr></table></figure>

<p>为linux3上的Flume-3配置一个avro source和一个logger sink。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">a3.sources = r1</span><br><span class="line">a3.sinks = k1</span><br><span class="line">a3.channels = c1</span><br><span class="line"></span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line">a3.sources.r1.bind = linux3</span><br><span class="line">a3.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line">a3.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line">a3.channels.c1.type = memory</span><br><span class="line">a3.channels.c1.capacity = 1000</span><br><span class="line">a3.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line">a3.sinks.k1.channel = c1</span><br><span class="line">a3.sources.r1.channels = c1</span><br></pre></td></tr></table></figure>

<p>（4）分别在linux3，linux2，linux1上启动flume进程，注意先后顺序。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux3 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/interceptor/a3.conf -Dflume.root.logger=INFO,console</span><br><span class="line">[vincent@linux2 flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/interceptor/a2.conf -Dflume.root.logger=INFO,console</span><br><span class="line">[vincent@linux1 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/interceptor/a1.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<p>（5）像linux1的44444端口发送字母和数字。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 ~]$ nc linux1 4444</span><br></pre></td></tr></table></figure>

<p>（6）观察linux2和linux3打印的日志。</p>
<h3 id="自定义Source"><a href="#自定义Source" class="headerlink" title="自定义Source"></a>自定义Source</h3><p>Source是负责接收数据到Flume Agent的组件。Source组件可以处理各种类型、各种格式的日志数据，包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy。官方提供的source类型已经很多，但是有时候并不能满足实际开发当中的需求，此时我们就需要根据实际需求自定义某些source。</p>
<p>官方也提供了自定义source的接口：</p>
<p><a href="#source">https://flume.apache.org/FlumeDeveloperGuide.html#source</a> </p>
<p>根据官方说明自定义MySource需要继承AbstractSource类并实现Configurable和PollableSource接口。</p>
<p>实现相应方法：</p>
<p>getBackOffSleepIncrement()            //暂不用</p>
<p>getMaxBackOffSleepInterval()         //暂不用</p>
<p>configure(Context context)             //初始化context（读取配置文件内容）</p>
<p>process()                                         //获取数据封装成event并写入channel，这个方法将被循环调用。</p>
<p>使用场景：读取MySQL数据或者其他文件系统。</p>
<p>需求：使用flume接收数据，并给每条数据添加前缀，输出到控制台。前缀可从flume配置文件中配置。</p>
<p><img src="/Flume/12.png" alt="12"></p>
<p><img src="/Flume/14.png" alt="14"></p>
<p>1）导入pom依赖</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flume<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flume-ng-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.9.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>2）编写代码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.vincent.flume.source;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flume.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.channel.ChannelProcessor;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.conf.Configurable;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.event.SimpleEvent;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.source.AbstractSource;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> Vincent</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span> 2020-06-17</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MySource</span> <span class="keyword">extends</span> <span class="title">AbstractSource</span> <span class="keyword">implements</span> <span class="title">Configurable</span>, <span class="title">PollableSource</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String prefff;</span><br><span class="line">    <span class="keyword">private</span> Long interval;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 拉取事件并交给ChannelProcessor处理的方法，循环调用</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> EventDeliveryException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Status <span class="title">process</span><span class="params">()</span> <span class="keyword">throws</span> EventDeliveryException </span>&#123;</span><br><span class="line">        Status status = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">//通过外部方法拉取数据</span></span><br><span class="line">            Event e = getSomeData();</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Store the Event into this Source&#x27;s associated Channel(s)</span></span><br><span class="line">            <span class="comment">//getChannelProcessor().processEvent(e);</span></span><br><span class="line">            ChannelProcessor channelProcessor = getChannelProcessor();</span><br><span class="line">            channelProcessor.processEvent(e);</span><br><span class="line"></span><br><span class="line">            status = Status.READY;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">            <span class="comment">// Log exception, handle individual exceptions as needed</span></span><br><span class="line"></span><br><span class="line">            status = Status.BACKOFF;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// re-throw all Errors</span></span><br><span class="line">            <span class="keyword">if</span> (t <span class="keyword">instanceof</span> Error) &#123;</span><br><span class="line">                <span class="keyword">throw</span> (Error)t;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> status;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 拉取数据并包装成Event的过程</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span>  拉取到的Event</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> Event <span class="title">getSomeData</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">//通过随机数模拟拉取到的数据</span></span><br><span class="line">        <span class="keyword">int</span> i = (<span class="keyword">int</span>) (Math.random() * <span class="number">1000</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//添加前缀</span></span><br><span class="line">        String message = prefix + i;</span><br><span class="line"></span><br><span class="line">        Thread.sleep(interval);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//包装成Event</span></span><br><span class="line">        Event event = <span class="keyword">new</span> SimpleEvent();</span><br><span class="line">        event.setBody(message.getBytes());</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> event;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 如果拉取不到数据，Backoff时间的增长速度</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span>  增长量</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getBackOffSleepIncrement</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1000</span>;    <span class="comment">//1s</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 最大等待时间</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span>  时间</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getMaxBackOffSleepInterval</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">10000</span>;   <span class="comment">//10s</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 来自configurable，可以定义我们的自定义Source</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context   配置文件</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line">        prefix = context.getString(<span class="string">&quot;prefff&quot;</span>,<span class="string">&quot;xxxx&quot;</span>);</span><br><span class="line">        interval = context.getLong(<span class="string">&quot;interval&quot;</span>, <span class="number">500L</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>3）测试</p>
<p>（1）打包</p>
<p>将写好的代码打包，并放到flume的lib目录（/opt/module/flume）下。</p>
<p>（2）配置文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ vim job/custom-logger.conf </span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = com.vincent.flume.source.MySource</span><br><span class="line">a1.sources.r1.interval = 100</span><br><span class="line">a1.sources.r1.prefff = ABCD-</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p>3）开启任务</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ bin/flume-ng agent -c conf/ -f job/custom-logger.conf -n a1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<h3 id="自定义Sink"><a href="#自定义Sink" class="headerlink" title="自定义Sink"></a>自定义Sink</h3><p>1）介绍</p>
<p>Sink不断地轮询Channel中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另一个Flume Agent。</p>
<p>Sink是完全事务性的。在从Channel批量删除数据之前，每个Sink用Channel启动一个事务。批量事件一旦成功写出到存储系统或下一个Flume Agent，Sink就利用Channel提交事务。事务一旦被提交，该Channel从自己的内部缓冲区删除事件。</p>
<p>Sink组件目的地包括hdfs、logger、avro、thrift、ipc、file、null、HBase、solr、自定义。官方提供的Sink类型已经很多，但是有时候并不能满足实际开发当中的需求，此时我们就需要根据实际需求自定义某些Sink。</p>
<p>官方也提供了自定义sink的接口：</p>
<p><a href="https://flume.apache.org/FlumeDeveloperGuide.html#sink">https://flume.apache.org/FlumeDeveloperGuide.html#sink</a></p>
<p>根据官方说明自定义MySink需要继承AbstractSink类并实现Configurable接口。</p>
<p>实现相应方法：</p>
<p>configure(Context context)//初始化context（读取配置文件内容）</p>
<p>process()//从Channel读取获取数据（event），这个方法将被循环调用。</p>
<p>使用场景：读取Channel数据写入MySQL或者其他文件系统。</p>
<p>2）需求</p>
<p>使用flume接收数据，并在Sink端给每条数据添加前缀和后缀，输出到控制台。前后缀可在flume任务配置文件中配置。</p>
<p><img src="/Flume/15.png" alt="15"></p>
<p>3）编码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.vincent.flume.sink;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flume.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.conf.Configurable;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.sink.AbstractSink;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> Vincent</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span> 2020-06-17</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MySink</span> <span class="keyword">extends</span> <span class="title">AbstractSink</span> <span class="keyword">implements</span> <span class="title">Configurable</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">  	<span class="comment">//创建Logger对象</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOG = LoggerFactory.getLogger(MySink.class);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String prefix;</span><br><span class="line">    <span class="keyword">private</span> String suffix;</span><br><span class="line">  </span><br><span class="line">  	<span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 改方法被调用时，会从Channel中拉取数据并处理</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span>  处理的状态</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> EventDeliveryException   处理失败时候会抛出异常</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Status <span class="title">process</span><span class="params">()</span> <span class="keyword">throws</span> EventDeliveryException </span>&#123;</span><br><span class="line">        <span class="comment">//声明返回值状态信息</span></span><br><span class="line">        Status status;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取当前Sink绑定的Channel</span></span><br><span class="line">        Channel ch = getChannel();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取事务</span></span><br><span class="line">        Transaction txn = ch.getTransaction();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//声明事件</span></span><br><span class="line">        Event event;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//开启事务</span></span><br><span class="line">        txn.begin();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//读取Channel中的事件，直到读取到事件结束循环</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            event = ch.take();</span><br><span class="line">            <span class="keyword">if</span> (event != <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">//处理事件（打印）</span></span><br><span class="line">            LOG.info(prefix + <span class="keyword">new</span> String(event.getBody()) + suffix);</span><br><span class="line"></span><br><span class="line">            <span class="comment">//事务提交</span></span><br><span class="line">            txn.commit();</span><br><span class="line">            status = Status.READY;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">//遇到异常，事务回滚</span></span><br><span class="line">            txn.rollback();</span><br><span class="line">            status = Status.BACKOFF;</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">//关闭事务</span></span><br><span class="line">            txn.close();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> status;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 配置方法</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context   配置文件</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line">				<span class="comment">//读取配置文件内容，有默认值</span></span><br><span class="line">        prefix = context.getString(<span class="string">&quot;prefix&quot;</span>, <span class="string">&quot;hello:&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//读取配置文件内容，无默认值</span></span><br><span class="line">        suffix = context.getString(<span class="string">&quot;suffix&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>4）测试</p>
<p>（1）打包</p>
<p>将写好的代码打包，并放到flume的lib目录（/opt/module/flume）下。</p>
<p>（2）配置文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = 0.0.0.0</span><br><span class="line">a1.sources.r1.port = 4444</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = com.vincent.flume.sink.MySink</span><br><span class="line">a1.sinks.k1.prefix = xsh:</span><br><span class="line">a1.sinks.k1.suffix = :xsh</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p>（3）开启任务并向linux1的44444端口发送数据</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ bin/flume-ng agent -c conf/ -f job/mysink.conf -n a1 -Dflume.root.logger=INFO,console</span><br><span class="line">[vincent@linux1 ~]$ nc linux1 4444</span><br></pre></td></tr></table></figure>

<h2 id="Flume数据流监控"><a href="#Flume数据流监控" class="headerlink" title="Flume数据流监控"></a>Flume数据流监控</h2><h3 id="Ganglia的安装与部署"><a href="#Ganglia的安装与部署" class="headerlink" title="Ganglia的安装与部署"></a>Ganglia的安装与部署</h3><p>1）安装httpd服务与php</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ sudo yum -y install httpd php</span><br></pre></td></tr></table></figure>

<p>2）安装其他依赖</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ sudo yum -y install rrdtool perl-rrdtool rrdtool-devel</span><br><span class="line">[vincent@linux1 flume]$ sudo yum -y install apr-devel</span><br></pre></td></tr></table></figure>

<p>3）安装ganglia</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ sudo rpm -Uvh http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ sudo yum -y install ganglia-gmetad ganglia-web ganglia-gmond</span><br></pre></td></tr></table></figure>

<p>Ganglia由gmond、gmetad和gweb三部分组成。</p>
<p>gmond（Ganglia Monitoring Daemon）是一种轻量级服务，安装在每台需要收集指标数据的节点主机上。使用gmond，你可以很容易收集很多系统指标数据，如CPU、内存、磁盘、网络和活跃进程的数据等。</p>
<p>gmetad（Ganglia Meta Daemon）整合所有信息，并将其以RRD格式存储至磁盘的服务。</p>
<p>gweb（Ganglia Web）Ganglia可视化工具，gweb是一种利用浏览器显示gmetad所存储数据的PHP前端。在Web界面中以图表方式展现集群的运行状态下收集的多种不同指标数据。</p>
<p>4）修改配置文件/etc/httpd/conf.d/ganglia.conf</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ sudo vim /etc/httpd/conf.d/ganglia.conf</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Ganglia monitoring system php web frontend</span></span><br><span class="line">Alias /ganglia /usr/share/ganglia</span><br><span class="line">&lt;Location /ganglia&gt;</span><br><span class="line"><span class="meta">	#</span><span class="bash"> Require <span class="built_in">local</span></span></span><br><span class="line">  Require ip linux1</span><br><span class="line">&lt;/Location&gt;</span><br></pre></td></tr></table></figure>

<p>5）修改配置文件/etc/ganglia/gmetad.conf</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ sudo vim /etc/ganglia/gmetad.conf</span><br></pre></td></tr></table></figure>

<p>修改为：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">data_source &quot;flume_cluster&quot; linux1 linux2 linux3</span><br></pre></td></tr></table></figure>

<p>6）修改配置文件/etc/ganglia/gmond.conf</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ sudo vim /etc/ganglia/gmond.conf</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cluster &#123;</span><br><span class="line">  name = &quot;flume_cluster&quot;</span><br><span class="line">  owner = &quot;unspecified&quot;</span><br><span class="line">  latlong = &quot;unspecified&quot;</span><br><span class="line">  url = &quot;unspecified&quot;</span><br><span class="line">&#125;</span><br><span class="line">udp_send_channel &#123;</span><br><span class="line"><span class="meta">  #</span><span class="bash">bind_hostname = yes <span class="comment"># Highly recommended, soon to be default.</span></span></span><br><span class="line">                       # This option tells gmond to use a source address</span><br><span class="line">                       # that resolves to the machine&#x27;s hostname.  Without</span><br><span class="line">                       # this, the metrics may appear to come from any</span><br><span class="line">                       # interface and the DNS names associated with</span><br><span class="line">                       # those IPs will be used to create the RRDs.</span><br><span class="line"><span class="meta">  #</span><span class="bash"> mcast_join = 239.2.11.71</span></span><br><span class="line">  host = linux1</span><br><span class="line">  port = 8649</span><br><span class="line">  ttl = 1</span><br><span class="line">&#125;</span><br><span class="line">udp_recv_channel &#123;</span><br><span class="line"><span class="meta">  #</span><span class="bash"> mcast_join = 239.2.11.71</span></span><br><span class="line">  bind = linux1</span><br><span class="line">  port = 8649</span><br><span class="line">  retry_bind = true</span><br><span class="line"><span class="meta">  #</span><span class="bash"> Size of the UDP buffer. If you are handling lots of metrics you really</span></span><br><span class="line"><span class="meta">  #</span><span class="bash"> should bump it up to e.g. 10MB or even higher.</span></span><br><span class="line"><span class="meta">  #</span><span class="bash"> buffer = 10485760</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ xsync.sh /etc/ganglia/gmond.conf</span><br></pre></td></tr></table></figure>

<p>7）修改配置文件/etc/selinux/config</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ sudo vim /etc/selinux/config</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> This file controls the state of SELinux on the system.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> SELINUX= can take one of these three values:</span></span><br><span class="line"><span class="meta">#</span><span class="bash">     enforcing - SELinux security policy is enforced.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">     permissive - SELinux prints warnings instead of enforcing.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">     disabled - No SELinux policy is loaded.</span></span><br><span class="line">SELINUX=disabled</span><br><span class="line"><span class="meta">#</span><span class="bash"> SELINUXTYPE= can take one of these two values:</span></span><br><span class="line"><span class="meta">#</span><span class="bash">     targeted - Targeted processes are protected,</span></span><br><span class="line"><span class="meta">#</span><span class="bash">     mls - Multi Level Security protection.</span></span><br><span class="line">SELINUXTYPE=targeted</span><br></pre></td></tr></table></figure>

<p>尖叫提示：selinux本次生效关闭必须重启，如果此时不想重启，可以临时生效之：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ sudo setenforce 0</span><br></pre></td></tr></table></figure>

<p>8）启动ganglia</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ sudo systemctl start httpd</span><br><span class="line">[vincent@linux1 flume]$ sudo systemctl start gmetad</span><br><span class="line">[vincent@linux1 flume]$ sudo systemctl start gmond</span><br></pre></td></tr></table></figure>

<p>9）打开网页浏览ganglia页面</p>
<p><a href="http://linux3/ganglia">http://linux3/ganglia</a></p>
<p>尖叫提示：如果完成以上操作依然出现权限不足错误，请修改/var/lib/ganglia目录的权限：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ sudo chmod -R 777 /var/lib/ganglia</span><br></pre></td></tr></table></figure>

<h3 id="Ganglia-Reporting"><a href="#Ganglia-Reporting" class="headerlink" title="Ganglia Reporting"></a>Ganglia Reporting</h3><p>1）启动Flume任务</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ bin/flume-ng agent \</span><br><span class="line">--conf conf/ \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf-file job/flume-netcat-logger.conf \</span><br><span class="line">-Dflume.root.logger==INFO,console \</span><br><span class="line">-Dflume.monitoring.type=ganglia \</span><br><span class="line">-Dflume.monitoring.hosts=linux1:8649</span><br></pre></td></tr></table></figure>

<p>2）发送数据观察ganglia监测图</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ nc localhost 6666</span><br></pre></td></tr></table></figure>

<p>3）图例说明：</p>
<table>
<thead>
<tr>
<th>字段（图表名称）</th>
<th>字段含义</th>
</tr>
</thead>
<tbody><tr>
<td>EventPutAttemptCount</td>
<td>source尝试写入channel的事件总数量</td>
</tr>
<tr>
<td>EventPutSuccessCount</td>
<td>成功写入channel且提交的事件总数量</td>
</tr>
<tr>
<td>EventTakeAttemptCount</td>
<td>sink尝试从channel拉取事件的总数量。</td>
</tr>
<tr>
<td>EventTakeSuccessCount</td>
<td>sink成功读取的事件的总数量</td>
</tr>
<tr>
<td>StartTime</td>
<td>channel启动的时间（毫秒）</td>
</tr>
<tr>
<td>StopTime</td>
<td>channel停止的时间（毫秒）</td>
</tr>
<tr>
<td>ChannelSize</td>
<td>目前channel中事件的总数量</td>
</tr>
<tr>
<td>ChannelFillPercentage</td>
<td>channel占用百分比</td>
</tr>
<tr>
<td>ChannelCapacity</td>
<td>channel的容量</td>
</tr>
</tbody></table>
<h3 id="JSON-Reporting"><a href="#JSON-Reporting" class="headerlink" title="JSON Reporting"></a>JSON Reporting</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/flume-ng agent --conf-file example.conf --name a1 -Dflume.monitoring.type=http -Dflume.monitoring.port=34545</span><br></pre></td></tr></table></figure>

<figure class="highlight awk"><table><tr><td class="code"><pre><span class="line">http:<span class="regexp">//</span>&lt;hostname&gt;:&lt;port&gt;/metrics</span><br><span class="line">http:<span class="regexp">//</span>linux1:<span class="number">34545</span>/metrics</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>离线计算</category>
      </categories>
  </entry>
  <entry>
    <title>Kylin</title>
    <url>/Kylin/</url>
    <content><![CDATA[<h1 id="Kylin定义"><a href="#Kylin定义" class="headerlink" title="Kylin定义"></a>Kylin定义</h1><p>Apache Kylin是一个开源的分布式分析引擎，提供Hadoop/Spark之上的SQL查询接口及多维分析（OLAP）能力以支持超大规模数据，最初由eBay Inc开发并贡献至开源社区。它能在亚秒内查询巨大的Hive表。</p>
<h1 id="Kylin架构"><a href="#Kylin架构" class="headerlink" title="Kylin架构"></a>Kylin架构</h1><p><img src="/Kylin/148.png" alt="148"></p>
<p>1）REST Server</p>
<p>REST Server是一套面向应用程序开发的入口点，旨在实现针对Kylin平台的应用开发工作。 此类应用程序可以提供查询、获取结果、触发cube构建任务、获取元数据以及获取用户权限等等。另外可以通过Restful接口实现SQL查询。</p>
<p>2）查询引擎（Query Engine）</p>
<p>当cube准备就绪后，查询引擎就能够获取并解析用户查询。它随后会与系统中的其它组件进行交互，从而向用户返回对应的结果。 </p>
<p>3）路由器（Routing）</p>
<p>在最初设计时曾考虑过将Kylin不能执行的查询引导去Hive中继续执行，但在实践后发现Hive与Kylin的速度差异过大，导致用户无法对查询的速度有一致的期望，很可能大多数查询几秒内就返回结果了，而有些查询则要等几分钟到几十分钟，因此体验非常糟糕。最后这个路由功能在发行版中默认关闭。</p>
<p>4）元数据管理工具（Metadata）</p>
<p>Kylin是一款元数据驱动型应用程序。元数据管理工具是一大关键性组件，用于对保存在Kylin当中的所有元数据进行管理，其中包括最为重要的cube元数据。其它全部组件的正常运作都需以元数据管理工具为基础。 Kylin的元数据存储在hbase中。 </p>
<p>5）任务引擎（Cube Build Engine）</p>
<p>这套引擎的设计目的在于处理所有离线任务，其中包括shell脚本、Java API以及Map Reduce任务等等。任务引擎对Kylin当中的全部任务加以管理与协调，从而确保每一项任务都能得到切实执行并解决其间出现的故障。</p>
<h1 id="Kylin特点"><a href="#Kylin特点" class="headerlink" title="Kylin特点"></a>Kylin特点</h1><p>Kylin的主要特点包括支持SQL接口、支持超大规模数据集、亚秒级响应、可伸缩性、高吞吐率、BI工具集成等。</p>
<p>1）标准SQL接口：Kylin是以标准的SQL作为对外服务的接口。</p>
<p>2）支持超大数据集：Kylin对于大数据的支撑能力可能是目前所有技术中最为领先的。早在2015年eBay的生产环境中就能支持百亿记录的秒级查询，之后在移动的应用场景中又有了千亿记录秒级查询的案例。</p>
<p>3）亚秒级响应：Kylin拥有优异的查询相应速度，这点得益于预计算，很多复杂的计算，比如连接、聚合，在离线的预计算过程中就已经完成，这大大降低了查询时刻所需的计算量，提高了响应速度。</p>
<p>4）可伸缩性和高吞吐率：单节点Kylin可实现每秒70个查询，还可以搭建Kylin的集群。</p>
<p>5）BI工具集成</p>
<p>Kylin可以与现有的BI工具集成，具体包括如下内容。</p>
<p>ODBC：与Tableau、Excel、PowerBI等工具集成</p>
<p>JDBC：与Saiku、BIRT等Java工具集成</p>
<p>RestAPI：与JavaScript、Web网页集成</p>
<p>Kylin开发团队还贡献了<strong>Zepplin</strong>的插件，也可以使用Zepplin来访问Kylin服务。</p>
<h1 id="Kylin安装"><a href="#Kylin安装" class="headerlink" title="Kylin安装"></a>Kylin安装</h1><p>安装Kylin前需先部署好Hadoop、Hive、Zookeeper、HBase，并且需要在/etc/profile中配置以下环境变量HADOOP_HOME，HIVE_HOME，HBASE_HOME，记得source使其生效。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vinent@linux1 sorfware]$ tar -zxvf apache-kylin-3.0.2-bin.tar.gz -C /opt/module/</span><br><span class="line">[vinent@linux1 module]$ mv /opt/module/apache-kylin-3.0.2-bin /opt/module/kylin</span><br></pre></td></tr></table></figure>

<p>（1）启动Kylin之前，需先启动Hadoop（hdfs，yarn，jobhistoryserver）、Zookeeper、Hbase</p>
<p>（2）启动Kylin</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vinent@linux1 kylin]$ bin/kylin.sh start</span><br></pre></td></tr></table></figure>

<p>在<a href="http://linux1:7070/kylin%E6%9F%A5%E7%9C%8BWeb%E9%A1%B5%E9%9D%A2">http://linux1:7070/kylin查看Web页面</a></p>
<p>用户名为：ADMIN，密码为：KYLIN</p>
<p>（3）关闭Kylin</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vinent@linux1 kylin]$ bin/kylin.sh stop</span><br></pre></td></tr></table></figure>

<h1 id="使用进阶"><a href="#使用进阶" class="headerlink" title="使用进阶"></a>使用进阶</h1><p>1）每日全量维度表及拉链维度表<strong>重复Key问题</strong>如何处理</p>
<p>按照上述流程，会发现，在cube构建流程中出现以下错误</p>
<p>上述错误原因是model中的维度表dwd_dim_user_info_his为拉链表、dwd_dim_sku_info为每日全量表，故使用整张表作为维度表，必然会出现订单表中同一个user_id或者sku_id对应多条数据的问题，针对上述问题，有以下两种解决方案。</p>
<p>方案一：在hive中创建维度表的临时表，该临时表中只存放维度表最新的一份完整的数据，在kylin中创建模型时选择该临时表作为维度表。</p>
<p>方案二：与方案一思路相同，但不使用物理临时表，而选用视图（view）实现相同的功能。</p>
<p>此处采用方案二：</p>
<h2 id="创建维度表视图"><a href="#创建维度表视图" class="headerlink" title="创建维度表视图"></a>创建维度表视图</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create view dwd_dim_user_info_his_view as select * from dwd_dim_user_info_his where end_date=&#x27;9999-99-99&#x27;;</span><br><span class="line"></span><br><span class="line">--全量维度表视图</span><br><span class="line">create view dwd_dim_sku_info_view as select * from dwd_dim_sku_info where dt=date_add(current_date,-1);</span><br><span class="line"></span><br><span class="line">--当前情形我们先创建一个2020-06-25的视图</span><br><span class="line">create view dwd_dim_sku_info_view as select * from dwd_dim_sku_info where dt=&#x27;2020-06-25&#x27;;</span><br></pre></td></tr></table></figure>

<h2 id="查询结果"><a href="#查询结果" class="headerlink" title="查询结果"></a>查询结果</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select</span><br><span class="line">    ui.gender,</span><br><span class="line">    si.category3_id,</span><br><span class="line">    dp.region_id,</span><br><span class="line">    sum(od.final_amount.d)</span><br><span class="line">from</span><br><span class="line">    dwd_fact_order_detail od</span><br><span class="line">join</span><br><span class="line">    dwd_dim_user_info_his_view ui</span><br><span class="line">on</span><br><span class="line">    od.user_id=ui.id</span><br><span class="line">join</span><br><span class="line">    dwd_dim_sku_info_view si</span><br><span class="line">on</span><br><span class="line">    od.sku_id=si.id</span><br><span class="line">join</span><br><span class="line">    dwd_dim_base_province dp</span><br><span class="line">on</span><br><span class="line">    od.province_id=dp.id</span><br><span class="line">group by</span><br><span class="line">    ui.gender,si.category3_id,dp.region_id;</span><br></pre></td></tr></table></figure>

<h2 id="自动构建cube"><a href="#自动构建cube" class="headerlink" title="自动构建cube"></a>自动构建cube</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">cube_name=order_cube</span><br><span class="line">do_date=`date -d &#x27;-1 day&#x27; +%F`</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">获取00:00时间戳</span></span><br><span class="line">start_date_unix=`date -d &quot;$do_date 08:00:00&quot; +%s`</span><br><span class="line">start_date=$(($start_date_unix*1000))</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">获取24:00的时间戳</span></span><br><span class="line">stop_date=$(($start_date+86400000))</span><br><span class="line"></span><br><span class="line">curl -X PUT -H &quot;Authorization: Basic QURNSU46S1lMSU4=&quot; -H &#x27;Content-Type: application/json&#x27; -d &#x27;&#123;&quot;startTime&quot;:&#x27;$start_date&#x27;, &quot;endTime&quot;:&#x27;$stop_date&#x27;, &quot;buildType&quot;:&quot;BUILD&quot;&#125;&#x27; http://linux1:7070/kylin/api/cubes/$cube_name/build</span><br></pre></td></tr></table></figure>

<h1 id="Kylin-Cube构建原理"><a href="#Kylin-Cube构建原理" class="headerlink" title="Kylin Cube构建原理"></a>Kylin Cube构建原理</h1><h2 id="维度和度量"><a href="#维度和度量" class="headerlink" title="维度和度量"></a>维度和度量</h2><p>维度：即观察数据的角度。比如员工数据，可以从性别角度来分析，也可以更加细化，从入职时间或者地区的维度来观察。维度是一组离散的值，比如说性别中的男和女，或者时间维度上的每一个独立的日期。因此在统计时可以将维度值相同的记录聚合在一起，然后应用聚合函数做累加、平均、最大和最小值等聚合计算。</p>
<p>度量：即被聚合（观察）的统计值，也就是聚合运算的结果。比如说员工数据中不同性别员工的人数，又或者说在同一年入职的员工有多少。</p>
<h2 id="Cube和Cuboid"><a href="#Cube和Cuboid" class="headerlink" title="Cube和Cuboid"></a>Cube和Cuboid</h2><p>有了维度跟度量，一个数据表或者数据模型上的所有字段就可以分类了，它们要么是维度，要么是度量（可以被聚合）。于是就有了根据维度和度量做预计算的Cube理论。</p>
<p>给定一个数据模型，我们可以对其上的所有维度进行聚合，对于N个维度来说，组合的所有可能性共有2n种。对于每一种维度的组合，将度量值做聚合计算，然后将结果保存为一个物化视图，称为Cuboid。所有维度组合的Cuboid作为一个整体，称为Cube。</p>
<p>下面举一个简单的例子说明，假设有一个电商的销售数据集，其中<strong>维度包括时间[time]、商品[item]、地区[location]和供应商[supplier]，度量为销售额。那么所有维度的组合就有2^4 = 16种</strong>。</p>
<h2 id="Cube存储原理"><a href="#Cube存储原理" class="headerlink" title="Cube存储原理"></a>Cube存储原理</h2><p><img src="/Kylin/149.png" alt="149"></p>
<p><img src="/Kylin/150.png" alt="150"></p>
<h2 id="Cube构建算法"><a href="#Cube构建算法" class="headerlink" title="Cube构建算法"></a>Cube构建算法</h2><h3 id="1）逐层构建算法（layer）"><a href="#1）逐层构建算法（layer）" class="headerlink" title="1）逐层构建算法（layer）"></a>1）逐层构建算法（layer）</h3><p>我们知道，一个N维的Cube，是由1个N维子立方体、N个(N-1)维子立方体、N*(N-1)/2个(N-2)维子立方体、……、N个1维子立方体和1个0维子立方体构成，总共有2^N个子立方体组成，在逐层算法中，按维度数逐层减少来计算，每个层级的计算（除了第一层，它是从原始数据聚合而来），是基于它上一层级的结果来计算的。比如，**[Group by A, B]的结果，可以基于[Group by A, B, C]的结果，通过去掉C后聚合得来的**；这样可以减少重复计算；当 0维度Cuboid计算出来的时候，整个Cube的计算也就完成了。</p>
<p>每一轮的计算都是一个MapReduce任务，且串行执行；一个N维的Cube，至少需要N次MapReduce Job。</p>
<p><img src="/Kylin/151.png" alt="151"></p>
<p>算法优点：</p>
<p>1）此算法充分利用了MapReduce的优点，处理了中间复杂的排序和shuffle工作，故而算法代码清晰简单，易于维护；</p>
<p>2）受益于Hadoop的日趋成熟，此算法非常稳定，即便是集群资源紧张时，也能保证最终能够完成。</p>
<p>算法缺点：</p>
<p>1）当Cube有比较多维度的时候，所需要的MapReduce任务也相应增加；由于Hadoop的任务调度需要耗费额外资源，特别是集群较庞大的时候，反复递交任务造成的额外开销会相当可观；</p>
<p>2）由于Mapper逻辑中并未进行聚合操作，所以每轮MR的shuffle工作量都很大，导致效率低下。</p>
<p>3）对HDFS的读写操作较多：由于每一层计算的输出会用做下一层计算的输入，这些Key-Value需要写到HDFS上；当所有计算都完成后，Kylin还需要额外的一轮任务将这些文件转成HBase的HFile格式，以导入到HBase中去；</p>
<p>总体而言，该算法的效率较低，尤其是当Cube维度数较大的时候。</p>
<h3 id="2）快速构建算法（inmem）"><a href="#2）快速构建算法（inmem）" class="headerlink" title="2）快速构建算法（inmem）"></a>2）快速构建算法（inmem）</h3><p>也被称作“逐段”(By Segment) 或“逐块”(By Split) 算法，从1.5.x开始引入该算法，该算法的主要思想是，每个Mapper将其所分配到的数据块，计算成一个完整的小Cube 段（包含所有Cuboid）。每个Mapper将计算完的Cube段输出给Reducer做合并，生成大Cube，也就是最终结果。如图所示解释了此流程。</p>
<p><img src="/Kylin/152.png" alt="152"></p>
<p>与旧算法相比，快速算法主要有两点不同：</p>
<p>1） Mapper会利用内存做预聚合，算出所有组合；Mapper输出的每个Key都是不同的，这样会减少输出到Hadoop MapReduce的数据量，Combiner也不再需要；</p>
<p>2）一轮MapReduce便会完成所有层次的计算，减少Hadoop任务的调配。</p>
<h1 id="Kylin-Cube构建优化"><a href="#Kylin-Cube构建优化" class="headerlink" title="Kylin Cube构建优化"></a>Kylin Cube构建优化</h1><h2 id="使用衍生维度（derived-dimension）"><a href="#使用衍生维度（derived-dimension）" class="headerlink" title="使用衍生维度（derived dimension）"></a>使用衍生维度（derived dimension）</h2><p>衍生维度用于在有效维度内将维度表上的非主键维度排除掉，并使用维度表的主键（其实是事实表上相应的外键）来替代它们。Kylin会在底层记录维度表主键与维度表其他维度之间的映射关系，以便在查询时能够动态地将维度表的主键“翻译”成这些非主键维度，并进行实时聚合。</p>
<p><img src="/Kylin/153.png" alt="153"></p>
<p>虽然衍生维度具有非常大的吸引力，但这也并不是说所有维度表上的维度都得变成衍生维度，如果从维度表主键到某个维度表维度所需要的聚合工作量非常大，则不建议使用衍生维度。</p>
<h2 id="使用聚合组（Aggregation-group）"><a href="#使用聚合组（Aggregation-group）" class="headerlink" title="使用聚合组（Aggregation group）"></a>使用聚合组（Aggregation group）</h2><p>聚合组（Aggregation Group）是一种强大的剪枝工具。聚合组假设一个Cube的所有维度均可以根据业务需求划分成若干组（当然也可以是一个组），由于同一个组内的维度更可能同时被同一个查询用到，因此会表现出更加紧密的内在关联。每个分组的维度集合均是Cube所有维度的一个子集，不同的分组各自拥有一套维度集合，它们可能与其他分组有相同的维度，也可能没有相同的维度。每个分组各自独立地根据自身的规则贡献出一批需要被物化的Cuboid，所有分组贡献的Cuboid的并集就成为了当前Cube中所有需要物化的Cuboid的集合。不同的分组有可能会贡献出相同的Cuboid，构建引擎会察觉到这点，并且保证每一个Cuboid无论在多少个分组中出现，它都只会被物化一次。</p>
<p>对于每个分组内部的维度，用户可以使用如下三种可选的方式定义，它们之间的关系，具体如下。</p>
<p>1）强制维度（Mandatory），如果一个维度被定义为强制维度，那么这个分组产生的所有Cuboid中每一个Cuboid都会包含该维度。每个分组中都可以有0个、1个或多个强制维度。如果根据这个分组的业务逻辑，则相关的查询一定会在过滤条件或分组条件中，因此可以在该分组中把该维度设置为强制维度。</p>
<p><img src="/Kylin/154.png" alt="154"></p>
<p>2）层级维度（Hierarchy），每个层级包含两个或更多个维度。假设一个层级中包含D1，D2…Dn这n个维度，那么在该分组产生的任何Cuboid中， 这n个维度只会以（），（D1），（D1，D2）…（D1，D2…Dn）这n+1种形式中的一种出现。每个分组中可以有0个、1个或多个层级，不同的层级之间不应当有共享的维度。如果根据这个分组的业务逻辑，则多个维度直接存在层级关系，因此可以在该分组中把这些维度设置为层级维度。</p>
<p><img src="/Kylin/155.png" alt="155"></p>
<p>3）联合维度（Joint），每个联合中包含两个或更多个维度，如果某些列形成一个联合，那么在该分组产生的任何Cuboid中，这些联合维度要么一起出现，要么都不出现。每个分组中可以有0个或多个联合，但是不同的联合之间不应当有共享的维度（否则它们可以合并成一个联合）。如果根据这个分组的业务逻辑，多个维度在查询中总是同时出现，则可以在该分组中把这些维度设置为联合维度。</p>
<p><img src="/Kylin/156.png" alt="156"></p>
<p>这些操作可以在Cube Designer的Advanced Setting中的Aggregation Groups区域完成。</p>
<h2 id="Row-Key优化"><a href="#Row-Key优化" class="headerlink" title="Row Key优化"></a>Row Key优化</h2><p>1）被用作过滤的维度放在前边</p>
<p><img src="/Kylin/157.png" alt="157"></p>
<p>2）基数大的维度放在基数小的维度前边</p>
<p><img src="/Kylin/158.png" alt="158"></p>
<h2 id="并发粒度优化"><a href="#并发粒度优化" class="headerlink" title="并发粒度优化"></a>并发粒度优化</h2><p>当Segment中某一个Cuboid的大小超出一定的阈值时，系统会将该Cuboid的数据分片到多个分区中，以实现Cuboid数据读取的并行化，从而优化Cube的查询速度。具体的实现方式如下：构建引擎根据Segment估计的大小，以及参数“kylin.hbase.region.cut”的设置决定Segment在存储引擎中总共需要几个分区来存储，如果存储引擎是HBase，那么分区的数量就对应于HBase中的Region数量。kylin.hbase.region.cut的默认值是5.0，单位是GB，也就是说对于一个大小估计是50GB的Segment，构建引擎会给它分配10个分区。用户还可以通过设置kylin.hbase.region.count.min（默认为1）和kylin.hbase.region.count.max（默认为500）两个配置来决定每个Segment最少或最多被划分成多少个分区。</p>
<h1 id="Kylin-BI工具集成"><a href="#Kylin-BI工具集成" class="headerlink" title="Kylin BI工具集成"></a>Kylin BI工具集成</h1><p>可以与Kylin结合使用的可视化工具很多，例如：</p>
<p>ODBC：与Tableau、Excel、PowerBI等工具集成</p>
<p>JDBC：与Saiku、BIRT等Java工具集成</p>
<p>RestAPI：与JavaScript、Web网页集成</p>
<p>Kylin开发团队还贡献了Zepplin的插件，也可以使用Zepplin来访问Kylin服务。</p>
<h2 id="JDBC"><a href="#JDBC" class="headerlink" title="JDBC"></a>JDBC</h2><p>1）新建项目并导入依赖</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kylin<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kylin-jdbc<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.5.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>2）编码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.vincent;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestKylin</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//Kylin_JDBC 驱动</span></span><br><span class="line">        String KYLIN_DRIVER = <span class="string">&quot;org.apache.kylin.jdbc.Driver&quot;</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//Kylin_URL</span></span><br><span class="line">        String KYLIN_URL = <span class="string">&quot;jdbc:kylin://linux1:7070/FirstProject&quot;</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//Kylin的用户名</span></span><br><span class="line">        String KYLIN_USER = <span class="string">&quot;ADMIN&quot;</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//Kylin的密码</span></span><br><span class="line">        String KYLIN_PASSWD = <span class="string">&quot;KYLIN&quot;</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//添加驱动信息</span></span><br><span class="line">        Class.forName(KYLIN_DRIVER);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取连接</span></span><br><span class="line">        Connection connection = DriverManager.getConnection(KYLIN_URL, KYLIN_USER, KYLIN_PASSWD);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//预编译SQL</span></span><br><span class="line">        PreparedStatement ps = connection.prepareStatement(<span class="string">&quot;SELECT sum(sal) FROM emp group by deptno&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//执行查询</span></span><br><span class="line">        ResultSet resultSet = ps.executeQuery();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//遍历打印</span></span><br><span class="line">        <span class="keyword">while</span> (resultSet.next()) &#123;</span><br><span class="line">            System.out.println(resultSet.getInt(<span class="number">1</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Zepplin"><a href="#Zepplin" class="headerlink" title="Zepplin"></a>Zepplin</h2><p>1）Zepplin安装与启动</p>
<p>（1）将zeppelin-0.8.0-bin-all.tgz上传至Linux</p>
<p>（2）解压zeppelin-0.8.0-bin-all.tgz之/opt/module</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 sorfware]$ tar -zxvf zeppelin-0.8.0-bin-all.tgz -C /opt/module/</span><br></pre></td></tr></table></figure>

<p>（3）修改名称</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 module]$ mv zeppelin-0.8.0-bin-all/ zeppelin</span><br></pre></td></tr></table></figure>

<p>（4）启动</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 zeppelin]$ bin/zeppelin-daemon.sh start</span><br></pre></td></tr></table></figure>

<p>可登录网页查看，web默认端口号为8080，可以设置为7980</p>
<p>2）配置Zepplin支持Kylin</p>
<p>（1）点击右上角anonymous选择Interpreter</p>
]]></content>
      <categories>
        <category>离线计算</category>
      </categories>
  </entry>
  <entry>
    <title>Azkaban</title>
    <url>/Azkaban/</url>
    <content><![CDATA[<h1 id="Azkaban完整配置"><a href="#Azkaban完整配置" class="headerlink" title="Azkaban完整配置"></a>Azkaban完整配置</h1><p>见官网文档：<a href="https://azkaban.readthedocs.io/en/latest/configuration.html">https://azkaban.readthedocs.io/en/latest/configuration.html</a></p>
<h1 id="调度工具对比"><a href="#调度工具对比" class="headerlink" title="调度工具对比"></a>调度工具对比</h1><table>
<thead>
<tr>
<th>特性</th>
<th>Hamake</th>
<th>Oozie</th>
<th>Azkaban</th>
<th>Cascading</th>
</tr>
</thead>
<tbody><tr>
<td>工作流描述语言</td>
<td>XML</td>
<td>XML (xPDL based)</td>
<td>text file with key/value pairs</td>
<td>Java API</td>
</tr>
<tr>
<td>依赖机制</td>
<td>data-driven</td>
<td>explicit</td>
<td>explicit</td>
<td>explicit</td>
</tr>
<tr>
<td>是否要web容器</td>
<td>No</td>
<td>Yes</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>进度跟踪</td>
<td>console/log messages</td>
<td>web page</td>
<td>web page</td>
<td>Java API</td>
</tr>
<tr>
<td>Hadoop job调度支持</td>
<td>no</td>
<td>yes</td>
<td>yes</td>
<td>yes</td>
</tr>
<tr>
<td>运行模式</td>
<td>command line utility</td>
<td>daemon</td>
<td>daemon</td>
<td>API</td>
</tr>
<tr>
<td>Pig支持</td>
<td>yes</td>
<td>yes</td>
<td>yes</td>
<td>yes</td>
</tr>
<tr>
<td>事件通知</td>
<td>no</td>
<td>no</td>
<td>no</td>
<td>yes</td>
</tr>
<tr>
<td>需要安装</td>
<td>no</td>
<td>yes</td>
<td>yes</td>
<td>no</td>
</tr>
<tr>
<td>支持的hadoop版本</td>
<td>0.18+</td>
<td>0.20+</td>
<td>currently unknown</td>
<td>0.18+</td>
</tr>
<tr>
<td>重试支持</td>
<td>no</td>
<td>workflownode evel</td>
<td>yes</td>
<td>yes</td>
</tr>
<tr>
<td>运行任意命令</td>
<td>yes</td>
<td>yes</td>
<td>yes</td>
<td>yes</td>
</tr>
<tr>
<td>Amazon EMR支持</td>
<td>yes</td>
<td>no</td>
<td>currently unknown</td>
<td>yes</td>
</tr>
</tbody></table>
<h2 id="Azkaban与Oozie对比"><a href="#Azkaban与Oozie对比" class="headerlink" title="Azkaban与Oozie对比"></a>Azkaban与Oozie对比</h2><p>对市面上最流行的两种调度器，给出以下详细对比，以供技术选型参考。总体来说，ooize相比azkaban是一个重量级的任务调度系统，功能全面，但配置使用也更复杂。如果可以不在意某些功能的缺失，轻量级调度器azkaban是很不错的候选对象。</p>
<p>详情如下：</p>
<p>1）功能</p>
<p>两者均可以调度mapreduce，pig，java，脚本工作流任务</p>
<p>两者均可以定时执行工作流任务</p>
<p>2）工作流定义</p>
<p>Azkaban使用Properties文件定义工作流</p>
<p>Oozie使用XML文件定义工作流</p>
<p>3）工作流传参</p>
<p>Azkaban支持直接传参，例如${input}</p>
<p>Oozie支持参数和EL表达式，例如${fs:dirSize(myInputDir)}</p>
<p>4）定时执行</p>
<p>Azkaban的定时执行任务是基于时间的</p>
<p>Oozie的定时执行任务基于时间和输入数据</p>
<p>5）资源管理</p>
<p>Azkaban有较严格的权限控制，如用户对工作流进行读/写/执行等操作</p>
<p>Oozie暂无严格的权限控制</p>
<p>6）工作流执行</p>
<p>Azkaban有两种运行模式，分别是solo server mode(executor server和web server部署在同一台节点)和multi server mode(executor server和web server可以部署在不同节点)</p>
<p>Oozie作为工作流服务器运行，支持多用户和多工作流</p>
<p>7）工作流管理</p>
<p>Azkaban支持浏览器以及ajax方式操作工作流</p>
<p>Oozie支持命令行、HTTP REST、Java API、浏览器操作工作流</p>
<h2 id="Azkaban特点"><a href="#Azkaban特点" class="headerlink" title="Azkaban特点"></a>Azkaban特点</h2><p>Azkaban是由Linkedin开源的一个批量工作流任务调度器。用于在一个工作流内以一个特定的顺序运行一组工作和流程。Azkaban定义了一种KV文件格式来建立任务之间的依赖关系，并提供一个易于使用的web用户界面维护和跟踪你的工作流。</p>
<p>它有如下功能特点：</p>
<p>1）Web用户界面</p>
<p>2）方便上传工作流</p>
<p>3）方便设置任务之间的关系</p>
<p>4）调度工作流</p>
<p>5）认证/授权(权限的工作)</p>
<p>6）能够杀死并重新启动工作流</p>
<p>7）模块化和可插拔的插件机制</p>
<p>8）项目工作区</p>
<p>9）工作流和任务的日志记录和审计</p>
<h1 id="集群模式"><a href="#集群模式" class="headerlink" title="集群模式"></a>集群模式</h1><p>1.上传tar包到集群</p>
<p>2.设置mysql</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; CREATE DATABASE azkaban;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mysql&gt; CREATE USER &#x27;azkaban&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;211819&#x27;;</span><br><span class="line">mysql&gt; GRANT SELECT,INSERT,UPDATE,DELETE ON azkaban.* to &#x27;azkaban&#x27;@&#x27;%&#x27; WITH GRANT OPTION;</span><br><span class="line"></span><br><span class="line">mysql&gt; use azkaban;</span><br><span class="line">-- 创建azkaban表</span><br><span class="line">mysql&gt; source /opt/module/azkaban/azkaban-db-3.84.4/create-all-sql-3.84.4.sql</span><br><span class="line">mysql&gt; quit;</span><br><span class="line"></span><br><span class="line">-- 更改mysql表大小</span><br><span class="line">sudo vim /etc/my.cnf</span><br><span class="line"></span><br><span class="line">[mysqld]</span><br><span class="line">...</span><br><span class="line">max_allowed_packet=1024M</span><br><span class="line"></span><br><span class="line">-- 重启</span><br><span class="line">sudo systemctl restart mysqld</span><br></pre></td></tr></table></figure>

<p>3.设置Executor Server并同步节点</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim /opt/module/azkaban/azkaban-exec-server-3.84.4/conf/azkaban.properties</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Azkaban Personalization Settings</span></span><br><span class="line">azkaban.name=vincent</span><br><span class="line">azkaban.label=My Local Azkaban</span><br><span class="line">azkaban.color=#FF3601</span><br><span class="line">azkaban.default.servlet.path=/index</span><br><span class="line">web.resource.dir=web/</span><br><span class="line">default.timezone.id=Asia/Shanghai</span><br><span class="line"><span class="meta">#</span><span class="bash"> Azkaban UserManager class</span></span><br><span class="line">user.manager.class=azkaban.user.XmlUserManager</span><br><span class="line">user.manager.xml.file=conf/azkaban-users.xml</span><br><span class="line"><span class="meta">#</span><span class="bash"> Loader <span class="keyword">for</span> projects</span></span><br><span class="line">executor.global.properties=conf/global.properties</span><br><span class="line">azkaban.project.dir=projects</span><br><span class="line"><span class="meta">#</span><span class="bash"> Velocity dev mode</span></span><br><span class="line">velocity.dev.mode=false</span><br><span class="line"><span class="meta">#</span><span class="bash"> Azkaban Jetty server properties.</span></span><br><span class="line">jetty.use.ssl=false</span><br><span class="line">jetty.maxThreads=25</span><br><span class="line">jetty.port=8081</span><br><span class="line"><span class="meta">#</span><span class="bash"> Where the Azkaban web server is located</span></span><br><span class="line">azkaban.webserver.url=http://linux1:8081</span><br><span class="line"><span class="meta">#</span><span class="bash"> mail settings</span></span><br><span class="line">mail.sender=</span><br><span class="line">mail.host=</span><br><span class="line"><span class="meta">#</span><span class="bash"> User facing web server configurations used to construct the user facing server URLs. They are useful when there is a reverse proxy between Azkaban web servers and users.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> enduser -&gt; myazkabanhost:443 -&gt; proxy -&gt; localhost:8081</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> when this parameters <span class="built_in">set</span> <span class="keyword">then</span> these parameters are used to generate email links.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">if</span> these parameters are not <span class="built_in">set</span> <span class="keyword">then</span> jetty.hostname, and jetty.port(<span class="keyword">if</span> ssl configured jetty.ssl.port) are used.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> azkaban.webserver.external_hostname=myazkabanhost.com</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> azkaban.webserver.external_ssl_port=443</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> azkaban.webserver.external_port=8081</span></span><br><span class="line">job.failure.email=</span><br><span class="line">job.success.email=</span><br><span class="line">lockdown.create.projects=false</span><br><span class="line">cache.directory=cache</span><br><span class="line"><span class="meta">#</span><span class="bash"> JMX stats</span></span><br><span class="line">jetty.connector.stats=true</span><br><span class="line">executor.connector.stats=true</span><br><span class="line"><span class="meta">#</span><span class="bash"> Azkaban plugin settings</span></span><br><span class="line">azkaban.jobtype.plugin.dir=plugins/jobtypes</span><br><span class="line"><span class="meta">#</span><span class="bash"> Azkaban mysql settings by default. Users should configure their own username and password.</span></span><br><span class="line">database.type=mysql</span><br><span class="line">mysql.port=3306</span><br><span class="line">mysql.host=linux1</span><br><span class="line">mysql.database=azkaban</span><br><span class="line">mysql.user=azkaban</span><br><span class="line">mysql.password=211819</span><br><span class="line">mysql.numconnections=100</span><br><span class="line"><span class="meta">#</span><span class="bash"> Azkaban Executor settings</span></span><br><span class="line">executor.maxThreads=50</span><br><span class="line">executor.flow.threads=30</span><br><span class="line">executor.metric.reports=true</span><br><span class="line">executor.metric.milisecinterval.default=60000</span><br></pre></td></tr></table></figure>

<p>4.三台机器上启动executor server</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd /opt/module/azkaban/azkaban-exec-server-3.84.4</span><br><span class="line">bin/start-exec.sh</span><br></pre></td></tr></table></figure>

<p>5.如果在目录下出现executor.port文件，说明启动成功，下面激活executor</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">curl -G &quot;localhost:$(&lt;./executor.port)/executor?action=activate&quot; &amp;&amp; echo</span><br></pre></td></tr></table></figure>

<p>6.设置web server</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim /opt/module/azkaban/azkaban-web-server-3.84.4/conf/azkaban.properties</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">default.timezone.id=Asia/Shanghai</span><br><span class="line">...</span><br><span class="line">database.type=mysql</span><br><span class="line">mysql.port=3306</span><br><span class="line">mysql.host=linux1</span><br><span class="line">mysql.database=azkaban</span><br><span class="line">mysql.user=azkaban</span><br><span class="line">mysql.password=211819</span><br><span class="line">mysql.numconnections=100</span><br><span class="line">...</span><br><span class="line">azkaban.executorselector.filters=StaticRemainingFlowSize,CpuStatus</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim /opt/module/azkaban/azkaban-web-server-3.84.4/conf/azkaban-users.xml</span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">azkaban-users</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">user</span> <span class="attr">groups</span>=<span class="string">&quot;azkaban&quot;</span> <span class="attr">password</span>=<span class="string">&quot;azkaban&quot;</span> <span class="attr">roles</span>=<span class="string">&quot;admin&quot;</span> <span class="attr">username</span>=<span class="string">&quot;azkaban&quot;</span>/&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">user</span> <span class="attr">password</span>=<span class="string">&quot;metrics&quot;</span> <span class="attr">roles</span>=<span class="string">&quot;metrics&quot;</span> <span class="attr">username</span>=<span class="string">&quot;metrics&quot;</span>/&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">user</span> <span class="attr">password</span>=<span class="string">&quot;211819&quot;</span> <span class="attr">roles</span>=<span class="string">&quot;metrics,admin&quot;</span> <span class="attr">username</span>=<span class="string">&quot;vincent&quot;</span>/&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">role</span> <span class="attr">name</span>=<span class="string">&quot;admin&quot;</span> <span class="attr">permissions</span>=<span class="string">&quot;ADMIN&quot;</span>/&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">role</span> <span class="attr">name</span>=<span class="string">&quot;metrics&quot;</span> <span class="attr">permissions</span>=<span class="string">&quot;METRICS&quot;</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">azkaban-users</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>7.linux1上启动web server</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/start-web.sh</span><br></pre></td></tr></table></figure>

<h1 id="workflow"><a href="#workflow" class="headerlink" title="workflow"></a>workflow</h1><h2 id="入门"><a href="#入门" class="headerlink" title="入门"></a>入门</h2><p>1.新建 first.project 文件</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">azkaban</span>-flow-version: <span class="number">2</span>.<span class="number">0</span></span><br></pre></td></tr></table></figure>

<p>2.新建 basic.flow 文件</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">nodes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">jobA</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">echo</span> <span class="string">&quot;This is an echoed text.&quot;</span></span><br></pre></td></tr></table></figure>

<p>3.在webserver新建项目</p>
<p>4.将两个文件打包为zip并上传执行</p>
<h2 id="作业设置"><a href="#作业设置" class="headerlink" title="作业设置"></a>作业设置</h2><p>Azkaban作业以Key:Value形式定义，每个作业有如下配置：</p>
<ol>
<li><p>name：作业名称</p>
</li>
<li><p>type：作业类型（详细类型配置见第3章）</p>
</li>
<li><p>config：和作业类型相关的配置，也以KV值形式</p>
</li>
<li><p>dependsOn：作业依赖</p>
</li>
</ol>
<h2 id="作业依赖案例"><a href="#作业依赖案例" class="headerlink" title="作业依赖案例"></a>作业依赖案例</h2><p>1.修改basic.flow为如下内容</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">nodes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">jobC</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">    <span class="comment"># jobC 依赖 JobA和JobB</span></span><br><span class="line">    <span class="attr">dependsOn:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">jobA</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">jobB</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">echo</span> <span class="string">&quot;I’m JobC&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">jobA</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">echo</span> <span class="string">&quot;I’m JobA&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">jobB</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">echo</span> <span class="string">&quot;I’m JobB&quot;</span></span><br></pre></td></tr></table></figure>

<p>2.打包上传并执行</p>
<h2 id="工作流配置"><a href="#工作流配置" class="headerlink" title="工作流配置"></a>工作流配置</h2><p>可以在工作流配置文件中添加config标签，对工作流进行全局配置，这些配置会应用到该工作流的<strong>所有</strong>作业中。例如，在basic.flow前面添加config：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">config:</span></span><br><span class="line">  <span class="attr">words.to.print:</span> <span class="string">&quot;This is for test!&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">nodes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">jobA</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">echo</span> <span class="string">$&#123;words.to.print&#125;</span></span><br></pre></td></tr></table></figure>

<h2 id="失败重试"><a href="#失败重试" class="headerlink" title="失败重试"></a>失败重试</h2><p>在Job配置中添加两个参数，即可实现任务失败自动重试：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment">#任务重试次数</span></span><br><span class="line"><span class="attr">retries:</span> <span class="number">3</span></span><br><span class="line"><span class="comment">#任务重试延迟</span></span><br><span class="line"><span class="attr">retry.backoff:</span> <span class="number">10000</span></span><br></pre></td></tr></table></figure>

<p>案例：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">nodes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">JobA</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">sh</span> <span class="string">/not_exists.sh</span></span><br><span class="line">      <span class="attr">retries:</span> <span class="number">3</span></span><br><span class="line">      <span class="attr">retry.backoff:</span> <span class="number">10000</span></span><br></pre></td></tr></table></figure>

<p>也可以在Flow全局配置中添加任务失败重试配置，此时重试配置会应用到所有Job。</p>
<p>案例如下：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">config:</span></span><br><span class="line">  <span class="attr">retries:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">retry.backoff:</span> <span class="number">10000</span></span><br><span class="line">  </span><br><span class="line"><span class="attr">nodes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">JobA</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">sh</span> <span class="string">/not_exists.sh</span></span><br></pre></td></tr></table></figure>

<h2 id="内嵌工作流"><a href="#内嵌工作流" class="headerlink" title="内嵌工作流"></a>内嵌工作流</h2><p>工作流定义文件中可以添加子工作流，例如：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">nodes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">jobC</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">    <span class="comment"># jobC 依赖embedded_flow</span></span><br><span class="line">    <span class="attr">dependsOn:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">embedded_flow</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">echo</span> <span class="string">&quot;I’m JobC&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">embedded_flow</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">flow</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">    <span class="comment"># prop: value 是工作流的一个全局配置</span></span><br><span class="line">      <span class="attr">prop:</span> <span class="string">value</span></span><br><span class="line">    <span class="attr">nodes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">jobB</span></span><br><span class="line">      <span class="comment"># noop 什么都不干</span></span><br><span class="line">        <span class="attr">type:</span> <span class="string">noop</span>					</span><br><span class="line">        <span class="attr">dependsOn:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">jobA</span></span><br><span class="line"></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">jobA</span></span><br><span class="line">        <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">        <span class="attr">config:</span></span><br><span class="line">          <span class="attr">command:</span> <span class="string">echo</span> <span class="string">$&#123;prop&#125;</span></span><br></pre></td></tr></table></figure>

<h1 id="作业类型"><a href="#作业类型" class="headerlink" title="作业类型"></a>作业类型</h1><h2 id="命令行类型"><a href="#命令行类型" class="headerlink" title="命令行类型"></a>命令行类型</h2><p>命令行类型是最基本的内建类型，type类型为command，可用的配置为：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">nodes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">jobA</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">echo</span> <span class="string">&quot;Command 1&quot;</span></span><br></pre></td></tr></table></figure>

<h2 id="JavaProcess"><a href="#JavaProcess" class="headerlink" title="JavaProcess"></a>JavaProcess</h2><p>JavaProcess类型可以运行一个自定义主类方法，type类型为javaprocess，可用的配置为：</p>
<p><strong>1.</strong> Xms：最小堆</p>
<p><strong>2.</strong> Xmx：最大堆</p>
<p><strong>3.</strong> java.class：要运行的Java对象，其中必须包含Main方法</p>
<p>案例：</p>
<ol>
<li>首先新建一个Java项目，新建一个包含主方法的主类，内容如下：</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.vincent;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AzTest</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;This is for testing!&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>打包成jar包</p>
<ol>
<li>新建testJava.flow，内容如下</li>
</ol>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">nodes:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">test_java</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">javaprocess</span></span><br><span class="line">  <span class="attr">config:</span></span><br><span class="line">   <span class="attr">Xms:</span> <span class="string">96M</span></span><br><span class="line">   <span class="attr">Xmx:</span> <span class="string">200M</span></span><br><span class="line">   <span class="attr">java.class:</span> <span class="string">com.vincent.AzTest</span></span><br></pre></td></tr></table></figure>

<p>将Jar包、flow文件和project文件打包成zip，上传到集群并执行</p>
<h1 id="条件工作流"><a href="#条件工作流" class="headerlink" title="条件工作流"></a>条件工作流</h1><p>条件工作流功能允许用户根据条件指定是否运行某些作业。条件由先前作业的运行时参数（例如输出）和预定义宏组成。在这些条件下，用户可以在确定作业执行逻辑时获得更大的灵活性。例如，只要父作业之一成功，他们就可以运行当前作业。他们可以在工作流内部实现分支逻辑。</p>
<h2 id="运行时参数"><a href="#运行时参数" class="headerlink" title="运行时参数"></a>运行时参数</h2><p>运行时参数一般指作业的输出，使用时有以下几个条件：</p>
<ol>
<li><p>使用 ${jobName:param} 来定义作业运行时参数的条件</p>
</li>
<li><p>“:” 用于分隔jobName和参数</p>
</li>
<li><p>job运行时，使用参数与条件中的字符串或数字进行比较</p>
</li>
<li><p>用户需要事先将参数的值写入 $JOB_OUTPUT_PROP_FILE</p>
</li>
</ol>
<p>支持的运算符：</p>
<ol>
<li><p>==    等于</p>
</li>
<li><p>!=    不等于</p>
</li>
<li><blockquote>
<p>   大于</p>
</blockquote>
</li>
<li><blockquote>
<p>=    大于等于</p>
</blockquote>
</li>
<li><p>&lt;    小于</p>
</li>
<li><p>&lt;=    小于等于</p>
</li>
<li><p>&amp;&amp;    与</p>
</li>
<li><p>||    或</p>
</li>
<li><p>!    非</p>
</li>
</ol>
<p>案例：</p>
<ol>
<li>新建basic.flow</li>
</ol>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">nodes:</span></span><br><span class="line"> <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">JobA</span></span><br><span class="line">   <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">   <span class="attr">config:</span></span><br><span class="line">     <span class="attr">command:</span> <span class="string">sh</span> <span class="string">/opt/module/write_to_props.sh</span></span><br><span class="line"></span><br><span class="line"> <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">JobB</span></span><br><span class="line">   <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">   <span class="attr">dependsOn:</span></span><br><span class="line">     <span class="bullet">-</span> <span class="string">JobA</span></span><br><span class="line">   <span class="attr">config:</span></span><br><span class="line">     <span class="attr">command:</span> <span class="string">echo</span> <span class="string">&quot;This is JobB.&quot;</span></span><br><span class="line">   <span class="attr">condition:</span> <span class="string">$&#123;JobA:param1&#125;</span> <span class="string">==</span> <span class="string">&quot;AAA&quot;</span></span><br><span class="line"></span><br><span class="line"> <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">JobC</span></span><br><span class="line">   <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">   <span class="attr">dependsOn:</span></span><br><span class="line">     <span class="bullet">-</span> <span class="string">JobA</span></span><br><span class="line">   <span class="attr">config:</span></span><br><span class="line">     <span class="attr">command:</span> <span class="string">echo</span> <span class="string">&quot;This is JobC.&quot;</span></span><br><span class="line">   <span class="attr">condition:</span> <span class="string">$&#123;JobA:param1&#125;</span> <span class="string">==</span> <span class="string">&quot;BBB&quot;</span></span><br></pre></td></tr></table></figure>

<ol start="2">
<li>新建/opt/module/write_to_props.sh</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim /opt/module/write_to_props.sh</span><br><span class="line">内容为：</span><br><span class="line">echo &#x27;&#123;&quot;param1&quot;:&quot;AAA&quot;&#125;&#x27; &gt; $JOB_OUTPUT_PROP_FILE</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>按照我们设定的条件，由于JobA输出为AAA（单引号会将里边字符自动转义），所以会执行JobB分支。上传执行，注意观察分支条件：</li>
</ol>
<h2 id="预定义宏"><a href="#预定义宏" class="headerlink" title="预定义宏"></a>预定义宏</h2><p>预定义宏将会在所有父作业上评估，即YAML文件中的dependsOn部分。可用的预定义宏如下：</p>
<ol>
<li><p>all_success: 全部成功(默认)</p>
</li>
<li><p>all_done：全部完成</p>
</li>
<li><p>all_failed：全部失败</p>
</li>
<li><p>one_success：至少一个成功</p>
</li>
<li><p>one_failed：至少一个失败</p>
</li>
</ol>
<p>案例：</p>
<ol>
<li>修改上个案例的basic.flow:</li>
</ol>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">nodes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">JobA</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">sh</span> <span class="string">/opt/module/write_to_props.sh</span></span><br><span class="line"></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">JobB</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">    <span class="attr">dependsOn:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">JobA</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">echo</span> <span class="string">&quot;This is JobB.&quot;</span></span><br><span class="line">    <span class="attr">condition:</span> <span class="string">$&#123;JobA:param1&#125;</span> <span class="string">==</span> <span class="string">&quot;AAA&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">JobC</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">    <span class="attr">dependsOn:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">JobA</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">echo</span> <span class="string">&quot;This is JobC.&quot;</span></span><br><span class="line">    <span class="attr">condition:</span> <span class="string">$&#123;JobA:param1&#125;</span> <span class="string">==</span> <span class="string">&quot;BBB&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">JobD</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">    <span class="attr">dependsOn:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">JobB</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">JobC</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">echo</span> <span class="string">&quot;This is JobD.&quot;</span></span><br><span class="line">    <span class="attr">condition:</span> <span class="string">one_success</span></span><br><span class="line"></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">JobE</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">    <span class="attr">dependsOn:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">JobB</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">JobC</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">echo</span> <span class="string">&quot;This is JobE.&quot;</span></span><br><span class="line">    <span class="attr">condition:</span> <span class="string">all_success</span></span><br><span class="line"></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">JobF</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">    <span class="attr">dependsOn:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">JobB</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">JobC</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">JobD</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">JobE</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">echo</span> <span class="string">&quot;This is JobF.&quot;</span></span><br><span class="line"><span class="attr">condition:</span> <span class="string">all_done</span></span><br></pre></td></tr></table></figure>

<ol start="2">
<li>打包执行，注意观察任务的执行情况</li>
</ol>
<h1 id="失败报警"><a href="#失败报警" class="headerlink" title="失败报警"></a>失败报警</h1><h2 id="默认邮件报警"><a href="#默认邮件报警" class="headerlink" title="默认邮件报警"></a>默认邮件报警</h2><p>1.在web-server节点（linux1）编辑 /opt/module/azkaban/azkaban-web-server-3.84.4/conf/azkaban.properties，修改如下内容：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">这里设置邮件发送服务器，需要 申请邮箱，且开通stmp服务，以下只是例子</span></span><br><span class="line">mail.sender=vincent@126.com</span><br><span class="line">mail.host=smtp.126.com</span><br><span class="line">mail.user=vincent@126.com</span><br><span class="line">mail.passwd=password</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">这里设置工作流成功或者失败默认向哪里发送服务</span></span><br><span class="line">job.failure.email=vincent@126.com</span><br><span class="line">job.success.email=vincent@126.com</span><br></pre></td></tr></table></figure>

<p>保存并重启web-server。</p>
<p>2.编辑basic.flow，加入如下属性：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">nodes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">jobA</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">echo</span> <span class="string">&quot;This is an echoed text.&quot;</span></span><br><span class="line">      <span class="attr">failure.emails:</span> <span class="string">vincent@126.com</span></span><br><span class="line">      <span class="attr">success.emails:</span> <span class="string">vincent@126.com</span></span><br><span class="line">      <span class="attr">notify.emails:</span> <span class="string">vincent@126.com</span> </span><br></pre></td></tr></table></figure>

<h2 id="自定义报警"><a href="#自定义报警" class="headerlink" title="自定义报警"></a>自定义报警</h2><p>有时任务执行失败后邮件报警接收不及时，需要自定义报警装置，比如电话报警。此时需要首先找一个电话通知服务商，比如onealter.com，购买相应服务后，获取通知API。然后进行MailAlter二次开发。</p>
<p>1.新建一个普通的Java项目</p>
<p>2.在项目的lib里添加4个Jar包：</p>
<p>3.新建com.vincent.PhoneAlter类实现azkaban.alert.Alerter</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> azkaban.alert.Alerter;</span><br><span class="line"><span class="keyword">import</span> azkaban.executor.ExecutableFlow;</span><br><span class="line"><span class="keyword">import</span> azkaban.executor.Executor;</span><br><span class="line"><span class="keyword">import</span> azkaban.executor.ExecutorManagerException;</span><br><span class="line"><span class="keyword">import</span> azkaban.sla.SlaOption;</span><br><span class="line"><span class="keyword">import</span> azkaban.utils.Props;</span><br><span class="line"><span class="keyword">import</span> com.google.gson.JsonObject;</span><br><span class="line"><span class="keyword">import</span> org.apache.log4j.Logger;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PhoneAlterter</span> <span class="keyword">implements</span> <span class="title">Alerter</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger logger = Logger.getLogger(PhoneAlterter.class);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String appKey;</span><br><span class="line">    <span class="keyword">private</span> String url;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">PhoneAlterter</span><span class="params">(Props props)</span> </span>&#123;</span><br><span class="line">        appKey = props.getString(<span class="string">&quot;my.alert.appKey&quot;</span>, <span class="string">&quot;&quot;</span>);</span><br><span class="line">        url = props.getString(<span class="string">&quot;my.alert.url&quot;</span>, <span class="string">&quot;&quot;</span>);</span><br><span class="line">        logger.info(<span class="string">&quot;Appkey: &quot;</span> + appKey);</span><br><span class="line">        logger.info(<span class="string">&quot;URL: &quot;</span> + url);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 成功的通知</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> exflow</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">alertOnSuccess</span><span class="params">(ExecutableFlow exflow)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 出现问题的通知</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> exflow</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> extraReasons</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">alertOnError</span><span class="params">(ExecutableFlow exflow, String... extraReasons)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//一般来说网络电话服务都是通过HTTP请求发送的，这里可以调用shell发送HTTP请求</span></span><br><span class="line">        JsonObject alert = <span class="keyword">new</span> JsonObject();</span><br><span class="line">        alert.addProperty(<span class="string">&quot;app&quot;</span>, appKey);</span><br><span class="line">        alert.addProperty(<span class="string">&quot;eventId&quot;</span>, exflow.getId());</span><br><span class="line">        alert.addProperty(<span class="string">&quot;eventType&quot;</span>, <span class="string">&quot;trigger&quot;</span>);</span><br><span class="line">        alert.addProperty(<span class="string">&quot;alarmContent&quot;</span>, exflow.getId() + <span class="string">&quot; fails!&quot;</span>);</span><br><span class="line">        alert.addProperty(<span class="string">&quot;priority&quot;</span>, <span class="string">&quot;2&quot;</span>);</span><br><span class="line">        String[] cmd = <span class="keyword">new</span> String[<span class="number">8</span>];</span><br><span class="line">        cmd[<span class="number">0</span>] = <span class="string">&quot;curl&quot;</span>;</span><br><span class="line">        cmd[<span class="number">1</span>] = <span class="string">&quot;-H&quot;</span>;</span><br><span class="line">        cmd[<span class="number">2</span>] = <span class="string">&quot;Content-type: application/json&quot;</span>;</span><br><span class="line">        cmd[<span class="number">3</span>] = <span class="string">&quot;-X&quot;</span>;</span><br><span class="line">        cmd[<span class="number">4</span>] = <span class="string">&quot;POST&quot;</span>;</span><br><span class="line">        cmd[<span class="number">5</span>] = <span class="string">&quot;-d&quot;</span>;</span><br><span class="line">        cmd[<span class="number">6</span>] = alert.toString();</span><br><span class="line">        cmd[<span class="number">7</span>] = url;</span><br><span class="line">        logger.info(<span class="string">&quot;Sending phone alert!&quot;</span>);</span><br><span class="line">        Runtime.getRuntime().exec(cmd);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 首次出现问题的通知</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> exflow</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">alertOnFirstError</span><span class="params">(ExecutableFlow exflow)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">alertOnSla</span><span class="params">(SlaOption slaOption, String slaMessage)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">alertOnFailedUpdate</span><span class="params">(Executor executor, List&lt;ExecutableFlow&gt; executions, ExecutorManagerException e)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>4.新建/opt/module/azkaban/azkaban-web-server-3.84.4/plugin/alerter/phone-alerter文件夹，并在内部新建conf和lib两个目录</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mkdir -p /opt/module/azkaban/azkaban-web-server-3.84.4/plugins/alerter/phone-alerter/conf /opt/module/azkaban/azkaban-web-server-3.84.4/plugins/alerter/phone-alerter/lib</span><br></pre></td></tr></table></figure>

<p>5.在新建的conf目录里，新建plugin.properties</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">name一定要设置email，用以覆盖默认的邮件报警</span></span><br><span class="line">alerter.name=email</span><br><span class="line">alerter.external.classpaths=lib</span><br><span class="line">alerter.class=com.vincent.PhoneAlterter</span><br><span class="line"><span class="meta">#</span><span class="bash">这两个参数和你使用的AlertAPI有关系</span></span><br><span class="line">my.alert.appKey=cf3e2ce2-40ba-c3cd-1c74-xxxxxxxxxxxx</span><br><span class="line">my.alert.url=http://some.example.url</span><br></pre></td></tr></table></figure>

<p>6.将代码打包，并将Jar上传到/opt/module/azkaban/azkaban-web-server-3.84.4/lib文件夹，并重启web服务。</p>
]]></content>
      <categories>
        <category>工具</category>
      </categories>
  </entry>
  <entry>
    <title>Git</title>
    <url>/Git/</url>
    <content><![CDATA[<p>Git是目前世界上最先进的分布式版本控制系统</p>
<p>功能：版本还原、代码备份、分支管理、协同开发、历史追查、版本记录、权限管理</p>
<h2 id="集中式版本管理"><a href="#集中式版本管理" class="headerlink" title="集中式版本管理"></a>集中式版本管理</h2><p>经典产品： CVS、VSS、SVN</p>
<p>特点：由中央仓库统一管理，结构简单，上手容易！</p>
<p><img src="/Git/16.png" alt="16"></p>
<p>不足：</p>
<p>版本管理的服务器一旦崩溃，硬盘损坏，代码如何恢复？</p>
<p>程序员上传到服务器的代码要求是完整版本，但是程序员开发过程中想做小版本的管理，以便追溯查询，怎么破？</p>
<p>系统正在上线运行，时不时还要修改bug，要增加好几个功能要几个月，如何管理几个版本？</p>
<p>如何管理一个分布在世界各地、互不相识的大型开发团队？</p>
<h2 id="Git的安装"><a href="#Git的安装" class="headerlink" title="Git的安装"></a>Git的安装</h2><p>Git官网：<a href="https://git-scm.com/">https://git-scm.com/</a></p>
<p>GitHub网站:<a href="http://www.github.com/">http://www.github.com</a></p>
<p>命令行工具：Git for windows</p>
<p>下载地址：<a href="https://git-for-windows.github.io/">https://git-for-windows.github.io/</a></p>
<p>在 macOS 上安装：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git --version</span></span><br></pre></td></tr></table></figure>

<p>在 Linux 上安装：</p>
<p> 基于 RPM 的发行版，如 RHEL 或 CentOS</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo dnf install git-all</span></span><br></pre></td></tr></table></figure>

<p>基于 Debian 的发行版上，如 Ubuntu:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo apt install git-all</span></span><br></pre></td></tr></table></figure>

<p>从源代码安装：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo dnf install dh-autoreconf curl-devel expat-devel gettext-devel \</span></span><br><span class="line"><span class="bash">  openssl-devel perl-devel zlib-devel</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo apt-get install dh-autoreconf libcurl4-gnutls-dev libexpat1-dev \</span></span><br><span class="line"><span class="bash">  gettext libz-dev libssl-dev</span></span><br></pre></td></tr></table></figure>

<p>为了添加文档的多种格式（doc、html、info），需要以下附加的依赖：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo dnf install asciidoc xmlto docbook2X</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo apt-get install asciidoc xmlto docbook2x</span></span><br></pre></td></tr></table></figure>

<p>如果你使用基于 Debian 的发行版（Debian/Ubuntu/Ubuntu-derivatives），你也需要 install-info 包：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo apt-get install install-info</span></span><br></pre></td></tr></table></figure>

<p>如果你使用基于 RPM 的发行版（Fedora/RHEL/RHEL衍生版），你还需要 getopt 包 （它已经在基于 Debian</p>
<p>的发行版中预装了）：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo dnf install getopt</span></span><br></pre></td></tr></table></figure>

<p>此外，如果你使用 Fedora/RHEL/RHEL衍生版，那么你需要执行以下命令：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo ln -s /usr/bin/db2x_docbook2texi /usr/bin/docbook2x-texi</span></span><br></pre></td></tr></table></figure>

<p>完成后，你可以使用 Git 来获取 Git 的更新：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git <span class="built_in">clone</span> git://git.kernel.org/pub/scm/git/git.git</span></span><br></pre></td></tr></table></figure>

<h3 id="设置Git账户"><a href="#设置Git账户" class="headerlink" title="设置Git账户"></a>设置Git账户</h3><table>
<thead>
<tr>
<th>命令</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>git config –list</td>
<td>查看所有配置</td>
</tr>
<tr>
<td>git config –list –show-origin</td>
<td>查看所有配置以及所在文件位置</td>
</tr>
<tr>
<td>git config –global user.name xxx</td>
<td>设置git用户名</td>
</tr>
<tr>
<td>git config –global user.email xxx</td>
<td>设置git邮箱</td>
</tr>
<tr>
<td>git init</td>
<td>初始化本地库</td>
</tr>
<tr>
<td>git config core.autocrlf false</td>
<td>取消换行符转换的warning提醒</td>
</tr>
</tbody></table>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git config --global user.name <span class="string">&quot;Vincent&quot;</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> git config --global user.email 18635372669@163.com</span></span><br></pre></td></tr></table></figure>

<h3 id="本地仓库初始化"><a href="#本地仓库初始化" class="headerlink" title="本地仓库初始化"></a>本地仓库初始化</h3><p>1、新建一个本地仓库，也就是一个文件夹。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> /Users/vincent</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mkdir my_project</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> /Users/vincent/my_project</span></span><br></pre></td></tr></table></figure>

<p>2、在此文件夹目录下执行git init命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git init</span></span><br></pre></td></tr></table></figure>

<h2 id="常用Git命令"><a href="#常用Git命令" class="headerlink" title="常用Git命令"></a>常用Git命令</h2><table>
<thead>
<tr>
<th>命令</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>git status</td>
<td>查看本地库的状态(git status -s 简化输出结果)</td>
</tr>
<tr>
<td>git add [file]</td>
<td>多功能命令: 1. 开始跟踪新文件 2. 把已跟踪的文件添加到暂存区 3. 合并时把有冲突的文件标记为已解决状态</td>
</tr>
<tr>
<td>git commit -m “xxx” [file]</td>
<td>将暂存区的文件提交到本地库,-m 后面为修改的说明（注释）</td>
</tr>
</tbody></table>
<p>工作区—–&gt;暂存区—–&gt;本地库</p>
<p>工作区—–&gt;暂存区：git add [file]</p>
<p>暂存区—–&gt;工作区：git rm cached [file]</p>
<p>暂存区—–&gt;本地库：git commit –m “xxx” [file]</p>
<h3 id="忽略文件"><a href="#忽略文件" class="headerlink" title="忽略文件"></a>忽略文件</h3><p>一般我们总会有些文件无需纳入 Git 的管理，也不希望它们总出现在未跟踪文件列表。 </p>
<p>通常都是些自动生成的文 件，比如日志文件，或者编译过程中创建的临时文件等。 </p>
<p>在这种情况下，我们可以创建一个名为 .gitignore 的文件，列出要忽略的文件的模式。</p>
<p>一个.gitignore 的案例:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 忽略所有的 .a 文件</span> </span><br><span class="line">*.a</span><br><span class="line"><span class="meta">#</span><span class="bash"> 但跟踪所有的 lib.a，即便你在前面忽略了 .a 文件</span> </span><br><span class="line">!lib.a</span><br><span class="line"><span class="meta">#</span><span class="bash"> 只忽略当前目录下的 TODO 文件，而不忽略 subdir/TODO. 不递归的忽略</span></span><br><span class="line">/TODO</span><br><span class="line"><span class="meta">#</span><span class="bash"> 忽略任何目录下名为 build 的文件夹 递归的忽略</span></span><br><span class="line">build/</span><br><span class="line"><span class="meta">#</span><span class="bash"> 忽略 doc/notes.txt，但不忽略 doc/server/arch.txt</span> </span><br><span class="line">doc/*.txt</span><br><span class="line"><span class="meta">#</span><span class="bash"> 忽略 doc/ 目录及其所有子目录下的 .pdf 文件</span> </span><br><span class="line">doc/**/*.pdf</span><br></pre></td></tr></table></figure>

<h2 id="版本切换"><a href="#版本切换" class="headerlink" title="版本切换"></a>版本切换</h2><h3 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h3><table>
<thead>
<tr>
<th>命令</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>git log</td>
<td>以完整格式查看本地库状态(查看历史版本)</td>
</tr>
<tr>
<td>git log –pretty=oneline</td>
<td>以单行模式查看本地库状态</td>
</tr>
<tr>
<td>git reset –hard HEAD^</td>
<td>回退一个版本</td>
</tr>
<tr>
<td>git reset –hard HEAD~n</td>
<td>回退N个版本</td>
</tr>
<tr>
<td>git reflog</td>
<td>查看所有操作的历史记录</td>
</tr>
<tr>
<td>git reset –hard [具体版本号，例如：1f9a527等]</td>
<td>回到（回退和前进都行）指定版本号的版本，</td>
</tr>
<tr>
<td>git checkout – [file]</td>
<td>从本地库检出最新文件覆盖工作区的文件(文件还没有提交到暂存区, 否则无效)</td>
</tr>
<tr>
<td>git reset [file]  或者 git restore –staged [file]</td>
<td>从暂存区撤销文件</td>
</tr>
<tr>
<td>git restore <file></file></td>
<td>放弃在工作区的修改(还没有提交到暂存区)</td>
</tr>
<tr>
<td>git rm –cache [file]</td>
<td>撤销对文件的跟踪.</td>
</tr>
</tbody></table>
<h3 id="比较文件"><a href="#比较文件" class="headerlink" title="比较文件"></a>比较文件</h3><p>将工作区中的文件和暂存区进行比较:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git diff &lt;file&gt;</span></span><br></pre></td></tr></table></figure>

<p>将工作区中的文件和本地库当前版本进行比较:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git diff HEAD &lt;file&gt;</span></span><br></pre></td></tr></table></figure>

<p>查看暂存区和本地库最新提交版本的差别:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git diff --cached &lt;file&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="删除操作"><a href="#删除操作" class="headerlink" title="删除操作"></a>删除操作</h3><p>1、删除文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> rm a.txt</span></span><br></pre></td></tr></table></figure>

<p>2、更新删除操作</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git add/rm a.txt</span></span><br></pre></td></tr></table></figure>

<p>3、提交更新</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git commit -m <span class="string">&#x27;delete a.txt&#x27;</span></span></span><br></pre></td></tr></table></figure>

<h2 id="Git的工作机制"><a href="#Git的工作机制" class="headerlink" title="Git的工作机制"></a>Git的工作机制</h2><h3 id="三区"><a href="#三区" class="headerlink" title="三区"></a>三区</h3><p><strong>工作区(Working Directory):就是你电脑本地硬盘目录</strong></p>
<p><strong>本地库(Repository):工作区有个隐藏目录.git，它就是Git的本地版本库</strong></p>
<p><strong>暂存区(stage):一般存放在”git目录”下的index文件（.git/index）中，所以我们把暂存区有时也叫作索引（index）。</strong></p>
<h3 id="git目录"><a href="#git目录" class="headerlink" title=".git目录"></a>.git目录</h3><p>HEAD    git项目当前指针位置</p>
<p>index        git索引文件</p>
<p>config        git配置信息</p>
<p>refs/        标示git项目分支指向那次提交commitId</p>
<p>objects/    git本地仓库对象（commit tree blob类型）</p>
<p>logs/        每次refs的历史记录</p>
<h2 id="分支操作"><a href="#分支操作" class="headerlink" title="分支操作"></a>分支操作</h2><h3 id="分支的概念"><a href="#分支的概念" class="headerlink" title="分支的概念"></a>分支的概念</h3><p>不使用分支，就是人与人之间协作；</p>
<p>使用分支，就是小组与小组之间的协作；</p>
<p>从主干中拉取分支，开发完成，将工作，合并到主干。</p>
<h3 id="常用命令汇总"><a href="#常用命令汇总" class="headerlink" title="常用命令汇总"></a>常用命令汇总</h3><table>
<thead>
<tr>
<th>命令</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>git branch [分支名]</td>
<td>创建分支</td>
</tr>
<tr>
<td>git branch -v</td>
<td>查看分支,可以使用-v参数查看详细信息</td>
</tr>
<tr>
<td>git checkout [分支名]</td>
<td>切换分支</td>
</tr>
<tr>
<td>git merge [分支名]</td>
<td>合并分支；将merge命令中指定的分支合并到当前分支上例如：如果想将dev分支合并到master分支，那么必须在master分支上执行merge命令</td>
</tr>
<tr>
<td>git branch -d[分支名]</td>
<td>删除分支</td>
</tr>
<tr>
<td>git checkout -b [分支名]</td>
<td>新建并切换到当前分支</td>
</tr>
<tr>
<td>git log –oneline –decorate –graph –all</td>
<td>它会输出你的提交历史、各个分支的指向以及项目的分支分叉情况</td>
</tr>
<tr>
<td>git merge –abort</td>
<td>取消合并</td>
</tr>
</tbody></table>
<h2 id="GitHub"><a href="#GitHub" class="headerlink" title="GitHub"></a>GitHub</h2><p><a href="https://github.com/Vincent-ski/bigdata.git">https://github.com/Vincent-ski/bigdata.git</a></p>
<p><a href="mailto:&#103;&#105;&#116;&#x40;&#103;&#105;&#116;&#x68;&#x75;&#98;&#x2e;&#99;&#111;&#x6d;">&#103;&#105;&#116;&#x40;&#103;&#105;&#116;&#x68;&#x75;&#98;&#x2e;&#99;&#111;&#x6d;</a>:Vincent-ski/bigdata.git</p>
<p>GitHub是一个Git项目托管网站,主要提供基于Git的版本托管服务。</p>
<h3 id="本地库联通GitHub"><a href="#本地库联通GitHub" class="headerlink" title="本地库联通GitHub"></a>本地库联通GitHub</h3><p>1、查看本地是否配置了密钥</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> ~/.ssh</span></span><br></pre></td></tr></table></figure>

<p>2、生成密钥</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ssh-keygen</span></span><br></pre></td></tr></table></figure>

<p>3、将公钥告诉github账户，相当于约定一个接头暗号</p>
<p>4、测试本地仓库和github的联通性：ssh -T <a href="mailto:&#x67;&#x69;&#x74;&#64;&#x67;&#x69;&#116;&#x68;&#117;&#98;&#46;&#x63;&#111;&#x6d;">&#x67;&#x69;&#x74;&#64;&#x67;&#x69;&#116;&#x68;&#117;&#98;&#46;&#x63;&#111;&#x6d;</a></p>
<p>5、.ssh文件夹中会多一个文件known_hosts，其中记录了连接的github的ip账号</p>
<h3 id="push"><a href="#push" class="headerlink" title="push"></a>push</h3><p>本地库推送到GitHub</p>
<p>1、准备本地库</p>
<p>2、在GitHub上创建仓库</p>
<p>3、增加远程地址</p>
<p>git remote add  &lt;远端代号&gt;  &lt;远端地址&gt;</p>
<p>&lt;远端代号&gt; 是指远程链接的代号，一般直接用origin作代号，也可以自定义</p>
<p>&lt;远端地址&gt; 默认远程链接的url</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git remote add origin git@github.com:Vincent-ski/bigdata.git</span></span><br></pre></td></tr></table></figure>

<p>4、本地库推送到远程库</p>
<p>git  push  -u  &lt;远端代号&gt;   &lt;本地分支名称&gt;</p>
<p>&lt;远端代号&gt; 是指远程链接的代号</p>
<p>&lt;分支名称&gt;  是指要提交的分支名字，比如master</p>
<p>我们第一次推送master分支时，加上了-u参数，Git不但会把本地的master分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命令。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git push -u origin master</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> git push origin color</span></span><br></pre></td></tr></table></figure>

<p>5、查看远程分支</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git remote -v</span></span><br></pre></td></tr></table></figure>

<h3 id="pull"><a href="#pull" class="headerlink" title="pull"></a>pull</h3><p>如果远程库的版本新于当前库，那么此时为了使当前库和远程库保持一致，可以执行pull命令</p>
<p>git pull &lt;远端代号&gt;  &lt;远端分支名&gt;</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git pull origin master</span></span><br></pre></td></tr></table></figure>

<h3 id="fetch"><a href="#fetch" class="headerlink" title="fetch"></a>fetch</h3><p>从远程库获取更新，但是并不合并</p>
<h3 id="clone"><a href="#clone" class="headerlink" title="clone"></a>clone</h3><p>刚开始做项目的时候，需要从远程库将项目先整到本机</p>
<p>执行命令：</p>
<p>git  clone  &lt;远端地址&gt;  &lt;新项目目录名&gt;</p>
<p>&lt;远端地址&gt; 是指远程链接的地址;</p>
<p>&lt;项目目录名&gt;  是指为克隆的项目在本地新建的目录名称，可以不填，默认是GitHub的项目名;</p>
<p>命令执行完后，会自动为这个远端地址建一个名为origin的代号。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git <span class="built_in">clone</span> https://github.com/Vincent-ski/bigdata.git hahaha</span></span><br></pre></td></tr></table></figure>

<h3 id="fork"><a href="#fork" class="headerlink" title="fork"></a>fork</h3><p>如果其他人，搜索到了你的项目，想对其做一些编辑时，必须先执行fork操作。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="/Git/17.png" alt="17"></p>
]]></content>
      <categories>
        <category>工具</category>
      </categories>
  </entry>
  <entry>
    <title>HDFS配置</title>
    <url>/Hadoop%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<h1 id="准备虚拟机"><a href="#准备虚拟机" class="headerlink" title="准备虚拟机"></a>准备虚拟机</h1><h2 id="1-克隆虚拟机"><a href="#1-克隆虚拟机" class="headerlink" title="1. 克隆虚拟机"></a>1. 克隆虚拟机</h2><p>单台虚拟机：内存4G，硬盘50G，安装必要环境(最小化安装)</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo yum install -y epel-release</span><br><span class="line">sudo yum install -y psmisc nc net-tools rsync vim lrzsz ntp libzstd openssl-static tree iotop</span><br></pre></td></tr></table></figure>

<p>修改克隆虚拟机的静态IP(按照自己机器的网络设置进行修改)</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo vim /etc/sysconfig/network-scripts/ifcfg-ens33  或</span><br><span class="line">sudo vim /etc/sysconfig/network-scripts/ifcfg-eth0</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">TYPE=&quot;Ethernet&quot;</span><br><span class="line">PROXY_METHOD=&quot;none&quot;</span><br><span class="line">BROWSER_ONLY=&quot;no&quot;</span><br><span class="line">BOOTPROTO=static</span><br><span class="line">DEFROUTE=&quot;yes&quot;</span><br><span class="line">IPV4_FAILURE_FATAL=&quot;no&quot;</span><br><span class="line">IPV6INIT=&quot;yes&quot;</span><br><span class="line">IPV6_AUTOCONF=&quot;yes&quot;</span><br><span class="line">IPV6_DEFROUTE=&quot;yes&quot;</span><br><span class="line">IPV6_FAILURE_FATAL=&quot;no&quot;</span><br><span class="line">IPV6_ADDR_GEN_MODE=&quot;stable-privacy&quot;</span><br><span class="line">NAME=&quot;eth0&quot;</span><br><span class="line">UUID=&quot;19c440dc-9a12-4fd7-bc46-0a121ef6af96&quot;</span><br><span class="line">DEVICE=&quot;eth0&quot;</span><br><span class="line">ONBOOT=&quot;yes&quot;</span><br><span class="line">IPADDR=10.211.55.10</span><br><span class="line">GATEWAY=10.211.55.1</span><br><span class="line">DNS1=10.211.55.1</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="2-修改主机名"><a href="#2-修改主机名" class="headerlink" title="2. 修改主机名"></a>2. 修改主机名</h2><p>修改主机名称</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo vim /etc/hostname</span><br></pre></td></tr></table></figure>

<p>配置主机名称映射，打开/etc/hosts</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo vim /etc/hosts</span><br></pre></td></tr></table></figure>

<figure class="highlight accesslog"><table><tr><td class="code"><pre><span class="line"><span class="number">10.211.55.10</span>	linux1</span><br><span class="line"><span class="number">10.211.55.11</span>	linux2</span><br><span class="line"><span class="number">10.211.55.12</span>	linux3</span><br></pre></td></tr></table></figure>

<p>修改window7的主机映射文件（hosts文件）</p>
<p>进入C:\Windows\System32\drivers\etc</p>
<h2 id="3-关闭防火墙"><a href="#3-关闭防火墙" class="headerlink" title="3. 关闭防火墙"></a>3. 关闭防火墙</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo systemctl stop firewalld</span><br><span class="line">sudo systemctl disable firewalld</span><br></pre></td></tr></table></figure>

<h2 id="4-创建vincent用户"><a href="#4-创建vincent用户" class="headerlink" title="4. 创建vincent用户"></a>4. 创建vincent用户</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo useradd vincent</span><br><span class="line">sudo passwd vincent</span><br></pre></td></tr></table></figure>

<h2 id="5-配置vincent用户具有root权限"><a href="#5-配置vincent用户具有root权限" class="headerlink" title="5. 配置vincent用户具有root权限"></a>5. 配置vincent用户具有root权限</h2><p>修改/etc/sudoers文件，如下所示</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo vim /etc/sudoers</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># Allows people in group wheel to run all commands</span></span></span><br><span class="line"><span class="meta">%</span><span class="bash">wheel  ALL=(ALL)       ALL</span></span><br><span class="line">vincent ALL=(ALL)       ALL</span><br></pre></td></tr></table></figure>

<h2 id="6-在-opt目录下创建文件夹"><a href="#6-在-opt目录下创建文件夹" class="headerlink" title="6. 在/opt目录下创建文件夹"></a>6. 在/opt目录下创建文件夹</h2><p>在/opt目录下创建module、software文件夹</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo mkdir module</span><br><span class="line">sudo mkdir software</span><br></pre></td></tr></table></figure>

<p>修改module、software文件夹的所有者</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo chown vincent:vincent /opt/module /opt/software</span><br></pre></td></tr></table></figure>

<h2 id="7-安装JDK"><a href="#7-安装JDK" class="headerlink" title="7. 安装JDK"></a>7. 安装JDK</h2><ol>
<li><p>将JDK安装包上传到Linux /opt/software目录下</p>
</li>
<li><p>解压JDK到/opt/module目录下</p>
</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -zxvf jdk-8u212-linux-x64.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>配置JDK环境变量,两种方式:</li>
</ol>
<p>新建/etc/profile.d/my_env.sh文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo vim /etc/profile.d/my_env.sh</span><br></pre></td></tr></table></figure>

<p>添加如下内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">JAVA_HOME</span></span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_212</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br></pre></td></tr></table></figure>

<p>或者</p>
<p>直接将环境变量配置到 /etc/profile 文件中,在/etc/profile文件的末尾追加如下内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">JAVA_HOME=/opt/module/jdk1.8.0_212</span><br><span class="line">PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line">export PATH JAVA_HOME</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">source /etc/profile	</span><br></pre></td></tr></table></figure>

<p>测试JDK是否安装成功</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">java -version</span><br></pre></td></tr></table></figure>

<h2 id="8-安装Hadoop"><a href="#8-安装Hadoop" class="headerlink" title="8. 安装Hadoop"></a>8. 安装Hadoop</h2><p>Hadoop下载地址：<a href="https://archive.apache.org/dist/hadoop/common/hadoop-2.7.2/">https://archive.apache.org/dist/hadoop/common/hadoop-3.1.3/</a></p>
<p>将hadoop安装包上传到/opt/software目录下</p>
<p>解压安装文件到/opt/module下面</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -zxvf hadoop-3.1.3.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>

<p>将Hadoop添加到环境变量</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">HADOOP_HOME</span></span><br><span class="line">HADOOP_HOME=/opt/module/hadoop-3.1.3</span><br><span class="line">PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br><span class="line">export PATH JAVA_HOME HADOOP_HOME</span><br></pre></td></tr></table></figure>

<p>测试是否安装成功</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop version</span><br></pre></td></tr></table></figure>

<h1 id="完全分布式集群搭建"><a href="#完全分布式集群搭建" class="headerlink" title="完全分布式集群搭建"></a>完全分布式集群搭建</h1><h2 id="1-编写集群分发脚本xsync"><a href="#1-编写集群分发脚本xsync" class="headerlink" title="1. 编写集群分发脚本xsync"></a>1. 编写集群分发脚本xsync</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash">1. 判断参数个数</span></span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line">  echo Not Enough Arguement!</span><br><span class="line">  exit;</span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span><span class="bash">2. 遍历集群所有机器</span></span><br><span class="line">for host in linux1 linux2 linux3</span><br><span class="line">do</span><br><span class="line">  echo ====================  $host  ====================</span><br><span class="line"><span class="meta">  #</span><span class="bash">3. 遍历所有目录，挨个发送</span></span><br><span class="line">  for file in $@</span><br><span class="line">  do</span><br><span class="line">    #4 判断文件是否存在</span><br><span class="line">    if [ -e $file ]</span><br><span class="line">    then</span><br><span class="line">    #5. 获取父目录</span><br><span class="line">    # cd -P 的意义是防止文件路径是软连接</span><br><span class="line">      pdir=$(cd -P $(dirname $file); pwd)</span><br><span class="line">      #6. 获取当前文件的名称</span><br><span class="line">      fname=$(basename $file)</span><br><span class="line">      ssh $host &quot;mkdir -p $pdir&quot;</span><br><span class="line">      rsync -av $pdir/$fname $host:$pdir</span><br><span class="line">    else</span><br><span class="line">      echo $file does not exists!</span><br><span class="line">    fi</span><br><span class="line">  done</span><br><span class="line">done</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">chmod +x xsync</span><br></pre></td></tr></table></figure>

<h2 id="2-集群配置"><a href="#2-集群配置" class="headerlink" title="2. 集群配置"></a>2. 集群配置</h2><table>
<thead>
<tr>
<th></th>
<th align="center">linux1</th>
<th align="center">linux2</th>
<th align="center">linux3</th>
</tr>
</thead>
<tbody><tr>
<td>HDFS</td>
<td align="center">NameNode                     DataNode</td>
<td align="center">DataNode</td>
<td align="center">SecondaryNameNode       DataNode</td>
</tr>
<tr>
<td>YARN</td>
<td align="center">NodeManager</td>
<td align="center">ResourceManager  NodeManager</td>
<td align="center">NodeManager</td>
</tr>
</tbody></table>
<h3 id="配置hadoop-env-sh-第54行"><a href="#配置hadoop-env-sh-第54行" class="headerlink" title="配置hadoop-env.sh(第54行)"></a>配置hadoop-env.sh(第54行)</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">:set nu							//设置行号</span><br><span class="line">:set nonu						//取消行号</span><br><span class="line">:noh								//取消高亮</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_212</span><br></pre></td></tr></table></figure>

<h3 id="配置core-site-xml"><a href="#配置core-site-xml" class="headerlink" title="配置core-site.xml"></a>配置core-site.xml</h3><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定NameNode的地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://linux1:9820<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定hadoop数据的存储目录,官方配置文件中的配置项是hadoop.tmp.dir,</span></span><br><span class="line"><span class="comment">     用来指定hadoop数据的存储目录,此次配置用的hadoop.data.dir是自己定义的变量,</span></span><br><span class="line"><span class="comment">     因为在hdfs-site.xml中会使用此配置的值来具体指定namenode 和 datanode存储数据的目录--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.1.3/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.http.staticuser.user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>vincent<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 下面是兼容性配置 --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置该vincent(superUser)允许通过代理访问的主机节点 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.vincent.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置该vincent(superuser)允许代理的用户所属组 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.vincent.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置该vincent(superuser)允许代理的用户--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.vincent.users<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="配置hdfs-site-xml"><a href="#配置hdfs-site-xml" class="headerlink" title="配置hdfs-site.xml"></a>配置hdfs-site.xml</h3><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定NameNode数据的存储目录 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file://$&#123;hadoop.data.dir&#125;/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"> <span class="comment">&lt;!-- 指定Datanode数据的存储目录 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file://$&#123;hadoop.data.dir&#125;/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 备份数 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定SecondaryNameNode数据的存储目录 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file://$&#123;hadoop.data.dir&#125;/namesecondary<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- nn web端访问地址--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>linux1:9870<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">&lt;!-- 2nn web端访问地址--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>linux3:9868<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="配置yarn-site-xml"><a href="#配置yarn-site-xml" class="headerlink" title="配置yarn-site.xml"></a>配置yarn-site.xml</h3><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定mapreduce走shuffle --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="comment">&lt;!-- 指定ResourceManager的地址--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>linux2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 环境变量的继承 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.env-whitelist<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置日志的聚集 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log.server.url<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>http://linux1:19888/jobhistory/logs<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>604800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--node manager 配置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.cpu-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">discription</span>&gt;</span>每个nodemanager可分配的cpu总核数<span class="tag">&lt;/<span class="name">discription</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>4096<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">discription</span>&gt;</span>每个nodemanager可分配的内存总量<span class="tag">&lt;/<span class="name">discription</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- </span></span><br><span class="line"><span class="comment">优化建议：</span></span><br><span class="line"><span class="comment">1. cpu核数=逻辑核数-其他应用数（datanode？work？zk？等）</span></span><br><span class="line"><span class="comment">cat /proc/cpuinfo | grep &quot;processor&quot; | wc -l</span></span><br><span class="line"><span class="comment">可以查看集群的逻辑核数</span></span><br><span class="line"><span class="comment">2. 内存建议是CPU的整数倍，给系统预留好足够用的内存</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--AppMaster 配置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.app.mapreduce.am.resource.cpu-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span>	</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.app.mapreduce.am.resource.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>2048<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">discription</span>&gt;</span>ApplicationMaster的占用的内存大小<span class="tag">&lt;/<span class="name">discription</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- </span></span><br><span class="line"><span class="comment">优化建议</span></span><br><span class="line"><span class="comment">1. cpu和内存比例和 nm的分配比例保持一致</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Container 配置优化 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">discription</span>&gt;</span>单个任务可申请的最多虚拟CPU个数<span class="tag">&lt;/<span class="name">discription</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">discription</span>&gt;</span>单个任务可申请的最小虚拟CPU个数<span class="tag">&lt;/<span class="name">discription</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>512<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">discription</span>&gt;</span>单个任务可申请最小内存<span class="tag">&lt;/<span class="name">discription</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.shceduler.maximum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>4096<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">discription</span>&gt;</span>单个任务可申请最大内存，默认8192MB<span class="tag">&lt;/<span class="name">discription</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">优化建议</span></span><br><span class="line"><span class="comment">1. 在调度器中，很多资源计算部分会转化为这个最小值的N倍进行计算。所以，设定可分配内存等资源的时候，最好是刚好为这个最小值的倍数</span></span><br><span class="line"><span class="comment">2. cpu/内存比例保持一致</span></span><br><span class="line"><span class="comment">3. YARN采用了线程监控的方法判断任务是否超量使用内存，一旦发现超量，则直接将其杀死。由于Cgroups对内存的控制缺乏灵活性（即任务任何时刻不能超过内存上限，如果超过，则直接将其杀死或者报OOM），而Java进程在创建瞬间内存将翻倍，之后骤降到正常值，这种情况下，采用线程监控的方式更加灵活（当发现进程树内存瞬间翻倍超过设定值时，可认为是正常现象，不会将任务杀死），因此YARN未提供Cgroups内存隔离机制来控制容器。</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.fair.assignmultiple<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">discription</span>&gt;</span>是否允许NodeManager一次分配多个容器<span class="tag">&lt;/<span class="name">discription</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.fair.max.assign<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>8<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">discription</span>&gt;</span>如果允许一次分配多个,一次最多可分配多少个,这里按照一个最小分配yarn.scheduler.minimum-allocation-mb0.5gb来计算总共内存4/0.5=8<span class="tag">&lt;/<span class="name">discription</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="配置mapred-site-xml"><a href="#配置mapred-site-xml" class="headerlink" title="配置mapred-site.xml"></a>配置mapred-site.xml</h3><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定MapReduce程序运行在Yarn上 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- mapreduce参数设置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>2048<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">discription</span>&gt;</span>map的内存大小<span class="tag">&lt;/<span class="name">discription</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.java.opts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>-Xmx1024M<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">discription</span>&gt;</span>用户设定的map/reduce阶段申请的container的JVM参数。最大堆设定要比申请的内存少一些，用于JVM的非堆部分使用0.80-0.85建议<span class="tag">&lt;/<span class="name">discription</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>3072<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.java.opts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>-Xmx2048M<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- </span></span><br><span class="line"><span class="comment">优化参考</span></span><br><span class="line"><span class="comment">1. 如果集群主要使用mr进行计算，那么建议map的内存和cpu和容器最小的相等。</span></span><br><span class="line"><span class="comment">2. 一个容器里面最多跑几个map？yarn.scheduler.maximum-allocation-mb/mapreduce.map.memory.mb=2</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">&lt;!-- 历史服务器端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>linux1:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 历史服务器web端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>linux1:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="在集群上分发配置好的hadoop"><a href="#在集群上分发配置好的hadoop" class="headerlink" title="在集群上分发配置好的hadoop"></a>在集群上分发配置好的hadoop</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">xsync /opt/module/hadoop-3.1.3</span><br></pre></td></tr></table></figure>

<h2 id="3-集群单点启动"><a href="#3-集群单点启动" class="headerlink" title="3. 集群单点启动"></a>3. 集群单点启动</h2><p>如果集群是第一次启动，需要格式化NameNode。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure>

<p>在linux1上启动NameNode</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs --daemon start namenode</span><br></pre></td></tr></table></figure>

<p>在linux1、linux2以及linux3上执行如下命令（三台都要执行）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs --daemon start datanode</span><br></pre></td></tr></table></figure>

<h2 id="4-SSH无密登录配置"><a href="#4-SSH无密登录配置" class="headerlink" title="4. SSH无密登录配置"></a>4. SSH无密登录配置</h2><p>生成公钥和私钥</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br></pre></td></tr></table></figure>

<p>然后敲（三个回车），就会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥）</p>
<p>将公钥拷贝到本机</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ssh-copy-id linux1</span><br></pre></td></tr></table></figure>

<p>分发公钥到集群各个节点</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">xsync .ssh</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>known_hosts</th>
<th>记录ssh访问过计算机的公钥(public key)</th>
</tr>
</thead>
<tbody><tr>
<td>id_rsa</td>
<td>生成的私钥</td>
</tr>
<tr>
<td>id_rsa.pub</td>
<td>生成的公钥</td>
</tr>
<tr>
<td>authorized_keys</td>
<td>存放授权过的无密登录服务器公钥</td>
</tr>
</tbody></table>
<h2 id="5-群起集群"><a href="#5-群起集群" class="headerlink" title="5. 群起集群"></a>5. 群起集群</h2><h3 id="配置workers"><a href="#配置workers" class="headerlink" title="配置workers"></a>配置workers</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim /opt/module/hadoop-3.1.3/etc/hadoop/workers</span><br></pre></td></tr></table></figure>

<p>在该文件中增加如下内容：</p>
<figure class="highlight gcode"><table><tr><td class="code"><pre><span class="line">li<span class="symbol">nux1</span></span><br><span class="line">li<span class="symbol">nux2</span></span><br><span class="line">li<span class="symbol">nux3</span></span><br></pre></td></tr></table></figure>

<p>同步所有节点配置文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">xsync /opt/module/hadoop-3.1.3/etc/hadoop/workers</span><br></pre></td></tr></table></figure>

<h3 id="启动集群"><a href="#启动集群" class="headerlink" title="启动集群"></a>启动集群</h3><ol>
<li>如果集群是第一次启动，需要在linux1节点格式化NameNode（注意格式化之前，一定要先停止上次启动的所有namenode和datanode进程，然后再删除data和log数据）</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>启动HDFS</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">start-dfs.sh</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>在配置了ResourceManager的节点(linux2)启动YARN</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">start-yarn.sh</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>集群基本测试</li>
</ol>
<p>上传文件到集群</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop fs -mkdir -p /user/vincent/input</span><br><span class="line">hadoop fs -put $HADOOP_HOME/wcinput/wc.input /user/vincent/input</span><br></pre></td></tr></table></figure>

<h2 id="6-集群启动-停止方式总结"><a href="#6-集群启动-停止方式总结" class="headerlink" title="6. 集群启动/停止方式总结"></a>6. 集群启动/停止方式总结</h2><h3 id="各个服务组件逐一启动-停止"><a href="#各个服务组件逐一启动-停止" class="headerlink" title="各个服务组件逐一启动/停止"></a>各个服务组件逐一启动/停止</h3><p>分别启动/停止HDFS组件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs --daemon start/stop namenode/datanode/secondarynamenode</span><br></pre></td></tr></table></figure>

<p>启动/停止YARN</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yarn --daemon start/stop  resourcemanager/nodemanager</span><br></pre></td></tr></table></figure>



<h3 id="各个模块分开启动-停止（配置ssh是前提）常用"><a href="#各个模块分开启动-停止（配置ssh是前提）常用" class="headerlink" title="各个模块分开启动/停止（配置ssh是前提）常用"></a>各个模块分开启动/停止（配置ssh是前提）常用</h3><p>整体启动/停止HDFS</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">start-dfs.sh/stop-dfs.sh</span><br></pre></td></tr></table></figure>

<p>整体启动/停止YARN</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">start-yarn.sh/stop-yarn.sh</span><br></pre></td></tr></table></figure>

<h2 id="7-配置历史服务器"><a href="#7-配置历史服务器" class="headerlink" title="7. 配置历史服务器"></a>7. 配置历史服务器</h2><p>配置mapred-site.xml</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 历史服务器端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>linux1:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">&lt;!-- 历史服务器web端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>linux1:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>分发配置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">xsync $HADOOP_HOME/etc/hadoop/mapred-site.xml</span><br></pre></td></tr></table></figure>

<p>在linux1上启动历史服务器</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mapred --daemon start historyserver</span><br></pre></td></tr></table></figure>

<p>查看JobHistory</p>
<p><a href="http://linux1:19888/jobhistory">http://linux1:19888/jobhistory</a></p>
<h2 id="8-配置日志的聚集"><a href="#8-配置日志的聚集" class="headerlink" title="8. 配置日志的聚集"></a>8. 配置日志的聚集</h2><p>配置yarn-site.xml</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log.server.url<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>http://linux1:19888/jobhistory/logs<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>604800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>分发配置</p>
<p>重启NodeManager 、ResourceManager和HistoryServer</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">stop-yarn.sh</span><br><span class="line"></span><br><span class="line">mapred --daemon stop historyserver</span><br><span class="line"></span><br><span class="line">start-yarn.sh</span><br><span class="line"></span><br><span class="line">mapred --daemon start historyserver</span><br></pre></td></tr></table></figure>

<p>删除HDFS上已经存在的输出文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs dfs -rm -R /user/vincent/output</span><br></pre></td></tr></table></figure>

<p>执行WordCount程序</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /user/vincent/input /user/vincent/output</span><br></pre></td></tr></table></figure>

<p>查看日志</p>
<p><a href="http://linux1:19888/jobhistory">http://linux1:19888/jobhistory</a></p>
<h2 id="9-集群时间同步"><a href="#9-集群时间同步" class="headerlink" title="9. 集群时间同步"></a>9. 集群时间同步</h2><p>时间同步的方式：找一个机器，作为时间服务器，所有的机器与这台集群时间进行定时的同步，比如，每隔十分钟，同步一次时间。</p>
<h3 id="时间服务器配置（必须root用户）"><a href="#时间服务器配置（必须root用户）" class="headerlink" title="时间服务器配置（必须root用户）"></a>时间服务器配置（必须root用户）</h3><h4 id="1-在所有节点关闭ntp服务和自启动"><a href="#1-在所有节点关闭ntp服务和自启动" class="headerlink" title="1. 在所有节点关闭ntp服务和自启动"></a>1. 在所有节点关闭ntp服务和自启动</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo systemctl stop ntpd</span><br><span class="line">sudo systemctl disable ntpd</span><br></pre></td></tr></table></figure>

<h4 id="2-修改ntp配置文件"><a href="#2-修改ntp配置文件" class="headerlink" title="2. 修改ntp配置文件"></a>2. 修改ntp配置文件</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim /etc/ntp.conf</span><br></pre></td></tr></table></figure>

<p>a）修改1（授权192.168.1.0-192.168.1.255网段上的所有机器可以从这台机器上查询和同步时间）</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">修改</span><br><span class="line">#restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap</span><br><span class="line">为</span><br><span class="line">restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap</span><br></pre></td></tr></table></figure>

<p>b）修改2（集群在局域网中，不使用其他互联网上的时间）</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">修改</span><br><span class="line">server 0.centos.pool.ntp.org iburst</span><br><span class="line">server 1.centos.pool.ntp.org iburst</span><br><span class="line">server 2.centos.pool.ntp.org iburst</span><br><span class="line">server 3.centos.pool.ntp.org iburst</span><br><span class="line">为</span><br><span class="line">#server 0.centos.pool.ntp.org iburst</span><br><span class="line">#server 1.centos.pool.ntp.org iburst</span><br><span class="line">#server 2.centos.pool.ntp.org iburst</span><br><span class="line">#server 3.centos.pool.ntp.org iburst</span><br></pre></td></tr></table></figure>

<p>c）添加3（当该节点丢失网络连接，依然可以采用本地时间作为时间服务器为集群中的其他节点提供时间同步）</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">server 127.127.1.0</span><br><span class="line">fudge 127.127.1.0 stratum 10</span><br></pre></td></tr></table></figure>

<h4 id="3-修改-etc-sysconfig-ntpd-文件"><a href="#3-修改-etc-sysconfig-ntpd-文件" class="headerlink" title="3. 修改/etc/sysconfig/ntpd 文件"></a>3. 修改/etc/sysconfig/ntpd 文件</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim /etc/sysconfig/ntpd</span><br></pre></td></tr></table></figure>

<p>增加内容如下（让硬件时间与系统时间一起同步）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">SYNC_HWCLOCK=yes</span><br></pre></td></tr></table></figure>

<p>Tips:</p>
<p>系统时间: 一般说来就是我们执行 date命令看到的时间，linux系统下所有的时间调用（除了直接访问硬件时间的命令）都是使用的这个时间。 </p>
<p>硬件时间: 主板上BIOS中的时间，由主板电池供电来维持运行，系统开机时要读取这个时间，并根据它来设定系统时间（注意：系统启动时根据硬件时间设定系统时间的过程可能存在时区换算，这要视具体的系统及相关设置而定）</p>
<h4 id="4-重新启动ntpd服务"><a href="#4-重新启动ntpd服务" class="headerlink" title="4. 重新启动ntpd服务"></a>4. 重新启动ntpd服务</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl start ntpd</span><br></pre></td></tr></table></figure>

<h4 id="5-设置ntpd服务开机启动"><a href="#5-设置ntpd服务开机启动" class="headerlink" title="5. 设置ntpd服务开机启动"></a>5. 设置ntpd服务开机启动</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl enable ntpd</span><br></pre></td></tr></table></figure>

<h3 id="其他机器配置（必须root用户）"><a href="#其他机器配置（必须root用户）" class="headerlink" title="其他机器配置（必须root用户）"></a>其他机器配置（必须root用户）</h3><p>在其他机器配置10分钟与时间服务器同步一次</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">crontab -e</span><br></pre></td></tr></table></figure>

<p>编写定时任务如下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">*/10 * * * * /usr/sbin/ntpdate linux1</span><br></pre></td></tr></table></figure>

<p>修改任意机器时间</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">date -s &quot;2017-9-11 11:11:11&quot;</span><br></pre></td></tr></table></figure>

<p>十分钟后查看机器是否与时间服务器同步</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">date</span><br></pre></td></tr></table></figure>

<h2 id="10、HDFS存储多目录"><a href="#10、HDFS存储多目录" class="headerlink" title="10、HDFS存储多目录"></a>10、HDFS存储多目录</h2><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///dfs/data1,file:///hd2/dfs/data2,file:///hd3/dfs/data3,file:///hd4/dfs/data4<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>hd1、hd2、hd3、hd4为一个节点上的多个磁盘。</p>
<h2 id="11、集群数据均衡"><a href="#11、集群数据均衡" class="headerlink" title="11、集群数据均衡"></a>11、集群数据均衡</h2><h3 id="1）节点间数据均衡"><a href="#1）节点间数据均衡" class="headerlink" title="1）节点间数据均衡"></a>1）节点间数据均衡</h3><p>开启数据均衡命令：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">start-balancer.sh -threshold 10</span><br></pre></td></tr></table></figure>

<p>对于参数10，代表的是集群中各个节点的磁盘空间利用率相差不超过10%，可根据实际情况进行调整。</p>
<p>停止数据均衡命令：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">stop-balancer.sh</span><br></pre></td></tr></table></figure>

<h3 id="2）磁盘间数据均衡"><a href="#2）磁盘间数据均衡" class="headerlink" title="2）磁盘间数据均衡"></a>2）磁盘间数据均衡</h3><p>（1）生成均衡计划</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs diskbalancer -plan linux2</span><br></pre></td></tr></table></figure>

<p>在当前路径生成linux2.plan.json</p>
<p>（2）执行均衡计划</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs diskbalancer -execute linux2.plan.json</span><br></pre></td></tr></table></figure>

<p>（3）查看当前均衡任务的执行情况</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs diskbalancer -query linux2</span><br></pre></td></tr></table></figure>

<p>（4）取消均衡任务</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs diskbalancer -cancel linux2.plan.json</span><br></pre></td></tr></table></figure>

<h2 id="12、支持LZO压缩配置"><a href="#12、支持LZO压缩配置" class="headerlink" title="12、支持LZO压缩配置"></a>12、支持LZO压缩配置</h2><p>1）hadoop本身并不支持lzo压缩，故需要使用twitter提供的hadoop-lzo开源组件。hadoop-lzo需依赖hadoop和lzo进行编译。</p>
<p>2）将编译好后的hadoop-lzo-0.4.20.jar 放入hadoop-3.1.3/share/hadoop/common/</p>
<p>3）同步hadoop-lzo-0.4.20.jar到linux2、linux3</p>
<p>4）core-site.xml增加配置支持LZO压缩</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>io.compression.codecs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span></span><br><span class="line">        org.apache.hadoop.io.compress.GzipCodec,</span><br><span class="line">        org.apache.hadoop.io.compress.DefaultCodec,</span><br><span class="line">        org.apache.hadoop.io.compress.BZip2Codec,</span><br><span class="line">        org.apache.hadoop.io.compress.SnappyCodec,</span><br><span class="line">        com.hadoop.compression.lzo.LzoCodec,</span><br><span class="line">        com.hadoop.compression.lzo.LzopCodec</span><br><span class="line">    <span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>io.compression.codec.lzo.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.hadoop.compression.lzo.LzoCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>5）同步core-site.xml到linux2、linux3</p>
<p>6）测试wordcount</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount -Dmapreduce.output.fileoutputformat.compress=true -Dmapreduce.output.fileoutputformat.compress.codec=com.hadoop.compression.lzo.LzopCodec input /output</span><br></pre></td></tr></table></figure>

<h1 id="问题总结"><a href="#问题总结" class="headerlink" title="问题总结"></a>问题总结</h1><p>1）ERROR: Cannot set priority of datanode process</p>
<figure class="highlight gcode"><table><tr><td class="code"><pre><span class="line">将克隆的li<span class="symbol">nux2</span>和li<span class="symbol">nux3</span>上的hadoop卸载，重新安装。</span><br></pre></td></tr></table></figure>

<p>2）linux2，linux3没有没有datanode</p>
<figure class="highlight haskell"><table><tr><td class="code"><pre><span class="line">由于datanode的<span class="type">VERSION</span>不同导致，删除linux2，linux3的<span class="class"><span class="keyword">data</span>目录下文件即可。</span></span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title>HDFS</title>
    <url>/HDFS/</url>
    <content><![CDATA[<h1 id="HDFS概述"><a href="#HDFS概述" class="headerlink" title="HDFS概述"></a>HDFS概述</h1><p><img src="/HDFS/38.png" alt="38"></p>
<h1 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h1><p><img src="/HDFS/39.png" alt="39"></p>
<p><img src="/HDFS/40.png" alt="40"></p>
<h1 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h1><p><img src="/HDFS/41.png" alt="41"></p>
<p><img src="/HDFS/42.png" alt="42"></p>
<h1 id="文件块大小"><a href="#文件块大小" class="headerlink" title="文件块大小"></a>文件块大小</h1><p><img src="/HDFS/43.png" alt="43"></p>
<p><img src="/HDFS/44.png" alt="44"></p>
<h1 id="Shell操作"><a href="#Shell操作" class="headerlink" title="Shell操作"></a>Shell操作</h1><p>bin/hadoop fs 具体命令   </p>
<p>bin/hdfs dfs 具体命令</p>
<p>两个是完全相同的。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">//-help</span><br><span class="line">hadoop fs -help rm</span><br><span class="line"></span><br><span class="line">//-ls</span><br><span class="line">hadoop fs -ls /</span><br><span class="line"></span><br><span class="line">//-mkdir</span><br><span class="line">hadoop fs -mkdir -p /sanguo/shuguo</span><br><span class="line"></span><br><span class="line">//-moveFromLocal	从本地剪切粘贴到HDFS</span><br><span class="line">hadoop fs -moveFromLocal ./kongming.txt  /sanguo/shuguo</span><br><span class="line"></span><br><span class="line">//-appendToFile	追加一个文件到已经存在的文件末尾</span><br><span class="line">hadoop fs -appendToFile liubei.txt /sanguo/shuguo/kongming.txt</span><br><span class="line"></span><br><span class="line">//-cat 显示文件内容</span><br><span class="line">hadoop fs -cat /sanguo/shuguo/kongming.txt</span><br><span class="line"></span><br><span class="line">//-chgrp 、-chmod、-chown Linux文件系统中的用法一样，修改文件所属权限</span><br><span class="line">hadoop fs	-chmod	666	/sanguo/shuguo/kongming.txt</span><br><span class="line">hadoop fs	-chown	vincent:vincent	/sanguo/shuguo/kongming.txt</span><br><span class="line"></span><br><span class="line">//-copyFromLocal	从本地文件系统中拷贝文件到HDFS路径去</span><br><span class="line">hadoop fs -copyFromLocal README.txt /</span><br><span class="line"></span><br><span class="line">//-put 等同于copyFromLocal</span><br><span class="line">hadoop fs -put ./zaiyiqi.txt /user/vincent/test/</span><br><span class="line"></span><br><span class="line">//-copyToLocal 从HDFS拷贝到本地</span><br><span class="line">hadoop fs -copyToLocal /sanguo/shuguo/kongming.txt ./</span><br><span class="line"></span><br><span class="line">//-get 等同于copyToLocal，就是从HDFS下载文件到本地</span><br><span class="line">hadoop fs -get /sanguo/shuguo/kongming.txt ./</span><br><span class="line"></span><br><span class="line">//-cp 从HDFS的一个路径拷贝到HDFS的另一个路径</span><br><span class="line">hadoop fs -cp /sanguo/shuguo/kongming.txt /zhuge.txt</span><br><span class="line"></span><br><span class="line">//-mv 在HDFS目录中移动文件</span><br><span class="line">hadoop fs -mv /zhuge.txt /sanguo/shuguo/</span><br><span class="line"></span><br><span class="line">//-getmerge 合并下载多个文件，比如HDFS的目录 /user/vincent/test下有多个文件:log1, log2,log3,...</span><br><span class="line">hadoop fs -getmerge /user/vincent/test/* ./zaiyiqi.txt</span><br><span class="line"></span><br><span class="line">//-tail 显示一个文件的末尾</span><br><span class="line">hadoop fs -tail /sanguo/shuguo/kongming.txt</span><br><span class="line"></span><br><span class="line">//-rm 删除文件或文件夹</span><br><span class="line">hadoop fs -rm /user/vincent/test/jinlian2.txt</span><br><span class="line"></span><br><span class="line">//-rmdir 删除空目录</span><br><span class="line">hadoop fs -rmdir /test</span><br><span class="line"></span><br><span class="line">//-du 统计文件夹的大小信息</span><br><span class="line">hadoop fs -du -s -h /user/vincent/test</span><br><span class="line"></span><br><span class="line">//-setrep 设置HDFS中文件的副本数量</span><br><span class="line">hadoop fs -setrep 5 /README.txt			//README.txt 存了5份</span><br><span class="line"></span><br><span class="line">//判断文件、路径是否存在 参数有-e -f等</span><br><span class="line">hadoop fs -test /aaa/bbb</span><br></pre></td></tr></table></figure>

<h1 id="HDFS客户端权限"><a href="#HDFS客户端权限" class="headerlink" title="HDFS客户端权限"></a>HDFS客户端权限</h1><p>hadoop默认情况下开启了权限检查，且默认使用dir.who作为http访问的静态用户,因此可通过关闭权限检查或者配置http访问的静态用户为vincent，二选一即可。</p>
<p>在core-site.xml中修改http访问的静态用户为vincent：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.http.staticuser.user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>vincent<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>在hdfs-site.xml中关闭权限检查：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h1 id="HDFS客户端操作"><a href="#HDFS客户端操作" class="headerlink" title="HDFS客户端操作"></a>HDFS客户端操作</h1><h2 id="准备操作"><a href="#准备操作" class="headerlink" title="准备操作"></a>准备操作</h2><p>1）配置Hadoop环境变量</p>
<p>2）创建maven</p>
<p>3）导入依赖</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.12<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.logging.log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j-slf4j-impl<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.12.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>项目的src/main/resources目录下，新建一个文件，命名为“log4j2.xml”，在文件中填入：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">Configuration</span> <span class="attr">status</span>=<span class="string">&quot;error&quot;</span> <span class="attr">strict</span>=<span class="string">&quot;true&quot;</span> <span class="attr">name</span>=<span class="string">&quot;XMLConfig&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">Appenders</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 类型名为Console，名称为必须属性 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">Appender</span> <span class="attr">type</span>=<span class="string">&quot;Console&quot;</span> <span class="attr">name</span>=<span class="string">&quot;STDOUT&quot;</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!-- 布局为PatternLayout的方式，</span></span><br><span class="line"><span class="comment">            输出样式为[INFO] [2018-01-22 17:34:01][org.test.Console]I&#x27;m here --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">Layout</span> <span class="attr">type</span>=<span class="string">&quot;PatternLayout&quot;</span></span></span><br><span class="line"><span class="tag">                    <span class="attr">pattern</span>=<span class="string">&quot;[%p] [%d&#123;yyyy-MM-dd HH:mm:ss&#125;][%c&#123;10&#125;]%m%n&quot;</span> /&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">Appender</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;/<span class="name">Appenders</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">Loggers</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 可加性为false --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">Logger</span> <span class="attr">name</span>=<span class="string">&quot;test&quot;</span> <span class="attr">level</span>=<span class="string">&quot;info&quot;</span> <span class="attr">additivity</span>=<span class="string">&quot;false&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">AppenderRef</span> <span class="attr">ref</span>=<span class="string">&quot;STDOUT&quot;</span> /&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">Logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!-- root loggerConfig设置 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">Root</span> <span class="attr">level</span>=<span class="string">&quot;info&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">AppenderRef</span> <span class="attr">ref</span>=<span class="string">&quot;STDOUT&quot;</span> /&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">Root</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">Loggers</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">Configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>4）创建包名 com.vincent.hdfs</p>
<p>5）创建HdfsClient类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HdfsClient</span></span>&#123;	</span><br><span class="line">   <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testHdfsClient</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">//1. 创建HDFS客户端对象,传入uri， configuration , user</span></span><br><span class="line">        FileSystem fileSystem =</span><br><span class="line">                FileSystem.get(URI.create(<span class="string">&quot;hdfs://linux1:9820&quot;</span>), <span class="keyword">new</span> Configuration(), <span class="string">&quot;vincent&quot;</span>);</span><br><span class="line">        <span class="comment">//2. 操作集群</span></span><br><span class="line">        <span class="comment">// 例如：在集群的/目录下创建 testHDFS目录</span></span><br><span class="line">        fileSystem.mkdirs(<span class="keyword">new</span> Path(<span class="string">&quot;/testHDFS&quot;</span>));</span><br><span class="line">        <span class="comment">//3. 关闭资源</span></span><br><span class="line">        fileSystem.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="API操作"><a href="#API操作" class="headerlink" title="API操作"></a>API操作</h2><h3 id="HDFS文件上传（测试参数优先级）"><a href="#HDFS文件上传（测试参数优先级）" class="headerlink" title="HDFS文件上传（测试参数优先级）"></a>HDFS文件上传（测试参数优先级）</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testCopyFromLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 1 获取文件系统</span></span><br><span class="line">		Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    <span class="comment">// 设置副本数为2个</span></span><br><span class="line">		configuration.set(<span class="string">&quot;dfs.replication&quot;</span>, <span class="string">&quot;2&quot;</span>);</span><br><span class="line">		FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">&quot;hdfs://linux1:9820&quot;</span>), configuration, <span class="string">&quot;vincent&quot;</span>);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 2 上传文件</span></span><br><span class="line">		fs.copyFromLocalFile(<span class="keyword">new</span> Path(<span class="string">&quot;e:/banzhang.txt&quot;</span>), <span class="keyword">new</span> Path(<span class="string">&quot;/banzhang.txt&quot;</span>));</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 3 关闭资源</span></span><br><span class="line">		fs.close();</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在项目的resources中新建hdfs-site.xml文件，并将如下内容拷贝进去，再次测试</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>参数优先级：</p>
<p>（1）客户端代码中设置的值 &gt;（2）ClassPath下的用户自定义配置文件 &gt;（3）然后是服务器的自定义配置(xxx-site.xml) &gt;（4）服务器的默认配置(xxx-default.xml)</p>
<h3 id="HDFS文件下载"><a href="#HDFS文件下载" class="headerlink" title="HDFS文件下载"></a>HDFS文件下载</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testCopyToLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 1 获取文件系统</span></span><br><span class="line">		Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">		FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">&quot;hdfs://linux1:9820&quot;</span>), configuration, <span class="string">&quot;vincent&quot;</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 2 执行下载操作</span></span><br><span class="line">		<span class="comment">// boolean delSrc 指是否将原文件删除</span></span><br><span class="line">		<span class="comment">// Path src 指要下载的文件路径</span></span><br><span class="line">		<span class="comment">// Path dst 指将文件下载到的路径</span></span><br><span class="line">		<span class="comment">// boolean useRawLocalFileSystem 是否开启文件校验</span></span><br><span class="line">		fs.copyToLocalFile(<span class="keyword">false</span>, <span class="keyword">new</span> Path(<span class="string">&quot;/banzhang.txt&quot;</span>), <span class="keyword">new</span> Path(<span class="string">&quot;e:/banhua.txt&quot;</span>), <span class="keyword">true</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 3 关闭资源</span></span><br><span class="line">		fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="HDFS文件夹删除"><a href="#HDFS文件夹删除" class="headerlink" title="HDFS文件夹删除"></a>HDFS文件夹删除</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testDelete</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 1 获取文件系统</span></span><br><span class="line">	Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">	FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">&quot;hdfs://linux1:9820&quot;</span>), configuration, <span class="string">&quot;vincent&quot;</span>);</span><br><span class="line">		</span><br><span class="line">	<span class="comment">// 2 执行删除</span></span><br><span class="line">	fs.delete(<span class="keyword">new</span> Path(<span class="string">&quot;/0213/&quot;</span>), <span class="keyword">true</span>);</span><br><span class="line">		</span><br><span class="line">	<span class="comment">// 3 关闭资源</span></span><br><span class="line">	fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="HDFS文件名更改-移动"><a href="#HDFS文件名更改-移动" class="headerlink" title="HDFS文件名更改/移动"></a>HDFS文件名更改/移动</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testRename</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 1 获取文件系统</span></span><br><span class="line">	Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">	FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">&quot;hdfs://linux1:9820&quot;</span>), configuration, <span class="string">&quot;vincent&quot;</span>); </span><br><span class="line">		</span><br><span class="line">	<span class="comment">// 2 修改文件名称</span></span><br><span class="line">	fs.rename(<span class="keyword">new</span> Path(<span class="string">&quot;/banzhang.txt&quot;</span>), <span class="keyword">new</span> Path(<span class="string">&quot;/banhua.txt&quot;</span>));</span><br><span class="line">		</span><br><span class="line">	<span class="comment">// 3 关闭资源</span></span><br><span class="line">	fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="HDFS文件详情查看"><a href="#HDFS文件详情查看" class="headerlink" title="HDFS文件详情查看"></a>HDFS文件详情查看</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testListFiles</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 1获取文件系统</span></span><br><span class="line">	Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">	FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">&quot;hdfs://linux1:9820&quot;</span>), configuration, <span class="string">&quot;vincent&quot;</span>); </span><br><span class="line">		</span><br><span class="line">	<span class="comment">// 2 获取文件详情</span></span><br><span class="line">	RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(<span class="keyword">new</span> Path(<span class="string">&quot;/&quot;</span>), <span class="keyword">true</span>);</span><br><span class="line">		</span><br><span class="line">	<span class="keyword">while</span>(listFiles.hasNext())&#123;</span><br><span class="line">		LocatedFileStatus status = listFiles.next();</span><br><span class="line">			</span><br><span class="line">		<span class="comment">// 输出详情</span></span><br><span class="line">		<span class="comment">// 文件名称</span></span><br><span class="line">		System.out.println(status.getPath().getName());</span><br><span class="line">		<span class="comment">// 长度</span></span><br><span class="line">		System.out.println(status.getLen());</span><br><span class="line">		<span class="comment">// 权限</span></span><br><span class="line">		System.out.println(status.getPermission());</span><br><span class="line">		<span class="comment">// 分组</span></span><br><span class="line">		System.out.println(status.getGroup());</span><br><span class="line">			</span><br><span class="line">		<span class="comment">// 获取存储的块信息</span></span><br><span class="line">		BlockLocation[] blockLocations = status.getBlockLocations();</span><br><span class="line">			</span><br><span class="line">		<span class="keyword">for</span> (BlockLocation blockLocation : blockLocations) &#123;</span><br><span class="line">				</span><br><span class="line">			<span class="comment">// 获取块存储的主机节点</span></span><br><span class="line">			String[] hosts = blockLocation.getHosts();</span><br><span class="line">				</span><br><span class="line">			<span class="keyword">for</span> (String host : hosts) &#123;</span><br><span class="line">				System.out.println(host);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">			</span><br><span class="line">		System.out.println(<span class="string">&quot;-----------漂亮的分割线----------&quot;</span>);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3 关闭资源</span></span><br><span class="line">fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="HDFS文件和文件夹判断"><a href="#HDFS文件和文件夹判断" class="headerlink" title="HDFS文件和文件夹判断"></a>HDFS文件和文件夹判断</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testListStatus</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line">        </span><br><span class="line">    <span class="comment">// 1 获取文件配置信息</span></span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">&quot;hdfs://linux1:9820&quot;</span>), configuration, <span class="string">&quot;vincent&quot;</span>);</span><br><span class="line">        </span><br><span class="line">    <span class="comment">// 2 判断是文件还是文件夹</span></span><br><span class="line">    FileStatus[] listStatus = fs.listStatus(<span class="keyword">new</span> Path(<span class="string">&quot;/&quot;</span>));</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">for</span> (FileStatus fileStatus : listStatus) &#123;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 如果是文件</span></span><br><span class="line">        <span class="keyword">if</span> (fileStatus.isFile()) &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;f:&quot;</span>+fileStatus.getPath().getName());</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;d:&quot;</span>+fileStatus.getPath().getName());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">        </span><br><span class="line">    <span class="comment">// 3 关闭资源</span></span><br><span class="line">    fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="HDFS的数据流"><a href="#HDFS的数据流" class="headerlink" title="HDFS的数据流"></a>HDFS的数据流</h1><h2 id="写入数据"><a href="#写入数据" class="headerlink" title="写入数据"></a>写入数据</h2><p><img src="/HDFS/45.png" alt="45"></p>
<p>1）客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。</p>
<p>2）NameNode返回是否可以上传。</p>
<p>3）客户端请求第一个 Block上传到哪几个DataNode服务器上。</p>
<p>4）NameNode返回3个DataNode节点，分别为dn1、dn2、dn3。</p>
<p>5）客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。</p>
<p>6）dn1、dn2、dn3逐级应答客户端。</p>
<p>7）客户端开始往dn1上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。</p>
<p>8）当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器。（重复执行3-7步）。</p>
<h3 id="网络拓扑-节点距离计算"><a href="#网络拓扑-节点距离计算" class="headerlink" title="网络拓扑-节点距离计算"></a>网络拓扑-节点距离计算</h3><p><strong>在HDFS写数据的过程中，NameNode会选择距离待上传数据最近距离的DataNode接收数据</strong>。那么这个最近距离怎么计算呢？</p>
<p>节点距离：两个节点到达最近的共同祖先的距离总和。</p>
<p><img src="/HDFS/46.png" alt="46"></p>
<h3 id="副本节点选择"><a href="#副本节点选择" class="headerlink" title="副本节点选择"></a>副本节点选择</h3><p><img src="/HDFS/47.png" alt="47"></p>
<h3 id="机架感知"><a href="#机架感知" class="headerlink" title="机架感知"></a>机架感知</h3><p>分布式的集群通常包含非常多的机器，由于受到机架槽位和交换机网口的限制，通常大型的分布式集群都会跨好几个机架，由多个机架上的机器共同组成一个分布式集群。机架内的机器之间的网络速度通常都会高于跨机架机器之间的网络速度，并且机架之间机器的网络通信通常受到上层交换机间网络带宽的限制。</p>
<p><strong>Hadoop在设计时考虑到数据的安全与高效，数据文件默认在HDFS上存放三份，存储策略为:</strong></p>
<p>第一个block副本放在<strong>客户端所在的数据节点里</strong>（如果客户端不在集群范围内，则从整个集群中随机选择一个合适的数据节点来存放）。</p>
<p>第二个副本放置在<strong>与第一个副本所在节点相同机架内的其它数据节点上</strong></p>
<p>第三个副本放置在<strong>不同机架的节点上</strong></p>
<p>这样如果本地数据损坏，节点可以从同一机架内的相邻节点拿到数据，速度肯定比从跨机架节点上拿数据要快；<br>同时，如果整个机架的网络出现异常，也能保证在其它机架的节点上找到数据。<br>为了降低整体的带宽消耗和读取延时，HDFS会尽量让读取程序读取离它最近的副本。<br>如果在读取程序的同一个机架上有一个副本，那么就读取该副本。<br>如果一个HDFS集群跨越多个数据中心，那么客户端也将首先读本地数据中心的副本。<br>那么Hadoop是如何确定任意两个节点是位于同一机架，还是跨机架的呢？答案就是机架感知。</p>
<p><strong>默认情况下，hadoop的机架感知是没有被启用的</strong>。所有的机器hadoop都默认在同一个默认的机架下，名为 “/default-rack”，这种情况下，任何一台datanode机器，不管物理上是否属于同一个机架，都会被认为是在同一个机架下，此时，就很容易出现增添机架间网络负载的情况。因为此时hadoop集群的HDFS在选机器的时候，是随机选择的，也就是说，<br>很有可能在写数据时，hadoop将第一块数据block1写到了rack1上，然后随机的选择下将block2写入到了rack2下，<br>此时两个rack之间产生了数据传输的流量，再接下来，在随机的情况下，又将block3重新又写回了rack1，此时，两个rack之间又产生了一次数据流量。</p>
<p>在job处理的数据量非常的大，或者往hadoop推送的数据量非常大的时候，这种情况会造成rack之间的网络流量成倍的上升，成为性能的瓶颈，<br>进而影响作业的性能以至于整个集群的服务。</p>
<p><strong>配置</strong></p>
<p>默认情况下，namenode启动时候日志是这样的：</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">INFO org<span class="selector-class">.apache</span><span class="selector-class">.hadoop</span><span class="selector-class">.net</span><span class="selector-class">.NetworkTopology</span>: Adding <span class="selector-tag">a</span> new node: /default-rack/ <span class="number">172.16</span>.<span class="number">145.35</span>:<span class="number">50010</span></span><br></pre></td></tr></table></figure>

<p>每个IP 对应的机架ID都是 /default-rack ，说明hadoop的机架感知没有被启用。<br>要将hadoop机架感知的功能启用，配置非常简单，在 NameNode所在节点的/etc/hadoop/conf下的core-site.xml配置文件中配置一个选项:</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>topology.script.file.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>/etc/hadoop/conf/RackAware.py<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><strong>这个配置选项的value指定为一个可执行程序，通常为一个脚本，该脚本接受一个参数，输出一个值。</strong><br>接受的参数通常为某台datanode机器的ip地址，而输出的值通常为该ip地址对应的datanode所在的rack，例如”/rack1”。<br>Namenode启动时，会判断该配置选项是否为空，如果非空，则表示已经启用机架感知的配置，此时namenode会根据配置寻找该脚本，<br>并在接收到每一个datanode的heartbeat时，将该datanode的ip地址作为参数传给该脚本运行，并将得到的输出作为该datanode所属的机架ID，保存到内存的一个map中。</p>
<p>脚本：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/usr/bin/python</span></span><br><span class="line"><span class="meta">#</span><span class="bash">-*-coding:UTF-8 -*-</span></span><br><span class="line">import sys</span><br><span class="line"> </span><br><span class="line">rack = &#123;&quot;NN01&quot;:&quot;rack2&quot;,</span><br><span class="line">        &quot;NN02&quot;:&quot;rack3&quot;,</span><br><span class="line">        &quot;DN01&quot;:&quot;rack4&quot;,</span><br><span class="line">        &quot;DN02&quot;:&quot;rack4&quot;,</span><br><span class="line">        &quot;DN03&quot;:&quot;rack1&quot;,</span><br><span class="line">        &quot;DN04&quot;:&quot;rack3&quot;,</span><br><span class="line">        &quot;DN05&quot;:&quot;rack1&quot;,</span><br><span class="line">        &quot;DN06&quot;:&quot;rack4&quot;,</span><br><span class="line">        &quot;DN07&quot;:&quot;rack1&quot;,</span><br><span class="line">        &quot;DN08&quot;:&quot;rack2&quot;,</span><br><span class="line">        &quot;DN09&quot;:&quot;rack1&quot;,</span><br><span class="line">        &quot;DN10&quot;:&quot;rack2&quot;,</span><br><span class="line">        &quot;172.16.145.32&quot;:&quot;rack2&quot;,</span><br><span class="line">        &quot;172.16.145.33&quot;:&quot;rack3&quot;,</span><br><span class="line">        &quot;172.16.145.34&quot;:&quot;rack4&quot;,</span><br><span class="line">        &quot;172.16.145.35&quot;:&quot;rack4&quot;,</span><br><span class="line">        &quot;172.16.145.36&quot;:&quot;rack1&quot;,</span><br><span class="line">        &quot;172.16.145.37&quot;:&quot;rack3&quot;,</span><br><span class="line">        &quot;172.16.145.38&quot;:&quot;rack1&quot;,</span><br><span class="line">        &quot;172.16.145.39&quot;:&quot;rack4&quot;,</span><br><span class="line">        &quot;172.16.145.40&quot;:&quot;rack1&quot;,</span><br><span class="line">        &quot;172.16.145.41&quot;:&quot;rack2&quot;,</span><br><span class="line">        &quot;172.16.145.42&quot;:&quot;rack1&quot;,</span><br><span class="line">        &quot;172.16.145.43&quot;:&quot;rack2&quot;,</span><br><span class="line">        &#125;</span><br><span class="line"> </span><br><span class="line">if __name__==&quot;__main__&quot;:</span><br><span class="line">    print &quot;/&quot; + rack.get(sys.argv[1],&quot;rack0&quot;)</span><br></pre></td></tr></table></figure>

<p>这样配置后，namenode启动时候日志是这样的：</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">INFO org<span class="selector-class">.apache</span><span class="selector-class">.hadoop</span><span class="selector-class">.net</span><span class="selector-class">.NetworkTopology</span>: Adding <span class="selector-tag">a</span> new node: /rack4/ <span class="number">172.16</span>.<span class="number">145.35</span>:<span class="number">50010</span></span><br></pre></td></tr></table></figure>

<p>说明hadoop的机架感知已经被启用了。<br>查看HADOOP机架信息命令: <strong>hdfs  dfsadmin  -printTopology</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@NN01 hadoop-hdfs]$ hdfs dfsadmin -printTopology</span><br><span class="line">Rack: /rack1</span><br><span class="line">   172.16.145.36:50010 (DN03)</span><br><span class="line">   172.16.145.38:50010 (DN05)</span><br><span class="line">   172.16.145.40:50010 (DN07)</span><br><span class="line">   172.16.145.42:50010 (DN09)</span><br><span class="line">   172.16.145.44:50010 (DN11)</span><br><span class="line">   172.16.145.54:50010 (DN17)</span><br><span class="line">   172.16.145.56:50010 (DN19)</span><br><span class="line">   172.16.145.58:50010 (DN21)</span><br><span class="line"> </span><br><span class="line">Rack: /rack2</span><br><span class="line">   172.16.145.41:50010 (DN08)</span><br><span class="line">   172.16.145.43:50010 (DN10)</span><br><span class="line">   172.16.145.45:50010 (DN12)</span><br><span class="line">   172.16.145.60:50010 (DN23)</span><br><span class="line">   172.16.145.62:50010 (DN25)</span><br><span class="line"> </span><br><span class="line">Rack: /rack3</span><br><span class="line">   172.16.145.37:50010 (DN04)</span><br><span class="line">   172.16.145.51:50010 (DN14)</span><br><span class="line">   172.16.145.53:50010 (DN16)</span><br><span class="line">   172.16.145.55:50010 (DN18)</span><br><span class="line">   172.16.145.57:50010 (DN20)</span><br><span class="line"> </span><br><span class="line">Rack: /rack4</span><br><span class="line">   172.16.145.34:50010 (DN01)</span><br><span class="line">   172.16.145.35:50010 (DN02)</span><br><span class="line">   172.16.145.39:50010 (DN06)</span><br><span class="line">   172.16.145.50:50010 (DN13)</span><br><span class="line">   172.16.145.52:50010 (DN15)</span><br><span class="line">   172.16.145.59:50010 (DN22)</span><br><span class="line">   172.16.145.61:50010 (DN24)</span><br></pre></td></tr></table></figure>

<p><strong>hdfs 三个副本的这种存放策略减少了机架间的数据传输，提高了写操作的效率。机架的错误远远比节点的错误少，所以这种策略不会影响到数据的可靠性和可用性。与此同时，因为数据块只存放在两个不同的机架上，所以此策略减少了读取数据时需要的网络传输总带宽。在这种策略下，副本并不是均匀的分布在不同的机架上，这种策略在不损害数据可靠性和读取性能的情况下改进了写的性能。</strong></p>
<h2 id="读出数据"><a href="#读出数据" class="headerlink" title="读出数据"></a>读出数据</h2><p><img src="/HDFS/48.png" alt="48"></p>
<p>1）客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。</p>
<p>2）挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。</p>
<p>3）DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）。</p>
<p>4）客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。</p>
<h1 id="NameNode和SecondaryNameNode"><a href="#NameNode和SecondaryNameNode" class="headerlink" title="NameNode和SecondaryNameNode"></a>NameNode和SecondaryNameNode</h1><h2 id="NN和2NN工作机制"><a href="#NN和2NN工作机制" class="headerlink" title="NN和2NN工作机制"></a>NN和2NN工作机制</h2><p>思考：<strong>NameNode中的元数据是存储在哪里的</strong>？</p>
<p>首先，我们做个假设，如果存储在NameNode节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。因此产生在磁盘中备份元数据的Fsimage。</p>
<p>这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新Fsimage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦NameNode节点断电，就会产生数据丢失。因此，引入<strong>Edits文件(只进行追加操作，效率很高)<strong>。</strong>每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中</strong>。这样，一旦NameNode节点断电，可以通过Fsimage和Edits的合并，合成元数据。</p>
<p>但是，如果长时间添加数据到Edits中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要<strong>定期进行Fsimage和Edits的合并</strong>，如果这个操作由NameNode节点完成，又会效率过低。因此，引入一个新的节点<strong>SecondaryNamenode，专门用于FsImage和Edits的合并</strong>。</p>
<p><img src="/HDFS/49.png" alt="49"></p>
<ol>
<li>第一阶段：NameNode启动</li>
</ol>
<p>（1）第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。</p>
<p>（2）客户端对元数据进行增删改的请求。</p>
<p>（3）NameNode记录操作日志，更新滚动日志。</p>
<p>（4）NameNode在内存中对元数据进行增删改。</p>
<ol start="2">
<li>第二阶段：Secondary NameNode工作</li>
</ol>
<p>（1）Secondary NameNode询问NameNode是否需要CheckPoint。直接带回NameNode是否检查结果。</p>
<p>（2）Secondary NameNode请求执行CheckPoint。</p>
<p>（3）NameNode滚动正在写的Edits日志。</p>
<p>（4）将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。</p>
<p>（5）Secondary NameNode加载编辑日志和镜像文件到内存，并合并。</p>
<p>（6）生成新的镜像文件fsimage.chkpoint。</p>
<p>（7）拷贝fsimage.chkpoint到NameNode。</p>
<p>（8）NameNode将fsimage.chkpoint重新命名成fsimage。</p>
<p>NN和2NN工作机制详解：</p>
<p><strong>Fsimage：NameNode内存中元数据序列化后形成的文件。</strong><br><strong>Edits：记录客户端更新元数据信息的每一步操作（可通过Edits运算出元数据）。</strong></p>
<p>NameNode启动时，先滚动Edits并生成一个空的edits.inprogress，然后加载Edits和Fsimage到内存中，此时NameNode内存就持有最新的元数据信息。Client开始对NameNode发送元数据的增删改的请求，这些请求的操作首先会被记录到edits.inprogress中（<strong>查询元数据的操作不会被记录在Edits中，因为查询操作不会更改元数据信息</strong>），如果此时NameNode挂掉，重启后会从Edits中读取元数据的信息。然后，NameNode会在内存中执行元数据的增删改的操作。<br>由于Edits中记录的操作会越来越多，Edits文件会越来越大，导致NameNode在启动加载Edits时会很慢，所以需要对Edits和Fsimage进行合并（所谓合并，就是将Edits和Fsimage加载到内存中，照着Edits中的操作一步步执行，最终形成新的Fsimage）。<strong>SecondaryNameNode的作用就是帮助NameNode进行Edits和Fsimage的合并工作。</strong><br>SecondaryNameNode首先会询问NameNode是否需要CheckPoint（触发CheckPoint需要满足两个条件中的任意一个，定时时间到和Edits中数据写满了）。直接带回NameNode是否检查结果。SecondaryNameNode执行CheckPoint操作，首先会让NameNode滚动Edits并生成一个空的edits.inprogress，滚动Edits的目的是给Edits打个标记，以后所有新的操作都写入edits.inprogress，其他未合并的Edits和Fsimage会拷贝到SecondaryNameNode的本地，然后将拷贝的Edits和Fsimage加载到内存中进行合并，生成fsimage.chkpoint，然后将fsimage.chkpoint拷贝给NameNode，重命名为Fsimage后替换掉原来的Fsimage。NameNode在启动时就只需要加载之前未合并的Edits和Fsimage即可，因为合并过的Edits中的元数据信息已经被记录在Fsimage中。</p>
<h2 id="Fsimage和Edits解析"><a href="#Fsimage和Edits解析" class="headerlink" title="Fsimage和Edits解析"></a>Fsimage和Edits解析</h2><p><img src="/HDFS/50.png" alt="50"></p>
<h3 id="oiv查看Fsimage文件"><a href="#oiv查看Fsimage文件" class="headerlink" title="oiv查看Fsimage文件"></a>oiv查看Fsimage文件</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs oiv -p 文件类型 -i 镜像文件 -o 转换后文件输出路径</span><br><span class="line"></span><br><span class="line">[vincent@linux1 current]$ pwd</span><br><span class="line">/opt/module/hadoop-3.1.3/data/tmp/dfs/name/current</span><br><span class="line"></span><br><span class="line">[vincent@linux1 current]$ hdfs oiv -p XML -i fsimage_0000000000000000025 -o /opt/module/hadoop-3.1.3/fsimage.xml</span><br><span class="line"></span><br><span class="line">[vincent@linux1 current]$ cat /opt/module/hadoop-3.1.3/fsimage.xml</span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">inode</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">id</span>&gt;</span>16386<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">type</span>&gt;</span>DIRECTORY<span class="tag">&lt;/<span class="name">type</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">mtime</span>&gt;</span>1512722284477<span class="tag">&lt;/<span class="name">mtime</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">permission</span>&gt;</span>vincent:supergroup:rwxr-xr-x<span class="tag">&lt;/<span class="name">permission</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">nsquota</span>&gt;</span>-1<span class="tag">&lt;/<span class="name">nsquota</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">dsquota</span>&gt;</span>-1<span class="tag">&lt;/<span class="name">dsquota</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">inode</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">inode</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">id</span>&gt;</span>16387<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">type</span>&gt;</span>DIRECTORY<span class="tag">&lt;/<span class="name">type</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>vincent<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">mtime</span>&gt;</span>1512790549080<span class="tag">&lt;/<span class="name">mtime</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">permission</span>&gt;</span>vincent:supergroup:rwxr-xr-x<span class="tag">&lt;/<span class="name">permission</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">nsquota</span>&gt;</span>-1<span class="tag">&lt;/<span class="name">nsquota</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">dsquota</span>&gt;</span>-1<span class="tag">&lt;/<span class="name">dsquota</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">inode</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">inode</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">id</span>&gt;</span>16389<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">type</span>&gt;</span>FILE<span class="tag">&lt;/<span class="name">type</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>wc.input<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">replication</span>&gt;</span>3<span class="tag">&lt;/<span class="name">replication</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">mtime</span>&gt;</span>1512722322219<span class="tag">&lt;/<span class="name">mtime</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">atime</span>&gt;</span>1512722321610<span class="tag">&lt;/<span class="name">atime</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">perferredBlockSize</span>&gt;</span>134217728<span class="tag">&lt;/<span class="name">perferredBlockSize</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">permission</span>&gt;</span>vincent:supergroup:rw-r--r--<span class="tag">&lt;/<span class="name">permission</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">blocks</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">block</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">id</span>&gt;</span>1073741825<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">genstamp</span>&gt;</span>1001<span class="tag">&lt;/<span class="name">genstamp</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">numBytes</span>&gt;</span>59<span class="tag">&lt;/<span class="name">numBytes</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">block</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">blocks</span>&gt;</span></span><br><span class="line">&lt;/inode &gt;</span><br></pre></td></tr></table></figure>

<p>Fsimage中没有记录块所对应DataNode，为什么？</p>
<p>在集群启动后，要求DataNode上报数据块信息，并间隔一段时间后再次上报。</p>
<h3 id="oev查看Edits文件"><a href="#oev查看Edits文件" class="headerlink" title="oev查看Edits文件"></a>oev查看Edits文件</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs oev -p 文件类型 -i 编辑日志 -o 转换后文件输出路径</span><br><span class="line"></span><br><span class="line">[vincent@hadoop102 current]$ hdfs oev -p XML -i edits_0000000000000000012-0000000000000000013 -o /opt/module/hadoop-3.1.3/edits.xml</span><br><span class="line"></span><br><span class="line">[vincent@linux1 current]$ cat /opt/module/hadoop-3.1.3/edits.xml</span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">EDITS</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">EDITS_VERSION</span>&gt;</span>-63<span class="tag">&lt;/<span class="name">EDITS_VERSION</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_START_LOG_SEGMENT<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">TXID</span>&gt;</span>129<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_ADD<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">TXID</span>&gt;</span>130<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">LENGTH</span>&gt;</span>0<span class="tag">&lt;/<span class="name">LENGTH</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">INODEID</span>&gt;</span>16407<span class="tag">&lt;/<span class="name">INODEID</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">PATH</span>&gt;</span>/hello7.txt<span class="tag">&lt;/<span class="name">PATH</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">REPLICATION</span>&gt;</span>2<span class="tag">&lt;/<span class="name">REPLICATION</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">MTIME</span>&gt;</span>1512943607866<span class="tag">&lt;/<span class="name">MTIME</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">ATIME</span>&gt;</span>1512943607866<span class="tag">&lt;/<span class="name">ATIME</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">BLOCKSIZE</span>&gt;</span>134217728<span class="tag">&lt;/<span class="name">BLOCKSIZE</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">CLIENT_NAME</span>&gt;</span>DFSClient_NONMAPREDUCE_-1544295051_1<span class="tag">&lt;/<span class="name">CLIENT_NAME</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">CLIENT_MACHINE</span>&gt;</span>192.168.1.5<span class="tag">&lt;/<span class="name">CLIENT_MACHINE</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">OVERWRITE</span>&gt;</span>true<span class="tag">&lt;/<span class="name">OVERWRITE</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">PERMISSION_STATUS</span>&gt;</span></span><br><span class="line">				<span class="tag">&lt;<span class="name">USERNAME</span>&gt;</span>vincent<span class="tag">&lt;/<span class="name">USERNAME</span>&gt;</span></span><br><span class="line">				<span class="tag">&lt;<span class="name">GROUPNAME</span>&gt;</span>supergroup<span class="tag">&lt;/<span class="name">GROUPNAME</span>&gt;</span></span><br><span class="line">				<span class="tag">&lt;<span class="name">MODE</span>&gt;</span>420<span class="tag">&lt;/<span class="name">MODE</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;/<span class="name">PERMISSION_STATUS</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">RPC_CLIENTID</span>&gt;</span>908eafd4-9aec-4288-96f1-e8011d181561<span class="tag">&lt;/<span class="name">RPC_CLIENTID</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">RPC_CALLID</span>&gt;</span>0<span class="tag">&lt;/<span class="name">RPC_CALLID</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_ALLOCATE_BLOCK_ID<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">TXID</span>&gt;</span>131<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">BLOCK_ID</span>&gt;</span>1073741839<span class="tag">&lt;/<span class="name">BLOCK_ID</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_SET_GENSTAMP_V2<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">TXID</span>&gt;</span>132<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">GENSTAMPV2</span>&gt;</span>1016<span class="tag">&lt;/<span class="name">GENSTAMPV2</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_ADD_BLOCK<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">TXID</span>&gt;</span>133<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">PATH</span>&gt;</span>/hello7.txt<span class="tag">&lt;/<span class="name">PATH</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">BLOCK</span>&gt;</span></span><br><span class="line">				<span class="tag">&lt;<span class="name">BLOCK_ID</span>&gt;</span>1073741839<span class="tag">&lt;/<span class="name">BLOCK_ID</span>&gt;</span></span><br><span class="line">				<span class="tag">&lt;<span class="name">NUM_BYTES</span>&gt;</span>0<span class="tag">&lt;/<span class="name">NUM_BYTES</span>&gt;</span></span><br><span class="line">				<span class="tag">&lt;<span class="name">GENSTAMP</span>&gt;</span>1016<span class="tag">&lt;/<span class="name">GENSTAMP</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;/<span class="name">BLOCK</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">RPC_CLIENTID</span>&gt;</span><span class="tag">&lt;/<span class="name">RPC_CLIENTID</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">RPC_CALLID</span>&gt;</span>-2<span class="tag">&lt;/<span class="name">RPC_CALLID</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_CLOSE<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">TXID</span>&gt;</span>134<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">LENGTH</span>&gt;</span>0<span class="tag">&lt;/<span class="name">LENGTH</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">INODEID</span>&gt;</span>0<span class="tag">&lt;/<span class="name">INODEID</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">PATH</span>&gt;</span>/hello7.txt<span class="tag">&lt;/<span class="name">PATH</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">REPLICATION</span>&gt;</span>2<span class="tag">&lt;/<span class="name">REPLICATION</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">MTIME</span>&gt;</span>1512943608761<span class="tag">&lt;/<span class="name">MTIME</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">ATIME</span>&gt;</span>1512943607866<span class="tag">&lt;/<span class="name">ATIME</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">BLOCKSIZE</span>&gt;</span>134217728<span class="tag">&lt;/<span class="name">BLOCKSIZE</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">CLIENT_NAME</span>&gt;</span><span class="tag">&lt;/<span class="name">CLIENT_NAME</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">CLIENT_MACHINE</span>&gt;</span><span class="tag">&lt;/<span class="name">CLIENT_MACHINE</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">OVERWRITE</span>&gt;</span>false<span class="tag">&lt;/<span class="name">OVERWRITE</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">BLOCK</span>&gt;</span></span><br><span class="line">				<span class="tag">&lt;<span class="name">BLOCK_ID</span>&gt;</span>1073741839<span class="tag">&lt;/<span class="name">BLOCK_ID</span>&gt;</span></span><br><span class="line">				<span class="tag">&lt;<span class="name">NUM_BYTES</span>&gt;</span>25<span class="tag">&lt;/<span class="name">NUM_BYTES</span>&gt;</span></span><br><span class="line">				<span class="tag">&lt;<span class="name">GENSTAMP</span>&gt;</span>1016<span class="tag">&lt;/<span class="name">GENSTAMP</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;/<span class="name">BLOCK</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">PERMISSION_STATUS</span>&gt;</span></span><br><span class="line">				<span class="tag">&lt;<span class="name">USERNAME</span>&gt;</span>vincent<span class="tag">&lt;/<span class="name">USERNAME</span>&gt;</span></span><br><span class="line">				<span class="tag">&lt;<span class="name">GROUPNAME</span>&gt;</span>supergroup<span class="tag">&lt;/<span class="name">GROUPNAME</span>&gt;</span></span><br><span class="line">				<span class="tag">&lt;<span class="name">MODE</span>&gt;</span>420<span class="tag">&lt;/<span class="name">MODE</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;/<span class="name">PERMISSION_STATUS</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">&lt;/EDITS &gt;</span><br></pre></td></tr></table></figure>

<h2 id="CheckPoint时间设置"><a href="#CheckPoint时间设置" class="headerlink" title="CheckPoint时间设置"></a>CheckPoint时间设置</h2><p>（1）通常情况下，SecondaryNameNode每隔一小时执行一次。</p>
<p>hdfs-default.xml</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>3600<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>（2）一分钟检查一次操作次数，3当操作次数达到1百万时，SecondaryNameNode执行一次。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.txns<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>操作动作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.check.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span> 1分钟检查一次操作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="NameNode故障处理"><a href="#NameNode故障处理" class="headerlink" title="NameNode故障处理"></a>NameNode故障处理</h2><p><strong>方法一：将SecondaryNameNode中数据拷贝到NameNode存储数据的目录</strong></p>
<ol>
<li><p>kill -9 NameNode进程</p>
</li>
<li><p>删除NameNode存储的数据（/opt/module/hadoop-3.1.3/data/name）</p>
</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">rm -rf /opt/module/hadoop-3.1.3/data/name/*</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>拷贝SecondaryNameNode中数据到原NameNode存储数据目录</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">scp -r vincent@linux3:/opt/module/hadoop-3.1.3/data/namesecondary/* /opt/module/hadoop-3.1.3/data/name/</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>重新启动NameNode</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs --daemon start namenode</span><br></pre></td></tr></table></figure>

<p><strong>方法二：使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中</strong></p>
<ol>
<li>修改hdfs-site.xml中的</li>
</ol>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>120<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.1.3/data/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ol start="2">
<li><p>kill -9 NameNode进程</p>
</li>
<li><p>删除NameNode存储的数据（/opt/module/hadoop-3.1.3/data/name）</p>
</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hadoop-3.1.3]$ rm -rf /opt/module/hadoop-3.1.3/data/name/*</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>如果SecondaryNameNode不和NameNode在一个主机节点上，需要将SecondaryNameNode存储数据的目录拷贝到NameNode存储数据的平级目录，并删除in_use.lock文件</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 data]$ scp -r vincent@linux3:/opt/module/hadoop-3.1.3/data/namesecondary ./</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在linux1中删除拷贝过来的in_use.lock</span></span><br><span class="line">[vincent@linux1 namesecondary]$ rm -rf in_use.lock</span><br><span class="line"></span><br><span class="line">[vincent@linux1 data]$ pwd</span><br><span class="line">/opt/module/hadoop-3.1.3/data</span><br><span class="line"></span><br><span class="line">[vincent@linux1 data]$ ls</span><br><span class="line">data  name  namesecondary</span><br></pre></td></tr></table></figure>

<ol start="5">
<li>   导入检查点数据（等待一会ctrl+c结束掉）</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hadoop-3.1.3]$ bin/hdfs namenode -importCheckpoint</span><br></pre></td></tr></table></figure>

<ol start="6">
<li>   启动NameNode</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hadoop-3.1.3]$ hdfs --daemon start namenode</span><br></pre></td></tr></table></figure>

<h2 id="集群安全模式"><a href="#集群安全模式" class="headerlink" title="集群安全模式"></a>集群安全模式</h2><p><img src="/HDFS/51.png" alt="51"></p>
<p>集群处于安全模式，不能执行重要操作（写操作）。集群启动完成后，自动退出安全模式。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/hdfs dfsadmin -safemode get		（功能描述：查看安全模式状态）</span><br><span class="line">bin/hdfs dfsadmin -safemode enter  （功能描述：进入安全模式状态）</span><br><span class="line">bin/hdfs dfsadmin -safemode leave	（功能描述：离开安全模式状态）</span><br><span class="line">bin/hdfs dfsadmin -safemode wait	（功能描述：等待安全模式状态）</span><br></pre></td></tr></table></figure>

<h2 id="NameNode多目录配置"><a href="#NameNode多目录配置" class="headerlink" title="NameNode多目录配置"></a>NameNode多目录配置</h2><ol>
<li><p>NameNode的本地目录可以配置成多个，且每个目录存放内容相同，增加了可靠性</p>
</li>
<li><p>具体配置如下</p>
<p>1）在hdfs-site.xml文件中修改如下内容</p>
</li>
</ol>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///$&#123;hadoop.tmp.dir&#125;/name1,file:///$&#123;hadoop.tmp.dir&#125;/name2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>​        2）停止集群，删除data和logs中所有数据。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hadoop-3.1.3]$ rm -rf data/ logs/</span><br><span class="line">[vincent@linux2 hadoop-3.1.3]$ rm -rf data/ logs/</span><br><span class="line">[vincent@linux3 hadoop-3.1.3]$ rm -rf data/ logs/</span><br></pre></td></tr></table></figure>

<p>​        3）格式化集群并启动</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hadoop-3.1.3]$ bin/hdfs namenode –format</span><br><span class="line">[vincent@linux1 hadoop-3.1.3]$ sbin/start-dfs.sh</span><br></pre></td></tr></table></figure>

<p>​        4）查看结果</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 data]$ ll</span><br><span class="line">总用量 12</span><br><span class="line">drwx------. 3 vincent vincent 4096 12月 11 08:03 data</span><br><span class="line">drwxrwxr-x. 3 vincent vincent 4096 12月 11 08:03 name1</span><br><span class="line">drwxrwxr-x. 3 vincent vincent 4096 12月 11 08:03 name2</span><br></pre></td></tr></table></figure>

<h1 id="DataNode"><a href="#DataNode" class="headerlink" title="DataNode"></a>DataNode</h1><h2 id="DataNode工作机制"><a href="#DataNode工作机制" class="headerlink" title="DataNode工作机制"></a>DataNode工作机制</h2><p><img src="/HDFS/52.png" alt="52"></p>
<p>1）<strong>一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据</strong>包括数据块的长度，块数据的校验和，以及时间戳。</p>
<p>2）DataNode启动后向NameNode注册，通过后，周期性（1小时）的向NameNode上报所有的块信息。</p>
<p>3）<strong>心跳是每3秒一次</strong>，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。<strong>如果超过10分钟30秒没有收到某个DataNode的心跳，则认为该节点不可用。</strong></p>
<p>4）集群运行中可以安全加入和退出一些机器。</p>
<h2 id="数据完整性"><a href="#数据完整性" class="headerlink" title="数据完整性"></a>数据完整性</h2><p><img src="/HDFS/53.png" alt="53"></p>
<p>思考：如果电脑磁盘里面存储的数据是控制高铁信号灯的红灯信号（1）和绿灯信号（0），但是存储该数据的磁盘坏了，一直显示是绿灯，是否很危险？同理DataNode节点上的数据损坏了，却没有发现，是否也很危险，那么如何解决呢？</p>
<p>如下是DataNode节点保证数据完整性的方法。</p>
<p>1）当DataNode读取Block的时候，它会计算CheckSum。</p>
<p>2）如果计算后的CheckSum，与Block创建时值不一样，说明Block已经损坏。</p>
<p>3）Client读取其他DataNode上的Block。</p>
<p>4）常见的校验算法 crc（32）  md5（128）  sha1（160）。</p>
<p>5）DataNode在其文件创建后周期验证CheckSum。</p>
<h2 id="掉线时限参数设置"><a href="#掉线时限参数设置" class="headerlink" title="掉线时限参数设置"></a>掉线时限参数设置</h2><p><img src="/HDFS/54.png" alt="54"></p>
<p>需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.heartbeat.recheck-interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>300000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.heartbeat.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="服役新数据节点"><a href="#服役新数据节点" class="headerlink" title="服役新数据节点"></a>服役新数据节点</h2><p>1.在新节点中进行操作系统配置，包括主机名、网络、防火墙和无密码登录等。</p>
<p>2.在所有节点/etc/hosts文件中添加新节点。</p>
<p>3.把namenode的有关配置文件复制到该节点。</p>
<p>4.修改master节点slaves/works文件,增加该节点。</p>
<p>5.单独启动该节点上的datanode和nodemanager</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux4 hadoop-3.1.3]$ hdfs --daemon start datanode</span><br><span class="line">[vincent@linux4 hadoop-3.1.3]$ yarn --daemon start nodemanager</span><br></pre></td></tr></table></figure>

<p>6.运行start-balancer.sh 进行数据负载均衡</p>
<p>（默认阀值:单个节点的使用率和整个集群的使用率差值是10%以下,超过百分之十集群就会进行负载均衡）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux4 hadoop-3.1.3]$ sbin/start-balancer.sh</span><br></pre></td></tr></table></figure>

<h2 id="退役旧数据节点"><a href="#退役旧数据节点" class="headerlink" title="退役旧数据节点"></a>退役旧数据节点</h2><p>添加到白名单或者黑名单。</p>
<p>添加到白名单的主机节点，都允许访问NameNode，不在白名单的主机节点，都会被直接退出。</p>
<p>添加到黑名单的主机节点，不允许访问NameNode，会在数据迁移后退出。</p>
<p>实际情况下，白名单用于确定允许访问NameNode的DataNode节点，内容配置一般与workers文件内容一致。 黑名单用于在集群运行过程中退役DataNode节点。</p>
<p>配置白名单和黑名单的具体步骤如下：</p>
<p>（1）在NameNode的/opt/module/hadoop-3.1.3/etc/hadoop目录下分别创建whitelist 和 blacklist 文件</p>
<p>在whitelist中添加如下主机名称,假如集群正常工作的节点为linux1，linux2，linux3</p>
<figure class="highlight gcode"><table><tr><td class="code"><pre><span class="line">li<span class="symbol">nux1</span></span><br><span class="line">li<span class="symbol">nux2</span></span><br><span class="line">li<span class="symbol">nux3</span></span><br></pre></td></tr></table></figure>

<p>黑名单暂时为空。</p>
<p>（2）在NameNode的hdfs-site.xml配置文件中增加dfs.hosts 和 dfs.hosts.exclude配置</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- whitelist --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.1.3/etc/hadoop/whitelist<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- blacklist --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.hosts.exclude<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.1.3/etc/hadoop/blacklist<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>（3）配置文件分发</p>
<p>（4）重新启动集群</p>
<p>白名单退役会直接将节点抛弃，没有迁移数据的过程，会造成数据丢失。</p>
<p>黑名单会进行数据迁移，推荐使用黑名单。</p>
<h2 id="Datanode多目录配置"><a href="#Datanode多目录配置" class="headerlink" title="Datanode多目录配置"></a>Datanode多目录配置</h2><ol>
<li><p>DataNode也可以配置成多个目录，每个目录存储的数据不一样。即：数据不是副本。</p>
</li>
<li><p>具体配置如下</p>
</li>
</ol>
<p>（1）在hdfs-site.xml中修改如下内容:</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///$&#123;hadoop.tmp.dir&#125;/data1,file:///$&#123;hadoop.tmp.dir&#125;/data2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>（2）停止集群，删除data和logs中所有数据。</p>
<p>（3）格式化集群并启动。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hadoop-3.1.3]$ bin/hdfs namenode -format</span><br><span class="line">[vincent@linux1 hadoop-3.1.3]$ sbin/start-dfs.sh</span><br></pre></td></tr></table></figure>

<h1 id="小文件存档"><a href="#小文件存档" class="headerlink" title="小文件存档"></a>小文件存档</h1><p><img src="/HDFS/55.png" alt="55"></p>
<p>1.归档文件:</p>
<p>把/user/vincent/input目录里面的所有文件归档成一个叫input.har的归档文件，并把归档后文件存储到/user/vincent/output路径下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hadoop-3.1.3]$ bin/hadoop archive -archiveName input.har -p  /user/vincent/input /user/vincent/output</span><br></pre></td></tr></table></figure>

<p>2.查看归档</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hadoop-3.1.3]$ hadoop fs -lsr /user/vincent/output/input.har</span><br><span class="line">[vincent@linux1 hadoop-3.1.3]$ hadoop fs -lsr har:///user/vincent/output/input.har</span><br></pre></td></tr></table></figure>

<p>3.解归档文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hadoop-3.1.3]$ hadoop fs -cp har:///user/vincent/output/input.har/* /user/vincent</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title>Hbase</title>
    <url>/Hbase/</url>
    <content><![CDATA[<p><a href="http://hbase.apache.org/2.0/book.html">http://hbase.apache.org/2.0/book.html</a></p>
<h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>HBase是一种分布式、可扩展、支持海量数据（PB）存储的NoSQL数据库。</p>
<h2 id="数据模型"><a href="#数据模型" class="headerlink" title="数据模型"></a>数据模型</h2><p>传统数据库：数据库-表-行</p>
<p>hbase：命名空间-table-列族-行-列</p>
<p>逻辑上，HBase的数据模型同关系型数据库很类似，数据存储在一张表中，有行有列。但从HBase的<strong>底层物理存储结构（K-V）</strong>来看，HBase更像是一个multi-dimensional map。</p>
<p>hbase<strong>逻辑结构</strong>：</p>
<p><img src="/Hbase/22.png" alt="22"></p>
<p>region存在不同的数据库中。</p>
<p>hbase<strong>物理存储结构</strong>：</p>
<p><img src="/Hbase/23.png" alt="23"></p>
<p><strong>1）Name Space</strong></p>
<p>命名空间，<strong>类似于关系型数据库的database概念</strong>，<strong>每个命名空间下有多个表</strong>。HBase两个自带的命名空间，分别是hbase和default，hbase中存放的是HBase内置的表，default是用户默认使用的命名空间。</p>
<p><strong>2）Table</strong></p>
<p>类似于关系型数据库的表概念。不同的是，<strong>HBase定义表时只需要声明列族即可</strong>，<strong>不需要声明具体的列</strong>。这意味着，往HBase写入数据时，字段可以动态、按需指定。因此，<strong>和关系型数据库相比，HBase能够轻松应对字段变更的场景。</strong></p>
<p><strong>3）Row</strong></p>
<p>HBase表中的每行数据都由一个<strong>RowKey</strong>和多个<strong>Column（列）</strong>组成，数据是按照RowKey的字典顺序存储的，并且<strong>查询数据时只能根据RowKey进行检索，所以RowKey的设计十分重要</strong>。</p>
<p><strong>4）Column</strong></p>
<p>HBase中的每个列都由Column Family(列族)和Column Qualifier（列限定符）进行限定，例如info：name，info：age。<strong>建表时，只需指明列族，而列限定符无需预先定义。列族不建议超过2个。</strong></p>
<p><strong>5）TimeStamp</strong></p>
<p>用于标识数据的不同版本（version），每条数据写入时，系统会自动为其加上该字段，其值为写入HBase的时间。</p>
<p><strong>6）Cell</strong></p>
<p><strong>由{rowkey, column Family：column Qualifier, time Stamp} 确定唯一的单元。cell中的数据全部是字节码形式存贮。</strong></p>
<h2 id="HBase基本架构"><a href="#HBase基本架构" class="headerlink" title="HBase基本架构"></a>HBase基本架构</h2><p><img src="/Hbase/24.png" alt="24"></p>
<p>架构角色：</p>
<p><strong>1）RegionServer</strong></p>
<p>Region Server为 Region的管理者，其实现类为HRegionServer，主要作用如下:</p>
<p>对于数据的操作：get, put, delete；</p>
<p>对于Region的操作：splitRegion、compactRegion。</p>
<p><strong>2）Master</strong></p>
<p>Master是所有Region Server的管理者，其实现类为HMaster，主要作用如下：</p>
<p>对于表的操作：create, delete, alter</p>
<p>对于RegionServer的操作：分配regions到每个RegionServer，监控每个RegionServer的状态，负载均衡和故障转移。</p>
<p><strong>3）Zookeeper</strong></p>
<p>HBase通过Zookeeper来做master的高可用、RegionServer的监控、元数据的入口以及集群配置的维护等工作。</p>
<p><strong>4）HDFS</strong></p>
<p>HDFS为Hbase提供最终的底层数据存储服务，同时为HBase提供高可用的支持。</p>
<h2 id="HBase安装部署"><a href="#HBase安装部署" class="headerlink" title="HBase安装部署"></a>HBase安装部署</h2><h3 id="1、Zookeeper正常部署"><a href="#1、Zookeeper正常部署" class="headerlink" title="1、Zookeeper正常部署"></a>1、Zookeeper正常部署</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 zookeeper-3.5.7]$ zk start</span><br></pre></td></tr></table></figure>

<h3 id="2、Hadoop正常部署"><a href="#2、Hadoop正常部署" class="headerlink" title="2、Hadoop正常部署"></a>2、Hadoop正常部署</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hadoop-3.1.3]$ mycluster start</span><br></pre></td></tr></table></figure>

<h3 id="3、HBase的解压"><a href="#3、HBase的解压" class="headerlink" title="3、HBase的解压"></a>3、HBase的解压</h3><p>解压Hbase到指定目录</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 software]$ tar -zxvf hbase-2.0.5-bin.tar.gz -C /opt/module</span><br><span class="line">[vincent@linux1 software]$ mv /opt/module/hbase-2.0.5 /opt/module/hbase</span><br></pre></td></tr></table></figure>

<p>配置环境变量</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">HBASE_HOME</span></span><br><span class="line">export HBASE_HOME=/opt/module/hbase</span><br><span class="line">export PATH=$PATH:$HBASE_HOME/bin</span><br></pre></td></tr></table></figure>

<h3 id="4、HBase的配置文件"><a href="#4、HBase的配置文件" class="headerlink" title="4、HBase的配置文件"></a>4、HBase的配置文件</h3><p>1.hbase-env.sh修改内容(第125行)</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export HBASE_MANAGES_ZK=false</span><br></pre></td></tr></table></figure>

<p>2.hbase-site.xml修改内容</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://linux1:9820/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>linux1,linux2,linux3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.unsafe.stream.capability.enforce<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.wal.provider<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>filesystem<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.master.maxclockskew<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>180000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Time difference of regionserver from master<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>3.regionservers</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">linux1</span><br><span class="line">linux2</span><br><span class="line">linux3</span><br></pre></td></tr></table></figure>

<h3 id="5、HBase远程发送到其他集群"><a href="#5、HBase远程发送到其他集群" class="headerlink" title="5、HBase远程发送到其他集群"></a>5、HBase远程发送到其他集群</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 module]$ xsync hbase</span><br></pre></td></tr></table></figure>

<h3 id="6、HBase服务的启动"><a href="#6、HBase服务的启动" class="headerlink" title="6、HBase服务的启动"></a>6、HBase服务的启动</h3><p>单点启动</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hbase]$ bin/hbase-daemon.sh start master</span><br><span class="line">[vincent@linux1 hbase]$ bin/hbase-daemon.sh start regionserver</span><br></pre></td></tr></table></figure>

<p>群启群停</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hbase]$ bin/start-hbase.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> bin/start-hbase.sh  在当前节点启动master，其他节点启动region server</span></span><br><span class="line">[vincent@linux1 hbase]$ bin/stop-hbase.sh</span><br></pre></td></tr></table></figure>

<h3 id="7、查看HBase页面"><a href="#7、查看HBase页面" class="headerlink" title="7、查看HBase页面"></a>7、查看HBase页面</h3><figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">http</span>://linux<span class="number">1</span>:<span class="number">16010</span> </span><br></pre></td></tr></table></figure>

<h3 id="8、高可用-可选"><a href="#8、高可用-可选" class="headerlink" title="8、高可用(可选)"></a>8、高可用(可选)</h3><p>在HBase中HMaster负责监控HRegionServer的生命周期，均衡RegionServer的负载，如果HMaster挂掉了，那么整个HBase集群将陷入不健康的状态，并且此时的工作状态并不会维持太久。所以HBase支持对HMaster的高可用配置。</p>
<p>1.关闭HBase集群（如果没有开启则跳过此步）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hbase]$ bin/stop-hbase.sh</span><br></pre></td></tr></table></figure>

<p>2.在conf目录下创建backup-masters文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hbase]$ touch conf/backup-masters</span><br></pre></td></tr></table></figure>

<p>3.在backup-masters文件中配置高可用HMaster节点</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hbase]$ echo linux2 &gt; conf/backup-masters</span><br></pre></td></tr></table></figure>

<p>4.将整个conf目录scp到其他节点</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hbase]$ scp -r conf/ linux2:/opt/module/hbase/</span><br><span class="line">[vincent@linux1 hbase]$ scp -r conf/ linux3:/opt/module/hbase/</span><br></pre></td></tr></table></figure>

<h2 id="HBase-Shell操作"><a href="#HBase-Shell操作" class="headerlink" title="HBase Shell操作"></a>HBase Shell操作</h2><h3 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h3><p>进入HBase客户端命令行</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hbase]$ bin/hbase shell</span><br></pre></td></tr></table></figure>

<p>查看帮助命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase(main):001:0&gt; help</span><br></pre></td></tr></table></figure>

<p>列出所有用户表</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase(main):002:0&gt; list</span><br></pre></td></tr></table></figure>

<h3 id="表的操作"><a href="#表的操作" class="headerlink" title="表的操作"></a>表的操作</h3><h4 id="1、创建表"><a href="#1、创建表" class="headerlink" title="1、创建表"></a>1、创建表</h4><p>create ‘表名’, ‘列族’</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hbase(main):002:0&gt; create &#x27;student&#x27;,&#x27;info&#x27;</span><br></pre></td></tr></table></figure>

<h4 id="2、插入数据到表"><a href="#2、插入数据到表" class="headerlink" title="2、插入数据到表"></a>2、插入数据到表</h4><p>put ‘表名’, ‘RowKey’, ‘列族:列标识符’, ‘数据’</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hbase(main):003:0&gt; put &#x27;student&#x27;,&#x27;1001&#x27;,&#x27;info:sex&#x27;,&#x27;male&#x27;</span><br><span class="line">hbase(main):004:0&gt; put &#x27;student&#x27;,&#x27;1001&#x27;,&#x27;info:age&#x27;,&#x27;18&#x27;</span><br><span class="line">hbase(main):005:0&gt; put &#x27;student&#x27;,&#x27;1002&#x27;,&#x27;info:name&#x27;,&#x27;Janna&#x27;</span><br><span class="line">hbase(main):006:0&gt; put &#x27;student&#x27;,&#x27;1002&#x27;,&#x27;info:sex&#x27;,&#x27;female&#x27;</span><br><span class="line">hbase(main):007:0&gt; put &#x27;student&#x27;,&#x27;1002&#x27;,&#x27;info:age&#x27;,&#x27;20&#x27;</span><br></pre></td></tr></table></figure>

<h4 id="3、扫描查看表数据"><a href="#3、扫描查看表数据" class="headerlink" title="3、扫描查看表数据"></a>3、扫描查看表数据</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hbase(main):008:0&gt; scan &#x27;student&#x27;</span><br><span class="line">hbase(main):009:0&gt; scan &#x27;student&#x27;,&#123;STARTROW =&gt; &#x27;1001&#x27;, STOPROW  =&gt; &#x27;1002&#x27;&#125;</span><br><span class="line">hbase(main):010:0&gt; scan &#x27;student&#x27;,&#123;STARTROW =&gt; &#x27;1001&#x27;&#125;</span><br></pre></td></tr></table></figure>

<h4 id="4、查看表结构"><a href="#4、查看表结构" class="headerlink" title="4、查看表结构"></a>4、查看表结构</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase(main):011:0&gt; describe &#x27;student&#x27;</span><br></pre></td></tr></table></figure>

<h4 id="5、更新指定字段的数据"><a href="#5、更新指定字段的数据" class="headerlink" title="5、更新指定字段的数据"></a>5、更新指定字段的数据</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hbase(main):012:0&gt; put &#x27;student&#x27;,&#x27;1001&#x27;,&#x27;info:name&#x27;,&#x27;Nick&#x27;</span><br><span class="line">hbase(main):013:0&gt; put &#x27;student&#x27;,&#x27;1001&#x27;,&#x27;info:age&#x27;,&#x27;100&#x27;</span><br></pre></td></tr></table></figure>

<h4 id="6、查看“指定行”或“指定列族-列”的数据"><a href="#6、查看“指定行”或“指定列族-列”的数据" class="headerlink" title="6、查看“指定行”或“指定列族:列”的数据"></a>6、查看“指定行”或“指定列族:列”的数据</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase(main):014:0&gt; get &#x27;student&#x27;,&#x27;1001&#x27;</span><br><span class="line">hbase(main):015:0&gt; get &#x27;student&#x27;,&#x27;1001&#x27;,&#x27;info:name&#x27;</span><br></pre></td></tr></table></figure>

<h4 id="7、统计表数据行数"><a href="#7、统计表数据行数" class="headerlink" title="7、统计表数据行数"></a>7、统计表数据行数</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hbase(main):021:0&gt; count &#x27;student&#x27;</span><br></pre></td></tr></table></figure>

<h4 id="8、删除数据"><a href="#8、删除数据" class="headerlink" title="8、删除数据"></a>8、删除数据</h4><p>删除某rowkey的全部数据：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hbase(main):016:0&gt; deleteall &#x27;student&#x27;,&#x27;1001&#x27;</span><br></pre></td></tr></table></figure>

<p>删除某rowkey的某一列数据：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase(main):017:0&gt; delete &#x27;student&#x27;,&#x27;1002&#x27;,&#x27;info:sex&#x27;</span><br></pre></td></tr></table></figure>

<h4 id="9、清空表数据"><a href="#9、清空表数据" class="headerlink" title="9、清空表数据"></a>9、清空表数据</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase(main):018:0&gt; truncate &#x27;student&#x27;</span><br></pre></td></tr></table></figure>

<p>提示：清空表的操作顺序为先disable，然后再truncate。</p>
<h4 id="10、删除表"><a href="#10、删除表" class="headerlink" title="10、删除表"></a>10、删除表</h4><p>首先需要先让该表为disable状态</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hbase(main):019:0&gt; disable &#x27;student&#x27;</span><br></pre></td></tr></table></figure>

<p>然后才能drop这个表</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hbase(main):020:0&gt; drop &#x27;student&#x27;</span><br></pre></td></tr></table></figure>

<p>提示：如果直接drop表，会报错：ERROR: Table student is enabled. Disable it first.</p>
<h4 id="11、变更表信息"><a href="#11、变更表信息" class="headerlink" title="11、变更表信息"></a>11、变更表信息</h4><p>将<strong>info列族</strong>中的数据存放3个版本</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hbase(main):022:0&gt; alter &#x27;student&#x27;,&#123;NAME=&gt;&#x27;info&#x27;,VERSIONS=&gt;3&#125;</span><br><span class="line">hbase(main):022:0&gt; get &#x27;student&#x27;,&#x27;1001&#x27;,&#123;COLUMN=&gt;&#x27;info:name&#x27;,VERSIONS=&gt;3&#125;</span><br></pre></td></tr></table></figure>

<h4 id="12、namespace操作"><a href="#12、namespace操作" class="headerlink" title="12、namespace操作"></a>12、namespace操作</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 列出所有namespace</span><br><span class="line">list_namespace</span><br><span class="line"># 创建namespace</span><br><span class="line">create_namespace &#x27;name_test&#x27;</span><br><span class="line"># 删除namespace</span><br><span class="line">drop_namespace &#x27;name_test&#x27;</span><br><span class="line"># 查看namespace</span><br><span class="line">describe_namespace &#x27;name_test&#x27;</span><br><span class="line"># 在namespace下创建表</span><br><span class="line">create &#x27;name_test:test1&#x27;, &#x27;cf1&#x27;</span><br><span class="line"># 查看namespace下的表</span><br><span class="line">list_namespace_tables &#x27;name_test&#x27;</span><br><span class="line"># 删除命名空间时 必须保证命名空间中没有表</span><br><span class="line">disable &#x27;name_test:test1&#x27;</span><br><span class="line">drop &#x27;name_test:test1&#x27;</span><br><span class="line">drop_namespace &#x27;name_test&#x27;</span><br></pre></td></tr></table></figure>

<h2 id="HBase进阶"><a href="#HBase进阶" class="headerlink" title="HBase进阶"></a>HBase进阶</h2><h3 id="RegionServer-架构"><a href="#RegionServer-架构" class="headerlink" title="RegionServer 架构"></a>RegionServer 架构</h3><p><img src="/Hbase/25.png" alt="25"></p>
<p><strong>1）StoreFile</strong></p>
<p>保存实际数据的物理文件，StoreFile以Hfile的形式存储在HDFS上。每个Store会有一个或多个StoreFile（HFile），<strong>数据在每个StoreFile中都是有序的</strong>。</p>
<p><strong>2）MemStore</strong></p>
<p>写缓存，由于HFile中的数据要求是有序的，所以数据是先存储在MemStore中，排好序后，等到达刷写时机才会刷写到HFile，每次刷写都会形成一个新的HFile。</p>
<p><strong>3）WAL</strong></p>
<p>由于数据要经MemStore排序后才能刷写到HFile，但把数据保存在内存中会有很高的概率导致数据丢失，为了解决这个问题，数据会先写在一个叫做Write-Ahead logfile的文件中，然后再写入MemStore中。所以在系统出现故障的时候，数据可以通过这个日志文件重建。</p>
<p><strong>4）BlockCache</strong></p>
<p>读缓存，每次查询出的数据会缓存在BlockCache中，方便下次查询。</p>
<h3 id="写流程"><a href="#写流程" class="headerlink" title="写流程"></a>写流程</h3><p><img src="/Hbase/26.png" alt="26"></p>
<p>1）Client先访问zookeeper，获取hbase:meta表位于哪个Region Server。</p>
<p>2）访问对应的Region Server，<strong>获取hbase:meta表</strong>，根据读请求的namespace:table/rowkey，查询出目标数据位于哪个Region Server中的哪个Region中。并将该table的region信息以及meta表的位置信息缓存在客户端的meta cache，方便下次访问。</p>
<p>3）与目标Region Server进行通讯；</p>
<p>4）将数据<strong>顺序写入（追加）到WAL</strong>；</p>
<p>5）将数据<strong>写入对应的MemStore</strong>，数据会在MemStore进行<strong>排序</strong>；</p>
<p>6）向客户端发送ack；</p>
<p>7）等达到MemStore的刷写时机后，将<strong>数据刷写到HFile</strong>。</p>
<h3 id="MemStore-Flush"><a href="#MemStore-Flush" class="headerlink" title="MemStore Flush"></a>MemStore Flush</h3><p><img src="/Hbase/27.png" alt="27"></p>
<p><strong>MemStore刷写时机：</strong></p>
<p><strong>所有的flush都是以Region为单位刷新</strong></p>
<p><strong>1）MemStore级别</strong><br>当 Region中 某个 MemStore 的大小达到了hbase.hregion.memstore.flush.size（默认值128M），会触发Region的刷写（若Region中有多个Store，只要有其中一个达到hbase.hregion.memstore.flush.size（默认值128M）值，就会触发flush，每个Store都会生成一个StroeFile文件，可能会生成多个小文件，所以一般情况下，一个Region只设置一个列簇（即一个Store））</p>
<p><strong>2）Region级别</strong><br>当处于写高峰的时候，会延迟触发第一个时机（MemStore级别）</p>
<p>当 Region 中的MemStore的总大小达到了hbase.hregion.memstore.flush.size（默认值128M）* hbase.hregion.memstore.block.multiplier（默认值4）时，会阻塞所有写入该 Region 的写请求，优先flush！这时候如果往 MemStore 写数据，会出现 RegionTooBusyException 异常。</p>
<p>举个例子：当 Region 中的某个 MemStore 占用内存达到128M ，会触发flush ，此时是允许写入操作的；若写入操作大于 flush 的速度，当 Region 中的所有 MemStore 占用内存达到 128 * 4 = 512M 时，阻止所有写入该 Region 的写请求，直到Region 中的所有 MemStore ，flush完毕，才取消阻塞，允许写入。</p>
<p><strong>3）RegionServer级别</strong><br>当RegionServer中，所有Region中的MemStore的总大小达到<br>java_heapsize * hbase.regionserver.global.memstore.size（默认值0.4）* hbase.regionserver.global.memstore.size.lower.limit（默认值0.95）会flush，flush的时候根据每个Region中，总MemStore占用的大小进行降序排序，依次flush；flush的时候优先flush占用空间大的region，每flush一个region，会查看总的占用大小是否小于 java_heapsize * hbase.regionserver.global.memstore.size（默认值0.4）* hbase.regionserver.global.memstore.size.lower.limit（默认值0.95）；如果还是大于，则继续flush region，若小于，则停止flush（注：此情况是允许memStore写入的）</p>
<p>当处于写高峰的时候，会延迟触发第三个时机（HRegionServer级别）</p>
<p>当RegionServer中，所有Region中的MemStore的总大小达到了 java_heapsize * hbase.regionserver.global.memstore.size（默认值0.4）时，会阻塞当前 RegionServer 的所有写请求（无法往Region中的MemStore写入数据），直到RegionServer中，所有Region中的MemStore的总大小 低于 java_heapsize * hbase.regionserver.global.memstore.size（默认值0.4）* hbase.regionserver.global.memstore.size.lower.limit（默认值0.95）时，才取消阻塞（允许写入数据）</p>
<p>举个例子，如果 HBase 堆内存总共是 10G，按照默认的比例，那么触发 RegionServer级别的flush ，是 RegionServer 中所有的 MemStore 占用内存为：10 * 0.4 * 0.95 = 3.8G时触发flush，此时是允许写入操作的，若写入操作大于flush的速度，当占用内存达到 10 * 0.4 = 4G 时，阻止所有写入操作，直到占用内存低于 3.8G ，才取消阻塞，允许写入。</p>
<p><strong>4）HLog级别</strong><br>当WAL文件的数量超过hbase.regionserver.maxlogs，region会按照时间顺序依次进行刷写，直到WAL文件数量减小到hbase.regionserver.maxlogs以下（该属性名已经废弃，现无需手动设置，最大值为32）。</p>
<p>WAL 数量触发的刷写策略是，找到最旧的 un-archived WAL 文件，并找到这个 WAL 文件对应的 Regions， 然后对这些 Regions 进行刷写。</p>
<p><strong>5）定期刷写</strong><br>到达自动刷写的时间，也会触发MemStore flush。自动刷新的时间间隔由该属性进行配置hbase.regionserver.optionalcacheflushinterval（默认1小时），指的是当前 MemStore 1小时会进行一次刷写。如果设定为0，则关闭定时自动刷写。</p>
<p><strong>6）手动刷写</strong><br>可以通过 shell 命令”flush ‘table’”或者”flush ‘regionname’”分别对一个Region或者多个Region进行flush</p>
<h3 id="读流程"><a href="#读流程" class="headerlink" title="读流程"></a>读流程</h3><p><img src="/Hbase/28.png" alt="28"></p>
<p><img src="/Hbase/29.png" alt="29"></p>
<p>1）Client先访问zookeeper，获取hbase:meta表位于哪个Region Server。</p>
<p>2）访问对应的Region Server，获取hbase:meta表，根据读请求的namespace:table/rowkey，查询出目标数据位于哪个Region Server中的哪个Region中。并将该table的region信息以及meta表的位置信息缓存在客户端的meta cache，方便下次访问。</p>
<p>3）与目标Region Server进行通讯；</p>
<p>4）分别在MemStore和Store File（HFile）中查询目标数据，并将查到的所有数据进行合并。此处所有数据是指同一条数据的不同版本（time stamp）或者不同的类型（Put/Delete）。</p>
<p>5）将查询到的新的数据块（Block，HFile数据存储单元，默认大小为64KB）缓存到Block Cache。</p>
<p>6）将合并后的最终结果返回给客户端。</p>
<h3 id="StoreFile-Compaction"><a href="#StoreFile-Compaction" class="headerlink" title="StoreFile Compaction"></a>StoreFile Compaction</h3><p><strong>由于memstore每次刷写都会生成一个新的HFile</strong>，<strong>且同一个字段的不同版本（timestamp）和不同类型（Put/Delete）有可能会分布在不同的HFile中</strong>，因此<strong>查询时需要遍历所有的HFile</strong>。<strong>为了减少HFile的个数，以及清理掉过期和删除的数据，会进行StoreFile Compaction</strong>。</p>
<p>Compaction分为两种，分别是<strong>Minor Compaction</strong>和<strong>Major Compaction</strong>。Minor Compaction会<strong>将临近的若干个较小的HFile合并成一个较大的HFile</strong>，并清理掉部分过期和删除的数据。Major Compaction会将<strong>一个Store下的所有的HFile合并成一个大HFile</strong>，并且清理掉所有过期和删除的数据。</p>
<p><img src="/Hbase/30.png" alt="30"></p>
<h3 id="Region-Split"><a href="#Region-Split" class="headerlink" title="Region Split"></a>Region Split</h3><p>默认情况下，每个Table起初只有一个Region，随着数据的不断写入，Region会自动进行拆分。刚拆分时，两个子Region都位于当前的Region Server，但处于负载均衡的考虑，HMaster有可能会将某个Region转移给其他的Region Server。</p>
<p>Region Split时机：</p>
<p>1.当1个region中的某个Store下所有StoreFile的总大小超过hbase.hregion.max.filesize（默认10G），该Region就会进行拆分（0.94版本之前）。</p>
<p>2.当1个region中的某个Store下所有StoreFile的总大小超过Min(initialSize<em>R^3 ,hbase.hregion.max.filesize”)，该Region就会进行拆分。其中initialSize的默认值为2</em>hbase.hregion.memstore.flush.size，R为当前Region Server中属于该Table的Region个数（0.94版本之后）。</p>
<p>具体的切分策略为：</p>
<p>第一次split：1^3 * 256 = 256MB </p>
<p>第二次split：2^3 * 256 = 2048MB </p>
<p>第三次split：3^3 * 256 = 6912MB </p>
<p>第四次split：4^3 * 256 = 16384MB &gt; 10GB，因此取较小的值10GB </p>
<p>后面每次split的size都是10GB了。</p>
<p>3.Hbase 2.0引入了新的split策略：如果当前RegionServer上该表只有一个Region，按照                                    2 * hbase.hregion.memstore.flush.size（256M）分裂，否则按照hbase.hregion.max.filesize（10G）分裂。</p>
<p><img src="/Hbase/31.png" alt="31"></p>
<h2 id="HBase-API"><a href="#HBase-API" class="headerlink" title="HBase API"></a>HBase API</h2><p>Maven依赖</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.0.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="HbaseDDL"><a href="#HbaseDDL" class="headerlink" title="HbaseDDL"></a>HbaseDDL</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HbaseDDL</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//获取hbase连接</span></span><br><span class="line">  <span class="keyword">val</span> conf: <span class="type">Configuration</span> = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line">  conf.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;linux1,linux2,linux3&quot;</span>)</span><br><span class="line">  <span class="keyword">val</span> conn: <span class="type">Connection</span> = <span class="type">ConnectionFactory</span>.createConnection(conf)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    deleteNS(<span class="string">&quot;xsh&quot;</span>)</span><br><span class="line"></span><br><span class="line">    closeConnection()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 判断表存在</span></span><br><span class="line"><span class="comment">   * @param name</span></span><br><span class="line"><span class="comment">   * @return</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">tableExists</span></span>(name: <span class="type">String</span>) = &#123;</span><br><span class="line">    <span class="comment">//获取管理对象 Admin</span></span><br><span class="line">    <span class="keyword">val</span> admin: <span class="type">Admin</span> = conn.getAdmin</span><br><span class="line">    <span class="comment">//利用admin操作</span></span><br><span class="line">    <span class="keyword">val</span> tableName: <span class="type">TableName</span> = <span class="type">TableName</span>.valueOf(name)</span><br><span class="line">    <span class="keyword">val</span> bool: <span class="type">Boolean</span> = admin.tableExists(tableName)</span><br><span class="line">    <span class="comment">//关闭admin</span></span><br><span class="line">    admin.close()</span><br><span class="line">    bool</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 创建表</span></span><br><span class="line"><span class="comment">   * @param name</span></span><br><span class="line"><span class="comment">   * @param cfs</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createTable</span></span>(name: <span class="type">String</span>, cfs: <span class="type">String</span>*): <span class="type">Boolean</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> admin: <span class="type">Admin</span> = conn.getAdmin</span><br><span class="line">    <span class="keyword">val</span> tableName: <span class="type">TableName</span> = <span class="type">TableName</span>.valueOf(name)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(tableExists(name)) <span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> table: <span class="type">TableDescriptorBuilder</span> = <span class="type">TableDescriptorBuilder</span>.newBuilder(tableName)</span><br><span class="line">    cfs.foreach(cf =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> descriptor: <span class="type">ColumnFamilyDescriptor</span> = <span class="type">ColumnFamilyDescriptorBuilder</span>.newBuilder(<span class="type">Bytes</span>.toBytes(cf)).build()</span><br><span class="line">      table.setColumnFamily(descriptor)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    admin.createTable(table.build())</span><br><span class="line">    admin.close()</span><br><span class="line"></span><br><span class="line">    <span class="literal">true</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 删除表</span></span><br><span class="line"><span class="comment">   * @param name</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">deleteTable</span></span>(name: <span class="type">String</span>) = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> admin: <span class="type">Admin</span> = conn.getAdmin</span><br><span class="line">    <span class="keyword">if</span>(tableExists(name))&#123;</span><br><span class="line">      admin.disableTable(<span class="type">TableName</span>.valueOf(name))</span><br><span class="line">      admin.deleteTable(<span class="type">TableName</span>.valueOf(name))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    admin.close()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 判断namespace存在</span></span><br><span class="line"><span class="comment">   * @param name</span></span><br><span class="line"><span class="comment">   * @return</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">nsExists</span></span>(name: <span class="type">String</span>) = &#123;</span><br><span class="line">    <span class="keyword">val</span> admin: <span class="type">Admin</span> = conn.getAdmin</span><br><span class="line">    <span class="keyword">val</span> descriptors: <span class="type">Array</span>[<span class="type">NamespaceDescriptor</span>] = admin.listNamespaceDescriptors()</span><br><span class="line">    <span class="keyword">val</span> bool: <span class="type">Boolean</span> = descriptors.map(_.getName).contains(name)</span><br><span class="line">    admin.close()</span><br><span class="line">    bool</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 创建namespace</span></span><br><span class="line"><span class="comment">   * @param name</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createNS</span></span>(name: <span class="type">String</span>) = &#123;</span><br><span class="line">    <span class="keyword">val</span> admin: <span class="type">Admin</span> = conn.getAdmin</span><br><span class="line">    <span class="keyword">if</span>(!nsExists(name))&#123;</span><br><span class="line">      <span class="keyword">val</span> descriptor: <span class="type">NamespaceDescriptor</span> = <span class="type">NamespaceDescriptor</span>.create(name).build()</span><br><span class="line">      admin.createNamespace(descriptor)</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">      println(<span class="string">s&quot;您要创建的namespace：<span class="subst">$&#123;name&#125;</span>已经存在.&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    admin.close()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 删除namespace</span></span><br><span class="line"><span class="comment">   * @param name</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">deleteNS</span></span>(name: <span class="type">String</span>) = &#123;</span><br><span class="line">    <span class="keyword">val</span> admin: <span class="type">Admin</span> = conn.getAdmin</span><br><span class="line">    <span class="keyword">if</span> (nsExists(name))&#123;</span><br><span class="line">      admin.deleteNamespace(name)</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">      println(<span class="string">s&quot;<span class="subst">$&#123;name&#125;</span>不存在&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">closeConnection</span></span>() = conn.close()</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="HbaseDML"><a href="#HbaseDML" class="headerlink" title="HbaseDML"></a>HbaseDML</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HbaseDML</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 1. 先获取到hbase的连接</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line">    conf.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;linux1,linux2,linux3&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> conn: <span class="type">Connection</span> = <span class="type">ConnectionFactory</span>.createConnection(conf)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//        putData(&quot;user&quot;, &quot;1001&quot;, &quot;info&quot;, &quot;name&quot;, &quot;lisi&quot;)</span></span><br><span class="line">        <span class="comment">//        putData(&quot;user&quot;, &quot;1002&quot;, &quot;info&quot;, &quot;name&quot;, &quot;ww&quot;)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">//        deleteData(&quot;user1&quot;, &quot;1001&quot;, &quot;info&quot;, &quot;age&quot;)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">//        getData(&quot;user&quot;, &quot;1001&quot;, &quot;info&quot;, &quot;name&quot;)</span></span><br><span class="line">        scanData(<span class="string">&quot;user&quot;</span>)</span><br><span class="line">        closeConnection()</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">scanData</span></span>(tableName: <span class="type">String</span>) = &#123;</span><br><span class="line">        <span class="keyword">val</span> table: <span class="type">Table</span> = conn.getTable(<span class="type">TableName</span>.valueOf(tableName))</span><br><span class="line">        <span class="keyword">val</span> scan = <span class="keyword">new</span> <span class="type">Scan</span>()</span><br><span class="line">        <span class="keyword">val</span> filter =</span><br><span class="line">            <span class="keyword">new</span> <span class="type">SingleColumnValueFilter</span>(<span class="type">Bytes</span>.toBytes(<span class="string">&quot;info&quot;</span>), <span class="type">Bytes</span>.toBytes(<span class="string">&quot;name&quot;</span>), <span class="type">CompareOperator</span>.<span class="type">EQUAL</span>, <span class="type">Bytes</span>.toBytes(<span class="string">&quot;abc&quot;</span>))</span><br><span class="line">        scan.setFilter(filter)</span><br><span class="line">        <span class="keyword">val</span> results: <span class="type">ResultScanner</span> = table.getScanner(scan)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">import</span> scala.collection.<span class="type">JavaConversions</span>._</span><br><span class="line">        <span class="comment">// 从scanner拿到所有数据</span></span><br><span class="line">        <span class="keyword">for</span> (result &lt;- results) &#123;</span><br><span class="line">            <span class="keyword">val</span> cells: util.<span class="type">List</span>[<span class="type">Cell</span>] = result.listCells() <span class="comment">// rawCells</span></span><br><span class="line">            <span class="keyword">if</span> (cells != <span class="literal">null</span>) &#123;</span><br><span class="line">                <span class="keyword">for</span> (cell &lt;- cells) &#123;</span><br><span class="line">                    println(</span><br><span class="line">                        <span class="string">s&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">                           |row = $&#123;Bytes.toString(CellUtil.cloneRow(cell))&#125;</span></span><br><span class="line"><span class="string">                           |cf = $&#123;Bytes.toString(CellUtil.cloneFamily(cell))&#125;</span></span><br><span class="line"><span class="string">                           |name = $&#123;Bytes.toString(CellUtil.cloneQualifier(cell))&#125;</span></span><br><span class="line"><span class="string">                           |value = $&#123;Bytes.toString(CellUtil.cloneValue(cell))&#125;</span></span><br><span class="line"><span class="string">                           |----------------</span></span><br><span class="line"><span class="string">                           |&quot;</span><span class="string">&quot;&quot;</span>.stripMargin)</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        table.close()</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getData</span></span>(tableName: <span class="type">String</span>, rowKey: <span class="type">String</span>, cf: <span class="type">String</span>, columnName: <span class="type">String</span>) = &#123;</span><br><span class="line">        <span class="keyword">val</span> table: <span class="type">Table</span> = conn.getTable(<span class="type">TableName</span>.valueOf(tableName))</span><br><span class="line">        <span class="keyword">val</span> get = <span class="keyword">new</span> <span class="type">Get</span>(<span class="type">Bytes</span>.toBytes(rowKey))</span><br><span class="line">        get.addColumn(<span class="type">Bytes</span>.toBytes(cf), <span class="type">Bytes</span>.toBytes(columnName))</span><br><span class="line">        <span class="keyword">val</span> result: <span class="type">Result</span> = table.get(get)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 这个是用来在java的集合和scala的集合之间互转  (隐式转换)</span></span><br><span class="line">        <span class="keyword">import</span> scala.collection.<span class="type">JavaConversions</span>._</span><br><span class="line">        <span class="keyword">val</span> cells: util.<span class="type">List</span>[<span class="type">Cell</span>] = result.listCells() <span class="comment">// rawCells</span></span><br><span class="line">        <span class="keyword">if</span> (cells != <span class="literal">null</span>) &#123;</span><br><span class="line">            <span class="keyword">for</span> (cell &lt;- cells) &#123;</span><br><span class="line">                </span><br><span class="line">                println(</span><br><span class="line">                    <span class="string">s&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">                       |row = $&#123;Bytes.toString(CellUtil.cloneRow(cell))&#125;</span></span><br><span class="line"><span class="string">                       |cf = $&#123;Bytes.toString(CellUtil.cloneFamily(cell))&#125;</span></span><br><span class="line"><span class="string">                       |name = $&#123;Bytes.toString(CellUtil.cloneQualifier(cell))&#125;</span></span><br><span class="line"><span class="string">                       |value = $&#123;Bytes.toString(CellUtil.cloneValue(cell))&#125;</span></span><br><span class="line"><span class="string">                       |----------------</span></span><br><span class="line"><span class="string">                       |&quot;</span><span class="string">&quot;&quot;</span>.stripMargin)</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        table.close()</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">deleteData</span></span>(tableName: <span class="type">String</span>, rowKey: <span class="type">String</span>, cf: <span class="type">String</span>, columnName: <span class="type">String</span>) = &#123;</span><br><span class="line">        <span class="keyword">val</span> table: <span class="type">Table</span> = conn.getTable(<span class="type">TableName</span>.valueOf(tableName))</span><br><span class="line">        <span class="keyword">val</span> delete = <span class="keyword">new</span> <span class="type">Delete</span>(<span class="type">Bytes</span>.toBytes(rowKey))</span><br><span class="line">        <span class="comment">//        delete.addColumn(Bytes.toBytes(cf), Bytes.toBytes(columnName))</span></span><br><span class="line">        delete.addColumns(<span class="type">Bytes</span>.toBytes(cf), <span class="type">Bytes</span>.toBytes(columnName)) <span class="comment">// 删除所有版本</span></span><br><span class="line">        table.delete(delete)</span><br><span class="line">        </span><br><span class="line">        table.close()</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">putData</span></span>(tableName: <span class="type">String</span>, rowKey: <span class="type">String</span>, cf: <span class="type">String</span>, columnName: <span class="type">String</span>, value: <span class="type">String</span>) = &#123;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 1. 先获取到表对象,客户端到表的连接</span></span><br><span class="line">        <span class="keyword">val</span> table: <span class="type">Table</span> = conn.getTable(<span class="type">TableName</span>.valueOf(tableName))</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 2. 调用表对象的put</span></span><br><span class="line">        <span class="comment">// 2.1 把需要添加的数据封装到一个Put对象   put &#x27;&#x27;, rowKey, &#x27;&#x27;, &#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">val</span> put = <span class="keyword">new</span> <span class="type">Put</span>(<span class="type">Bytes</span>.toBytes(rowKey))</span><br><span class="line">        put.addColumn(<span class="type">Bytes</span>.toBytes(cf), <span class="type">Bytes</span>.toBytes(columnName), <span class="type">Bytes</span>.toBytes(value))</span><br><span class="line">        <span class="comment">//        put.addColumn(Bytes.toBytes(cf), Bytes.toBytes(columnName + &quot;abc&quot;), Bytes.toBytes(value + &quot;efg&quot;))</span></span><br><span class="line">        <span class="comment">// 2.2 提交Put对象</span></span><br><span class="line">        table.put(put)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 3. 关闭到table的连接</span></span><br><span class="line">        table.close()</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">closeConnection</span></span>() = conn.close()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="HBase优化"><a href="#HBase优化" class="headerlink" title="HBase优化"></a>HBase优化</h2><h3 id="预分区"><a href="#预分区" class="headerlink" title="预分区"></a>预分区</h3><p>每一个region维护着startRow与endRowKey，如果加入的数据符合某个region维护的rowKey范围，则该数据交给这个region维护。那么依照这个原则，我们可以将数据所要投放的分区提前大致的规划好，以提高HBase性能。</p>
<p>1.手动设定预分区</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hbase&gt; create &#x27;staff1&#x27;,&#x27;info&#x27;,&#x27;partition1&#x27;,SPLITS =&gt; [&#x27;1000&#x27;,&#x27;2000&#x27;,&#x27;3000&#x27;,&#x27;4000&#x27;]</span><br></pre></td></tr></table></figure>

<p>2.生成16进制序列预分区</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create &#x27;staff2&#x27;,&#x27;info&#x27;,&#x27;partition2&#x27;,&#123;NUMREGIONS =&gt; 15, SPLITALGO =&gt; &#x27;HexStringSplit&#x27;&#125;</span><br></pre></td></tr></table></figure>

<p>3.按照文件中设置的规则预分区</p>
<p>创建splits.txt文件内容如下：</p>
<figure class="highlight ebnf"><table><tr><td class="code"><pre><span class="line"><span class="attribute">aaaa</span></span><br><span class="line"><span class="attribute">bbbb</span></span><br><span class="line"><span class="attribute">cccc</span></span><br><span class="line"><span class="attribute">dddd</span></span><br></pre></td></tr></table></figure>

<p>然后执行：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create &#x27;staff3&#x27;,&#x27;partition3&#x27;,SPLITS_FILE =&gt; &#x27;splits.txt&#x27;</span><br></pre></td></tr></table></figure>

<p>4.使用JavaAPI创建预分区</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//自定义算法，产生一系列Hash散列值存储在二维数组中</span></span><br><span class="line"><span class="keyword">byte</span>[][] splitKeys = 某个散列值函数</span><br><span class="line"><span class="comment">//创建HbaseAdmin实例</span></span><br><span class="line">HBaseAdmin hAdmin = <span class="keyword">new</span> HBaseAdmin(HbaseConfiguration.create());</span><br><span class="line"><span class="comment">//创建HTableDescriptor实例</span></span><br><span class="line">HTableDescriptor tableDesc = <span class="keyword">new</span> HTableDescriptor(tableName);</span><br><span class="line"><span class="comment">//通过HTableDescriptor实例和散列值二维数组创建带有预分区的Hbase表</span></span><br><span class="line">hAdmin.createTable(tableDesc, splitKeys);</span><br></pre></td></tr></table></figure>

<h3 id="RowKey设计"><a href="#RowKey设计" class="headerlink" title="RowKey设计"></a>RowKey设计</h3><p>一条数据的唯一标识就是rowkey，那么这条数据存储于哪个分区，取决于rowkey处于哪个一个预分区的区间内，设计rowkey的主要目的 ，就是让数据均匀的分布于所有的region中，在一定程度上<strong>防止数据倾斜</strong>。接下来我们就谈一谈rowkey常用的设计方案。</p>
<p>1.生成随机数、hash、散列值</p>
<figure class="highlight llvm"><table><tr><td class="code"><pre><span class="line">比如：</span><br><span class="line">原本rowKey为<span class="number">1001</span>的，SHA<span class="number">1</span>后变成：dd<span class="number">01903921</span>ea<span class="number">24941</span><span class="keyword">c</span><span class="number">26</span>a<span class="number">48</span>f<span class="number">2</span>cec<span class="number">24e0</span>bb<span class="number">0e8</span><span class="keyword">cc</span><span class="number">7</span></span><br><span class="line">原本rowKey为<span class="number">3001</span>的，SHA<span class="number">1</span>后变成：<span class="number">49042</span><span class="keyword">c</span><span class="number">54</span>de<span class="number">64</span>a<span class="number">1e9</span>bf<span class="number">0</span>b<span class="number">33e00245660</span>ef<span class="number">92</span>dc<span class="number">7</span>bd</span><br><span class="line">原本rowKey为<span class="number">5001</span>的，SHA<span class="number">1</span>后变成：<span class="number">7</span>b<span class="number">61</span>dec<span class="number">07e02</span><span class="keyword">c</span><span class="number">188790670</span>af<span class="number">43e717</span>f<span class="number">0</span>f<span class="number">46e8913</span></span><br><span class="line">在做此操作之前，一般我们会选择从数据集中抽取样本，来决定什么样的rowKey来Hash后作为每个分区的临界值。</span><br></pre></td></tr></table></figure>

<p>2.字符串反转</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">20170524000001</span>转成<span class="number">10000042507102</span></span><br><span class="line"><span class="attribute">20170524000002</span>转成<span class="number">20000042507102</span></span><br></pre></td></tr></table></figure>

<p>3.字符串拼接</p>
<figure class="highlight asciidoc"><table><tr><td class="code"><pre><span class="line">20170524000001<span class="emphasis">_a12e</span></span><br><span class="line"><span class="emphasis">20170524000001_</span>93i7</span><br></pre></td></tr></table></figure>

<h3 id="内存优化"><a href="#内存优化" class="headerlink" title="内存优化"></a>内存优化</h3><p>HBase操作过程中需要大量的内存开销，毕竟Table是可以缓存在内存中的，但是不建议分配非常大的堆内存，因为GC过程持续太久会导致RegionServer处于长期不可用状态，一般16~36G内存就可以了，如果因为框架占用内存过高导致系统内存不足，框架一样会被系统服务拖死。</p>
<p>1.Zookeeper会话超时时间</p>
<p>hbase-site.xml</p>
<figure class="highlight crmsh"><table><tr><td class="code"><pre><span class="line">属性：zookeeper.session.timeout</span><br><span class="line">解释：默认值为<span class="number">90000</span>毫秒（<span class="number">90s</span>）。当某个RegionServer挂掉，<span class="number">90s</span>之后<span class="literal">Master</span>才能察觉到。可适当减小此值，以加快<span class="literal">Master</span>响应，可调整至<span class="number">600000</span>毫秒。</span><br></pre></td></tr></table></figure>

<p>2.设置RPC监听数量</p>
<p>hbase-site.xml</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">属性：hbase<span class="selector-class">.regionserver</span><span class="selector-class">.handler</span><span class="selector-class">.count</span></span><br><span class="line">解释：默认值为<span class="number">30</span>，用于指定RPC监听的数量，可以根据客户端的请求数进行调整，读写请求较多时，增加此值。</span><br></pre></td></tr></table></figure>

<p>3.手动控制Major Compaction</p>
<p>hbase-site.xml</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">属性：hbase<span class="selector-class">.hregion</span><span class="selector-class">.majorcompaction</span></span><br><span class="line">解释：默认值：<span class="number">604800000</span>秒（<span class="number">7</span>天）， Major Compaction的周期，若关闭自动Major Compaction，可将其设为<span class="number">0</span></span><br></pre></td></tr></table></figure>

<p>4.优化HStore文件大小</p>
<p>hbase-site.xml</p>
<figure class="highlight maxima"><table><tr><td class="code"><pre><span class="line">属性：hbase.hregion.<span class="built_in">max</span>.filesize解释：默认值<span class="number">10737418240</span>（10GB），如果需要运行HBase的MR任务，可以减小此值，因为一个<span class="built_in">region</span>对应一个<span class="built_in">map</span>任务，如果单个<span class="built_in">region</span>过大，会导致<span class="built_in">map</span>任务执行时间过长。该值的意思就是，如果HFile的大小达到这个数值，则这个<span class="built_in">region</span>会被切分为两个Hfile。</span><br></pre></td></tr></table></figure>

<p>5.优化HBase客户端缓存</p>
<p>hbase-site.xml</p>
<figure class="highlight arduino"><table><tr><td class="code"><pre><span class="line">属性：hbase.client.write.buffer解释：默认值<span class="number">2097152b</span>ytes（<span class="number">2</span>M）用于指定HBase客户端缓存，增大该值可以减少RPC调用次数，但是会消耗更多内存，反之则反之。一般我们需要设定一定的缓存大小，以达到减少RPC次数的目的。</span><br></pre></td></tr></table></figure>

<p>6.指定scan.next扫描HBase所获取的行数</p>
<p>hbase-site.xml</p>
<figure class="highlight axapta"><table><tr><td class="code"><pre><span class="line">属性：hbase.<span class="keyword">client</span>.scanner.caching解释：用于指定scan.<span class="keyword">next</span>方法获取的默认行数，值越大，消耗内存越大。</span><br></pre></td></tr></table></figure>

<p>7.BlockCache占用RegionServer堆内存的比例</p>
<p>hbase-site.xml</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">属性：hfile<span class="selector-class">.block</span><span class="selector-class">.cache</span><span class="selector-class">.size</span></span><br><span class="line">解释：默认<span class="number">0.4</span>，读请求比较多的情况下，可适当调大</span><br></pre></td></tr></table></figure>

<p>8.MemStore占用RegionServer堆内存的比例</p>
<p>hbase-site.xml</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">属性：hbase<span class="selector-class">.regionserver</span><span class="selector-class">.global</span><span class="selector-class">.memstore</span><span class="selector-class">.size</span></span><br><span class="line">解释：默认<span class="number">0.4</span>，写请求较多的情况下，可适当调大</span><br></pre></td></tr></table></figure>

<h2 id="HBase与Hive集成使用"><a href="#HBase与Hive集成使用" class="headerlink" title="HBase与Hive集成使用"></a>HBase与Hive集成使用</h2><p>在hive-site.xml中添加zookeeper的属性，如下：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>linux1,linux2,linux3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.zookeeper.client.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>案例1：建立Hive表，关联HBase表，插入数据到Hive表的同时能够影响HBase表。</p>
<p>1.在Hive中创建表同时关联HBase</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> hive_hbase_emp_table(</span><br><span class="line">empno <span class="type">int</span>,</span><br><span class="line">ename string,</span><br><span class="line">job string,</span><br><span class="line">mgr <span class="type">int</span>,</span><br><span class="line">hiredate string,</span><br><span class="line">sal <span class="keyword">double</span>,</span><br><span class="line">comm <span class="keyword">double</span>,</span><br><span class="line">deptno <span class="type">int</span>)</span><br><span class="line">STORED <span class="keyword">BY</span> <span class="string">&#x27;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#x27;</span></span><br><span class="line"><span class="keyword">WITH</span> SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; <span class="operator">=</span> &quot;:key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno&quot;)</span><br><span class="line">TBLPROPERTIES (&quot;hbase.table.name&quot; <span class="operator">=</span> &quot;hbase_emp_table&quot;);</span><br></pre></td></tr></table></figure>

<p>提示：完成之后，可以分别进入Hive和HBase查看，都生成了对应的表</p>
<p>2.在Hive中<strong>创建临时中间表</strong>，用于load文件中的数据</p>
<p>提示：<strong>不能将数据直接load进Hive所关联HBase的那张表中</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CREATE TABLE emp(</span><br><span class="line">empno int,</span><br><span class="line">ename string,</span><br><span class="line">job string,</span><br><span class="line">mgr int,</span><br><span class="line">hiredate string,</span><br><span class="line">sal double,</span><br><span class="line">comm double,</span><br><span class="line">deptno int)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure>

<p>3.向Hive中间表中load数据</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> load data <span class="built_in">local</span> inpath <span class="string">&#x27;/home/admin/softwares/data/emp.txt&#x27;</span> into table emp;</span></span><br></pre></td></tr></table></figure>

<p>4.通过insert命令将中间表中的数据导入到Hive关联Hbase的那张表中</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive&gt; insert overwrite table hive_hbase_emp_table select * from emp;</span><br></pre></td></tr></table></figure>

<p>5.查看Hive以及关联的HBase表中是否已经成功的同步插入了数据</p>
<p>Hive：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive&gt; select * from hive_hbase_emp_table;</span><br></pre></td></tr></table></figure>

<p>HBase：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">Hbase&gt;</span><span class="bash"> scan <span class="string">&#x27;hbase_emp_table&#x27;</span></span></span><br></pre></td></tr></table></figure>

<p>案例2：在HBase中已经存储了某一张表hbase_emp_table，然后在Hive中创建一个外部表来关联HBase中的hbase_emp_table这张表，使之可以借助Hive来分析HBase这张表中的数据。</p>
<p>1.在Hive中创建外部表</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CREATE EXTERNAL TABLE relevance_hbase_emp(</span><br><span class="line">empno int,</span><br><span class="line">ename string,</span><br><span class="line">job string,</span><br><span class="line">mgr int,</span><br><span class="line">hiredate string,</span><br><span class="line">sal double,</span><br><span class="line">comm double,</span><br><span class="line">deptno int)</span><br><span class="line">STORED BY </span><br><span class="line">&#x27;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#x27;</span><br><span class="line">WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = </span><br><span class="line">&quot;:key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno&quot;) </span><br><span class="line">TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;hbase_emp_table&quot;);</span><br></pre></td></tr></table></figure>

<p>2.关联后就可以使用Hive函数进行一些分析操作了</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from relevance_hbase_emp;</span><br></pre></td></tr></table></figure>

<h2 id="整合Phoenix"><a href="#整合Phoenix" class="headerlink" title="整合Phoenix"></a>整合Phoenix</h2><h3 id="Phoenix定义"><a href="#Phoenix定义" class="headerlink" title="Phoenix定义"></a>Phoenix定义</h3><p>Phoenix是HBase的开源SQL皮肤。可以使用标准JDBC API代替HBase客户端API来创建表，插入数据和查询HBase数据。</p>
<h3 id="Phoenix特点"><a href="#Phoenix特点" class="headerlink" title="Phoenix特点"></a>Phoenix特点</h3><p>1）容易集成：如Spark，Hive，Pig，Flume和Map Reduce；</p>
<p>2）操作简单：DML命令以及通过DDL命令创建和操作表和版本化增量更改；</p>
<p>3）<strong>支持HBase二级索引创建</strong>。</p>
<h3 id="Phoenix架构"><a href="#Phoenix架构" class="headerlink" title="Phoenix架构"></a>Phoenix架构</h3><p><img src="/Hbase/34.png" alt="34"></p>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>1.官网地址</p>
<p><a href="http://phoenix.apache.org/">http://phoenix.apache.org/</a></p>
<p>2.Phoenix部署</p>
<p>1）上传并解压tar包</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 software]$ tar -zxvf apache-phoenix-5.0.0-HBase-2.0-bin.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 module]$ mv apache-phoenix-5.0.0-HBase-2.0-bin phoenix</span><br></pre></td></tr></table></figure>

<p>2）复制server包并拷贝到各个节点的hbase/lib</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 module]$ cd /opt/module/phoenix/</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 phoenix]$ cp /opt/module/phoenix/phoenix-5.0.0-HBase-2.0-server.jar /opt/module/hbase/lib/</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 phoenix]$ xsync /opt/module/hbase/lib/phoenix-5.0.0-HBase-2.0-server.jar</span><br></pre></td></tr></table></figure>

<p>4）配置环境变量</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">phoenix</span></span><br><span class="line">export PHOENIX_HOME=/opt/module/phoenix</span><br><span class="line">export PHOENIX_CLASSPATH=$PHOENIX_HOME</span><br><span class="line">export PATH=$PATH:$PHOENIX_HOME/bin</span><br></pre></td></tr></table></figure>

<p>5）重启HBase</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 ~]$ stop-hbase.sh</span><br><span class="line">[vincent@linux1 ~]$ start-hbase.sh</span><br></pre></td></tr></table></figure>

<ol start="6">
<li>连接Phoenix</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 phoenix]$ sqlline.py linux1,linux2,linux3:2181</span><br></pre></td></tr></table></figure>

<h3 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h3><h4 id="1-表的操作"><a href="#1-表的操作" class="headerlink" title="1.表的操作"></a>1.表的操作</h4><p>1）显示所有表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator">!</span><span class="keyword">table</span> 或 <span class="operator">!</span>tables</span><br></pre></td></tr></table></figure>

<p>2）创建表</p>
<p>直接指定单个列作为RowKey</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CREATE TABLE IF NOT EXISTS student(</span><br><span class="line">id VARCHAR primary key,</span><br><span class="line">name VARCHAR,</span><br><span class="line">addr VARCHAR</span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<p>在phoenix中，表名等会自动转换为大写，若要小写，使用双引号，如”us_population”。</p>
<p>指定多个列的联合作为RowKey</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CREATE TABLE IF NOT EXISTS &quot;us_population&quot; (</span><br><span class="line">State CHAR(2) NOT NULL,</span><br><span class="line">City VARCHAR NOT NULL,</span><br><span class="line">Population BIGINT</span><br><span class="line">CONSTRAINT my_pk PRIMARY KEY (state, city)</span><br><span class="line">);</span><br><span class="line">//联合主键</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table &quot;test&quot;(</span><br><span class="line">  id varchar primary key,</span><br><span class="line">  &quot;info1&quot;.&quot;name&quot; varchar, </span><br><span class="line">  &quot;info2&quot;.&quot;address&quot; varchar</span><br><span class="line">) split on (&#x27;100&#x27;, &#x27;200&#x27;, &#x27;300&#x27;);</span><br><span class="line">//预分区</span><br><span class="line">//列族</span><br></pre></td></tr></table></figure>

<p>3）插入数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">upsert into student values(&#x27;1001&#x27;,&#x27;zhangsan&#x27;,&#x27;beijing&#x27;);</span><br></pre></td></tr></table></figure>

<p>4）查询记录</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select * from student;</span><br><span class="line">select * from student where id=&#x27;1001&#x27;;</span><br></pre></td></tr></table></figure>

<p>5）删除记录</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">delete from student where id=&#x27;1001&#x27;;</span><br></pre></td></tr></table></figure>

<p>6）删除表</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">drop table student;</span><br></pre></td></tr></table></figure>

<p>7）退出命令行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">!quit</span><br></pre></td></tr></table></figure>

<h4 id="2-表的映射"><a href="#2-表的映射" class="headerlink" title="2.表的映射"></a>2.表的映射</h4><p>1）表的关系</p>
<p>默认情况下，直接在HBase中创建的表，通过Phoenix是查看不到的。如果要在Phoenix中操作直接在HBase中创建的表，则需要在Phoenix中进行表的映射。映射方式有两种：视图映射和表映射。</p>
<p>2）命令行中创建表test</p>
<p>HBase中test的表结构如下，两个列族info1、info2。</p>
<table>
<thead>
<tr>
<th>Rowkey</th>
<th>info1</th>
<th>info2</th>
</tr>
</thead>
<tbody><tr>
<td>id</td>
<td>name</td>
<td>address</td>
</tr>
</tbody></table>
<p>启动HBase Shell</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 ~]$ hbase shell</span><br></pre></td></tr></table></figure>

<p>创建HBase表test</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hbase(main):001:0&gt; create &#x27;test&#x27;,&#x27;info1&#x27;,&#x27;info2&#x27;</span><br></pre></td></tr></table></figure>

<p>3）视图映射</p>
<p>Phoenix创建的视图是只读的，所以只能用来做查询，无法通过视图对源数据进行修改等操作。在phoenix中创建关联test表的视图</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">0: jdbc:phoenix:linux1,linux2,linux3&gt; create view &quot;test&quot;(id varchar primary key,&quot;info1&quot;.&quot;name&quot; varchar, &quot;info2&quot;.&quot;address&quot; varchar);</span><br></pre></td></tr></table></figure>

<p>删除视图</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">0: jdbc:phoenix:linux1,linux2,linux3&gt; drop view &quot;test&quot;;</span><br></pre></td></tr></table></figure>

<p>4）表映射</p>
<p>使用Apache Phoenix创建对HBase的表映射，有两种方法：</p>
<p>（1）HBase中不存在表时，可以直接使用create table指令创建需要的表,系统将会自动在Phoenix和HBase中创建person_infomation的表，并会根据指令内的参数对表结构进行初始化。</p>
<p>（2）当HBase中已经存在表时，可以以类似创建视图的方式创建关联表，只需要将create view改为create table即可。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">0: jdbc:phoenix:linux1,linux2,linux3&gt; create table &quot;test&quot;(id varchar primary key,&quot;info1&quot;.&quot;name&quot; varchar, &quot;info2&quot;.&quot;address&quot; varchar) column_encoded_bytes=0;</span><br></pre></td></tr></table></figure>

<p><strong>Phoenix双引号””：限制小写</strong></p>
<p><strong>Phoenix单引号’’：字符串</strong></p>
<h3 id="Phoenix-JDBC操作"><a href="#Phoenix-JDBC操作" class="headerlink" title="Phoenix JDBC操作"></a>Phoenix JDBC操作</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">jdbc:phoenix:thin:url<span class="operator">=</span>http:<span class="operator">/</span><span class="operator">/</span>linux1:<span class="number">8765</span>;serialization<span class="operator">=</span>PROTOBUF</span><br></pre></td></tr></table></figure>

<h4 id="1）thin连接"><a href="#1）thin连接" class="headerlink" title="1）thin连接"></a>1）thin连接</h4><p>1.启动queryserver</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 ~]$ queryserver.py start</span><br></pre></td></tr></table></figure>

<p>2.创建项目并导入依赖</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.phoenix/phoenix-queryserver-client --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.phoenix<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>phoenix-queryserver-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.0.0-HBase-2.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">  			<span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.httpcomponents/httpclient --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.httpcomponents<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>httpclient<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.3.4<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- https://mvnrepository.com/artifact/com.google.protobuf/protobuf-java --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.google.protobuf<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>protobuf-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.4.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>3.编写代码</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">phoenixTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> url: <span class="type">String</span> = <span class="type">ThinClientUtil</span>.getConnectionUrl(<span class="string">&quot;linux1&quot;</span>, <span class="number">8765</span>)</span><br><span class="line">    <span class="keyword">val</span> conn: <span class="type">Connection</span> = <span class="type">DriverManager</span>.getConnection(url)</span><br><span class="line">    <span class="keyword">val</span> statement: <span class="type">PreparedStatement</span> = conn.prepareStatement(<span class="string">&quot;select * from student&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> set: <span class="type">ResultSet</span> = statement.executeQuery()</span><br><span class="line">    <span class="keyword">while</span>(set.next())&#123;</span><br><span class="line">      println(set.getString(<span class="number">1</span>) + <span class="string">&quot;\t&quot;</span> + set.getString(<span class="number">2</span>)+ <span class="string">&quot;\t&quot;</span> + set.getString(<span class="number">3</span>))</span><br><span class="line">    &#125;</span><br><span class="line">    conn.close()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="2）thick连接"><a href="#2）thick连接" class="headerlink" title="2）thick连接"></a>2）thick连接</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">phoenixTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> url: <span class="type">String</span> = <span class="string">&quot;jdbc:phoenix:linux1,linux2,linux3:2181&quot;</span></span><br><span class="line">    <span class="keyword">val</span> conn: <span class="type">Connection</span> = <span class="type">DriverManager</span>.getConnection(url)</span><br><span class="line">    <span class="keyword">val</span> statement: <span class="type">PreparedStatement</span> = conn.prepareStatement(<span class="string">&quot;select * from student&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> set: <span class="type">ResultSet</span> = statement.executeQuery()</span><br><span class="line">    <span class="keyword">while</span>(set.next())&#123;</span><br><span class="line">      println(set.getString(<span class="number">1</span>) + <span class="string">&quot;\t&quot;</span> + set.getString(<span class="number">2</span>)+ <span class="string">&quot;\t&quot;</span> + set.getString(<span class="number">3</span>))</span><br><span class="line">    &#125;</span><br><span class="line">    conn.close()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Phoenix二级索引"><a href="#Phoenix二级索引" class="headerlink" title="Phoenix二级索引"></a>Phoenix二级索引</h3><p>添加如下配置到HBase的HRegionserver节点的hbase-site.xml</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- phoenix regionserver 配置参数--&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.regionserver.wal.codec<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.region.server.rpc.scheduler.factory.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hbase.ipc.PhoenixRpcSchedulerFactory<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Factory to create the Phoenix RPC Scheduler that uses separate queues for index and metadata updates<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rpc.controllerfactory.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hbase.ipc.controller.ServerRpcControllerFactory<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Factory to create the Phoenix RPC Scheduler that uses separate queues for index and metadata updates<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="全局二级索引-多读少写"><a href="#全局二级索引-多读少写" class="headerlink" title="全局二级索引(多读少写)"></a>全局二级索引(多读少写)</h4><p>select后面的字段中如果出现未加索引的字段，那么整个语句全局扫描。</p>
<p>Global Index是默认的索引格式，创建全局索引时，会在HBase中建立一张新表。也就是说索引数据和数据表是存放在不同的表中的，因此全局索引适用于多读少写的业务场景。</p>
<p>写数据的时候会消耗大量开销，因为索引表也要更新，而索引表是分布在不同的数据节点上的，跨节点的数据传输带来了较大的性能消耗。</p>
<p>在读数据的时候Phoenix会选择索引表来降低查询消耗的时间。</p>
<p>1.创建单个字段的全局索引</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CREATE INDEX my_index ON my_table (my_col);</span><br></pre></td></tr></table></figure>

<p>2.创建携带其他字段的全局索引</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CREATE INDEX my_index ON my_table (v1) INCLUDE (v2);</span><br></pre></td></tr></table></figure>

<h4 id="局部二级索引-多写"><a href="#局部二级索引-多写" class="headerlink" title="局部二级索引(多写)"></a>局部二级索引(多写)</h4><p>只要where后面的字段添加了索引，那么select后面的字段即使没有索引也不会全局扫描。</p>
<p>Local Index适用于写操作频繁的场景。</p>
<p><strong>索引数据和数据表的数据是存放在同一张表中（且是同一个Region）</strong>，避免了在写操作的时候往不同服务器的索引表中写索引带来的额外开销。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CREATE LOCAL INDEX my_index ON my_table (my_column);</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title>MapReduce</title>
    <url>/MapReduce/</url>
    <content><![CDATA[<h1 id="MapReduce概述"><a href="#MapReduce概述" class="headerlink" title="MapReduce概述"></a>MapReduce概述</h1><p><img src="/MapReduce/56.png" alt="56"></p>
<h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><p><img src="/MapReduce/57.png" alt="57"></p>
<p><img src="/MapReduce/58.png" alt="58"></p>
<p><img src="/MapReduce/59.png" alt="59"></p>
<h2 id="核心编程思想"><a href="#核心编程思想" class="headerlink" title="核心编程思想"></a>核心编程思想</h2><p><img src="/MapReduce/60.png" alt="60"></p>
<p>1）分布式的运算程序往往需要分成至少2个阶段。</p>
<p>2）第一个阶段的MapTask并发实例，完全并行运行，互不相干。</p>
<p>3）第二个阶段的ReduceTask并发实例互不相干，但是他们的数据依赖于上一个阶段的所有MapTask并发实例的输出。</p>
<p>4）MapReduce编程模型只能包含一个Map阶段和一个Reduce阶段，如果用户的业务逻辑非常复杂，那就只能多个MapReduce程序，串行运行。</p>
<h2 id="MapReduce进程"><a href="#MapReduce进程" class="headerlink" title="MapReduce进程"></a>MapReduce进程</h2><p><img src="/MapReduce/61.png" alt="61"></p>
<h2 id="常用数据序列化类型"><a href="#常用数据序列化类型" class="headerlink" title="常用数据序列化类型"></a>常用数据序列化类型</h2><table>
<thead>
<tr>
<th>Java类型</th>
<th>Hadoop Writable类型</th>
</tr>
</thead>
<tbody><tr>
<td>Boolean</td>
<td>BooleanWritable</td>
</tr>
<tr>
<td>Byte</td>
<td>ByteWritable</td>
</tr>
<tr>
<td>Int</td>
<td>IntWritable</td>
</tr>
<tr>
<td>Float</td>
<td>FloatWritable</td>
</tr>
<tr>
<td>Long</td>
<td>LongWritable</td>
</tr>
<tr>
<td>Double</td>
<td>DoubleWritable</td>
</tr>
<tr>
<td>String</td>
<td>Text</td>
</tr>
<tr>
<td>Map</td>
<td>MapWritable</td>
</tr>
<tr>
<td>Array</td>
<td>ArrayWritable</td>
</tr>
</tbody></table>
<h2 id="MapReduce编程规范"><a href="#MapReduce编程规范" class="headerlink" title="MapReduce编程规范"></a>MapReduce编程规范</h2><p>用户编写的程序分成三个部分：Mapper、Reducer和Driver</p>
<p><img src="/MapReduce/62.png" alt="62"></p>
<p><img src="/MapReduce/63.png" alt="63"></p>
<h2 id="WordCount案例实操"><a href="#WordCount案例实操" class="headerlink" title="WordCount案例实操"></a>WordCount案例实操</h2><p>1．需求</p>
<p>在给定的文本文件中统计输出每一个单词出现的总次数。</p>
<p><img src="/MapReduce/64.png" alt="64"></p>
<p>创建maven工程</p>
<p>在pom.xml文件中添加如下依赖：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.12<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.logging.log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j-slf4j-impl<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.12.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>在项目的src/main/resources目录下，新建一个文件，命名为“log4j2.xml”，在文件中填入：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">Configuration</span> <span class="attr">status</span>=<span class="string">&quot;error&quot;</span> <span class="attr">strict</span>=<span class="string">&quot;true&quot;</span> <span class="attr">name</span>=<span class="string">&quot;XMLConfig&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">Appenders</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 类型名为Console，名称为必须属性 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">Appender</span> <span class="attr">type</span>=<span class="string">&quot;Console&quot;</span> <span class="attr">name</span>=<span class="string">&quot;STDOUT&quot;</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!-- 布局为PatternLayout的方式，</span></span><br><span class="line"><span class="comment">            输出样式为[INFO] [2018-01-22 17:34:01][org.test.Console]I&#x27;m here --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">Layout</span> <span class="attr">type</span>=<span class="string">&quot;PatternLayout&quot;</span></span></span><br><span class="line"><span class="tag">                    <span class="attr">pattern</span>=<span class="string">&quot;[%p] [%d&#123;yyyy-MM-dd HH:mm:ss&#125;][%c&#123;10&#125;]%m%n&quot;</span> /&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">Appender</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;/<span class="name">Appenders</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">Loggers</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 可加性为false --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">Logger</span> <span class="attr">name</span>=<span class="string">&quot;test&quot;</span> <span class="attr">level</span>=<span class="string">&quot;info&quot;</span> <span class="attr">additivity</span>=<span class="string">&quot;false&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">AppenderRef</span> <span class="attr">ref</span>=<span class="string">&quot;STDOUT&quot;</span> /&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">Logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!-- root loggerConfig设置 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">Root</span> <span class="attr">level</span>=<span class="string">&quot;info&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">AppenderRef</span> <span class="attr">ref</span>=<span class="string">&quot;STDOUT&quot;</span> /&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">Root</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">Loggers</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">Configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>编写Mapper类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.vincent.mr.wordcount;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *  继承Mapper类，指定4个泛型，4个泛型表示两组KV，一组输入，一组输出</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *  输入的KV：</span></span><br><span class="line"><span class="comment"> *  KEYIN: LongWritable, 表示从文件读取数据的偏移量</span></span><br><span class="line"><span class="comment"> *  VALUEIN: Text, 表示从文件读取一行数据</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *  输出的KV：</span></span><br><span class="line"><span class="comment"> *  KEYOUT: Text, 表示一个单词</span></span><br><span class="line"><span class="comment"> *  VALUEOUT: IntWritable, 表示单词出现1次</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> Vincent</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span> 2020-07-01</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordcountMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">    Text k = <span class="keyword">new</span> Text();</span><br><span class="line">    IntWritable v = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * called once for each key/value pairs in the input split</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key   输入的k，偏移量</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value 输入的v，从文件中读取的一行内容</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context   负责调度Mapper运行</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> InterruptedException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取一行</span></span><br><span class="line">        String line = value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 切割</span></span><br><span class="line">        String[] words = line.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 输出</span></span><br><span class="line">        <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line"></span><br><span class="line">            k.set(word);</span><br><span class="line">            context.write(k, v);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>编写Reducer类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.vincent.mr.wordcount;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 自定义Reducer，需要继承Reducer类，指定4个泛型，表示两组KV，一对输入，一对输出</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 输入的kv</span></span><br><span class="line"><span class="comment"> * KEYIN: Text 对应Map输出的k，表示一个单词</span></span><br><span class="line"><span class="comment"> * VALUEIN: IntWritable 对应Map输出的v，表示单词出现的次数（1次）</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 输出的kv</span></span><br><span class="line"><span class="comment"> * KEYOUT: Text 表示一个单词</span></span><br><span class="line"><span class="comment"> * VALUEOUT: IntWritable 表示这个单词出现的总次数</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> Vincent</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span> 2020-07-01</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordcountReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> sum;</span><br><span class="line">    IntWritable v = <span class="keyword">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * This method is called once for each key. Most applications will define</span></span><br><span class="line"><span class="comment">     * their reduce class by overriding this method. The default implementation</span></span><br><span class="line"><span class="comment">     * is an identity function.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key   表示输入的key，这里是一个单词</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> values 表示封装了当前key对应的所有value的一个迭代器对象</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context   负责调度Reducer运行</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> InterruptedException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 累加求和</span></span><br><span class="line">        sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (IntWritable count : values) &#123;</span><br><span class="line">            sum += count.get();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 输出</span></span><br><span class="line">        v.set(sum);</span><br><span class="line">        context.write(key,v);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>编写Driver驱动类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.vincent.mr.wordcount;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> Vincent</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span> 2020-07-01</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordcountDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取配置信息以及封装任务</span></span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        Job job = Job.getInstance(configuration);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 设置jar加载路径</span></span><br><span class="line">        job.setJarByClass(WordcountDriver.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 设置map和reduce类</span></span><br><span class="line">        job.setMapperClass(WordcountMapper.class);</span><br><span class="line">        job.setReducerClass(WordcountReducer.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4 设置map输出</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5 设置最终输出kv类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 6 设置输入和输出路径</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">//本地模式运行</span></span><br><span class="line">        <span class="comment">//FileInputFormat.setInputPaths(job, new Path(&quot;/Users/vincent/Downloads/inputword&quot;));</span></span><br><span class="line">        <span class="comment">//FileOutputFormat.setOutputPath(job, new Path(&quot;/Users/vincent/Downloads/inputword/output&quot;));</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">//动态传参</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 7 提交</span></span><br><span class="line">        <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">        System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>本地测试：直接运行Driver</p>
<p>集群测试：</p>
<p>1.打包jar并放在集群节点中</p>
<p>2.执行命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop jar wc.jar com.vincent.mr.wordcount.WordcountDriver /input /input/wcOutput</span><br></pre></td></tr></table></figure>

<p>wc.jar 是打好的jar包</p>
<h1 id="Hadoop序列化"><a href="#Hadoop序列化" class="headerlink" title="Hadoop序列化"></a>Hadoop序列化</h1><p><img src="/MapReduce/65.png" alt="65"></p>
<p><img src="/MapReduce/66.png" alt="66"></p>
<h2 id="自定义bean对象实现序列化接口（Writable）"><a href="#自定义bean对象实现序列化接口（Writable）" class="headerlink" title="自定义bean对象实现序列化接口（Writable）"></a>自定义bean对象实现序列化接口（Writable）</h2><p>在企业开发中往往常用的基本序列化类型不能满足所有需求，比如在Hadoop框架内部传递一个bean对象，那么该对象就需要实现序列化接口。</p>
<p>Writable接口：</p>
<p>​    write(DataOutput out):    对象序列化</p>
<p>​    readFields(DataInput in):    对象的反序列化</p>
<p>1.实现Writable接口。</p>
<p>2.在类中提供无参数构造器，反序列化时会反射调用。</p>
<p>3.write方法写出的fields顺序与readFields读取的fields顺序要一致。</p>
<p>4.重写toString()方法，一般将各个属性用’\t’分割打印。</p>
<h2 id="序列化案例实操"><a href="#序列化案例实操" class="headerlink" title="序列化案例实操"></a>序列化案例实操</h2><p>统计每一个手机号耗费的上行流量、下行流量、总流量</p>
<p><img src="/MapReduce/67.png" alt="67"></p>
<p>1.数据文件    phone_data.txt</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">7</span> 	<span class="number">13560436666</span>	<span class="number">120.196.100.99</span>		<span class="number">1116</span>		 <span class="number">954</span>			<span class="number">200</span></span><br><span class="line"><span class="attribute">id</span>	手机号码		 网络ip			        上行流量  下行流量   网络状态码</span><br></pre></td></tr></table></figure>

<p>2.期望输出数据格式</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="number">13560436666</span> 		<span class="number">1116		      </span><span class="number">954</span> 			<span class="number">2070</span></span><br><span class="line"><span class="string">手机号码</span>		     <span class="string">上行流量</span>       <span class="string">下行流量</span>	  <span class="string">总流量</span></span><br></pre></td></tr></table></figure>

<p>3.编写MapReduce程序</p>
<p>FlowBean</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.vincent.mapreduce.flowsum;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Writable;</span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> Vincent</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span> 2020-07-01</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">// 1 实现writable接口</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowBean</span> <span class="keyword">implements</span> <span class="title">Writable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> upFlow;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> downFlow;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> sumFlow;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getUpFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setUpFlow</span><span class="params">(<span class="keyword">long</span> upFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getDownFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setDownFlow</span><span class="params">(<span class="keyword">long</span> downFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.downFlow = downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getSumFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setSumFlow</span><span class="params">(<span class="keyword">long</span> sumFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.sumFlow = sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setSumFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.sumFlow = <span class="keyword">this</span>.getUpFlow() + <span class="keyword">this</span>.getDownFlow();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.upFlow + <span class="string">&quot;\t&quot;</span> + <span class="keyword">this</span>.downFlow + <span class="string">&quot;\t&quot;</span> + <span class="keyword">this</span>.sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 无参数构造器，反序列化调用</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        out.writeLong(upFlow);</span><br><span class="line">        out.writeLong(downFlow);</span><br><span class="line">        out.writeLong(sumFlow);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.upFlow = in.readLong();</span><br><span class="line">        <span class="keyword">this</span>.downFlow = in.readLong();</span><br><span class="line">        <span class="keyword">this</span>.sumFlow = in.readLong();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>FlowCountMapper</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.vincent.mapreduce.flowsum;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> Vincent</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span> 2020-07-01</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowCountMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Text outK = <span class="keyword">new</span> Text();</span><br><span class="line">    <span class="keyword">private</span> FlowBean outV = <span class="keyword">new</span> FlowBean();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//1	13244894052	192.168.1.1	www.baidu.com	2000	3452	200</span></span><br><span class="line">        String line = value.toString();</span><br><span class="line"></span><br><span class="line">        String[] splits = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line"></span><br><span class="line">        outK.set(splits[<span class="number">1</span>]);</span><br><span class="line"></span><br><span class="line">        outV.setUpFlow(Long.parseLong(splits[splits.length - <span class="number">3</span>]));</span><br><span class="line">        outV.setDownFlow(Long.parseLong(splits[splits.length - <span class="number">2</span>]));</span><br><span class="line">        outV.setSumFlow();</span><br><span class="line"></span><br><span class="line">        context.write(outK, outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>FlowCountReducer</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.vincent.mapreduce.flowsum;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> Vincent</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span> 2020-07-01</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowCountReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">FlowBean</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> FlowBean outV = <span class="keyword">new</span> FlowBean();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;FlowBean&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">long</span> totalUpFlow = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">long</span> totalDownFlow = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">long</span> totalSumFlow = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (FlowBean value: values) &#123;</span><br><span class="line">            totalUpFlow += value.getUpFlow();</span><br><span class="line">            totalDownFlow += value.getDownFlow();</span><br><span class="line">            totalSumFlow += value.getSumFlow();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        outV.setUpFlow(totalUpFlow);</span><br><span class="line">        outV.setDownFlow(totalDownFlow);</span><br><span class="line">        outV.setSumFlow(totalSumFlow);</span><br><span class="line"></span><br><span class="line">        context.write(key, outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>FlowDriver</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.vincent.mapreduce.flowsum;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> Vincent</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span> 2020-07-01</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        job.setJarByClass(FlowDriver.class);</span><br><span class="line"></span><br><span class="line">        job.setMapperClass(FlowCountMapper.class);</span><br><span class="line">        job.setReducerClass(FlowCountReducer.class);</span><br><span class="line"></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line"></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="MapReduce框架原理"><a href="#MapReduce框架原理" class="headerlink" title="MapReduce框架原理"></a>MapReduce框架原理</h1><h2 id="InputFormat数据输入"><a href="#InputFormat数据输入" class="headerlink" title="InputFormat数据输入"></a>InputFormat数据输入</h2><p><img src="/MapReduce/68.png" alt="68"></p>
<h3 id="切片与MapTask并行度决定机制"><a href="#切片与MapTask并行度决定机制" class="headerlink" title="切片与MapTask并行度决定机制"></a>切片与MapTask并行度决定机制</h3><p>1．问题引出</p>
<p>MapTask的并行度决定Map阶段的任务处理并发度，进而影响到整个Job的处理速度。</p>
<p>思考：1G的数据，启动8个MapTask，可以提高集群的并发处理能力。那么1K的数据，也启动8个MapTask，会提高集群性能吗？MapTask并行任务是否越多越好呢？哪些因素影响了MapTask并行度？</p>
<p>2．MapTask并行度决定机制</p>
<p><strong>数据块</strong>：Block是HDFS物理上把数据分成一块一块。</p>
<p><strong>数据切片</strong>：数据切片只是在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行存储。</p>
<p><img src="/MapReduce/69.png" alt="69"></p>
<h3 id="Job提交流程源码"><a href="#Job提交流程源码" class="headerlink" title="Job提交流程源码"></a>Job提交流程源码</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">waitForCompletion()</span><br><span class="line"></span><br><span class="line">submit();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1建立连接</span></span><br><span class="line">	connect();	</span><br><span class="line">		<span class="comment">// 1）创建提交Job的代理</span></span><br><span class="line">		<span class="keyword">new</span> Cluster(getConfiguration());</span><br><span class="line">			<span class="comment">// （1）判断是本地yarn还是远程</span></span><br><span class="line">			initialize(jobTrackAddr, conf); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 2 提交job</span></span><br><span class="line">submitter.submitJobInternal(Job.<span class="keyword">this</span>, cluster)</span><br><span class="line">	<span class="comment">// 1）创建给集群提交数据的Stag路径</span></span><br><span class="line">	Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf);</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 2）获取jobid ，并创建Job路径</span></span><br><span class="line">	JobID jobId = submitClient.getNewJobID();</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 3）拷贝jar包到集群</span></span><br><span class="line">copyAndConfigureFiles(job, submitJobDir);	</span><br><span class="line">	rUploader.uploadFiles(job, jobSubmitDir);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4）计算切片，生成切片规划文件</span></span><br><span class="line">writeSplits(job, submitJobDir);</span><br><span class="line">		maps = writeNewSplits(job, jobSubmitDir);</span><br><span class="line">		input.getSplits(job);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 5）向Stag路径写XML配置文件</span></span><br><span class="line">writeConf(conf, submitJobFile);</span><br><span class="line">	conf.writeXml(out);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 6）提交Job,返回提交状态</span></span><br><span class="line">status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials());</span><br></pre></td></tr></table></figure>

<p><img src="/MapReduce/70.png" alt="70"></p>
<h3 id="FileInputFormat切片源码解析-input-getSplits-job"><a href="#FileInputFormat切片源码解析-input-getSplits-job" class="headerlink" title="FileInputFormat切片源码解析(input.getSplits(job))"></a>FileInputFormat切片源码解析(input.getSplits(job))</h3><p><img src="/MapReduce/71.png" alt="71"></p>
<h3 id="FileInputFormat切片机制"><a href="#FileInputFormat切片机制" class="headerlink" title="FileInputFormat切片机制"></a>FileInputFormat切片机制</h3><p><strong>切片默认是128M，与块大小相同，如果切片大小大于块大小，那么切片中的另外一块数据就需要通过网络传输到map任务节点，与使用本地数据运行map任务相比，效率则更低。</strong></p>
<p><img src="/MapReduce/72.png" alt="72"></p>
<p><img src="/MapReduce/73.png" alt="73"></p>
<h3 id="FileInputFormat实现类"><a href="#FileInputFormat实现类" class="headerlink" title="FileInputFormat实现类"></a>FileInputFormat实现类</h3><p><img src="/MapReduce/74.png" alt="74"></p>
<h4 id="1-TextInputFormat"><a href="#1-TextInputFormat" class="headerlink" title="1.TextInputFormat"></a>1.TextInputFormat</h4><p><img src="/MapReduce/75.png" alt="75"></p>
<h4 id="2-KeyValueTextInputFormat"><a href="#2-KeyValueTextInputFormat" class="headerlink" title="2.KeyValueTextInputFormat"></a>2.KeyValueTextInputFormat</h4><p><img src="/MapReduce/76.png" alt="76"></p>
<p>KeyValueTextInputFormat案例</p>
<p><img src="/MapReduce/79.png" alt="79"></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KVTextMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">LongWritable</span>&gt;</span>&#123;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">// 1 设置value等于1</span></span><br><span class="line">   LongWritable v = <span class="keyword">new</span> LongWritable(<span class="number">1</span>);  </span><br><span class="line">    </span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Text key, Text value, Context context)</span></span></span><br><span class="line"><span class="function">			<span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">				<span class="comment">// banzhang ni hao</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 2 写出</span></span><br><span class="line">        context.write(key, v);  </span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KVTextReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">LongWritable</span>&gt;</span>&#123;</span><br><span class="line">	</span><br><span class="line">    LongWritable v = <span class="keyword">new</span> LongWritable();  </span><br><span class="line">    </span><br><span class="line">		<span class="meta">@Override</span></span><br><span class="line">		<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;LongWritable&gt; values,	Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">		</span><br><span class="line">		 <span class="keyword">long</span> sum = <span class="number">0L</span>;  </span><br><span class="line"></span><br><span class="line">		 <span class="comment">// 1 汇总统计</span></span><br><span class="line">        <span class="keyword">for</span> (LongWritable value : values) &#123;  </span><br><span class="line">            sum += value.get();  </span><br><span class="line">        &#125;</span><br><span class="line">         </span><br><span class="line">        v.set(sum);  </span><br><span class="line">         </span><br><span class="line">        <span class="comment">// 2 输出</span></span><br><span class="line">        context.write(key, v);  </span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.KeyValueLineRecordReader;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KVTextDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">		</span><br><span class="line">		Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">		<span class="comment">// 设置切割符</span></span><br><span class="line">		conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, <span class="string">&quot; &quot;</span>);</span><br><span class="line">		<span class="comment">// 1 获取job对象</span></span><br><span class="line">		Job job = Job.getInstance(conf);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 2 设置jar包位置，关联mapper和reducer</span></span><br><span class="line">		job.setJarByClass(KVTextDriver.class);</span><br><span class="line">		job.setMapperClass(KVTextMapper.class);</span><br><span class="line">		job.setReducerClass(KVTextReducer.class);</span><br><span class="line">				</span><br><span class="line">		<span class="comment">// 3 设置map输出kv类型</span></span><br><span class="line">		job.setMapOutputKeyClass(Text.class);</span><br><span class="line">		job.setMapOutputValueClass(LongWritable.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 4 设置最终输出kv类型</span></span><br><span class="line">		job.setOutputKeyClass(Text.class);</span><br><span class="line">		job.setOutputValueClass(LongWritable.class);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 5 设置输入数据路径</span></span><br><span class="line">		FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 设置输入格式</span></span><br><span class="line">		job.setInputFormatClass(KeyValueTextInputFormat.class);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 6 设置输出数据路径</span></span><br><span class="line">		FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 7 提交job</span></span><br><span class="line">		job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="3-NLineInputFormat"><a href="#3-NLineInputFormat" class="headerlink" title="3.NLineInputFormat"></a>3.NLineInputFormat</h4><p><img src="/MapReduce/77.png" alt="77"></p>
<p>NLineInputFormat使用案例</p>
<p><img src="/MapReduce/80.png" alt="80"></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NLineMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">LongWritable</span>&gt;</span>&#123;</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">private</span> Text k = <span class="keyword">new</span> Text();</span><br><span class="line">	<span class="keyword">private</span> LongWritable v = <span class="keyword">new</span> LongWritable(<span class="number">1</span>);</span><br><span class="line">	</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span>	<span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">		</span><br><span class="line">		 <span class="comment">// 1 获取一行</span></span><br><span class="line">        String line = value.toString();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 2 切割</span></span><br><span class="line">        String[] splited = line.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 3 循环写出</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; splited.length; i++) &#123;</span><br><span class="line">        	</span><br><span class="line">        	 k.set(splited[i]);</span><br><span class="line">        	</span><br><span class="line">           context.write(k, v);</span><br><span class="line">        &#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NLineReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">LongWritable</span>&gt;</span>&#123;</span><br><span class="line">	</span><br><span class="line">	LongWritable v = <span class="keyword">new</span> LongWritable();</span><br><span class="line">	</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;LongWritable&gt; values,	Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">		</span><br><span class="line">        <span class="keyword">long</span> sum = <span class="number">0l</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 汇总</span></span><br><span class="line">        <span class="keyword">for</span> (LongWritable value : values) &#123;</span><br><span class="line">            sum += value.get();</span><br><span class="line">        &#125;  </span><br><span class="line">        </span><br><span class="line">        v.set(sum);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 2 输出</span></span><br><span class="line">        context.write(key, v);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.net.URISyntaxException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.NLineInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NLineDriver</span> </span>&#123;</span><br><span class="line">	</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, URISyntaxException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">		</span><br><span class="line">		 		<span class="comment">// 输入输出路径需要根据自己电脑上实际的输入输出路径设置</span></span><br><span class="line">		 		args = <span class="keyword">new</span> String[] &#123; <span class="string">&quot;e:/input/inputword&quot;</span>, <span class="string">&quot;e:/output1&quot;</span> &#125;;</span><br><span class="line"></span><br><span class="line">		 		<span class="comment">// 1 获取job对象</span></span><br><span class="line">		 		Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        Job job = Job.getInstance(configuration);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 2 设置每个切片InputSplit中划分三条记录</span></span><br><span class="line">        NLineInputFormat.setNumLinesPerSplit(job, <span class="number">3</span>);</span><br><span class="line">          </span><br><span class="line">        <span class="comment">// 3 使用NLineInputFormat处理记录数  </span></span><br><span class="line">        job.setInputFormatClass(NLineInputFormat.class);  </span><br><span class="line">          </span><br><span class="line">        <span class="comment">// 4 设置jar包位置，关联mapper和reducer</span></span><br><span class="line">        job.setJarByClass(NLineDriver.class);  </span><br><span class="line">        job.setMapperClass(NLineMapper.class);  </span><br><span class="line">        job.setReducerClass(NLineReducer.class);  </span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 5 设置map输出kv类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);  </span><br><span class="line">        job.setMapOutputValueClass(LongWritable.class);  </span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 6 设置最终输出kv类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);  </span><br><span class="line">        job.setOutputValueClass(LongWritable.class);  </span><br><span class="line">          </span><br><span class="line">        <span class="comment">// 7 设置输入输出数据路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));  </span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));  </span><br><span class="line">          </span><br><span class="line">        <span class="comment">// 8 提交job</span></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);  </span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="4-CombineTextInputFormat"><a href="#4-CombineTextInputFormat" class="headerlink" title="4.CombineTextInputFormat"></a>4.CombineTextInputFormat</h4><p>框架默认的TextInputFormat切片机制是对任务按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个MapTask，这样如果有大量小文件，就会产生大量的MapTask，处理效率极其低下。</p>
<p>1、应用场景：</p>
<p><strong>CombineTextInputFormat用于小文件过多的场景</strong>，它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个MapTask处理。</p>
<p>2、虚拟存储切片最大值设置</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">CombineTextInputFormat.setMaxInputSplitSize(job, <span class="number">4194304</span>);	<span class="comment">// 4M</span></span><br></pre></td></tr></table></figure>

<p>注意：虚拟存储切片最大值设置最好根据实际的小文件大小情况来设置具体的值。</p>
<p>3、切片机制</p>
<p>生成切片过程包括：虚拟存储过程和切片过程二部分。</p>
<p><img src="/MapReduce/78.png" alt="78"></p>
<p>CombineTextInputFormat案例</p>
<p>WordcountDriver:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 如果不设置InputFormat，它默认用的是TextInputFormat.class</span></span><br><span class="line">job.setInputFormatClass(CombineTextInputFormat.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">//虚拟存储切片最大值设置4m</span></span><br><span class="line">CombineTextInputFormat.setMaxInputSplitSize(job, <span class="number">4194304</span>);</span><br></pre></td></tr></table></figure>

<h4 id="5-自定义InputFormat"><a href="#5-自定义InputFormat" class="headerlink" title="5.自定义InputFormat"></a>5.自定义InputFormat</h4><p><img src="/MapReduce/81.png" alt="81"></p>
<p>无论HDFS还是MapReduce，在处理小文件时效率都非常低，但又难免面临处理大量小文件的场景，此时，就需要有相应解决方案，可以<strong>自定义InputFormat实现小文件的合并</strong>。</p>
<p>案例实操：</p>
<p>将多个小文件合并成一个SequenceFile文件（SequenceFile文件是Hadoop用来存储二进制形式的key-value对的文件格式），SequenceFile里面存储着多个文件，存储的形式为文件路径+名称为key，文件内容为value。</p>
<p>输入为三个小文件：one.txt    two.txt    three.txt</p>
<p>输出为一个SequenceFile文件：part-r-0000</p>
<p><img src="/MapReduce/82.png" alt="82"></p>
<p>自定义InputFromat</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.BytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.InputSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.JobContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.RecordReader;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义类继承FileInputFormat</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WholeFileInputformat</span> <span class="keyword">extends</span> <span class="title">FileInputFormat</span>&lt;<span class="title">Text</span>, <span class="title">BytesWritable</span>&gt;</span>&#123;</span><br><span class="line">	</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">boolean</span> <span class="title">isSplitable</span><span class="params">(JobContext context, Path filename)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> RecordReader&lt;Text, BytesWritable&gt; <span class="title">createRecordReader</span><span class="params">(InputSplit split, TaskAttemptContext context)</span>	<span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">		</span><br><span class="line">    <span class="comment">// 创建自定义RecordReader对象</span></span><br><span class="line">		WholeRecordReader recordReader = <span class="keyword">new</span> WholeRecordReader();	</span><br><span class="line">    </span><br><span class="line">		recordReader.initialize(split, context);</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">return</span> recordReader;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>自定义RecordReader类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataInputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.BytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.InputSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.RecordReader;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WholeRecordReader</span> <span class="keyword">extends</span> <span class="title">RecordReader</span>&lt;<span class="title">Text</span>, <span class="title">BytesWritable</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">private</span> Configuration configuration;</span><br><span class="line">	<span class="keyword">private</span> FileSplit split;</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">boolean</span> isProgress= <span class="keyword">true</span>;</span><br><span class="line">	<span class="keyword">private</span> BytesWritable value = <span class="keyword">new</span> BytesWritable();</span><br><span class="line">	<span class="keyword">private</span> Text k = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">(InputSplit split, TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">this</span>.split = (FileSplit)split;</span><br><span class="line">		configuration = context.getConfiguration();</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">nextKeyValue</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">if</span> (isProgress) &#123;</span><br><span class="line"></span><br><span class="line">			<span class="comment">// 1 定义缓存区</span></span><br><span class="line">			<span class="keyword">byte</span>[] contents = <span class="keyword">new</span> <span class="keyword">byte</span>[(<span class="keyword">int</span>)split.getLength()];</span><br><span class="line">			</span><br><span class="line">			FileSystem fs = <span class="keyword">null</span>;</span><br><span class="line">			FSDataInputStream fis = <span class="keyword">null</span>;</span><br><span class="line">			</span><br><span class="line">			<span class="keyword">try</span> &#123;</span><br><span class="line">				<span class="comment">// 2 获取文件系统</span></span><br><span class="line">				Path path = split.getPath();</span><br><span class="line">				fs = path.getFileSystem(configuration);</span><br><span class="line">				</span><br><span class="line">				<span class="comment">// 3 读取数据</span></span><br><span class="line">				fis = fs.open(path);</span><br><span class="line">				</span><br><span class="line">				<span class="comment">// 4 读取文件内容</span></span><br><span class="line">				IOUtils.readFully(fis, contents, <span class="number">0</span>, contents.length);</span><br><span class="line">				</span><br><span class="line">				<span class="comment">// 5 输出文件内容</span></span><br><span class="line">				value.set(contents, <span class="number">0</span>, contents.length);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 6 获取文件路径及名称</span></span><br><span class="line">        String name = split.getPath().toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 7 设置输出的key值</span></span><br><span class="line">        k.set(name);</span><br><span class="line"></span><br><span class="line">      &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line"></span><br><span class="line">      &#125;<span class="keyword">finally</span> &#123;</span><br><span class="line">        IOUtils.closeStream(fis);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      isProgress = <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Text <span class="title">getCurrentKey</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> k;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> BytesWritable <span class="title">getCurrentValue</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> value;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">float</span> <span class="title">getProgress</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>编写SequenceFileMapper类处理流程</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.BytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SequenceFileMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">Text</span>, <span class="title">BytesWritable</span>, <span class="title">Text</span>, <span class="title">BytesWritable</span>&gt;</span>&#123;</span><br><span class="line">	</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Text key, BytesWritable value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">		context.write(key, value);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>编写SequenceFileReducer类处理流程</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.BytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SequenceFileReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">BytesWritable</span>, <span class="title">Text</span>, <span class="title">BytesWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;BytesWritable&gt; values, Context context)</span>		<span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">		context.write(key, values.iterator().next());</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>编写SequenceFileDriver类处理流程</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.BytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SequenceFileDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">		</span><br><span class="line">       <span class="comment">// 输入输出路径需要根据自己电脑上实际的输入输出路径设置</span></span><br><span class="line">		args = <span class="keyword">new</span> String[] &#123; <span class="string">&quot;e:/input/inputinputformat&quot;</span>, <span class="string">&quot;e:/output1&quot;</span> &#125;;</span><br><span class="line"></span><br><span class="line">       <span class="comment">// 获取job对象</span></span><br><span class="line">		Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">		Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">       <span class="comment">// 设置jar包存储位置、关联自定义的mapper和reducer</span></span><br><span class="line">		job.setJarByClass(SequenceFileDriver.class);</span><br><span class="line">		job.setMapperClass(SequenceFileMapper.class);</span><br><span class="line">		job.setReducerClass(SequenceFileReducer.class);</span><br><span class="line"></span><br><span class="line">       <span class="comment">// 设置输入的inputFormat</span></span><br><span class="line">		job.setInputFormatClass(WholeFileInputformat.class);</span><br><span class="line"></span><br><span class="line">       <span class="comment">// 设置输出的outputFormat</span></span><br><span class="line">	  job.setOutputFormatClass(SequenceFileOutputFormat.class);</span><br><span class="line">       </span><br><span class="line">		   <span class="comment">// 设置map输出端的kv类型</span></span><br><span class="line">		job.setMapOutputKeyClass(Text.class);</span><br><span class="line">		job.setMapOutputValueClass(BytesWritable.class);</span><br><span class="line">		</span><br><span class="line">       <span class="comment">// 设置最终输出端的kv类型</span></span><br><span class="line">		job.setOutputKeyClass(Text.class);</span><br><span class="line">		job.setOutputValueClass(BytesWritable.class);</span><br><span class="line"></span><br><span class="line">       <span class="comment">// 设置输入输出路径</span></span><br><span class="line">		FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">		FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">       <span class="comment">// 提交job</span></span><br><span class="line">		<span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">		System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="MapReduce工作流程"><a href="#MapReduce工作流程" class="headerlink" title="MapReduce工作流程"></a>MapReduce工作流程</h2><p><img src="/MapReduce/83.png" alt="83"></p>
<p><img src="/MapReduce/84.png" alt="84"></p>
<p><strong>Map阶段：溢写过程中对key的索引进行快排（第一次）对溢写文件进行归并（第二次）</strong>combiner对每一个溢写文件相同key的值进行聚合</p>
<p><strong>Reduce阶段：归并（第三次）和分组排序</strong></p>
<p>上面的流程是整个MapReduce最全工作流程，但是Shuffle过程只是从第7步开始到第16步结束，具体Shuffle过程详解，如下：</p>
<p>1）MapTask收集我们的map()方法输出的kv对，放到内存缓冲区中</p>
<p>2）从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件</p>
<p>3）多个溢出文件会被合并成大的溢出文件</p>
<p>4）在溢出过程及合并的过程中，都要调用Partitioner进行分区和针对key进行排序</p>
<p>5）ReduceTask根据自己的分区号，去各个MapTask机器上取相应的结果分区数据</p>
<p>6）ReduceTask会取到同一个分区的来自不同MapTask的结果文件，ReduceTask会将这些文件再进行<strong>合并（归并排序）</strong></p>
<p>7）合并成大文件后，Shuffle的过程也就结束了，后面进入ReduceTask的逻辑运算过程（从文件中取出一个一个的键值对Group，调用用户自定义的reduce()方法）</p>
<p>注意</p>
<p>Shuffle中的缓冲区大小会影响到MapReduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快。</p>
<p>缓冲区的大小可以通过参数调整，参数：io.sort.mb默认100M。</p>
<h2 id="Shuffle机制"><a href="#Shuffle机制" class="headerlink" title="Shuffle机制"></a>Shuffle机制</h2><p>Map方法之后，Reduce方法之前的数据处理过程称之为Shuffle。</p>
<p><img src="/MapReduce/85.png" alt="85"></p>
<h3 id="Partition分区"><a href="#Partition分区" class="headerlink" title="Partition分区"></a>Partition分区</h3><p><img src="/MapReduce/86.png" alt="86"></p>
<p><img src="/MapReduce/87.png" alt="87"></p>
<p><img src="/MapReduce/89.png" alt="89"></p>
<p>案例实操</p>
<p>手机号136、137、138、139开头都分别放到一个独立的4个文件中，其他开头的放到一个文件中。</p>
<p><img src="/MapReduce/88.png" alt="88"></p>
<p>在序列化案例的基础上增加分区类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Partitioner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProvincePartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Text key, FlowBean value, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 1 获取电话号码的前三位</span></span><br><span class="line">		String preNum = key.toString().substring(<span class="number">0</span>, <span class="number">3</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">int</span> partition = <span class="number">5</span>;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 2 判断是哪个省</span></span><br><span class="line">		<span class="keyword">if</span> (<span class="string">&quot;136&quot;</span>.equals(preNum)) &#123;</span><br><span class="line">			partition = <span class="number">0</span>;</span><br><span class="line">		&#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;137&quot;</span>.equals(preNum)) &#123;</span><br><span class="line">			partition = <span class="number">1</span>;</span><br><span class="line">		&#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;138&quot;</span>.equals(preNum)) &#123;</span><br><span class="line">			partition = <span class="number">2</span>;</span><br><span class="line">		&#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;139&quot;</span>.equals(preNum)) &#123;</span><br><span class="line">			partition = <span class="number">3</span>;</span><br><span class="line">		&#125;<span class="keyword">else</span> &#123;</span><br><span class="line">      partition = <span class="number">4</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> partition;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在驱动函数中增加自定义数据分区设置和ReduceTask设置</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowsumDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IllegalArgumentException, IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 输入输出路径需要根据自己电脑上实际的输入输出路径设置</span></span><br><span class="line">		args = <span class="keyword">new</span> String[]&#123;<span class="string">&quot;e:/output1&quot;</span>,<span class="string">&quot;e:/output2&quot;</span>&#125;;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 1 获取配置信息，或者job对象实例</span></span><br><span class="line">		Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">		Job job = Job.getInstance(configuration);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 2 指定本程序的jar包所在的本地路径</span></span><br><span class="line">		job.setJarByClass(FlowsumDriver.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 3 指定本业务job要使用的mapper/Reducer业务类</span></span><br><span class="line">		job.setMapperClass(FlowCountMapper.class);</span><br><span class="line">		job.setReducerClass(FlowCountReducer.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 4 指定mapper输出数据的kv类型</span></span><br><span class="line">		job.setMapOutputKeyClass(Text.class);</span><br><span class="line">		job.setMapOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 5 指定最终输出的数据的kv类型</span></span><br><span class="line">		job.setOutputKeyClass(Text.class);</span><br><span class="line">		job.setOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 8 指定自定义数据分区</span></span><br><span class="line">		job.setPartitionerClass(ProvincePartitioner.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 9 同时指定相应数量的reduce task</span></span><br><span class="line">		job.setNumReduceTasks(<span class="number">5</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 6 指定job的输入原始文件所在目录</span></span><br><span class="line">		FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">		FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 7 将job中配置的相关参数，以及job所用的java类所在的jar包， 提交给yarn去运行</span></span><br><span class="line">		<span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">		System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="WritableComparable排序"><a href="#WritableComparable排序" class="headerlink" title="WritableComparable排序"></a>WritableComparable排序</h3><p><img src="/MapReduce/90.png" alt="90"></p>
<p><img src="/MapReduce/91.png" alt="91"></p>
<p><img src="/MapReduce/92.png" alt="92"></p>
<p>自定义排序WritableComparable</p>
<p>bean对象做为key传输，需要实现WritableComparable接口重写compareTo方法，就可以实现排序。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(FlowBean bean)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">int</span> result;</span><br><span class="line">		</span><br><span class="line">	<span class="comment">// 按照总流量大小，倒序排列</span></span><br><span class="line">	<span class="keyword">if</span> (sumFlow &gt; bean.getSumFlow()) &#123;</span><br><span class="line">		result = -<span class="number">1</span>;</span><br><span class="line">	&#125;<span class="keyword">else</span> <span class="keyword">if</span> (sumFlow &lt; bean.getSumFlow()) &#123;</span><br><span class="line">		result = <span class="number">1</span>;</span><br><span class="line">	&#125;<span class="keyword">else</span> &#123;</span><br><span class="line">		result = <span class="number">0</span>;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="WritableComparable排序案例实操（全排序）"><a href="#WritableComparable排序案例实操（全排序）" class="headerlink" title="WritableComparable排序案例实操（全排序）"></a>WritableComparable排序案例实操（全排序）</h3><p><img src="/MapReduce/93.png" alt="93"></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 实现 WritableComparable接口</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowBean</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">long</span> upFlow;</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">long</span> downFlow;</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">long</span> sumFlow;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 反序列化时，需要反射调用空参构造函数，所以必须有</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">super</span>();</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">(<span class="keyword">long</span> upFlow, <span class="keyword">long</span> downFlow)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">super</span>();</span><br><span class="line">		<span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line">		<span class="keyword">this</span>.downFlow = downFlow;</span><br><span class="line">		<span class="keyword">this</span>.sumFlow = upFlow + downFlow;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">set</span><span class="params">(<span class="keyword">long</span> upFlow, <span class="keyword">long</span> downFlow)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line">		<span class="keyword">this</span>.downFlow = downFlow;</span><br><span class="line">		<span class="keyword">this</span>.sumFlow = upFlow + downFlow;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getSumFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> sumFlow;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setSumFlow</span><span class="params">(<span class="keyword">long</span> sumFlow)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.sumFlow = sumFlow;</span><br><span class="line">	&#125;	</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setSumFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.sumFlow = <span class="keyword">this</span>.getUpFlow() + <span class="keyword">this</span>.getDownFlow();</span><br><span class="line">	&#125;	</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getUpFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> upFlow;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setUpFlow</span><span class="params">(<span class="keyword">long</span> upFlow)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getDownFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> downFlow;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setDownFlow</span><span class="params">(<span class="keyword">long</span> downFlow)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.downFlow = downFlow;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * 序列化方法</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> out</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">		out.writeLong(upFlow);</span><br><span class="line">		out.writeLong(downFlow);</span><br><span class="line">		out.writeLong(sumFlow);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * 反序列化方法 注意反序列化的顺序和序列化的顺序完全一致</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> in</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">		upFlow = in.readLong();</span><br><span class="line">		downFlow = in.readLong();</span><br><span class="line">		sumFlow = in.readLong();</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> upFlow + <span class="string">&quot;\t&quot;</span> + downFlow + <span class="string">&quot;\t&quot;</span> + sumFlow;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(FlowBean bean)</span> </span>&#123;</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">int</span> result;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 按照总流量大小，倒序排列</span></span><br><span class="line">		<span class="keyword">if</span> (sumFlow &gt; bean.getSumFlow()) &#123;</span><br><span class="line">			result = -<span class="number">1</span>;</span><br><span class="line">		&#125;<span class="keyword">else</span> <span class="keyword">if</span> (sumFlow &lt; bean.getSumFlow()) &#123;</span><br><span class="line">			result = <span class="number">1</span>;</span><br><span class="line">		&#125;<span class="keyword">else</span> &#123;</span><br><span class="line">			result = <span class="number">0</span>;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> result;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowCountSortMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>, <span class="title">Text</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">	FlowBean bean = <span class="keyword">new</span> FlowBean();</span><br><span class="line">	Text v = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span>	<span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 1 获取一行</span></span><br><span class="line">		String line = value.toString();</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 2 截取</span></span><br><span class="line">		String[] fields = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 3 封装对象</span></span><br><span class="line">		String phoneNbr = fields[<span class="number">0</span>];</span><br><span class="line">		<span class="keyword">long</span> upFlow = Long.parseLong(fields[<span class="number">1</span>]);</span><br><span class="line">		<span class="keyword">long</span> downFlow = Long.parseLong(fields[<span class="number">2</span>]);</span><br><span class="line">		</span><br><span class="line">		bean.set(upFlow, downFlow);</span><br><span class="line">		v.set(phoneNbr);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 4 输出</span></span><br><span class="line">		context.write(bean, v);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowCountSortReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">FlowBean</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(FlowBean key, Iterable&lt;Text&gt; values, Context context)</span>	<span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 循环输出，避免总流量相同情况</span></span><br><span class="line">    <span class="comment">// 遍历每一个手机号</span></span><br><span class="line">		<span class="keyword">for</span> (Text text : values) &#123;</span><br><span class="line">			context.write(text, key);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowCountSortDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ClassNotFoundException, IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 输入输出路径需要根据自己电脑上实际的输入输出路径设置</span></span><br><span class="line">		args = <span class="keyword">new</span> String[]&#123;<span class="string">&quot;e:/output1&quot;</span>,<span class="string">&quot;e:/output2&quot;</span>&#125;;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 1 获取配置信息，或者job对象实例</span></span><br><span class="line">		Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">		Job job = Job.getInstance(configuration);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 2 指定本程序的jar包所在的本地路径</span></span><br><span class="line">		job.setJarByClass(FlowCountSortDriver.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 3 指定本业务job要使用的mapper/Reducer业务类</span></span><br><span class="line">		job.setMapperClass(FlowCountSortMapper.class);</span><br><span class="line">		job.setReducerClass(FlowCountSortReducer.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 4 指定mapper输出数据的kv类型</span></span><br><span class="line">		job.setMapOutputKeyClass(FlowBean.class);</span><br><span class="line">		job.setMapOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 5 指定最终输出的数据的kv类型</span></span><br><span class="line">		job.setOutputKeyClass(Text.class);</span><br><span class="line">		job.setOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 6 指定job的输入原始文件所在目录</span></span><br><span class="line">		FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">		FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 7 将job中配置的相关参数，以及job所用的java类所在的jar包， 提交给yarn去运行</span></span><br><span class="line">		<span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">		System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="WritableComparable排序案例实操（区内排序）"><a href="#WritableComparable排序案例实操（区内排序）" class="headerlink" title="WritableComparable排序案例实操（区内排序）"></a>WritableComparable排序案例实操（区内排序）</h3><p>要求每个省份手机号输出的文件中按照总流量内部排序</p>
<p><img src="/MapReduce/94.png" alt="94"></p>
<p>增加自定义分区类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Partitioner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProvincePartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">FlowBean</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(FlowBean key, Text value, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 1 获取手机号码前三位</span></span><br><span class="line">		String preNum = value.toString().substring(<span class="number">0</span>, <span class="number">3</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">int</span> partition = <span class="number">4</span>;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 2 根据手机号归属地设置分区</span></span><br><span class="line">		<span class="keyword">if</span> (<span class="string">&quot;136&quot;</span>.equals(preNum)) &#123;</span><br><span class="line">			partition = <span class="number">0</span>;</span><br><span class="line">		&#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;137&quot;</span>.equals(preNum)) &#123;</span><br><span class="line">			partition = <span class="number">1</span>;</span><br><span class="line">		&#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;138&quot;</span>.equals(preNum)) &#123;</span><br><span class="line">			partition = <span class="number">2</span>;</span><br><span class="line">		&#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;139&quot;</span>.equals(preNum)) &#123;</span><br><span class="line">			partition = <span class="number">3</span>;</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      partition = <span class="number">4</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> partition;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在驱动类中添加分区类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 加载自定义分区类</span></span><br><span class="line">job.setPartitionerClass(ProvincePartitioner.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Reducetask个数</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">5</span>);</span><br></pre></td></tr></table></figure>

<h3 id="Combiner合并"><a href="#Combiner合并" class="headerlink" title="Combiner合并"></a>Combiner合并</h3><p><img src="/MapReduce/95.png" alt="95"></p>
<p>Combiner合并案例实操</p>
<p><img src="/MapReduce/96.png" alt="96"></p>
<p>方案一</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordcountCombiner</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">IntWritable v = <span class="keyword">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1 汇总</span></span><br><span class="line">		<span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">for</span>(IntWritable value :values)&#123;</span><br><span class="line">			sum += value.get();</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		v.set(sum);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 2 写出</span></span><br><span class="line">		context.write(key, v);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 指定需要使用combiner，以及用哪个类作为combiner的逻辑</span></span><br><span class="line">job.setCombinerClass(WordcountCombiner.class);</span><br></pre></td></tr></table></figure>

<p>方案二</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 指定需要使用Combiner，以及用哪个类作为Combiner的逻辑</span></span><br><span class="line">job.setCombinerClass(WordcountReducer.class);</span><br></pre></td></tr></table></figure>

<h3 id="GroupingComparator分组（辅助排序）"><a href="#GroupingComparator分组（辅助排序）" class="headerlink" title="GroupingComparator分组（辅助排序）"></a>GroupingComparator分组（辅助排序）</h3><p>对Reduce阶段的数据根据某一个或几个字段进行分组。</p>
<p>分组排序步骤：</p>
<p>（1）自定义类继承WritableComparator</p>
<p>（2）重写compare()方法</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(WritableComparable a, WritableComparable b)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 比较的业务逻辑</span></span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> result;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（3）创建一个构造将比较对象的类传给父类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="title">OrderGroupingComparator</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">super</span>(OrderBean.class, <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>案例实操</p>
<p>求出每一个订单中最贵的商品</p>
<table>
<thead>
<tr>
<th>订单id</th>
<th>商品id</th>
<th>成交金额</th>
</tr>
</thead>
<tbody><tr>
<td>0000001</td>
<td>Pdt_01</td>
<td>222.8</td>
</tr>
<tr>
<td></td>
<td>Pdt_02</td>
<td>33.8</td>
</tr>
<tr>
<td>0000002</td>
<td>Pdt_03</td>
<td>522.8</td>
</tr>
<tr>
<td></td>
<td>Pdt_04</td>
<td>122.4</td>
</tr>
<tr>
<td></td>
<td>Pdt_05</td>
<td>722.4</td>
</tr>
<tr>
<td>0000003</td>
<td>Pdt_06</td>
<td>232.8</td>
</tr>
<tr>
<td></td>
<td>Pdt_02</td>
<td>33.8</td>
</tr>
</tbody></table>
<p>期望输出</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">1</span>	<span class="number">222</span>.<span class="number">82</span>	<span class="number">722</span>.<span class="number">43</span>	<span class="number">232</span>.<span class="number">8</span></span><br></pre></td></tr></table></figure>

<p>（1）利用“订单id和成交金额”作为key，可以将Map阶段读取到的所有订单数据按照id升序排序，如果id相同再按照金额降序排序，发送到Reduce。</p>
<p>（2）在Reduce端利用groupingComparator将订单id相同的kv聚合成组，然后取第一个即是该订单中最贵商品，如图所示。</p>
<p><img src="/MapReduce/97.png" alt="97"></p>
<p>定义订单信息OrderBean类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderBean</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">OrderBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">int</span> order_id; <span class="comment">// 订单id号</span></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">double</span> price; <span class="comment">// 价格</span></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="title">OrderBean</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">super</span>();</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="title">OrderBean</span><span class="params">(<span class="keyword">int</span> order_id, <span class="keyword">double</span> price)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">super</span>();</span><br><span class="line">		<span class="keyword">this</span>.order_id = order_id;</span><br><span class="line">		<span class="keyword">this</span>.price = price;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">		out.writeInt(order_id);</span><br><span class="line">		out.writeDouble(price);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">		order_id = in.readInt();</span><br><span class="line">		price = in.readDouble();</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> order_id + <span class="string">&quot;\t&quot;</span> + price;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getOrder_id</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> order_id;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setOrder_id</span><span class="params">(<span class="keyword">int</span> order_id)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.order_id = order_id;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">double</span> <span class="title">getPrice</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> price;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setPrice</span><span class="params">(<span class="keyword">double</span> price)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.price = price;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 二次排序</span></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(OrderBean o)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">int</span> result;</span><br><span class="line">		</span><br><span class="line">    <span class="comment">// 订单编号升序排列</span></span><br><span class="line">		<span class="keyword">if</span> (order_id &gt; o.getOrder_id()) &#123;</span><br><span class="line">			result = <span class="number">1</span>;</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (order_id &lt; o.getOrder_id()) &#123;</span><br><span class="line">			result = -<span class="number">1</span>;</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			<span class="comment">// 价格倒序排序</span></span><br><span class="line">			result = price &gt; o.getPrice() ? -<span class="number">1</span> : <span class="number">1</span>;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> result;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>编写OrderSortMapper类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">OrderBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">	OrderBean k = <span class="keyword">new</span> OrderBean();</span><br><span class="line">	</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 1 获取一行</span></span><br><span class="line">		String line = value.toString();</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 2 截取</span></span><br><span class="line">		String[] fields = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 3 封装对象</span></span><br><span class="line">		k.setOrder_id(Integer.parseInt(fields[<span class="number">0</span>]));</span><br><span class="line">		k.setPrice(Double.parseDouble(fields[<span class="number">2</span>]));</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 4 写出</span></span><br><span class="line">		context.write(k, NullWritable.get());</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>编写OrderSortGroupingComparator类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparator;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderGroupingComparator</span> <span class="keyword">extends</span> <span class="title">WritableComparator</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="title">OrderGroupingComparator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">super</span>(OrderBean.class, <span class="keyword">true</span>);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(WritableComparable a, WritableComparable b)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">		OrderBean aBean = (OrderBean) a;</span><br><span class="line">		OrderBean bBean = (OrderBean) b;</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">int</span> result;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 只比较Order_id，相同的Order_id进入一个组</span></span><br><span class="line">		<span class="keyword">if</span> (aBean.getOrder_id() &gt; bBean.getOrder_id()) &#123;</span><br><span class="line">			result = <span class="number">1</span>;</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (aBean.getOrder_id() &lt; bBean.getOrder_id()) &#123;</span><br><span class="line">			result = -<span class="number">1</span>;</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			result = <span class="number">0</span>;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> aBean.compareTo(bBean);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>编写OrderSortReducer类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">OrderBean</span>, <span class="title">NullWritable</span>, <span class="title">OrderBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(OrderBean key, Iterable&lt;NullWritable&gt; values, Context context)</span>		<span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">		</span><br><span class="line">		context.write(key, NullWritable.get());</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>编写OrderSortDriver类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception, IOException </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输入输出路径需要根据自己电脑上实际的输入输出路径设置</span></span><br><span class="line">		args  = <span class="keyword">new</span> String[]&#123;<span class="string">&quot;e:/input/inputorder&quot;</span> , <span class="string">&quot;e:/output1&quot;</span>&#125;;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 1 获取配置信息</span></span><br><span class="line">		Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">		Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 2 设置jar包加载路径</span></span><br><span class="line">		job.setJarByClass(OrderDriver.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 3 加载map/reduce类</span></span><br><span class="line">		job.setMapperClass(OrderMapper.class);</span><br><span class="line">		job.setReducerClass(OrderReducer.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 4 设置map输出数据key和value类型</span></span><br><span class="line">		job.setMapOutputKeyClass(OrderBean.class);</span><br><span class="line">		job.setMapOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 5 设置最终输出数据的key和value类型</span></span><br><span class="line">		job.setOutputKeyClass(OrderBean.class);</span><br><span class="line">		job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 6 设置输入数据和输出数据路径</span></span><br><span class="line">		FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">		FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 8 设置reduce端的分组</span></span><br><span class="line">	  job.setGroupingComparatorClass(OrderGroupingComparator.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 7 提交</span></span><br><span class="line">		<span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">		System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="MapTask工作机制"><a href="#MapTask工作机制" class="headerlink" title="MapTask工作机制"></a>MapTask工作机制</h2><p><img src="/MapReduce/98.png" alt="98"></p>
<p>​    （1）Read阶段：MapTask通过用户编写的RecordReader，<strong>从输入InputSplit中解析出一个个key/value</strong>。</p>
<p>​    （2）Map阶段：该节点主要是将解析出的key/value交给用户编写map()函数处理，并<strong>产生一系列新的key/value</strong>。</p>
<p>​    （3）Collect收集阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的key/value分区（调用Partitioner），并写入一个环形内存缓冲区中。</p>
<p>​    （4）Spill阶段：即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。</p>
<p>​    溢写阶段详情：</p>
<p>​    步骤1：利用<strong>快速排序算法</strong>对缓存区内的数据进行排序，排序方式是，<strong>先按照分区编号Partition进行排序，然后按照key进行排序</strong>。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序。</p>
<p>​    步骤2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件<strong>output/spillN.out</strong>（N表示当前溢写次数）中。<strong>如果用户设置了Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作</strong>。</p>
<p>​    步骤3：将分区数据的元信息写到内存索引数据结构<strong>SpillRecord</strong>中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB，则将内存索引写到文件output/spillN.out.index中。</p>
<p>​    （5）Combine阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最<strong>终只会生成一个数据文件</strong>。</p>
<p>​    当所有数据处理完后，MapTask会将所有临时文件合并成一个大文件，并保存到文件<strong>output/file.out</strong>中，同时生成相应的索引文件<strong>output/file.out.index</strong>。</p>
<p>​    在进行文件合并过程中，MapTask以分区为单位进行合并。对于每个分区，它将采用多轮递归合并的方式。每轮合并io.sort.factor（默认10）个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。</p>
<p>​    <strong>让每个MapTask最终只生成一个数据文件</strong>，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。</p>
<h2 id="ReduceTask工作机制"><a href="#ReduceTask工作机制" class="headerlink" title="ReduceTask工作机制"></a>ReduceTask工作机制</h2><p><img src="/MapReduce/99.png" alt="99"></p>
<p>​    （1）Copy阶段：ReduceTask从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。</p>
<p>​    （2）Merge阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对<strong>内存和磁盘</strong>上的文件进行合并，以防止内存使用过多或磁盘上文件过多。</p>
<p>​    （3）Sort阶段：按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次<strong>归并排序</strong>即可。</p>
<p>​    （4）Reduce阶段：reduce()函数将计算结果写到HDFS上。</p>
<p>设置ReduceTask并行度（个数）</p>
<p>ReduceTask的并行度同样影响整个Job的执行并发度和执行效率，但与<strong>MapTask的并发数由切片数决定</strong>不同，<strong>ReduceTask数量的决定是可以直接手动设置</strong>：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 默认值是1，手动设置为4</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">4</span>);</span><br></pre></td></tr></table></figure>

<p><img src="/MapReduce/100.png" alt="100"></p>
<h2 id="OutputFormat数据输出"><a href="#OutputFormat数据输出" class="headerlink" title="OutputFormat数据输出"></a>OutputFormat数据输出</h2><h3 id="OutputFormat接口实现类"><a href="#OutputFormat接口实现类" class="headerlink" title="OutputFormat接口实现类"></a>OutputFormat接口实现类</h3><p><img src="/MapReduce/101.png" alt="101"></p>
<h3 id="自定义OutputFormat"><a href="#自定义OutputFormat" class="headerlink" title="自定义OutputFormat"></a>自定义OutputFormat</h3><p><img src="/MapReduce/102.png" alt="102"></p>
<p>案例实操</p>
<p><img src="/MapReduce/103.png" alt="103"></p>
<p>编写FilterMapper类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FilterMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt;</span>&#123;</span><br><span class="line">	</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span>	<span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 写出</span></span><br><span class="line">		context.write(value, NullWritable.get());</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>编写FilterReducer类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FilterReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">		Text k = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;NullWritable&gt; values, Context context)</span>		<span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">       <span class="comment">// 1 获取一行</span></span><br><span class="line">		String line = key.toString();</span><br><span class="line"></span><br><span class="line">       <span class="comment">// 2 拼接</span></span><br><span class="line">		line = line + <span class="string">&quot;\n&quot;</span>;</span><br><span class="line"></span><br><span class="line">       <span class="comment">// 3 设置key</span></span><br><span class="line">    k.set(line);</span><br><span class="line"></span><br><span class="line">       <span class="comment">// 4 输出</span></span><br><span class="line">		context.write(k, NullWritable.get());</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>自定义一个OutputFormat类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.RecordWriter;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FilterOutputFormat</span> <span class="keyword">extends</span> <span class="title">FileOutputFormat</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> RecordWriter&lt;Text, NullWritable&gt; <span class="title">getRecordWriter</span><span class="params">(TaskAttemptContext job)</span>			<span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 创建一个RecordWriter</span></span><br><span class="line">		<span class="keyword">return</span> <span class="keyword">new</span> FilterRecordWriter(job);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>编写RecordWriter类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataOutputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.RecordWriter;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FilterRecordWriter</span> <span class="keyword">extends</span> <span class="title">RecordWriter</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">	FSDataOutputStream atguiguOut = <span class="keyword">null</span>;</span><br><span class="line">	FSDataOutputStream otherOut = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="title">FilterRecordWriter</span><span class="params">(TaskAttemptContext job)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 1 获取文件系统</span></span><br><span class="line">		FileSystem fs;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">try</span> &#123;</span><br><span class="line">			fs = FileSystem.get(job.getConfiguration());</span><br><span class="line"></span><br><span class="line">			<span class="comment">// 2 创建输出文件路径</span></span><br><span class="line">			Path atguiguPath = <span class="keyword">new</span> Path(<span class="string">&quot;e:/atguigu.log&quot;</span>);</span><br><span class="line">			Path otherPath = <span class="keyword">new</span> Path(<span class="string">&quot;e:/other.log&quot;</span>);</span><br><span class="line"></span><br><span class="line">			<span class="comment">// 3 创建输出流</span></span><br><span class="line">			atguiguOut = fs.create(atguiguPath);</span><br><span class="line">			otherOut = fs.create(otherPath);</span><br><span class="line">		&#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">			e.printStackTrace();</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(Text key, NullWritable value)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 判断是否包含“atguigu”输出到不同文件</span></span><br><span class="line">		<span class="keyword">if</span> (key.toString().contains(<span class="string">&quot;atguigu&quot;</span>)) &#123;</span><br><span class="line">			atguiguOut.write(key.toString().getBytes());</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			otherOut.write(key.toString().getBytes());</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">(TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 关闭资源</span></span><br><span class="line">		IOUtils.closeStream(atguiguOut);</span><br><span class="line">		IOUtils.closeStream(otherOut);	</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>编写FilterDriver类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FilterDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 输入输出路径需要根据自己电脑上实际的输入输出路径设置</span></span><br><span class="line">		args = <span class="keyword">new</span> String[] &#123; <span class="string">&quot;e:/input/inputoutputformat&quot;</span>, <span class="string">&quot;e:/output2&quot;</span> &#125;;</span><br><span class="line"></span><br><span class="line">		Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">		Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">		job.setJarByClass(FilterDriver.class);</span><br><span class="line">		job.setMapperClass(FilterMapper.class);</span><br><span class="line">		job.setReducerClass(FilterReducer.class);</span><br><span class="line"></span><br><span class="line">		job.setMapOutputKeyClass(Text.class);</span><br><span class="line">		job.setMapOutputValueClass(NullWritable.class);</span><br><span class="line">		</span><br><span class="line">		job.setOutputKeyClass(Text.class);</span><br><span class="line">		job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 要将自定义的输出格式组件设置到job中</span></span><br><span class="line">		job.setOutputFormatClass(FilterOutputFormat.class);</span><br><span class="line"></span><br><span class="line">		FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 虽然我们自定义了outputformat，但是因为我们的outputformat继承自fileoutputformat</span></span><br><span class="line">		<span class="comment">// 而fileoutputformat要输出一个_SUCCESS文件，所以，在这还得指定一个输出目录</span></span><br><span class="line">		FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">		<span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">		System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Join多种应用"><a href="#Join多种应用" class="headerlink" title="Join多种应用"></a>Join多种应用</h2><h3 id="Reduce-Join"><a href="#Reduce-Join" class="headerlink" title="Reduce Join"></a>Reduce Join</h3><p><img src="/MapReduce/104.png" alt="104"></p>
<p>案例实操</p>
<p>将商品信息表中数据根据商品pid合并到订单数据表中</p>
<p><img src="/MapReduce/105.png" alt="105"></p>
<p>创建商品和订合并后的Bean类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Writable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 封装  order.txt  及  pd.txt中的字段信息</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderBean</span> <span class="keyword">implements</span> <span class="title">Writable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String orderId;  <span class="comment">//订单id</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String pid ;     <span class="comment">//商品id</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Integer amount ; <span class="comment">//购买数量</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String pname ;   <span class="comment">// 商品名字</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String flag ;    <span class="comment">// 用于标记数据来自于哪个文件</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getOrderId</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> orderId;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setOrderId</span><span class="params">(String orderId)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.orderId = orderId;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getPid</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> pid;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setPid</span><span class="params">(String pid)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.pid = pid;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">getAmount</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> amount;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setAmount</span><span class="params">(Integer amount)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.amount = amount;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getPname</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> pname;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setPname</span><span class="params">(String pname)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.pname = pname;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getFlag</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> flag;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setFlag</span><span class="params">(String flag)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.flag = flag;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">OrderBean</span><span class="params">()</span></span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        out.writeUTF(orderId);</span><br><span class="line">        out.writeUTF(pid);</span><br><span class="line">        out.writeInt(amount);</span><br><span class="line">        out.writeUTF(pname);</span><br><span class="line">        out.writeUTF(flag);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.orderId = in.readUTF();</span><br><span class="line">        <span class="keyword">this</span>.pid = in.readUTF();</span><br><span class="line">        <span class="keyword">this</span>.amount = in.readInt();</span><br><span class="line">        <span class="keyword">this</span>.pname = in.readUTF();</span><br><span class="line">        <span class="keyword">this</span>.flag = in.readUTF();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">       <span class="keyword">return</span> <span class="keyword">this</span>.orderId + <span class="string">&quot;\t&quot;</span> + <span class="keyword">this</span>.pname + <span class="string">&quot;\t&quot;</span> + <span class="keyword">this</span>.amount;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>编写ReduceJoinMapper类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.InputSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReduceJoinMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>,<span class="title">Text</span>,<span class="title">OrderBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String currentSplitFileName ;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> OrderBean outV = <span class="keyword">new</span> OrderBean() ;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Text outK = <span class="keyword">new</span> Text() ;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 在MapTask执行开始时执行一次</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * 获取当前处理的切片对应的文件是哪个</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">       <span class="comment">//获取当前的切片对象</span></span><br><span class="line">        InputSplit inputSplit = context.getInputSplit();</span><br><span class="line">       <span class="comment">//转换成FileSplit</span></span><br><span class="line">        FileSplit currentSplit = (FileSplit)inputSplit;</span><br><span class="line">       <span class="comment">//获取当前处理的文件名</span></span><br><span class="line">        currentSplitFileName = currentSplit.getPath().getName();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">//处理一行数据</span></span><br><span class="line">        String line  = value.toString();</span><br><span class="line">        String[] split = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">        <span class="keyword">if</span>(currentSplitFileName.contains(<span class="string">&quot;order&quot;</span>))&#123;</span><br><span class="line">            <span class="comment">//数据来自于 order.txt</span></span><br><span class="line">            <span class="comment">//  1001	01	1</span></span><br><span class="line">            <span class="comment">//封装key</span></span><br><span class="line">            outK.set(split[<span class="number">1</span>]);</span><br><span class="line">            <span class="comment">//封装value</span></span><br><span class="line">            outV.setOrderId(split[<span class="number">0</span>]);</span><br><span class="line">            outV.setPid(split[<span class="number">1</span>]);</span><br><span class="line">            outV.setAmount(Integer.parseInt(split[<span class="number">2</span>]));</span><br><span class="line">            outV.setPname(<span class="string">&quot;&quot;</span>);</span><br><span class="line">            outV.setFlag(<span class="string">&quot;order&quot;</span>);</span><br><span class="line"></span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="comment">//数据来自于 pd.txt</span></span><br><span class="line">            <span class="comment">// 01	小米</span></span><br><span class="line">            <span class="comment">//封装key</span></span><br><span class="line">            outK.set(split[<span class="number">0</span>]);</span><br><span class="line">            <span class="comment">//封装value</span></span><br><span class="line">            outV.setPid(split[<span class="number">0</span>]);</span><br><span class="line">            outV.setPname(split[<span class="number">1</span>]);</span><br><span class="line">            outV.setOrderId(<span class="string">&quot;&quot;</span>);</span><br><span class="line">            outV.setAmount(<span class="number">0</span>);</span><br><span class="line">            outV.setFlag(<span class="string">&quot;pd&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//写出</span></span><br><span class="line">        context.write(outK,outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>编写ReduceJoinReducer类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.commons.beanutils.BeanUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.lang.reflect.InvocationTargetException;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReduceJoinReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>,<span class="title">OrderBean</span>,<span class="title">OrderBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//定义存储Order数据的OrderBean集合</span></span><br><span class="line">    List&lt;OrderBean&gt; orders = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//定义OrderBean， 存储pd的数据</span></span><br><span class="line">    OrderBean pdBean = <span class="keyword">new</span> OrderBean();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;OrderBean&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">//pid相同的数据会进入到一个reduce方法</span></span><br><span class="line">        <span class="comment">//  1001	01	1</span></span><br><span class="line">        <span class="comment">//  1004	01	4</span></span><br><span class="line">        <span class="comment">//  01	小米</span></span><br><span class="line">        <span class="comment">//思路: 将所有order的数据全部都获取到，保存到一个集合中. 把pd的数据获取到，保存到一个对象中</span></span><br><span class="line">        <span class="comment">//      迭代保存order数据的集合， 获取到每个order数据的OrderBean对象， 把Pd对象中的						//			pname设置到每个order数据的OrderBean对象中</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (OrderBean orderBean : values) &#123;</span><br><span class="line">          <span class="keyword">if</span>(<span class="string">&quot;order&quot;</span>.equals(orderBean.getFlag()))&#123;</span><br><span class="line">              <span class="comment">//order数据</span></span><br><span class="line">              <span class="comment">//深拷贝</span></span><br><span class="line">              OrderBean currentOrderbean = <span class="keyword">new</span> OrderBean();</span><br><span class="line">              <span class="keyword">try</span> &#123;</span><br><span class="line">                  BeanUtils.copyProperties(currentOrderbean,orderBean);</span><br><span class="line">              &#125; <span class="keyword">catch</span> (IllegalAccessException e) &#123;</span><br><span class="line">                  e.printStackTrace();</span><br><span class="line">              &#125; <span class="keyword">catch</span> (InvocationTargetException e) &#123;</span><br><span class="line">                  e.printStackTrace();</span><br><span class="line">              &#125;</span><br><span class="line">              orders.add(currentOrderbean);</span><br><span class="line">          &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">              <span class="comment">//pd数据</span></span><br><span class="line">              <span class="keyword">try</span> &#123;</span><br><span class="line">                  BeanUtils.copyProperties(pdBean,orderBean);</span><br><span class="line">              &#125; <span class="keyword">catch</span> (IllegalAccessException e) &#123;</span><br><span class="line">                  e.printStackTrace();</span><br><span class="line">              &#125; <span class="keyword">catch</span> (InvocationTargetException e) &#123;</span><br><span class="line">                  e.printStackTrace();</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//写出</span></span><br><span class="line">        <span class="keyword">for</span> (OrderBean orderBean : orders) &#123;</span><br><span class="line">            <span class="comment">// 给OrderBean的pname赋值</span></span><br><span class="line">            orderBean.setPname(pdBean.getPname());</span><br><span class="line"></span><br><span class="line">            context.write(orderBean,NullWritable.get());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//清空集合</span></span><br><span class="line">        orders.clear();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>编写ReduceJoinDriver类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReduceJoinDriver</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">//1. 创建Job对象</span></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line">        <span class="comment">//2. 关联jar</span></span><br><span class="line">        job.setJarByClass(ReduceJoinDriver.class);</span><br><span class="line">        <span class="comment">//3. 关联mapper和reducer</span></span><br><span class="line">        job.setMapperClass(ReduceJoinMapper.class);</span><br><span class="line">        job.setReducerClass(ReduceJoinReducer.class);</span><br><span class="line">        <span class="comment">//4. 设置map输出的k 和 v的类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(OrderBean.class);</span><br><span class="line">        <span class="comment">//5. 设置最终输出的k 和 v的类型</span></span><br><span class="line">        job.setOutputKeyClass(OrderBean.class);</span><br><span class="line">        job.setOutputValueClass(NullWritable.class);</span><br><span class="line">        <span class="comment">//6. 设置输入和输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job,<span class="keyword">new</span> Path(<span class="string">&quot;D:/input/inputtable&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job,<span class="keyword">new</span> Path(<span class="string">&quot;D:/output&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//7. 提交job</span></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Map-Join"><a href="#Map-Join" class="headerlink" title="Map Join"></a>Map Join</h3><p>Map Join适用于一张表十分小、一张表很大的场景。</p>
<p>思考：在Reduce端处理过多的表，非常容易产生数据倾斜。怎么办？</p>
<p>在Map端缓存多张表，提前处理业务逻辑，这样增加Map端业务，减少Reduce端数据的压力，尽可能的减少数据倾斜。</p>
<p><strong>具体办法：采用DistributedCache</strong></p>
<p>​    （1）在Mapper的setup阶段，将文件读取到缓存集合中。</p>
<p>​    （2）在驱动函数中加载缓存。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 缓存普通文件到Task运行节点。</span></span><br><span class="line">job.addCacheFile(<span class="keyword">new</span> URI(<span class="string">&quot;file://e:/cache/pd.txt&quot;</span>));</span><br></pre></td></tr></table></figure>

<p><img src="/MapReduce/106.png" alt="106"></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataInputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> java.io.BufferedReader;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.io.InputStreamReader;</span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * MapJoin的思想:</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *    将小表的文件提前加载到内存中，接下来每读取一条大表的数据，就与内存中的小表的数据进行join,join完成后直接写出.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MapJoinMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>,<span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Map&lt;String ,String&gt; pdMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Text outK = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        context.getCounter(<span class="string">&quot;Map Join&quot;</span>,<span class="string">&quot;setup&quot;</span>).increment(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将小表的数据加载到内存中</span></span><br><span class="line">        <span class="comment">// 获取在driver中设置的缓存文件</span></span><br><span class="line">        URI[] cacheFiles = context.getCacheFiles();</span><br><span class="line">        URI currentCacheFile = cacheFiles[<span class="number">0</span>];</span><br><span class="line">        <span class="comment">//读取文件</span></span><br><span class="line">        FileSystem fs = FileSystem.get(context.getConfiguration());</span><br><span class="line">        <span class="comment">//获取输入流</span></span><br><span class="line">        FSDataInputStream in = fs.open(<span class="keyword">new</span> Path(currentCacheFile.getPath()));</span><br><span class="line">        <span class="comment">//一次读取文件中的一行数据</span></span><br><span class="line">        BufferedReader reader  = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(in));</span><br><span class="line">        String line ;</span><br><span class="line">        <span class="keyword">while</span>((line = reader.readLine())!=<span class="keyword">null</span>)&#123;</span><br><span class="line">            <span class="comment">//切割</span></span><br><span class="line">            <span class="comment">// 01	小米</span></span><br><span class="line">            String[] split = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">            pdMap.put(split[<span class="number">0</span>],split[<span class="number">1</span>]);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//pdMap : 01 - 小米    02 - 华为  03 - 格力</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 计数器</span></span><br><span class="line">        context.getCounter(<span class="string">&quot;Map Join&quot;</span>,<span class="string">&quot;map&quot;</span>).increment(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 读取大表的数据</span></span><br><span class="line">        String line = value.toString() ;</span><br><span class="line">        <span class="comment">// 切割</span></span><br><span class="line">        <span class="comment">// 1001	01	1</span></span><br><span class="line">        String[] split = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">        String currentPanme = pdMap.get(split[<span class="number">1</span>]);</span><br><span class="line">        String resultLine = split[<span class="number">0</span>] +<span class="string">&quot;\t&quot;</span> +currentPanme +<span class="string">&quot;\t&quot;</span> +split[<span class="number">2</span>];</span><br><span class="line"></span><br><span class="line">        <span class="comment">//封装key</span></span><br><span class="line">        outK.set(resultLine);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//写出</span></span><br><span class="line">        context.write(outK, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"><span class="keyword">import</span> java.net.URISyntaxException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MapJoinDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, URISyntaxException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        Job job  = Job.getInstance(conf);</span><br><span class="line">        <span class="comment">//设置缓存文件</span></span><br><span class="line">        job.addCacheFile(<span class="keyword">new</span> URI(<span class="string">&quot;file:///D:/input/inputcache/pd.txt&quot;</span>));</span><br><span class="line">        <span class="comment">// job.addCacheFile();  可以设置多个缓存文件</span></span><br><span class="line">        <span class="comment">//.....</span></span><br><span class="line">        job.setJarByClass(MapJoinDriver.class);</span><br><span class="line">        job.setMapperClass(MapJoinMapper.class);</span><br><span class="line"></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置ReduceTask的个数为0</span></span><br><span class="line">        job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">        FileInputFormat.setInputPaths(job,<span class="keyword">new</span> Path(<span class="string">&quot;d:/input/inputtable2&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job,<span class="keyword">new</span> Path(<span class="string">&quot;d:/output3&quot;</span>));</span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="计数器应用"><a href="#计数器应用" class="headerlink" title="计数器应用"></a>计数器应用</h2><p><img src="/MapReduce/107.png" alt="107"></p>
<h2 id="数据清洗（ETL）"><a href="#数据清洗（ETL）" class="headerlink" title="数据清洗（ETL）"></a>数据清洗（ETL）</h2><p>在运行核心业务MapReduce程序之前，往往要先对数据进行清洗，清理掉不符合用户要求的数据。<strong>清理的过程往往只需要运行Mapper程序，不需要运行Reduce程序。</strong></p>
<p>需求：去除日志中字段个数小于等于11的日志。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt;</span>&#123;</span><br><span class="line">	</span><br><span class="line">	Text k = <span class="keyword">new</span> Text();</span><br><span class="line">	</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 1 获取1行数据</span></span><br><span class="line">		String line = value.toString();</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 2 解析日志</span></span><br><span class="line">		<span class="keyword">boolean</span> result = parseLog(line,context);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 3 日志不合法退出</span></span><br><span class="line">		<span class="keyword">if</span> (!result) &#123;</span><br><span class="line">			<span class="keyword">return</span>;</span><br><span class="line">		&#125;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 4 设置key</span></span><br><span class="line">		k.set(line);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 5 写出数据</span></span><br><span class="line">		context.write(k, NullWritable.get());</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 2 解析日志</span></span><br><span class="line">	<span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">parseLog</span><span class="params">(String line, Context context)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 1 截取</span></span><br><span class="line">		String[] fields = line.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 2 日志长度大于11的为合法</span></span><br><span class="line">		<span class="keyword">if</span> (fields.length &gt; <span class="number">11</span>) &#123;</span><br><span class="line"></span><br><span class="line">			<span class="comment">// 系统计数器</span></span><br><span class="line">			context.getCounter(<span class="string">&quot;map&quot;</span>, <span class="string">&quot;true&quot;</span>).increment(<span class="number">1</span>);</span><br><span class="line">			<span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">		&#125;<span class="keyword">else</span> &#123;</span><br><span class="line">			context.getCounter(<span class="string">&quot;map&quot;</span>, <span class="string">&quot;false&quot;</span>).increment(<span class="number">1</span>);</span><br><span class="line">			<span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>编写LogDriver类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 输入输出路径需要根据自己电脑上实际的输入输出路径设置</span></span><br><span class="line">    args = <span class="keyword">new</span> String[] &#123; <span class="string">&quot;e:/input/inputlog&quot;</span>, <span class="string">&quot;e:/output1&quot;</span> &#125;;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 1 获取job信息</span></span><br><span class="line">		Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">		Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 2 加载jar包</span></span><br><span class="line">		job.setJarByClass(LogDriver.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 3 关联map</span></span><br><span class="line">		job.setMapperClass(LogMapper.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 4 设置最终输出类型</span></span><br><span class="line">		job.setOutputKeyClass(Text.class);</span><br><span class="line">		job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 设置reducetask个数为0</span></span><br><span class="line">		job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 5 设置输入和输出路径</span></span><br><span class="line">		FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">		FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 6 提交</span></span><br><span class="line">		job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>需求：对Web访问日志中的各字段识别切分，去除日志中不合法的记录。根据清洗规则，输出过滤后的数据。</p>
<p>定义一个bean，用来记录日志数据中的各数据字段</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogBean</span> </span>&#123;</span><br><span class="line">	<span class="keyword">private</span> String remote_addr;<span class="comment">// 记录客户端的ip地址</span></span><br><span class="line">	<span class="keyword">private</span> String remote_user;<span class="comment">// 记录客户端用户名称,忽略属性&quot;-&quot;</span></span><br><span class="line">	<span class="keyword">private</span> String time_local;<span class="comment">// 记录访问时间与时区</span></span><br><span class="line">	<span class="keyword">private</span> String request;<span class="comment">// 记录请求的url与http协议</span></span><br><span class="line">	<span class="keyword">private</span> String status;<span class="comment">// 记录请求状态；成功是200</span></span><br><span class="line">	<span class="keyword">private</span> String body_bytes_sent;<span class="comment">// 记录发送给客户端文件主体内容大小</span></span><br><span class="line">	<span class="keyword">private</span> String http_referer;<span class="comment">// 用来记录从那个页面链接访问过来的</span></span><br><span class="line">	<span class="keyword">private</span> String http_user_agent;<span class="comment">// 记录客户浏览器的相关信息</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">boolean</span> valid = <span class="keyword">true</span>;<span class="comment">// 判断数据是否合法</span></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> String <span class="title">getRemote_addr</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> remote_addr;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setRemote_addr</span><span class="params">(String remote_addr)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.remote_addr = remote_addr;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> String <span class="title">getRemote_user</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> remote_user;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setRemote_user</span><span class="params">(String remote_user)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.remote_user = remote_user;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> String <span class="title">getTime_local</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> time_local;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setTime_local</span><span class="params">(String time_local)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.time_local = time_local;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> String <span class="title">getRequest</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> request;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setRequest</span><span class="params">(String request)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.request = request;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> String <span class="title">getStatus</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> status;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setStatus</span><span class="params">(String status)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.status = status;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> String <span class="title">getBody_bytes_sent</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> body_bytes_sent;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setBody_bytes_sent</span><span class="params">(String body_bytes_sent)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.body_bytes_sent = body_bytes_sent;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> String <span class="title">getHttp_referer</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> http_referer;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setHttp_referer</span><span class="params">(String http_referer)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.http_referer = http_referer;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> String <span class="title">getHttp_user_agent</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> http_user_agent;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setHttp_user_agent</span><span class="params">(String http_user_agent)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.http_user_agent = http_user_agent;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isValid</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> valid;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setValid</span><span class="params">(<span class="keyword">boolean</span> valid)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.valid = valid;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">		StringBuilder sb = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">		sb.append(<span class="keyword">this</span>.valid);</span><br><span class="line">		sb.append(<span class="string">&quot;\001&quot;</span>).append(<span class="keyword">this</span>.remote_addr);</span><br><span class="line">		sb.append(<span class="string">&quot;\001&quot;</span>).append(<span class="keyword">this</span>.remote_user);</span><br><span class="line">		sb.append(<span class="string">&quot;\001&quot;</span>).append(<span class="keyword">this</span>.time_local);</span><br><span class="line">		sb.append(<span class="string">&quot;\001&quot;</span>).append(<span class="keyword">this</span>.request);</span><br><span class="line">		sb.append(<span class="string">&quot;\001&quot;</span>).append(<span class="keyword">this</span>.status);</span><br><span class="line">		sb.append(<span class="string">&quot;\001&quot;</span>).append(<span class="keyword">this</span>.body_bytes_sent);</span><br><span class="line">		sb.append(<span class="string">&quot;\001&quot;</span>).append(<span class="keyword">this</span>.http_referer);</span><br><span class="line">		sb.append(<span class="string">&quot;\001&quot;</span>).append(<span class="keyword">this</span>.http_user_agent);</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">return</span> sb.toString();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>编写LogMapper类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt;</span>&#123;</span><br><span class="line">	Text k = <span class="keyword">new</span> Text();</span><br><span class="line">	</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span>	<span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 1 获取1行</span></span><br><span class="line">		String line = value.toString();</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 2 解析日志是否合法</span></span><br><span class="line">		LogBean bean = parseLog(line);</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">if</span> (!bean.isValid()) &#123;</span><br><span class="line">			<span class="keyword">return</span>;</span><br><span class="line">		&#125;</span><br><span class="line">		</span><br><span class="line">		k.set(bean.toString());</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 3 输出</span></span><br><span class="line">		context.write(k, NullWritable.get());</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 解析日志</span></span><br><span class="line">	<span class="function"><span class="keyword">private</span> LogBean <span class="title">parseLog</span><span class="params">(String line)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">		LogBean logBean = <span class="keyword">new</span> LogBean();</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 1 截取</span></span><br><span class="line">		String[] fields = line.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">if</span> (fields.length &gt; <span class="number">11</span>) &#123;</span><br><span class="line"></span><br><span class="line">			<span class="comment">// 2封装数据</span></span><br><span class="line">			logBean.setRemote_addr(fields[<span class="number">0</span>]);</span><br><span class="line">			logBean.setRemote_user(fields[<span class="number">1</span>]);</span><br><span class="line">			logBean.setTime_local(fields[<span class="number">3</span>].substring(<span class="number">1</span>));</span><br><span class="line">			logBean.setRequest(fields[<span class="number">6</span>]);</span><br><span class="line">			logBean.setStatus(fields[<span class="number">8</span>]);</span><br><span class="line">			logBean.setBody_bytes_sent(fields[<span class="number">9</span>]);</span><br><span class="line">			logBean.setHttp_referer(fields[<span class="number">10</span>]);</span><br><span class="line">			</span><br><span class="line">			<span class="keyword">if</span> (fields.length &gt; <span class="number">12</span>) &#123;</span><br><span class="line">				logBean.setHttp_user_agent(fields[<span class="number">11</span>] + <span class="string">&quot; &quot;</span>+ fields[<span class="number">12</span>]);</span><br><span class="line">			&#125;<span class="keyword">else</span> &#123;</span><br><span class="line">				logBean.setHttp_user_agent(fields[<span class="number">11</span>]);</span><br><span class="line">			&#125;</span><br><span class="line">			</span><br><span class="line">			<span class="comment">// 大于400，HTTP错误</span></span><br><span class="line">			<span class="keyword">if</span> (Integer.parseInt(logBean.getStatus()) &gt;= <span class="number">400</span>) &#123;</span><br><span class="line">				logBean.setValid(<span class="keyword">false</span>);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;<span class="keyword">else</span> &#123;</span><br><span class="line">			logBean.setValid(<span class="keyword">false</span>);</span><br><span class="line">		&#125;</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">return</span> logBean;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>编写LogDriver类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogDriver</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 1 获取job信息</span></span><br><span class="line">		Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">		Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 2 加载jar包</span></span><br><span class="line">		job.setJarByClass(LogDriver.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 3 关联map</span></span><br><span class="line">		job.setMapperClass(LogMapper.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 4 设置最终输出类型</span></span><br><span class="line">		job.setOutputKeyClass(Text.class);</span><br><span class="line">		job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 5 设置输入和输出路径</span></span><br><span class="line">		FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">		FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 6 提交</span></span><br><span class="line">		job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="MapReduce开发总结"><a href="#MapReduce开发总结" class="headerlink" title="MapReduce开发总结"></a>MapReduce开发总结</h2><p><img src="/MapReduce/108.png" alt="108"></p>
<p><img src="/MapReduce/109.png" alt="109"></p>
<p><img src="/MapReduce/110.png" alt="110"></p>
<p><img src="/MapReduce/111.png" alt="111"></p>
<p><img src="/MapReduce/112.png" alt="112"></p>
<h1 id="Hadoop数据压缩"><a href="#Hadoop数据压缩" class="headerlink" title="Hadoop数据压缩"></a>Hadoop数据压缩</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p><img src="/MapReduce/113.png" alt="113"></p>
<p><img src="/MapReduce/114.png" alt="114"></p>
<h2 id="MR支持的压缩编码"><a href="#MR支持的压缩编码" class="headerlink" title="MR支持的压缩编码"></a>MR支持的压缩编码</h2><table>
<thead>
<tr>
<th>压缩格式</th>
<th>hadoop自带？</th>
<th>算法</th>
<th>文件扩展名</th>
<th>是否可切分</th>
<th>换成压缩格式后，原来的程序是否需要修改</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>是，直接使用</td>
<td>DEFLATE</td>
<td>.deflate</td>
<td>否</td>
<td>和文本处理一样，不需要修改</td>
</tr>
<tr>
<td>Gzip</td>
<td>是，直接使用</td>
<td>DEFLATE</td>
<td>.gz</td>
<td>否</td>
<td>和文本处理一样，不需要修改</td>
</tr>
<tr>
<td>bzip2</td>
<td>是，直接使用</td>
<td>bzip2</td>
<td>.bz2</td>
<td>是</td>
<td>和文本处理一样，不需要修改</td>
</tr>
<tr>
<td>LZO</td>
<td>否，需要安装</td>
<td>LZO</td>
<td>.lzo</td>
<td>是</td>
<td>需要建索引，还需要指定输入格式</td>
</tr>
<tr>
<td>Snappy</td>
<td>否，需要安装</td>
<td>Snappy</td>
<td>.snappy</td>
<td>否</td>
<td>和文本处理一样，不需要修改</td>
</tr>
</tbody></table>
<p>为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示：</p>
<table>
<thead>
<tr>
<th>压缩格式</th>
<th>对应的编码/解码器</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
</tr>
<tr>
<td>gzip</td>
<td>org.apache.hadoop.io.compress.GzipCodec</td>
</tr>
<tr>
<td>bzip2</td>
<td>org.apache.hadoop.io.compress.BZip2Codec</td>
</tr>
<tr>
<td>LZO</td>
<td>com.hadoop.compression.lzo.LzopCodec</td>
</tr>
<tr>
<td>Snappy</td>
<td>org.apache.hadoop.io.compress.SnappyCodec</td>
</tr>
</tbody></table>
<p>压缩性能的比较：</p>
<table>
<thead>
<tr>
<th>压缩算法</th>
<th>原始文件大小</th>
<th>压缩文件大小</th>
<th>压缩速度</th>
<th>解压速度</th>
</tr>
</thead>
<tbody><tr>
<td>gzip</td>
<td>8.3GB</td>
<td>1.8GB</td>
<td>17.5MB/s</td>
<td>58MB/s</td>
</tr>
<tr>
<td>bzip2</td>
<td>8.3GB</td>
<td>1.1GB</td>
<td>2.4MB/s</td>
<td>9.5MB/s</td>
</tr>
<tr>
<td>LZO</td>
<td>8.3GB</td>
<td>2.9GB</td>
<td>49.3MB/s</td>
<td>74.6MB/s</td>
</tr>
</tbody></table>
<h3 id="Gzip压缩"><a href="#Gzip压缩" class="headerlink" title="Gzip压缩"></a>Gzip压缩</h3><p><img src="/MapReduce/115.png" alt="115"></p>
<h3 id="Bzip2压缩"><a href="#Bzip2压缩" class="headerlink" title="Bzip2压缩"></a>Bzip2压缩</h3><p><img src="/MapReduce/116.png" alt="116"></p>
<h3 id="Lzo压缩"><a href="#Lzo压缩" class="headerlink" title="Lzo压缩"></a>Lzo压缩</h3><p><img src="/MapReduce/117.png" alt="117"></p>
<h3 id="Snappy压缩"><a href="#Snappy压缩" class="headerlink" title="Snappy压缩"></a>Snappy压缩</h3><p><img src="/MapReduce/118.png" alt="118"></p>
<h2 id="压缩位置选择"><a href="#压缩位置选择" class="headerlink" title="压缩位置选择"></a>压缩位置选择</h2><p>压缩可以在MapReduce作用的任意阶段启用</p>
<p><img src="/MapReduce/119.png" alt="119"></p>
<h2 id="压缩参数配置"><a href="#压缩参数配置" class="headerlink" title="压缩参数配置"></a>压缩参数配置</h2><table>
<thead>
<tr>
<th align="left">参数</th>
<th>默认值</th>
<th>阶段</th>
<th>建议</th>
</tr>
</thead>
<tbody><tr>
<td align="left">io.compression.codecs    （在core-site.xml中配置）</td>
<td>org.apache.hadoop.io.compress.DefaultCodec,  org.apache.hadoop.io.compress.GzipCodec,  org.apache.hadoop.io.compress.BZip2Codec</td>
<td>输入压缩</td>
<td>Hadoop使用文件扩展名判断是否支持某种编解码器</td>
</tr>
<tr>
<td align="left">mapreduce.map.output.compress（在mapred-site.xml中配置）</td>
<td>false</td>
<td>mapper输出</td>
<td>这个参数设为true启用压缩</td>
</tr>
<tr>
<td align="left">mapreduce.map.output.compress.codec（在mapred-site.xml中配置）</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
<td>mapper输出</td>
<td>企业多使用LZO或Snappy编解码器在此阶段压缩数据</td>
</tr>
<tr>
<td align="left">mapreduce.output.fileoutputformat.compress（在mapred-site.xml中配置）</td>
<td>false</td>
<td>reducer输出</td>
<td>这个参数设为true启用压缩</td>
</tr>
<tr>
<td align="left">mapreduce.output.fileoutputformat.compress.codec（在mapred-site.xml中配置）</td>
<td>org.apache.hadoop.io.compress.  DefaultCodec</td>
<td>reducer输出</td>
<td>使用标准工具或者编解码器，如gzip和bzip2</td>
</tr>
<tr>
<td align="left">mapreduce.output.fileoutputformat.compress.type（在mapred-site.xml中配置）</td>
<td>RECORD</td>
<td>reducer输出</td>
<td>SequenceFile输出使用的压缩类型：NONE和BLOCK</td>
</tr>
</tbody></table>
<h2 id="压缩实操案例"><a href="#压缩实操案例" class="headerlink" title="压缩实操案例"></a>压缩实操案例</h2><h3 id="数据流的压缩和解压缩"><a href="#数据流的压缩和解压缩" class="headerlink" title="数据流的压缩和解压缩"></a>数据流的压缩和解压缩</h3><p><img src="/MapReduce/120.png" alt="120"></p>
<table>
<thead>
<tr>
<th>DEFLATE</th>
<th>org.apache.hadoop.io.compress.DefaultCodec</th>
</tr>
</thead>
<tbody><tr>
<td>gzip</td>
<td>org.apache.hadoop.io.compress.GzipCodec</td>
</tr>
<tr>
<td>bzip2</td>
<td>org.apache.hadoop.io.compress.BZip2Codec</td>
</tr>
</tbody></table>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.File;</span><br><span class="line"><span class="keyword">import</span> java.io.FileInputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.FileNotFoundException;</span><br><span class="line"><span class="keyword">import</span> java.io.FileOutputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.CompressionCodec;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.CompressionCodecFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.CompressionInputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.CompressionOutputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ReflectionUtils;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestCompress</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">		compress(<span class="string">&quot;e:/hello.txt&quot;</span>,<span class="string">&quot;org.apache.hadoop.io.compress.BZip2Codec&quot;</span>);</span><br><span class="line">	<span class="comment">//		decompress(&quot;e:/hello.txt.bz2&quot;,&quot;e:/hello.txt&quot;);</span></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 1、压缩</span></span><br><span class="line">	<span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">compress</span><span class="params">(String filename, String method)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// （1）获取输入流</span></span><br><span class="line">		FileInputStream fis = <span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(filename));</span><br><span class="line">		</span><br><span class="line">		Class codecClass = Class.forName(method);</span><br><span class="line">		</span><br><span class="line">		CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, <span class="keyword">new</span> Configuration());</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// （2）获取输出流</span></span><br><span class="line">		FileOutputStream fos = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(filename + codec.getDefaultExtension()));</span><br><span class="line">		CompressionOutputStream cos = codec.createOutputStream(fos);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// （3）流的对拷</span></span><br><span class="line">		IOUtils.copyBytes(fis, cos, <span class="number">1024</span>*<span class="number">1024</span>*<span class="number">5</span>, <span class="keyword">false</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// （4）关闭资源</span></span><br><span class="line">		cos.close();</span><br><span class="line">		fos.close();</span><br><span class="line">		fis.close();</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 2、解压缩</span></span><br><span class="line">	<span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">decompress</span><span class="params">(String filename,String dest)</span> <span class="keyword">throws</span> FileNotFoundException, IOException </span>&#123;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// （0）校验是否能解压缩</span></span><br><span class="line">		CompressionCodecFactory factory = <span class="keyword">new</span> CompressionCodecFactory(<span class="keyword">new</span> Configuration());</span><br><span class="line"></span><br><span class="line">		CompressionCodec codec = factory.getCodec(<span class="keyword">new</span> Path(filename));</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">if</span> (codec == <span class="keyword">null</span>) &#123;</span><br><span class="line">			System.out.println(<span class="string">&quot;cannot find codec for file &quot;</span> + filename);</span><br><span class="line">			<span class="keyword">return</span>;</span><br><span class="line">		&#125;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// （1）获取输入流</span></span><br><span class="line">		CompressionInputStream cis = codec.createInputStream(<span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(filename)));</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// （2）获取输出流</span></span><br><span class="line">		FileOutputStream fos = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(dest));</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// （3）流的对拷</span></span><br><span class="line">		IOUtils.copyBytes(cis, fos, <span class="number">1024</span>*<span class="number">1024</span>*<span class="number">5</span>, <span class="keyword">false</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// （4）关闭资源</span></span><br><span class="line">		cis.close();</span><br><span class="line">		fos.close();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="Map输出端采用压缩"><a href="#Map输出端采用压缩" class="headerlink" title="Map输出端采用压缩"></a>Map输出端采用压缩</h3><p>即使你的MapReduce的输入输出文件都是未压缩的文件，你仍然可以对Map任务的中间结果输出做压缩，因为它要写在硬盘并且通过网络传输到Reduce节点，对其压缩可以提高很多性能，这些工作只要设置两个属性即可，我们来看下代码怎么设置。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.BZip2Codec;	</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.CompressionCodec;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.GzipCodec;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">		Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 开启map端输出压缩</span></span><br><span class="line">	  configuration.setBoolean(<span class="string">&quot;mapreduce.map.output.compress&quot;</span>, <span class="keyword">true</span>);</span><br><span class="line">		<span class="comment">// 设置map端输出压缩方式</span></span><br><span class="line">	  configuration.setClass(<span class="string">&quot;mapreduce.map.output.compress.codec&quot;</span>, BZip2Codec.class, CompressionCodec.class);</span><br><span class="line"></span><br><span class="line">		Job job = Job.getInstance(configuration);</span><br><span class="line"></span><br><span class="line">		job.setJarByClass(WordCountDriver.class);</span><br><span class="line"></span><br><span class="line">		job.setMapperClass(WordCountMapper.class);</span><br><span class="line">		job.setReducerClass(WordCountReducer.class);</span><br><span class="line"></span><br><span class="line">		job.setMapOutputKeyClass(Text.class);</span><br><span class="line">		job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">		job.setOutputKeyClass(Text.class);</span><br><span class="line">		job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">		FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">		FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">		<span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">		System.exit(result ? <span class="number">1</span> : <span class="number">0</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Mapper和Reducer保持不变。</p>
<h3 id="Reduce输出端采用压缩"><a href="#Reduce输出端采用压缩" class="headerlink" title="Reduce输出端采用压缩"></a>Reduce输出端采用压缩</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.BZip2Codec;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.DefaultCodec;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.GzipCodec;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.Lz4Codec;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.SnappyCodec;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">		</span><br><span class="line">		Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">		</span><br><span class="line">		Job job = Job.getInstance(configuration);</span><br><span class="line">		</span><br><span class="line">		job.setJarByClass(WordCountDriver.class);</span><br><span class="line">		</span><br><span class="line">		job.setMapperClass(WordCountMapper.class);</span><br><span class="line">		job.setReducerClass(WordCountReducer.class);</span><br><span class="line">		</span><br><span class="line">		job.setMapOutputKeyClass(Text.class);</span><br><span class="line">		job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line">		</span><br><span class="line">		job.setOutputKeyClass(Text.class);</span><br><span class="line">		job.setOutputValueClass(IntWritable.class);</span><br><span class="line">		</span><br><span class="line">		FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">		FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 设置reduce端输出压缩开启</span></span><br><span class="line">		FileOutputFormat.setCompressOutput(job, <span class="keyword">true</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 设置压缩的方式</span></span><br><span class="line">	  FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class); </span><br><span class="line"><span class="comment">//	    FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class); </span></span><br><span class="line"><span class="comment">//	    FileOutputFormat.setOutputCompressorClass(job, DefaultCodec.class); </span></span><br><span class="line">	    </span><br><span class="line">		<span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">		</span><br><span class="line">		System.exit(result?<span class="number">1</span>:<span class="number">0</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title>Redis</title>
    <url>/Redis/</url>
    <content><![CDATA[<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>1、下载tar包并解压</p>
<p>2、安装gcc环境</p>
<p>我们需要将源码编译后再安装，因此需要安装c语言的编译环境！不能直接make！</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum install –y gcc-c++</span><br><span class="line"><span class="meta">#</span><span class="bash"> 之后查看安装是否成功：</span></span><br><span class="line">rpm -qa | grep gcc</span><br></pre></td></tr></table></figure>

<p>常见错误：在没有安装gcc环境下，如果执行了make，不会成功！安装环境后，第二次make有可能报错</p>
<figure class="highlight gauss"><table><tr><td class="code"><pre><span class="line">Jemalloc/jemalloc.h:没有那个文件</span><br><span class="line">解决： 运行 <span class="built_in">make</span> distclean之后再<span class="built_in">make</span></span><br></pre></td></tr></table></figure>

<p>3、编译，执行make命令！</p>
<p>4、编译完成后，安装，执行make install命令！</p>
<p>5、文件会被安装到 /usr/local/bin目录</p>
<p>6、可以将redis的bin目录，加入到环境变量中</p>
<table>
<thead>
<tr>
<th>Redis-benchmark</th>
<th>压力测试。标准是每秒80000次写操作，110000次读操作 (服务启动起来后执行,类似安兔兔跑分)</th>
</tr>
</thead>
<tbody><tr>
<td>Redis-check-aof</td>
<td>修复有问题的AOF文件</td>
</tr>
<tr>
<td>Redis-check-dump</td>
<td>修复有问题的dump.rdb文件</td>
</tr>
<tr>
<td>Redis-sentinel</td>
<td>启动哨兵，集群使用</td>
</tr>
<tr>
<td>redis-server</td>
<td>启动服务器</td>
</tr>
<tr>
<td>redis-cli</td>
<td>启动客户端</td>
</tr>
</tbody></table>
<h1 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h1><h2 id="服务端启动"><a href="#服务端启动" class="headerlink" title="服务端启动"></a>服务端启动</h2><p>将配置文件，保留一份副本，进行启动。</p>
<p>命令： </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">redis-server /opt/module/redis-3.2.5/redis.conf</span><br></pre></td></tr></table></figure>

<p>修改配置文件，改为守护进程，在后台运行</p>
<figure class="highlight nginx"><table><tr><td class="code"><pre><span class="line"><span class="attribute">daemonize</span> <span class="literal">yes</span></span><br></pre></td></tr></table></figure>

<p>后台启动后，查看服务</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">netstat -anp | grep 6379</span><br><span class="line"><span class="meta">#</span><span class="bash"> 或者</span></span><br><span class="line">ps -aux | grep 6379</span><br><span class="line">ps -ef | grep redis</span><br></pre></td></tr></table></figure>

<h2 id="客户端登录"><a href="#客户端登录" class="headerlink" title="客户端登录"></a>客户端登录</h2><table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
<th>举例</th>
<th>备注</th>
</tr>
</thead>
<tbody><tr>
<td>redis-cli</td>
<td>启动客户端</td>
<td>redis-cli -h 主机名 –p 端口号 -a 密码 –raw(查看原始数据)</td>
<td>直接执行的话，默认端口号就是6379；</td>
</tr>
<tr>
<td>ping</td>
<td>测试联通</td>
<td></td>
<td>回复pong代表联通</td>
</tr>
<tr>
<td>exit</td>
<td>退出客户端</td>
<td></td>
<td></td>
</tr>
<tr>
<td>redis-cli shutdown</td>
<td>停止服务器</td>
<td>redis-cli -h 127.0.0.1 -p 6379 shutdown  停止指定ip指定端口号的服务器</td>
<td>redis是通过客户端发送停止服务器的命令</td>
</tr>
</tbody></table>
<h1 id="Redis基本操作"><a href="#Redis基本操作" class="headerlink" title="Redis基本操作"></a>Redis基本操作</h1><h2 id="redis命令参考"><a href="#redis命令参考" class="headerlink" title="redis命令参考"></a>redis命令参考</h2><p><a href="http://doc.redisfans.com/">http://doc.redisfans.com/</a></p>
<h2 id="数据库连接操作"><a href="#数据库连接操作" class="headerlink" title="数据库连接操作"></a>数据库连接操作</h2><table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
<th>举例</th>
<th>备注</th>
</tr>
</thead>
<tbody><tr>
<td>select <dbid></dbid></td>
<td>切换数据库</td>
<td>select 1：切换到1号库</td>
<td>开启redis服务后，一共有16（0-15）个库，默认在0号库</td>
</tr>
<tr>
<td>flushdb</td>
<td>清空当前库</td>
<td></td>
<td></td>
</tr>
<tr>
<td>dbsize</td>
<td>查看数据库数据个数</td>
<td></td>
<td></td>
</tr>
<tr>
<td>flushall</td>
<td>通杀全部库</td>
<td></td>
<td></td>
</tr>
</tbody></table>
<h2 id="key的操作"><a href="#key的操作" class="headerlink" title="key的操作"></a>key的操作</h2><p><strong>Redis中的数据以键值对（key-value）为基本存储方式，其中key都是字符串。</strong></p>
<table>
<thead>
<tr>
<th>表达式</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>KEYS  pattern</td>
<td>查询符合指定表达式的所有key，支持*，？等</td>
</tr>
<tr>
<td>TYPE  key</td>
<td>查看key对应值的类型</td>
</tr>
<tr>
<td>EXISTS key</td>
<td>指定的key是否存在，0代表不存在，dr</td>
</tr>
<tr>
<td>DEL key</td>
<td>删除指定key</td>
</tr>
<tr>
<td>RANDOMKEY</td>
<td>在现有的KEY中随机返回一个</td>
</tr>
<tr>
<td>EXPIRE key seconds</td>
<td>为键值设置过期时间，单位是秒，过期后key会被redis移除</td>
</tr>
<tr>
<td>TTL key</td>
<td>查看key还有多少秒过期，-1表示永不过期，-2表示已过期</td>
</tr>
<tr>
<td>RENAME key newkey</td>
<td>重命名一个key，NEWKEY不管是否是已经存在的都会执行，如果NEWKEY已经存在则会被覆盖</td>
</tr>
<tr>
<td>RENAMENX   key newkey</td>
<td>只有在NEWKEY不存在时能够执行成功，否则失败</td>
</tr>
</tbody></table>
<h2 id="常用五大数据类型"><a href="#常用五大数据类型" class="headerlink" title="常用五大数据类型"></a>常用五大数据类型</h2><p>Redis中的数据以键值对（key-value）为基本存储方式，其中<strong>key都是字符串</strong>，这里探讨数据类型都是探讨value的类型。</p>
<table>
<thead>
<tr>
<th>value</th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>string</td>
<td>字符串</td>
</tr>
<tr>
<td>list</td>
<td>可以重复的集合</td>
</tr>
<tr>
<td>set</td>
<td>不可以重复的集合</td>
</tr>
<tr>
<td>hash</td>
<td>类似于Map&lt;String,String&gt;</td>
</tr>
<tr>
<td>zset（sorted  set）</td>
<td>带分数的set</td>
</tr>
</tbody></table>
<h2 id="string操作"><a href="#string操作" class="headerlink" title="string操作"></a>string操作</h2><p>String类型是Redis中最基本的类型，它是key对应的一个单一值。</p>
<p>二进制安全，不必担心由于编码等问题导致二进制数据变化。所以<strong>redis的string可以包含任何数据，比如jpg图片或者序列化的对象</strong>。</p>
<p>Redis中一个字符串值的最大容量是512M。</p>
<table>
<thead>
<tr>
<th>SET  key value</th>
<th>添加键值对，如果存在就更新value</th>
</tr>
</thead>
<tbody><tr>
<td>GET  key</td>
<td>查询指定key的值</td>
</tr>
<tr>
<td>APPEND key value</td>
<td>将给定的value追加到原值的末尾</td>
</tr>
<tr>
<td>STRLEN key</td>
<td>获取值的长度</td>
</tr>
<tr>
<td>SETNX key value</td>
<td>只有在 key 不存在时设置 key 的值</td>
</tr>
<tr>
<td>INCR key</td>
<td>指定key的值自增1，只对数字有效</td>
</tr>
<tr>
<td>DECR key</td>
<td>指定key的值自减1，只对数字有效</td>
</tr>
<tr>
<td>INCRBY key num</td>
<td>自增num</td>
</tr>
<tr>
<td>DECRBY key num</td>
<td>自减num</td>
</tr>
<tr>
<td>MSET key1 value1  key2 value2…</td>
<td>同时设置多个key-value对</td>
</tr>
<tr>
<td>MGET key1 key2</td>
<td>同时获取一个或多个value</td>
</tr>
<tr>
<td>MSETNX key1 value1 key2 value2</td>
<td>当key不存在时，设置多个key-value对；其中一个失败全失败</td>
</tr>
<tr>
<td>GETRANGE key起始索引 结束索引</td>
<td>获取指定范围的值，都是闭区间</td>
</tr>
<tr>
<td>SETRANGE key起始索引 value</td>
<td>从起始位置开始覆写指定的值，给多少覆盖多少</td>
</tr>
<tr>
<td>GETSET key value</td>
<td>以新换旧，同时获取旧值，先get再set</td>
</tr>
<tr>
<td>SETEX  key 过期时间  value</td>
<td>设置键值的同时，设置过期时间，单位秒</td>
</tr>
</tbody></table>
<h2 id="list操作"><a href="#list操作" class="headerlink" title="list操作"></a>list操作</h2><p>在Java中list 一般是单向链表，如常见的Arraylist，只能从一侧插入。</p>
<p><strong>在Redis中，list是双向链表。可以从两侧插入。</strong></p>
<p>常见操作：</p>
<p> 遍历：遍历的时候，是从左往右取值；</p>
<p>删除：弹栈，POP；</p>
<p>添加：压栈，PUSH ；</p>
<p>Redis 列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素导列表的头部（左边）或者尾部（右边）。它的底层实际是个<strong>双向链表</strong>，对两端的操作性能很高，通过索引下标的操作中间的节点性能会较差。</p>
<table>
<thead>
<tr>
<th>LPUSH/RPUSH key value1 value2…</th>
<th>从左边/右边压入一个或多个值  头尾效率高，中间效率低</th>
</tr>
</thead>
<tbody><tr>
<td>LPOP/RPOP key</td>
<td>从左边/右边弹出一个值  值在键在，值光键亡  弹出=返回+删除</td>
</tr>
<tr>
<td>LRANGE key start stop</td>
<td>查看指定区间的元素  正着数：0,1,2,3,…  倒着数：-1,-2,-3,…</td>
</tr>
<tr>
<td>LINDEX key index</td>
<td>按照索引下标获取元素（从左到右）</td>
</tr>
<tr>
<td>LLEN key</td>
<td>获取列表长度</td>
</tr>
<tr>
<td>LINSERT key BEFORE|AFTER value newvalue</td>
<td>在指定value的前后插入newvalue</td>
</tr>
<tr>
<td>LREM key n value</td>
<td>从左边删除n个value</td>
</tr>
<tr>
<td>LSET key index value</td>
<td>把指定索引位置的元素替换为另一个值</td>
</tr>
<tr>
<td>LTRIM key start stop</td>
<td>仅保留指定区间的数据</td>
</tr>
<tr>
<td>RPOPLPUSH list1 list2</td>
<td>从list1右边弹出一个值，左侧压入到list2，list1可以和list2相同</td>
</tr>
</tbody></table>
<h2 id="set操作"><a href="#set操作" class="headerlink" title="set操作"></a>set操作</h2><p>set是无序的，且是不可重复的。</p>
<table>
<thead>
<tr>
<th>SADD key member [member …]</th>
<th>将一个或多个 member 元素加入到集合 key 当中，已经存在于集合的  member 元素将被忽略。</th>
</tr>
</thead>
<tbody><tr>
<td>SMEMBERS key</td>
<td>取出该集合的所有值</td>
</tr>
<tr>
<td>SISMEMBER key value</td>
<td>判断集合<key>是否为含有该<value>值，有返回1，没有返回0</value></key></td>
</tr>
<tr>
<td>SCARD key</td>
<td>返回集合中元素的数量</td>
</tr>
<tr>
<td>SREM key member [member …]</td>
<td>从集合中删除元素</td>
</tr>
<tr>
<td>SPOP key [count]</td>
<td>从集合中随机弹出count个数量的元素，count不指定就弹出1个</td>
</tr>
<tr>
<td>SRANDMEMBER key [count]</td>
<td>从集合中随机返回count个数量的元素，count不指定就返回1个</td>
</tr>
<tr>
<td>SINTER key [key …]</td>
<td>将指定的集合进行“交集”操作</td>
</tr>
<tr>
<td>SINTERSTORE dest key [key …]</td>
<td>取交集，另存为一个set</td>
</tr>
<tr>
<td>SUNION key [key …]</td>
<td>将指定的集合执行“并集”操作</td>
</tr>
<tr>
<td>SUNIONSTORE dest key [key …]</td>
<td>取并集，另存为set</td>
</tr>
<tr>
<td>SDIFF key [key …]</td>
<td>将指定的集合执行“差集”操作</td>
</tr>
<tr>
<td>SDIFFSTORE dest key [key …]</td>
<td>取差集，另存为set</td>
</tr>
</tbody></table>
<h2 id="hash操作"><a href="#hash操作" class="headerlink" title="hash操作"></a>hash操作</h2><p><strong>Hash数据类型的键值对中的值是“单列”的，不支持进一步的层次结构。</strong></p>
<table>
<thead>
<tr>
<th></th>
<th align="center">field:value</th>
</tr>
</thead>
<tbody><tr>
<td>key</td>
<td align="center">“k01”:”v01”, “k02”:”v02”</td>
</tr>
</tbody></table>
<p>从前到后的数据对应关系</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">stu</span>:&#123;<span class="string">&quot;stu_id&quot;</span>:<span class="number">10</span>,<span class="string">&quot;stu_name&quot;</span>:<span class="string">&quot;tom&quot;</span>,<span class="string">&quot;stu_age&quot;</span>:<span class="number">30</span>&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Java</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Student</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> Integer stuId;<span class="comment">//10</span></span><br><span class="line"><span class="keyword">private</span> String stuName;<span class="comment">//&quot;tom&quot;</span></span><br><span class="line"><span class="keyword">private</span> Integer stuAge;<span class="comment">//30</span></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Student stu=<span class="keyword">new</span> Student()’</span><br><span class="line">stu.setStuId=<span class="number">10</span>;</span><br><span class="line">stu.setStuName=”tom”;</span><br><span class="line">stu.setStuAge=<span class="number">30</span>;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>Redis hash</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">key</span>	       value(hash)</span><br><span class="line">	         <span class="attribute">stu_id</span>	   <span class="number">10</span></span><br><span class="line"><span class="attribute">stu</span>	       stu_name	 tom</span><br><span class="line">	         <span class="attribute">stu_age</span>	 <span class="number">30</span></span><br></pre></td></tr></table></figure>

<p>常用操作：</p>
<table>
<thead>
<tr>
<th>HDEL key field1 [field2]</th>
<th>删除一个或多个哈希表字段</th>
</tr>
</thead>
<tbody><tr>
<td>HEXISTS key field</td>
<td>查看哈希表 key 中，指定的字段是否存在。</td>
</tr>
<tr>
<td>HGET key field</td>
<td>获取存储在哈希表中指定字段的值。</td>
</tr>
<tr>
<td>HGETALL key</td>
<td>获取在哈希表中指定 key 的所有字段和值</td>
</tr>
<tr>
<td>HINCRBY key field increment</td>
<td>为哈希表 key 中的指定字段的整数值加上增量 increment 。</td>
</tr>
<tr>
<td>HINCRBYFLOAT key field increment</td>
<td>为哈希表 key 中的指定字段的浮点数值加上增量 increment 。</td>
</tr>
<tr>
<td>HKEYS key</td>
<td>获取所有哈希表中的字段</td>
</tr>
<tr>
<td>HLEN key</td>
<td>获取哈希表中字段的数量</td>
</tr>
<tr>
<td>HMGET key field1 [field2]</td>
<td>获取所有给定字段的值</td>
</tr>
<tr>
<td>HMSET key field1 value1 [field2 value2 ]</td>
<td>同时将多个 field-value (域-值)对设置到哈希表 key 中。</td>
</tr>
<tr>
<td>HSET key field value</td>
<td>将哈希表 key 中的字段 field 的值设为 value 。</td>
</tr>
<tr>
<td>HSETNX key field value</td>
<td>只有在字段 field 不存在时，设置哈希表字段的值。</td>
</tr>
<tr>
<td>HVALS key</td>
<td>获取哈希表中所有值。</td>
</tr>
<tr>
<td>HSCAN key cursor [MATCH pattern] [COUNT count]</td>
<td>迭代哈希表中的键值对。当Redis操作Hash中的大量数据时，需要用到分页操作hscan。数据量太小无法使用。</td>
</tr>
</tbody></table>
<h2 id="zset操作"><a href="#zset操作" class="headerlink" title="zset操作"></a>zset操作</h2><p>zset是一种特殊的set(sorted set)，在保存value的时候，<strong>为每个value多保存了一个score信息。根据score信息，可以进行排序</strong>。</p>
<p>这个评分（score）被用来按照<strong>从最低分到最高分</strong>的方式排序集合中的成员。集合的成员是唯一的，但是评分可以是重复的。</p>
<table>
<thead>
<tr>
<th>ZADD key [score  member …]</th>
<th>添加</th>
</tr>
</thead>
<tbody><tr>
<td>ZSCORE key member</td>
<td>返回指定值的分数</td>
</tr>
<tr>
<td>ZRANGE key start stop [WITHSCORES]</td>
<td>返回指定区间的值，可选择是否一起返回scores</td>
</tr>
<tr>
<td>ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset  count]</td>
<td>在分数的指定区间内返回数据，从小到大排列</td>
</tr>
<tr>
<td>ZREVRANGEBYSCORE key max min [WITHSCORES] [LIMIT offset  count]</td>
<td>在分数的指定区间内返回数据，从大到小排列</td>
</tr>
<tr>
<td>ZCARD key</td>
<td>返回集合中所有的元素的数量</td>
</tr>
<tr>
<td>ZCOUNT key min max</td>
<td>统计分数区间内的元素个数</td>
</tr>
<tr>
<td>ZREM key member</td>
<td>删除该集合下，指定值的元素</td>
</tr>
<tr>
<td>ZRANK key member</td>
<td>返回该值在集合中的排名，从0开始</td>
</tr>
<tr>
<td>ZINCRBY key increment member</td>
<td>为元素的score加上增量</td>
</tr>
</tbody></table>
<h1 id="持久化"><a href="#持久化" class="headerlink" title="持久化"></a>持久化</h1><p>Redis主要是工作在内存中。内存本身就不是一个持久化设备，断电后数据会清空。所以Redis在工作过程中，如果发生了意外停电事故，如何尽可能减少数据丢失。</p>
<h2 id="RDB"><a href="#RDB" class="headerlink" title="RDB"></a>RDB</h2><p>RDB：在指定的时间间隔内将内存中的数据集快照写入磁盘，也就是行话讲的Snapshot快照，它恢复时是将快照文件直接读到内存里。</p>
<p>工作机制：<strong>每隔一段时间，就把内存中的数据保存到硬盘上的指定文件中</strong>。</p>
<p>​    存储方式：</p>
<ol>
<li><p>隔段时间自动存储</p>
</li>
<li><p>使用save或者bgsave时也会持久化</p>
</li>
<li><p>flushall也会持久化（flushdb不会持久化）</p>
</li>
<li><p>shutdown也会持久化</p>
</li>
</ol>
<p>RDB是默认开启的！</p>
<p>Redis会单独创建（fork）一个子进程来进行持久化，会先将数据写入到一个临时文件中，待持久化过程都结束了，再用这个临时文件替换上次持久化好的文件。整个过程中，主进程是不进行任何IO操作的，这就确保了极高的性能如果需要进行大规模数据的恢复，且对于数据恢复的完整性不是非常敏感，那RDB方式要比AOF方式更加的高效。</p>
<p>RDB的缺点是最后一次持久化后的数据可能丢失。</p>
<h3 id="RDB保存策略"><a href="#RDB保存策略" class="headerlink" title="RDB保存策略"></a>RDB保存策略</h3><p>save 900 1                       900 秒内如果至少有 1 个 key 的值变化，则保存</p>
<p>save 300 10                     300 秒内如果至少有 10 个 key 的值变化，则保存</p>
<p>save 60 10000                 60 秒内如果至少有 10000 个 key 的值变化，则保存</p>
<p>save “”                              就是禁用RDB模式；</p>
<h3 id="RDB常用属性配置"><a href="#RDB常用属性配置" class="headerlink" title="RDB常用属性配置"></a>RDB常用属性配置</h3><table>
<thead>
<tr>
<th>属性</th>
<th>含义</th>
<th>备注</th>
</tr>
</thead>
<tbody><tr>
<td>save</td>
<td>保存策略</td>
<td></td>
</tr>
<tr>
<td>dbfilename</td>
<td>RDB快照文件名</td>
<td></td>
</tr>
<tr>
<td>dir</td>
<td>RDB快照保存的目录</td>
<td>必须是一个目录，不能是文件名。最好改为固定目录。默认为./代表执行redis-server命令时的当前目录！</td>
</tr>
<tr>
<td>stop-writes-on-bgsave-error</td>
<td>是否在备份出错时，继续接受写操作</td>
<td>如果用户开启了RDB快照功能，那么在redis持久化数据到磁盘时如果出现失败，默认情况下，redis会停止接受所有的写请求</td>
</tr>
<tr>
<td>rdbcompression</td>
<td>对于存储到磁盘中的快照，可以设置是否进行压缩存储。</td>
<td>如果是的话，redis会采用LZF算法进行压缩。如果你不想消耗CPU来进行压缩的话，可以设置为关闭此功能，但是存储在磁盘上的快照会比较大。</td>
</tr>
<tr>
<td>rdbchecksum</td>
<td>是否进行数据校验</td>
<td>在存储快照后，我们还可以让redis使用CRC64算法来进行数据校验，但是这样做会增加大约10%的性能消耗，   如果希望获取到最大的性能提升，可以关闭此功能。</td>
</tr>
</tbody></table>
<h3 id="RDB数据丢失的情况"><a href="#RDB数据丢失的情况" class="headerlink" title="RDB数据丢失的情况"></a>RDB数据丢失的情况</h3><p>两次保存的时间间隔内，服务器宕机，或者发生断电问题。</p>
<h3 id="RDB的触发"><a href="#RDB的触发" class="headerlink" title="RDB的触发"></a>RDB的触发</h3><p>1、基于自动保存的策略</p>
<p>2、执行save，或者bgsave命令！执行时，是阻塞状态。</p>
<p>3、执行flushall命令，也会产生dump.rdb，但里面是空的，没有意义。</p>
<p>4、当执行shutdown命令时，也会主动地备份数据。</p>
<h3 id="RDB的优缺点"><a href="#RDB的优缺点" class="headerlink" title="RDB的优缺点"></a>RDB的优缺点</h3><p>RDB的优点:</p>
<ul>
<li>RDB是一个紧凑压缩的二进制文件，代表Redis在某一个时间点上的数据快照。非常适合用于备份，全量复制等场景。比如每6小时执行bgsave备份，并把RDB文件拷贝到远程机器或者文件系统中（如hdfs），用于灾难恢复。</li>
<li>Redis加载RDB恢复数据远远快于AOF方式。</li>
</ul>
<p>RDB的缺点:</p>
<ul>
<li><p>RDB方式数据没办法做到实时持久化/秒级持久化。因为bgsave每次运行都要执行fork操作创建子进程，属于重量级操作，频繁执行成本过高。</p>
</li>
<li><p>RDB文件使用特定二进制格式保存，Redis版本演进过程中有多个格式的RDB笨笨，存在老版本Redis服务无法兼容新版RDB格式的问题。</p>
<p>针对RDB不适合实时持久化的问题，Redis提供了AOF持久化方式来解决</p>
</li>
</ul>
<h2 id="AOF"><a href="#AOF" class="headerlink" title="AOF"></a>AOF</h2><p><strong>AOF是以日志的形式来记录每个写操作</strong>，将每一次对数据进行修改，都把新建、修改数据的命令保存到指定文件中。<strong>Redis重新启动时读取这个文件，重新执行新建、修改数据的命令恢复数据</strong>。</p>
<p>默认不开启，需要手动开启</p>
<p>AOF文件的保存路径，同RDB的路径一致。</p>
<p>AOF在保存命令的时候，只会保存对数据有修改的命令，也就是写操作！</p>
<p>当RDB和AOF存的不一致的情况下，按照AOF来恢复。因为AOF是对RDB的补充。备份周期更短，也就更可靠。</p>
<h3 id="AOF保存策略"><a href="#AOF保存策略" class="headerlink" title="AOF保存策略"></a>AOF保存策略</h3><p>appendfsync always：每次产生一条新的修改数据的命令都执行保存操作；效率低，但是安全！</p>
<p>appendfsync everysec：每秒执行一次保存操作。如果在未保存当前秒内操作时发生了断电，仍然会导致一部分数据丢失（即1秒钟的数据）。</p>
<p>appendfsync no：从不保存，将数据交给操作系统来处理。更快，也更不安全的选择。</p>
<p>推荐（并且也是默认）的措施为每秒 fsync 一次， 这种 fsync 策略可以兼顾速度和安全性。</p>
<h3 id="AOF常用属性"><a href="#AOF常用属性" class="headerlink" title="AOF常用属性"></a>AOF常用属性</h3><table>
<thead>
<tr>
<th>属性</th>
<th>含义</th>
<th>备注</th>
</tr>
</thead>
<tbody><tr>
<td>appendonly</td>
<td>是否开启AOF功能</td>
<td>默认是关闭的</td>
</tr>
<tr>
<td>appendfilename</td>
<td>AOF文件名称</td>
<td></td>
</tr>
<tr>
<td>appendfsync</td>
<td>AOF保存策略</td>
<td>官方建议everysec</td>
</tr>
<tr>
<td>no-appendfsync-on-rewrite</td>
<td>在重写时，是否执行保存策略</td>
<td>执行重写，可以节省AOF文件的体积；而且在恢复的时候效率也更高。</td>
</tr>
<tr>
<td>auto-aof-rewrite-percentage</td>
<td>重写的触发条件</td>
<td>当目前aof文件大小超过上一次重写的aof文件大小的百分之多少进行重写</td>
</tr>
<tr>
<td>auto-aof-rewrite-min-size</td>
<td>设置允许重写的最小aof文件大小</td>
<td>避免了达到约定百分比但尺寸仍然很小的情况还要重写</td>
</tr>
<tr>
<td>aof-load-truncated</td>
<td>截断设置</td>
<td>如果选择的是yes，当截断的aof文件被导入的时候，会自动发布一个log给客户端然后load</td>
</tr>
</tbody></table>
<h3 id="AOF文件的修复"><a href="#AOF文件的修复" class="headerlink" title="AOF文件的修复"></a>AOF文件的修复</h3><p>如果AOF文件中出现了残余命令，会导致服务器无法重启。此时需要借助redis-check-aof工具来修复！</p>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><span class="line">命令： redis-<span class="keyword">check</span>-aof  <span class="comment">--fix 文件</span></span><br></pre></td></tr></table></figure>

<h3 id="AOF的优缺点"><a href="#AOF的优缺点" class="headerlink" title="AOF的优缺点"></a>AOF的优缺点</h3><p>优点：</p>
<p>备份机制更稳健，丢失数据概率更低</p>
<p>可读的日志文本，通过操作AOF稳健，可以处理误操作</p>
<p>缺点：</p>
<p>比起RDB占用更多的磁盘空间</p>
<p>恢复备份速度要慢</p>
<p>每次读写都同步的话，有一定的性能压力</p>
<p>存在个别Bug，造成恢复不能</p>
<h2 id="备份建议"><a href="#备份建议" class="headerlink" title="备份建议"></a>备份建议</h2><p>Redis作为内存数据库从本质上来说，如果不想牺牲性能，就不可能做到数据的“绝对”安全。</p>
<p>RDB和AOF都只是尽可能在兼顾性能的前提下降低数据丢失的风险，如果真的发生数据丢失问题，尽可能减少损失。</p>
<p>在整个项目的架构体系中，<strong>Redis大部分情况是扮演“二级缓存”角色</strong>。</p>
<p>二级缓存适合保存的数据</p>
<p>经常要查询，很少被修改的数据。</p>
<p>不是非常重要，允许出现偶尔的并发问题。</p>
<p>不会被其他应用程序修改。</p>
<p>如果Redis是作为缓存服务器，那么说明数据在MySQL这样的传统关系型数据库中是有正式版本的。数据最终以MySQL中的为准。</p>
<p>官方推荐两个都用；如果对数据不敏感，可以选单独用RDB；不建议单独用AOF，因为可能出现Bug；如果只是做纯内存缓存，可以都不用。</p>
<h1 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h1><p>ACID一个都不具有，原子性一致性持久性都没。</p>
<p>把多个指令封装到一个队列中按顺序执行。</p>
<h2 id="事务简介"><a href="#事务简介" class="headerlink" title="事务简介"></a>事务简介</h2><p>Redis中事务，不同于传统的关系型数据库中的事务。</p>
<p>Redis中的事务指的是一个单独的隔离操作。</p>
<p>Redis的事务中的所有命令都会序列化、按顺序地执行且不会被其他客户端发送来的命令请求所打断。</p>
<p>Redis事务的主要作用是串联多个命令防止别的命令插队</p>
<h2 id="事务常用命令"><a href="#事务常用命令" class="headerlink" title="事务常用命令"></a>事务常用命令</h2><table>
<thead>
<tr>
<th>MULTI</th>
<th>标记一个事务块的开始</th>
</tr>
</thead>
<tbody><tr>
<td>EXEC</td>
<td>执行事务中所有在排队等待的指令并将链接状态恢复到正常 当使用WATCH 时，只有当被监视的键没有被修改，且允许检查设定机制时，EXEC会被执行</td>
</tr>
<tr>
<td>DISCARD</td>
<td>刷新一个事务中所有在排队等待的指令，并且将连接状态恢复到正常。  如果已使用WATCH，DISCARD将释放所有被WATCH的key。  取消组队。</td>
</tr>
<tr>
<td>WATCH</td>
<td>标记所有指定的key 被监视起来，在事务中有条件的执行（乐观锁）</td>
</tr>
</tbody></table>
<h2 id="锁"><a href="#锁" class="headerlink" title="锁"></a>锁</h2><h3 id="悲观锁"><a href="#悲观锁" class="headerlink" title="悲观锁"></a>悲观锁</h3><p>适合写多读少场景。</p>
<p>执行操作前假设当前的操作肯定（或有很大几率）会被打断（悲观）。基于这个假设，我们在做操作前就会把相关资源锁定，不允许自己执行期间有其他操作干扰。</p>
<p>Redis不支持悲观锁。Redis作为缓存服务器使用时，以读操作为主，很少写操作，相应的操作被打断的几率较少。不采用悲观锁是为了防止降低性能。</p>
<h3 id="乐观锁"><a href="#乐观锁" class="headerlink" title="乐观锁"></a>乐观锁</h3><p>适合读多写少场景。</p>
<p>执行操作前假设当前操作不会被打断（乐观）。基于这个假设，我们在做操作前不会锁定资源，万一发生了其他操作的干扰，那么本次操作将被放弃。</p>
<h2 id="Redis中的锁策略"><a href="#Redis中的锁策略" class="headerlink" title="Redis中的锁策略"></a>Redis中的锁策略</h2><p>Redis采用了乐观锁策略（通过watch操作）。乐观锁支持读操作，适用于多读少写的情况！</p>
<p>在事务中，可以通过watch命令来加锁；使用 UNWATCH可以取消加锁；</p>
<p>如果在事务之前，执行了WATCH（加锁），那么执行EXEC 命令或 DISCARD 命令后，锁对自动释放，即不需要再执行 UNWATCH 了</p>
<h1 id="主从复制"><a href="#主从复制" class="headerlink" title="主从复制"></a>主从复制</h1><h2 id="主从简介"><a href="#主从简介" class="headerlink" title="主从简介"></a>主从简介</h2><p>配置多台Redis服务器，以主机和备机的身份分开。主机数据更新后，根据配置和策略，自动同步到备机的master/salver机制，<strong>Master以写为主，Slave以读为主，二者之间自动同步数据</strong>。</p>
<p>目的：</p>
<p>读写分离提高Redis性能</p>
<p>避免单点故障，容灾快速恢复</p>
<p>原理：</p>
<p>每次从机联通后，都会给主机发送sync指令，主机立刻进行存盘操作，发送RDB文件，给从机</p>
<p>从机收到RDB文件后，进行全盘加载。之后每次主机的写操作命令，都会立刻发送给从机，从机执行相同的命令来保证主从的数据一致！</p>
<p>注意：主库接收到SYNC的命令时会执行RDB过程，即使在配置文件中禁用RDB持久化也会生成，但是如果主库所在的服务器磁盘IO性能较差，那么这个复制过程就会出现瓶颈，庆幸的是，Redis在2.8.18版本开始实现了无磁盘复制功能（不过该功能还是处于试验阶段），设置repl-diskless-sync yes。即Redis在与从数据库进行复制初始化时将不会将快照存储到磁盘，而是直接通过网络发送给从数据库，避免了IO性能差问题。</p>
<h2 id="主从准备"><a href="#主从准备" class="headerlink" title="主从准备"></a>主从准备</h2><p>除非是不同的主机配置不同的Redis服务，否则在一台机器上面跑多个Redis服务，需要配置多个Redis配置文件。</p>
<p>1、准备多个Redis配置文件，每个配置文件，需要配置以下属性</p>
<figure class="highlight elm"><table><tr><td class="code"><pre><span class="line"><span class="title">daemonize</span> yes: 服务在后台运行<span class="keyword">port</span>：端口号pidfile:pid保存文件logfile：日志文件(如果没有指定的话，就不需要)dump.rdb: RDB appendonly 关掉，或者是更改appendonly文件的名称。</span><br></pre></td></tr></table></figure>

<p>样本</p>
<figure class="highlight gradle"><table><tr><td class="code"><pre><span class="line"><span class="keyword">include</span> <span class="regexp">/root/</span>redis_repilication<span class="regexp">/redis.confport 6379pidfile /</span>var<span class="regexp">/run/</span>redis_6379.piddbfilename dump_6379.rdb</span><br></pre></td></tr></table></figure>

<p>2、根据多个配置文件，启动多个Redis服务</p>
<p>原则是配从不配主。</p>
<h2 id="主从建立"><a href="#主从建立" class="headerlink" title="主从建立"></a>主从建立</h2><h3 id="临时建立"><a href="#临时建立" class="headerlink" title="临时建立"></a>临时建立</h3><p>原则：配从不配主。</p>
<p>配置：在从服务器上执行SLAVEOF ip:port命令；</p>
<p>查看：执行info replication命令；</p>
<h3 id="永久建立"><a href="#永久建立" class="headerlink" title="永久建立"></a>永久建立</h3><p>在从机的配置文件中，编写slaveof属性配置！</p>
<h3 id="恢复身份"><a href="#恢复身份" class="headerlink" title="恢复身份"></a>恢复身份</h3><p>执行命令slaveof no one恢复自由身！</p>
<h2 id="哨兵模式"><a href="#哨兵模式" class="headerlink" title="哨兵模式"></a>哨兵模式</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p><strong>作用</strong>：</p>
<ol>
<li>主从状态检测 </li>
<li>如果Master异常，则会进行Master-Slave切换，将其中一个Slave作为Master，将之前的Master作为Slave(如果重启成功 )的多个哨兵，不仅同时监控主从状态，且哨兵之间也互相监控！</li>
</ol>
<p><strong>下线</strong>：</p>
<ol>
<li><p>主观下线：Subjectively Down，简称 SDOWN，指的是当前 Sentinel 实例对某个redis服务器做出的下线判断。</p>
</li>
<li><p>客观下线：Objectively Down， 简称 ODOWN，指的是多个 Sentinel 实例在对Master Server做出 SDOWN 判断，并且通过 SENTINEL is-master-down-by-addr 命令互相交流之后，得出的Master Server下线判断，然后开启failover.</p>
</li>
</ol>
<p><strong>工作原理</strong>：</p>
<ol>
<li><p>每个Sentinel以每秒钟一次的频率向它所知的Master，Slave以及其他 Sentinel 实例发送一个 PING 命令 ；</p>
</li>
<li><p>如果一个实例（instance）距离最后一次有效回复 PING 命令的时间超过 down-after-milliseconds 选项所指定的值， 则这个实例会被 Sentinel 标记为主观下线；</p>
</li>
<li><p>如果一个Master被标记为主观下线，则正在监视这个Master的所有 Sentinel 要以每秒一次的频率确认Master的确进入了主观下线状态；</p>
</li>
<li><p>当有足够数量的 Sentinel（大于等于配置文件指定的值）在指定的时间范围内确认Master的确进入了主观下线状态， 则Master会被标记为客观下线 ；</p>
</li>
<li><p>在一般情况下， 每个 Sentinel 会以每 10 秒一次的频率向它已知的所有Master，Slave发送 INFO 命令</p>
</li>
<li><p>当Master被 Sentinel 标记为客观下线时，Sentinel 向下线的 Master 的所有 Slave 发送 INFO 命令的频率会从 10 秒一次改为每秒一次 ；</p>
</li>
<li><p>若没有足够数量的 Sentinel 同意 Master 已经下线， Master 的主观下线状态就会被移除； 若 Master 重新向 Sentinel 的 PING 命令返回有效回复， Master 的客观下线状态就会被移除；主机宕机后，新主机选举条件依次为：优先级靠前的、偏移量最大的、runid最小的，若新主复活后则变为从机。</p>
</li>
</ol>
<h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>哨兵模式需要配置哨兵的配置文件！</p>
<figure class="highlight smali"><table><tr><td class="code"><pre><span class="line">sentinel<span class="built_in"> monitor </span>mymaster 10.211.55.10 6379 1</span><br></pre></td></tr></table></figure>

<p>启动哨兵：</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><span class="line">redis-sentinel <span class="regexp">/opt/m</span>odule<span class="regexp">/redis-3.2.5/</span>sentinel.conf</span><br></pre></td></tr></table></figure>

<h1 id="集群"><a href="#集群" class="headerlink" title="集群"></a>集群</h1><p>Redis 集群实现了对Redis的水平扩容，即启动N个redis节点，将整个数据库分布存储在这N个节点中，每个节点存储总数据的1/N。</p>
<p>Redis 集群通过分区（partition）来提供一定程度的可用性（availability）： 即使集群中有一部分节点失效或者无法进行通讯， 集群也可以继续处理命令请求。</p>
<h2 id="安装ruby环境"><a href="#安装ruby环境" class="headerlink" title="安装ruby环境"></a>安装ruby环境</h2><p>本身redis集群的安装是很麻烦了，通过ruby工具，可以非常方便的将一系列命令打包为一个脚本！</p>
<p><strong>依次执行</strong>在安装光盘下的Package目录(/media/CentOS_6.8_Final/Packages)下的rpm包：</p>
<table>
<thead>
<tr>
<th>rpm -ivh compat-readline5-5.2-17.1.el6.x86_64.rpm</th>
</tr>
</thead>
<tbody><tr>
<td>rpm -ivh ruby-libs-1.8.7.374-4.el6_6.x86_64.rpm</td>
</tr>
<tr>
<td>rpm -ivh ruby-1.8.7.374-4.el6_6.x86_64.rpm</td>
</tr>
<tr>
<td>rpm -ivh ruby-irb-1.8.7.374-4.el6_6.x86_64.rpm</td>
</tr>
<tr>
<td>rpm -ivh ruby-rdoc-1.8.7.374-4.el6_6.x86_64.rpm</td>
</tr>
<tr>
<td>rpm -ivh rubygems-1.3.7-5.el6.noarch.rpm</td>
</tr>
</tbody></table>
<p>也可以在联网状态下，执行yum安装，执行</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum -y install ruby</span><br></pre></td></tr></table></figure>

<p>之后安装rubygem，rubygem是ruby的包管理框架。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum -y install rubygems</span><br></pre></td></tr></table></figure>

<h2 id="安装redis-gem"><a href="#安装redis-gem" class="headerlink" title="安装redis gem"></a>安装redis gem</h2><p>redis-3.2.0.gem是一个通过ruby操作redis的插件！</p>
<p>拷贝redis-3.2.0.gem到/opt/software目录下，在/opt/software目录下执行 </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">gem install --local redis-3.2.0.gem</span><br></pre></td></tr></table></figure>

<h2 id="制作6个redis配置文件"><a href="#制作6个redis配置文件" class="headerlink" title="制作6个redis配置文件"></a>制作6个redis配置文件</h2><p>端口号分别是：6379,6380,6381,6382,6383,6384</p>
<p>注意：每个配置文件中需要指定</p>
<figure class="highlight stata"><table><tr><td class="code"><pre><span class="line">daemonize yes: 服务在后台运行port：端口号pidfile:pid保存文件logfile：日志文件(如果没有指定的话，就不需要)dump.rdb: RDB备份文件的名称appendonly 关掉，或者是更改appendonly文件的名称。	<span class="keyword">cluster</span>-enabled yes    打开集群模式<span class="keyword">cluster</span>-config-<span class="keyword">file</span>  nodes-6379.<span class="keyword">conf</span>  设定节点配置文件名<span class="keyword">cluster</span>-node-timeout 15000   设定节点失联时间，超过该时间（毫秒），集群自动进行主从切换。</span><br></pre></td></tr></table></figure>

<p>样例</p>
<figure class="highlight gradle"><table><tr><td class="code"><pre><span class="line"><span class="keyword">include</span> <span class="regexp">/opt/m</span>odule<span class="regexp">/my_redis/</span>base.confpidfile <span class="string">&quot;/opt/module/my_redis/redis-6379.pid&quot;</span>port <span class="number">6379</span>dbfilename <span class="string">&quot;dump_6379.rdb&quot;</span>cluster-enabled yescluster-config-<span class="keyword">file</span> nodes-<span class="number">6379</span>.confcluster-node-timeout <span class="number">15000</span></span><br></pre></td></tr></table></figure>

<p>注意在创建集群的时候，初始化的时候，把所有节点的dump文件全部删掉。</p>
<h2 id="开启集群"><a href="#开启集群" class="headerlink" title="开启集群"></a>开启集群</h2><p>1、首先依次启动6个节点，启动后，会在当前文件夹生成nodes-xxxx.conf文件</p>
<p>2、配置集群</p>
<p>执行命令：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/opt/module/redis-3.2.5/src/redis-trib.rb create --replicas 1 10.211.55.10:6379 10.211.55.10:6380 10.211.55.10:6381 10.211.55.10:6382 10.211.55.10:6383 10.211.55.10:6384</span><br></pre></td></tr></table></figure>

<p>注意，此处不要用127.0.0.1和域名，请用真实IP地址！</p>
<p>3、之后登录到客户端，通过 cluster nodes 命令查看集群信息</p>
<p>4、6个节点，为什么是三主三从？</p>
<p>命令create,代表创建一个集群。参数–replicas 1 表示我们希望为集群中的每个主节点创建一个从节点。一个集群至少要有<strong>三个主节点</strong>，分配原则尽量保证每个主数据库运行在不同的IP地址，每个从库和主库不在一个IP地址上。</p>
<h2 id="slot"><a href="#slot" class="headerlink" title="slot"></a>slot</h2><p>进入集群后，如果我们，直接写入数据，可能会看到报错信息</p>
<p>这是因为，集群中多了slot(插槽)的设计。一个 Redis 集群包含 16384 个插槽（hash slot）， 数据库中的每个键都属于这 16384 个插槽的其中一个， 集群使用公式 CRC16(key) % 16384 来计算键 key 属于哪个槽， 其中 CRC16(key) 语句用于计算键 key 的 CRC16 校验和 。</p>
<p>集群中的每个节点负责处理一部分插槽。 举个例子， 如果一个集群可以有主节点， 其中：</p>
<p>​    节点 A 负责处理 0 号至 5500 号插槽。</p>
<p>​    节点 B 负责处理 5501 号至 11000 号插槽。</p>
<p>​    节点 C 负责处理 11001 号至 16383 号插槽。</p>
<h2 id="集群中写入数据"><a href="#集群中写入数据" class="headerlink" title="集群中写入数据"></a>集群中写入数据</h2><h3 id="客户端重定向"><a href="#客户端重定向" class="headerlink" title="客户端重定向"></a>客户端重定向</h3><p>1、在redis-cli每次录入、查询键值，redis都会计算出该key应该送往的插槽，如果不是该客户端对应服务器插槽，redis会报错，并告知应前往的redis实例地址和端口。</p>
<p>2、redis-cli客户端提供了 –c 参数实现自动重定向。如 redis-cli -c –p 6379 登入后，再录入、查询键值对可以自动重定向。</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">redis</span>-cli -h linux<span class="number">1</span> –p <span class="number">6379</span> -c</span><br></pre></td></tr></table></figure>

<p>3、每个slot可以存储一批键值对。</p>
<h3 id="如何多键操作"><a href="#如何多键操作" class="headerlink" title="如何多键操作"></a>如何多键操作</h3><p>采用哈希算法后，会自动地分配slot，而不在一个slot下的键值，是不能使用mget，mset等多键操作。</p>
<p>如果有需求，需要将一批业务数据一起插入呢？</p>
<p>解决：可以通过{}来定义组的概念，从而使key中{}内相同内容的键值对放到一个slot中去。</p>
<figure class="highlight puppet"><table><tr><td class="code"><pre><span class="line"><span class="keyword">mset</span> &#123;a&#125;<span class="keyword">k1</span> <span class="keyword">v1</span> &#123;a&#125;<span class="keyword">k2</span> <span class="keyword">v2</span> &#123;a&#125;<span class="keyword">k3</span> <span class="keyword">v3mget</span> &#123;a&#125;<span class="keyword">k1</span> <span class="keyword">v1</span> &#123;a&#125;<span class="keyword">k2</span> <span class="keyword">v2</span> &#123;a&#125;<span class="keyword">k3</span> <span class="keyword">v3</span></span><br></pre></td></tr></table></figure>

<h2 id="集群中读取数据"><a href="#集群中读取数据" class="headerlink" title="集群中读取数据"></a>集群中读取数据</h2><p>CLUSTER KEYSLOT <key> 计算键 key 应该被放置在哪个槽上</key></p>
<p>CLUSTER COUNTKEYSINSLOT <slot> 返回槽 slot 目前包含的键值对数量</slot></p>
<p>CLUSTER GETKEYSINSLOT <slot> <count> 返回 count 个 slot 槽中的键。</count></slot></p>
<h2 id="集群中故障恢复"><a href="#集群中故障恢复" class="headerlink" title="集群中故障恢复"></a>集群中故障恢复</h2><p>问题1：如果主节点下线？从节点能否自动升为主节点？</p>
<p>答：主节点下线，从节点自动升为主节点。</p>
<p>问题2：主节点恢复后，主从关系会如何？</p>
<p>主节点恢复后，主节点变为从节点！</p>
<p>问题3：如果所有某一段插槽的主从节点都宕掉，redis服务是否还能继续?</p>
<p>答：服务是否继续，可以通过redis.conf中的cluster-require-full-coverage参数(默认关闭)进行控制。</p>
<p>主从都宕掉，意味着有一片数据，会变成真空，没法再访问了！</p>
<p>如果无法访问的数据，是连续的业务数据，我们需要停止集群，避免缺少此部分数据，造成整个业务的异常。此时可以通过配置cluster-require-full-coverage为yes.</p>
<p>如果无法访问的数据，是相对独立的，对于其他业务的访问，并不影响，那么可以继续开启集群体提供服务。此时，可以配置cluster-require-full-coverage为no。</p>
<h2 id="集群的优缺点"><a href="#集群的优缺点" class="headerlink" title="集群的优缺点"></a>集群的优缺点</h2><p>优点：</p>
<p>实现扩容</p>
<p>分摊压力</p>
<p>无中心配置相对简单</p>
<p>缺点：</p>
<p>多键操作是不被支持的</p>
<p>多键的Redis事务是不被支持的。lua脚本不被支持。</p>
<p>由于集群方案出现较晚，很多公司已经采用了其他的集群方案，而代理或者客户端分片的方案想要迁移至redis cluster，需要整体迁移而不是逐步过渡，复杂度较大。</p>
<h1 id="Redis-Java客户端操作"><a href="#Redis-Java客户端操作" class="headerlink" title="Redis Java客户端操作"></a>Redis Java客户端操作</h1><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/redis.clients/jedis --&gt;</span><span class="tag">&lt;<span class="name">dependency</span>&gt;</span>    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>redis.clients<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span>    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jedis<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span>    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.3.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.utilimport redis.clients.jedis.<span class="type">Jedisimport</span> scala.collection.<span class="type">JavaConversions</span>._<span class="comment">/** * Author vincent * Date 2019/7/13 11:28   */</span>   <span class="class"><span class="keyword">object</span> <span class="title">Demo1</span> </span>&#123;   <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;       <span class="comment">// 1. 创建一个redis的客户端       val client = new Jedis(&quot;linux1&quot;, 8000)       // 2. 使用客户端进行各种操作       //        client.set(&quot;k1&quot;, &quot;v1&quot;)       //        client.set(&quot;name&quot;, &quot;李四&quot;)       //        client.lpush(&quot;list1&quot;, &quot;a&quot;, &quot;b&quot;, &quot;c&quot;)       //        val data: util.List[String] = client.lrange(&quot;list1&quot;, 0, -1)       //        import scala.collection.JavaConversions._       //        import scala.collection.JavaConverters._       //        for (ele &lt;- data.asScala) &#123;       //            println(ele)       //        &#125;   //        client.sadd(&quot;set1&quot;, &quot;a&quot;, &quot;b&quot;, &quot;c&quot;)   //        val set: util.Set[String] = client.smembers(&quot;set1&quot;)       /*for(e &lt;- set)&#123;           println(e)       &#125;*/       /*val it: util.Iterator[String] = set.iterator()       while(it.hasNext)&#123;           println(it.next())       &#125;*/   //        client.hset(&quot;h1&quot;, &quot;k1&quot;, &quot;v1&quot;)              /*val map = Map(&quot;k1&quot;-&gt; &quot;v1&quot;, &quot;k2&quot; -&gt; &quot;v2&quot;)       client.hmset(&quot;s2&quot;, map)*/              /*val map: util.Map[String, String] = client.hgetAll(&quot;s2&quot;)       for((k, v) &lt;- map)&#123;           println(v)       &#125;*/              // 3. 关闭客户端       client.close()   &#125;   &#125;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.redisimport redis.clients.jedis.&#123;<span class="type">HostAndPort</span>, <span class="type">Jedis</span>, <span class="type">JedisCluster</span>&#125;<span class="comment">/** * Author vincent * Date 2019/7/13 11:28   */</span>   <span class="class"><span class="keyword">object</span> <span class="title">Demo2</span> </span>&#123;   <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;       <span class="keyword">val</span> hosts = <span class="keyword">new</span> java.util.<span class="type">HashSet</span>[<span class="type">HostAndPort</span>]()       hosts.add(<span class="keyword">new</span> <span class="type">HostAndPort</span>(<span class="string">&quot;linux1&quot;</span>, <span class="number">6379</span>))       hosts.add(<span class="keyword">new</span> <span class="type">HostAndPort</span>(<span class="string">&quot;linux2&quot;</span>, <span class="number">6380</span>))            <span class="keyword">val</span> cluster = <span class="keyword">new</span> <span class="type">JedisCluster</span>(hosts)              cluster.set(<span class="string">&quot;c2&quot;</span>, <span class="string">&quot;v1&quot;</span>)              cluster.close()   &#125;   &#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Nosql</category>
      </categories>
  </entry>
  <entry>
    <title>Zookeeper</title>
    <url>/Zookeeper/</url>
    <content><![CDATA[<h1 id="Zookeeper入门"><a href="#Zookeeper入门" class="headerlink" title="Zookeeper入门"></a>Zookeeper入门</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Zookeeper是一个开源的分布式的，为分布式应用提供<strong>协调服务</strong>的Apache项目。</p>
<p>Zookeeper从设计模式角度来理解，是一个基于<strong>观察者模式</strong>设计的分布式服务管理框架，它负责<strong>存储和管理大家都关心的数据</strong>，然后接受观察者的注册，一旦这些数据的状态发生了变化，Zookeeper就负责通知已经在Zookeeper上注册的那些观察者做出相应的反应.</p>
<p>Zookeeper = 文件系统 + 通知机制</p>
<h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><p><img src="/Zookeeper/130.png" alt="130"></p>
<h2 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h2><p><img src="/Zookeeper/131.png" alt="131"></p>
<h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><p>提供的服务包括：统一命名服务、统一配置管理、统一集群管理、服务器节点动态上下线、软负载均衡等。</p>
<p>统一命名服务</p>
<p><img src="/Zookeeper/132.png" alt="132"></p>
<p>统一配置管理</p>
<p><img src="/Zookeeper/133.png" alt="133"></p>
<p>统一集群管理</p>
<p><img src="/Zookeeper/134.png" alt="134"></p>
<p>服务器节点动态上下线</p>
<p><img src="/Zookeeper/135.png" alt="135"></p>
<p>软负载均衡</p>
<p><img src="/Zookeeper/136.png" alt="136"></p>
<h1 id="Zookeeper安装部署"><a href="#Zookeeper安装部署" class="headerlink" title="Zookeeper安装部署"></a>Zookeeper安装部署</h1><h2 id="Zookeeper安装"><a href="#Zookeeper安装" class="headerlink" title="Zookeeper安装"></a>Zookeeper安装</h2><p>1）安装前准备</p>
<p>（1）安装Jdk</p>
<p>（2）拷贝Zookeeper安装包到Linux系统下</p>
<p>（3）解压到指定目录并改名</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 software]$ tar -zxvf apache-zookeeper-3.6.1-bin.tar.gz -C /opt/module/</span><br><span class="line">[vincent@linux1 software]$ mv apache-zookeeper-3.6.1-bin zookeeper</span><br></pre></td></tr></table></figure>

<p>2）配置修改</p>
<p>（1）将/opt/module/zookeeper/conf这个路径下的zoo_sample.cfg修改为zoo.cfg；</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 conf]$ cd /opt/module/zookeeper/conf</span><br><span class="line">[vincent@linux1 conf]$ mv zoo_sample.cfg zoo.cfg</span><br></pre></td></tr></table></figure>

<p>​    （2）打开zoo.cfg文件，修改dataDir路径：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 zookeeper-3.5.7]$ vim zoo.cfg</span><br></pre></td></tr></table></figure>

<p>修改如下内容：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">dataDir=/opt/module/zookeeper/zkData</span><br></pre></td></tr></table></figure>

<p>​    （3）在/opt/module/zookeeper/这个目录上创建zkData文件夹</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 zookeeper]$ cd /opt/module/zookeeper</span><br><span class="line">[vincent@linux1 zookeeper]$ mkdir zkData</span><br></pre></td></tr></table></figure>

<p>3）操作Zookeeper</p>
<p>（1）启动Zookeeper</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 zookeeper]$ bin/zkServer.sh start</span><br></pre></td></tr></table></figure>

<p>（2）查看进程是否启动</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 zookeeper]$ jps</span><br><span class="line">4020 Jps</span><br><span class="line">4001 QuorumPeerMain</span><br></pre></td></tr></table></figure>

<p>（3）查看状态：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 zookeeper]$ bin/zkServer.sh status</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper/bin/../conf/zoo.cfg</span><br><span class="line">Client port found: 2181. Client address: localhost.</span><br><span class="line">Mode: standalone</span><br></pre></td></tr></table></figure>

<p>（4）启动客户端：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 zookeeper]$ bin/zkCli.sh</span><br></pre></td></tr></table></figure>

<p>（5）退出客户端：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 0] quit</span><br></pre></td></tr></table></figure>

<p>（6）停止Zookeeper</p>
<h2 id="配置参数解读"><a href="#配置参数解读" class="headerlink" title="配置参数解读"></a>配置参数解读</h2><p>Zookeeper中的配置文件zoo.cfg中参数含义解读如下：</p>
<p>1）tickTime =2000：通信心跳数，Zookeeper服务器*与客户端心跳时间，单位毫秒</p>
<p>Zookeeper使用的基本时间，服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个tickTime时间就会发送一个心跳，时间单位为毫秒。</p>
<p>它用于心跳机制，并且设置最小的session超时时间为两倍心跳时间。(session的最小超时时间是2*tickTime)</p>
<p>2）initLimit =10：LF初始通信时限</p>
<p>集群中的Follower跟随者服务器与Leader领导者服务器之间初始连接时能容忍的最多心跳数（tickTime的数量），用它来限定集群中的Zookeeper服务器连接到Leader的时限。</p>
<p>3）syncLimit =5：LF同步通信时限</p>
<p>集群中Leader与Follower之间的最大响应时间单位，假如响应超过syncLimit * tickTime，Leader认为Follwer死掉，从服务器列表中删除Follwer。</p>
<p>4）dataDir：数据文件目录+数据持久化路径</p>
<p>主要用于保存Zookeeper中的数据。</p>
<p>5）clientPort =2181：客户端连接端口</p>
<p>监听客户端连接的端口。</p>
<h2 id="Zookeeper的四字命令"><a href="#Zookeeper的四字命令" class="headerlink" title="Zookeeper的四字命令"></a>Zookeeper的四字命令</h2><p>Zookeeper支持某些特定的四字命令(The Four Letter Words) 与其进行交互，它们大多是查询命令，用来获取Zookeeper服务的当前状态及相关信息，用户在客户端可以通过telnet</p>
<p>或nc 向Zookeeper提交相应的命令。</p>
<p>​    需要在Zookeeper的配置文件中加入如下配置:</p>
<figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="attr">4lw.commands.whitelist</span>=*</span><br></pre></td></tr></table></figure>

<p>​    Zookeeper常用四字命令主要如下:</p>
<table>
<thead>
<tr>
<th>ruok</th>
<th>测试服务是否处于正确状态，如果确实如此，那么服务返回 imok ,否则不做任何响应。</th>
</tr>
</thead>
<tbody><tr>
<td>conf</td>
<td>3.3.0版本引入的，打印出服务相关配置的详细信息</td>
</tr>
<tr>
<td>cons</td>
<td>列出所有连接到这台服务器的客户端全部会话详细信息。包括 接收/发送的包数量，会话id，操作延迟、最后的操作执行等等信息</td>
</tr>
<tr>
<td>crst</td>
<td>重置所有连接的连接和会话统计信息</td>
</tr>
<tr>
<td>dump</td>
<td>列出那些比较重要的会话和临时节点。这个命令只能在leader节点上有用</td>
</tr>
<tr>
<td>envi</td>
<td>打印出服务环境的详细信息</td>
</tr>
</tbody></table>
<h1 id="Zookeeper内部原理"><a href="#Zookeeper内部原理" class="headerlink" title="Zookeeper内部原理"></a>Zookeeper内部原理</h1><h2 id="节点类型"><a href="#节点类型" class="headerlink" title="节点类型"></a>节点类型</h2><p><img src="/Zookeeper/137.png" alt="137"></p>
<h2 id="Stat结构体"><a href="#Stat结构体" class="headerlink" title="Stat结构体"></a>Stat结构体</h2><p>（1）czxid-创建节点的事务zxid</p>
<p>每次修改ZooKeeper状态都会收到一个zxid形式的时间戳，也就是ZooKeeper事务ID。</p>
<p>事务ID是ZooKeeper中所有修改总的次序。每个修改都有唯一的zxid，如果zxid1小于zxid2，那么zxid1在zxid2之前发生。</p>
<p>（2）ctime - znode被创建的毫秒数(从1970年开始)</p>
<p>（3）mzxid - znode最后更新的事务zxid</p>
<p>（4）mtime - znode最后修改的毫秒数(从1970年开始)</p>
<p>（5）pZxid-znode最后更新的子节点zxid</p>
<p>（6）cversion - znode子节点变化号，znode子节点修改次数</p>
<p>（7）dataversion - znode数据变化号</p>
<p>（8）aclVersion - znode访问控制列表的变化号</p>
<p>（9）ephemeralOwner- 如果是临时节点，这个是znode拥有者的session id。如果不是临时节点则是0。</p>
<p>（10）dataLength- znode的数据长度</p>
<p>（11）numChildren - znode子节点数量</p>
<h2 id="监听器原理"><a href="#监听器原理" class="headerlink" title="监听器原理"></a>监听器原理</h2><p><img src="/Zookeeper/138.png" alt="138"></p>
<h2 id="选举机制"><a href="#选举机制" class="headerlink" title="选举机制"></a>选举机制</h2><ol>
<li>半数机制：集群中半数以上机器存活，集群可用。所以Zookeeper适合安装奇数台服务器。</li>
<li>Zookeeper虽然在配置文件中并没有指定Master和Slave。但是，Zookeeper工作时，是有一个节点为Leader，其他则为Follower，Leader是通过内部的选举机制临时产生的。</li>
<li>以一个简单的例子来说明整个选举的过程。</li>
</ol>
<p>假设有五台服务器组成的Zookeeper集群，它们的id从1-5，同时它们都是最新启动的，也就是没有历史数据，在存放数据量这一点上，都是一样的。假设这些服务器依序启动，来看看会发生什么。</p>
<p><img src="/Zookeeper/139.png" alt="139"></p>
<p>（1）服务器1启动，发起一次选举。服务器1投自己一票。此时服务器1票数一票，不够半数以上（3票），选举无法完成，服务器1状态保持为LOOKING；</p>
<p>（2）服务器2启动，再发起一次选举。服务器1和2分别投自己一票并交换选票信息：此时服务器1发现服务器2的ID比自己目前投票推举的（服务器1）大，更改选票为推举服务器2。此时服务器1票数0票，服务器2票数2票，没有半数以上结果，选举无法完成，服务器1，2状态保持LOOKING</p>
<p>（3）服务器3启动，发起一次选举。此时服务器1和2都会更改选票为服务器3。此次投票结果：服务器1为0票，服务器2为0票，服务器3为3票。此时服务器3的票数已经超过半数，服务器3当选Leader。服务器1，2更改状态为FOLLOWING，服务器3更改状态为LEADING；</p>
<p>（4）服务器4启动，发起一次选举。此时服务器1，2，3已经不是LOOKING状态，不会更改选票信息。交换选票信息结果：服务器3为3票，服务器4为1票。此时服务器4服从多数，更改选票信息为服务器3，并更改状态为FOLLOWING；</p>
<p>（5）服务器5启动，同4一样当小弟。</p>
<p>如果节点上有数据且zxid不一样，选举zxid最大的节点做leader。</p>
<h2 id="写数据流程"><a href="#写数据流程" class="headerlink" title="写数据流程"></a>写数据流程</h2><p><img src="/Zookeeper/140.png" alt="140"></p>
<h1 id="Zookeeper实战"><a href="#Zookeeper实战" class="headerlink" title="Zookeeper实战"></a>Zookeeper实战</h1><h2 id="分布式安装部署"><a href="#分布式安装部署" class="headerlink" title="分布式安装部署"></a>分布式安装部署</h2><p>1）集群规划</p>
<p>在linux1、linux2和linux3三个节点上部署Zookeeper。</p>
<p>2）解压安装</p>
<p>（1）在linux1解压Zookeeper安装包到/opt/module/目录下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 software]$ tar -zxvf apache-zookeeper-3.6.1-bin.tar.gz -C /opt/module/</span><br><span class="line">[vincent@linux1 software]$ mv apache-zookeeper-3.6.1-bin zookeeper</span><br></pre></td></tr></table></figure>

<p>3）配置服务器编号</p>
<p>（1）在/opt/module/zookeeper/这个目录下创建zkData</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 zookeeper]$ mkdir -p zkData</span><br></pre></td></tr></table></figure>

<p>（2）在/opt/module/zookeeper/zkData目录下创建一个myid的文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 zkData]$ touch myid</span><br></pre></td></tr></table></figure>

<p>添加myid文件，注意一定要在linux里面创建，在notepad++里面很可能乱码</p>
<p>（3）编辑myid文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 zkData]$ vim myid</span><br></pre></td></tr></table></figure>

<p>在文件中添加与server对应的编号：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">1</span><br></pre></td></tr></table></figure>

<p>（4）拷贝配置好的zookeeper到其他机器上</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 module ]$ xsync zookeeper</span><br></pre></td></tr></table></figure>

<p>并分别在linux2、linux3上修改myid文件中内容为2、3</p>
<p>4）配置zoo.cfg文件</p>
<p>（1）重命名/opt/module/zookeeper/conf这个目录下的zoo_sample.cfg为zoo.cfg</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 conf]$ mv zoo_sample.cfg zoo.cfg</span><br></pre></td></tr></table></figure>

<p>（2）打开zoo.cfg文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 conf]$ vim zoo.cfg</span><br></pre></td></tr></table></figure>

<p>修改数据存储路径配置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">dataDir=/opt/module/zookeeper/zkData</span><br></pre></td></tr></table></figure>

<p>增加如下配置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">######################cluster##########################</span></span></span><br><span class="line">server.1=linux1:2888:3888</span><br><span class="line">server.2=linux2:2888:3888</span><br><span class="line">server.3=linux3:2888:3888</span><br></pre></td></tr></table></figure>

<p>（3）同步zoo.cfg配置文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 conf]$ xsync zoo.cfg</span><br></pre></td></tr></table></figure>

<p>（4）配置参数解读</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">server.A=B:C:D</span><br></pre></td></tr></table></figure>

<p>A是一个数字，表示这个是第几号服务器；</p>
<p>集群模式下配置一个文件myid，这个文件在dataDir目录下，这个文件里面有一个数据就是A的值，Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server。</p>
<p>B是这个服务器的地址；</p>
<p>C是这个服务器Follower与集群中的Leader服务器交换信息的端口；</p>
<p>D是万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。</p>
<p>5）集群操作</p>
<p>（1）分别启动Zookeeper</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 zookeeper]$ bin/zkServer.sh start</span><br><span class="line">[vincent@linux2 zookeeper]$ bin/zkServer.sh start</span><br><span class="line">[vincent@linux3 zookeeper]$ bin/zkServer.sh start</span><br></pre></td></tr></table></figure>

<p>（2）查看状态</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 zookeeper]$ bin/zkServer.sh status</span><br><span class="line">[atguigu@linux2 zookeeper]$ bin/zkServer.sh status</span><br><span class="line">[atguigu@linux3 zookeeper]$ bin/zkServer.sh status</span><br></pre></td></tr></table></figure>

<p>（3）群起脚本zk</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line"> then</span><br><span class="line">        echo &quot;No Args Input!!!!&quot;</span><br><span class="line">        exit;</span><br><span class="line">fi</span><br><span class="line">for i in linux1 linux2 linux3</span><br><span class="line">do</span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)</span><br><span class="line">        echo &quot;=============== start $i zookeeper ==============&quot;</span><br><span class="line">        ssh $i /opt/module/zookeeper/bin/zkServer.sh start</span><br><span class="line">;;</span><br><span class="line">&quot;status&quot;)</span><br><span class="line">        echo &quot;=============== status $i zookeeper =============&quot;</span><br><span class="line">        ssh $i /opt/module/zookeeper/bin/zkServer.sh status</span><br><span class="line">;;</span><br><span class="line">&quot;stop&quot;)</span><br><span class="line">        echo &quot;=============== stop $i zookeeper ===============&quot;</span><br><span class="line">        ssh $i /opt/module/zookeeper/bin/zkServer.sh stop</span><br><span class="line">;;</span><br><span class="line">*)</span><br><span class="line">        echo &quot;Input Args Error!!!!&quot;</span><br><span class="line">;;</span><br><span class="line">esac</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<h2 id="客户端命令行操作"><a href="#客户端命令行操作" class="headerlink" title="客户端命令行操作"></a>客户端命令行操作</h2><table>
<thead>
<tr>
<th>命令基本语法</th>
<th>功能描述</th>
</tr>
</thead>
<tbody><tr>
<td>help</td>
<td>显示所有操作命令</td>
</tr>
<tr>
<td>ls path</td>
<td>使用 ls 命令来查看当前znode的子节点 [可监听]  -w 监听子节点变化  -s  附加次级信息</td>
</tr>
<tr>
<td>create</td>
<td>普通创建  -s 含有序列  -e 临时（重启或者超时消失）</td>
</tr>
<tr>
<td>get path</td>
<td>获得节点的值 [可监听]  -w 监听节点内容变化  -s  附加次级信息</td>
</tr>
<tr>
<td>set</td>
<td>设置节点的具体值</td>
</tr>
<tr>
<td>stat</td>
<td>查看节点状态</td>
</tr>
<tr>
<td>delete</td>
<td>删除节点</td>
</tr>
<tr>
<td>deleteall</td>
<td>递归删除节点</td>
</tr>
</tbody></table>
<p>1）启动客户端</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 zookeeper]$ bin/zkCli.sh</span><br></pre></td></tr></table></figure>

<p>2）显示所有操作命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 0] help</span><br></pre></td></tr></table></figure>

<p>3）查看当前znode中所包含的内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 1] ls /</span><br><span class="line"></span><br><span class="line">[zookeeper]</span><br></pre></td></tr></table></figure>

<p>4）查看当前节点详细数据</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 1] ls -s /</span><br><span class="line"></span><br><span class="line">&#x27;ls2&#x27; has been deprecated. Please use &#x27;ls [-s] path&#x27; instead.</span><br><span class="line">[zookeeper]</span><br><span class="line">cZxid = 0x0</span><br><span class="line">ctime = Thu Jan 01 08:00:00 CST 1970</span><br><span class="line">mZxid = 0x0</span><br><span class="line">mtime = Thu Jan 01 08:00:00 CST 1970</span><br><span class="line">pZxid = 0x0</span><br><span class="line">cversion = -1</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 0</span><br><span class="line">numChildren = 1</span><br></pre></td></tr></table></figure>

<p>5）分别创建2个普通节点</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 3] create /sanguo &quot;jinlian&quot;</span><br><span class="line">Created /sanguo</span><br><span class="line">[zk: localhost:2181(CONNECTED) 4] create /sanguo/shuguo &quot;liubei&quot;</span><br><span class="line">Created /sanguo/shuguo</span><br></pre></td></tr></table></figure>

<p>6）获得节点的值</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 5] get /sanguo</span><br><span class="line">jinlian</span><br><span class="line">cZxid = 0x100000003</span><br><span class="line">ctime = Wed Aug 29 00:03:23 CST 2018</span><br><span class="line">mZxid = 0x100000003</span><br><span class="line">mtime = Wed Aug 29 00:03:23 CST 2018</span><br><span class="line">pZxid = 0x100000004</span><br><span class="line">cversion = 1</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 7</span><br><span class="line">numChildren = 1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[zk: localhost:2181(CONNECTED) 6] get /sanguo/shuguo</span><br><span class="line">liubei</span><br><span class="line">cZxid = 0x100000004</span><br><span class="line">ctime = Wed Aug 29 00:04:35 CST 2018</span><br><span class="line">mZxid = 0x100000004</span><br><span class="line">mtime = Wed Aug 29 00:04:35 CST 2018</span><br><span class="line">pZxid = 0x100000004</span><br><span class="line">cversion = 0</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 6</span><br><span class="line">numChildren = 0</span><br></pre></td></tr></table></figure>

<p>7）创建短暂节点</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 7] create -e /sanguo/wuguo &quot;zhouyu&quot;</span><br><span class="line">Created /sanguo/wuguo</span><br></pre></td></tr></table></figure>

<p>8）创建带序号的节点</p>
<p>（1）先创建一个普通的根节点/sanguo/weiguo</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 1] create /sanguo/weiguo &quot;caocao&quot;</span><br><span class="line">Created /sanguo/weiguo</span><br></pre></td></tr></table></figure>

<p>（2）创建带序号的节点</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 2] create -s /sanguo/weiguo/xiaoqiao &quot;jinlian&quot;</span><br><span class="line">Created /sanguo/weiguo/xiaoqiao0000000000</span><br><span class="line"></span><br><span class="line">[zk: localhost:2181(CONNECTED) 3] create -s /sanguo/weiguo/daqiao &quot;jinlian&quot;</span><br><span class="line">Created /sanguo/weiguo/daqiao0000000001</span><br><span class="line"></span><br><span class="line">[zk: localhost:2181(CONNECTED) 4] create -s /sanguo/weiguo/diaocan &quot;jinlian&quot;</span><br><span class="line">Created /sanguo/weiguo/diaocan0000000002</span><br></pre></td></tr></table></figure>

<p>如果原来没有序号节点，序号从0开始依次递增。如果原节点下已有2个节点，则再排序时从2开始，以此类推。</p>
<p>9）修改节点数据值</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 6] set /sanguo/weiguo &quot;simayi&quot;</span><br></pre></td></tr></table></figure>

<p>10）节点的值变化监听</p>
<p>（1）在hadoop104主机上注册监听/sanguo节点数据变化</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 26] [zk: localhost:2181(CONNECTED) 8] get -w /sanguo </span><br></pre></td></tr></table></figure>

<p>（2）在hadoop103主机上修改/sanguo节点的数据</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 1] set /sanguo &quot;xisi&quot;</span><br></pre></td></tr></table></figure>

<p>​    （3）观察hadoop104主机收到数据变化的监听</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">WATCHER::</span><br><span class="line">WatchedEvent state:SyncConnected type:NodeDataChanged path:/sanguo</span><br></pre></td></tr></table></figure>

<p>11）节点的子节点变化监听（路径变化）</p>
<p>（1）在hadoop104主机上注册监听/sanguo节点的子节点变化</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 1] ls -w /sanguo</span><br><span class="line">[aa0000000001, server101]</span><br></pre></td></tr></table></figure>

<p>​    （2）在hadoop103主机/sanguo节点上创建子节点</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 2] create /sanguo/jin &quot;simayi&quot;</span><br><span class="line">Created /sanguo/jin</span><br></pre></td></tr></table></figure>

<p>​    （3）观察hadoop104主机收到子节点变化的监听</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">WATCHER::</span><br><span class="line">WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/sanguo</span><br></pre></td></tr></table></figure>

<p>12）删除节点</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 4] delete /sanguo/jin</span><br></pre></td></tr></table></figure>

<p>13）递归删除节点</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 15] deleteall /sanguo/shuguo</span><br></pre></td></tr></table></figure>

<p>14）查看节点状态</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 17] stat /sanguo</span><br><span class="line">cZxid = 0x100000003</span><br><span class="line">ctime = Wed Aug 29 00:03:23 CST 2018</span><br><span class="line">mZxid = 0x100000011</span><br><span class="line">mtime = Wed Aug 29 00:21:23 CST 2018</span><br><span class="line">pZxid = 0x100000014</span><br><span class="line">cversion = 9</span><br><span class="line">dataVersion = 1</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 4</span><br><span class="line">numChildren = 1</span><br></pre></td></tr></table></figure>

<h2 id="API应用"><a href="#API应用" class="headerlink" title="API应用"></a>API应用</h2><p>1.创建maven工程</p>
<p>2.修改pom文件</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">version</span>&gt;</span>RELEASE<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">		<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.logging.log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">version</span>&gt;</span>2.8.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">		<span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.zookeeper/zookeeper --&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.zookeeper<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>zookeeper<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">version</span>&gt;</span>3.5.7<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>3.拷贝log4j.properties文件到项目根目录</p>
<p>需要在项目的src/main/resources目录下，新建一个文件，命名为“log4j.properties”，在文件中填入：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">log4j.rootLogger=INFO, stdout  </span><br><span class="line">log4j.appender.stdout=org.apache.log4j.ConsoleAppender  </span><br><span class="line">log4j.appender.stdout.layout=org.apache.log4j.PatternLayout  </span><br><span class="line">log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n  </span><br><span class="line">log4j.appender.logfile=org.apache.log4j.FileAppender  </span><br><span class="line">log4j.appender.logfile.File=target/spring.log  </span><br><span class="line">log4j.appender.logfile.layout=org.apache.log4j.PatternLayout  </span><br><span class="line">log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n  </span><br></pre></td></tr></table></figure>

<p>4.创建ZooKeeper客户端</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//获取客户端连接对象</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> String connectString = <span class="string">&quot;linux1:2181,linux2:2181,linux3:2181&quot;</span>;</span><br><span class="line">	</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> sessionTimeout = <span class="number">10000</span>;</span><br><span class="line"><span class="keyword">private</span> ZooKeeper zkClient = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Before</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">	zkClient = <span class="keyword">new</span> ZooKeeper(connectString, sessionTimeout, <span class="keyword">new</span> Watcher() &#123;</span><br><span class="line">			<span class="comment">//回调方法，当watcher对象监听到感兴趣事件后，会调用prcess方法</span></span><br><span class="line">			<span class="comment">//WatchEvent：事件对象，封装了所发生的的事</span></span><br><span class="line">      <span class="meta">@Override</span></span><br><span class="line">			<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent event)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">				<span class="comment">// 收到事件通知后的回调函数（用户的业务逻辑）</span></span><br><span class="line">				System.out.println(event.getType() + <span class="string">&quot;--&quot;</span> + event.getPath());</span><br><span class="line"></span><br><span class="line">				<span class="comment">// 再次启动监听</span></span><br><span class="line">				<span class="keyword">try</span> &#123;</span><br><span class="line">					zkClient.getChildren(<span class="string">&quot;/&quot;</span>, <span class="keyword">true</span>);</span><br><span class="line">				&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">					e.printStackTrace();</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">   );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>5.创建子节点</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 创建子节点</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">create</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 参数1：要创建的节点的路径； 参数2：节点数据 ； 参数3：节点权限 ；参数4：节点的类型</span></span><br><span class="line">		String nodeCreated = zkClient.create(<span class="string">&quot;/vincent&quot;</span>, <span class="string">&quot;jinlian&quot;</span>.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>6.获取子节点并监听节点变化</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取子节点</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getChildren</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">		List&lt;String&gt; children = zkClient.getChildren(<span class="string">&quot;/&quot;</span>, <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">		<span class="keyword">for</span> (String child : children) &#123;</span><br><span class="line">			System.out.println(child);</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 延时阻塞</span></span><br><span class="line">  	<span class="comment">// 只管一次监听</span></span><br><span class="line">		Thread.sleep(Long.MAX_VALUE);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>7.判断Znode是否存在</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 判断znode是否存在</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">exist</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">	Stat stat = zkClient.exists(<span class="string">&quot;/eclipse&quot;</span>, <span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line">	System.out.println(stat == <span class="keyword">null</span> ? <span class="string">&quot;not exist&quot;</span> : <span class="string">&quot;exist&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="监听服务器节点动态上下线案例"><a href="#监听服务器节点动态上下线案例" class="headerlink" title="监听服务器节点动态上下线案例"></a>监听服务器节点动态上下线案例</h2><p>1）需求</p>
<p>某分布式系统中，主节点可以有多台，可以动态上下线，任意一台客户端都能实时感知到主节点服务器的上下线。</p>
<p>2）需求分析</p>
<p><img src="/Zookeeper/141.png" alt="141"></p>
<p>3）具体实现</p>
<p>0）先在集群上创建/servers节点</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 10] create /servers &quot;servers&quot;</span><br><span class="line">Created /servers</span><br></pre></td></tr></table></figure>

<p>1）服务器端向Zookeeper注册代码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.CreateMode;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.WatchedEvent;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.Watcher;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.ZooKeeper;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.ZooDefs.Ids;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DistributeServer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">static</span> String connectString = <span class="string">&quot;linux1:2181,linux2:2181,linux3:2181&quot;</span>;</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> sessionTimeout = <span class="number">10000</span>;</span><br><span class="line">	<span class="keyword">private</span> ZooKeeper zk = <span class="keyword">null</span>;</span><br><span class="line">	<span class="keyword">private</span> String parentNode = <span class="string">&quot;/servers&quot;</span>;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">// 创建到zk的客户端连接</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getConnect</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line">		</span><br><span class="line">		zk = <span class="keyword">new</span> ZooKeeper(connectString, sessionTimeout, <span class="keyword">new</span> Watcher() &#123;</span><br><span class="line"></span><br><span class="line">			<span class="meta">@Override</span></span><br><span class="line">			<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent event)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">			&#125;</span><br><span class="line">		&#125;);</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">// 注册服务器</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">registServer</span><span class="params">(String hostname)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line"></span><br><span class="line">		String create = zk.create(parentNode + <span class="string">&quot;/server&quot;</span>, hostname.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL);</span><br><span class="line">		</span><br><span class="line">		System.out.println(hostname +<span class="string">&quot; is online &quot;</span>+ create);</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">// 业务功能</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">business</span><span class="params">(String hostname)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">		System.out.println(hostname+<span class="string">&quot; is working ...&quot;</span>);</span><br><span class="line">		</span><br><span class="line">		Thread.sleep(Long.MAX_VALUE);</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 1获取zk连接</span></span><br><span class="line">		DistributeServer server = <span class="keyword">new</span> DistributeServer();</span><br><span class="line">		server.getConnect();</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 2 利用zk连接注册服务器信息</span></span><br><span class="line">		server.registServer(args[<span class="number">0</span>]);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 3 启动业务功能</span></span><br><span class="line">		server.business(args[<span class="number">0</span>]);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>2）客户端代码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.WatchedEvent;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.Watcher;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.ZooKeeper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DistributeClient</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">static</span> String connectString = <span class="string">&quot;hadoop102:2181,hadoop103:2181,hadoop104:2181&quot;</span>;</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> sessionTimeout = <span class="number">10000</span>;</span><br><span class="line">	<span class="keyword">private</span> ZooKeeper zk = <span class="keyword">null</span>;</span><br><span class="line">	<span class="keyword">private</span> String parentNode = <span class="string">&quot;/servers&quot;</span>;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 创建到zk的客户端连接</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getConnect</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">		zk = <span class="keyword">new</span> ZooKeeper(connectString, sessionTimeout, <span class="keyword">new</span> Watcher() &#123;</span><br><span class="line"></span><br><span class="line">			<span class="meta">@Override</span></span><br><span class="line">			<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent event)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">				<span class="comment">// 再次启动监听</span></span><br><span class="line">				<span class="keyword">try</span> &#123;</span><br><span class="line">					getServerList();</span><br><span class="line">				&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">					e.printStackTrace();</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 获取服务器列表信息</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getServerList</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 1获取服务器子节点信息，并且对父节点进行监听</span></span><br><span class="line">		List&lt;String&gt; children = zk.getChildren(parentNode, <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2存储服务器信息列表</span></span><br><span class="line">		ArrayList&lt;String&gt; servers = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">		</span><br><span class="line">        <span class="comment">// 3遍历所有节点，获取节点中的主机名称信息</span></span><br><span class="line">		<span class="keyword">for</span> (String child : children) &#123;</span><br><span class="line">			<span class="keyword">byte</span>[] data = zk.getData(parentNode + <span class="string">&quot;/&quot;</span> + child, <span class="keyword">false</span>, <span class="keyword">null</span>);</span><br><span class="line"></span><br><span class="line">			servers.add(<span class="keyword">new</span> String(data));</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4打印服务器列表信息</span></span><br><span class="line">		System.out.println(servers);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 业务功能</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">business</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line"></span><br><span class="line">		System.out.println(<span class="string">&quot;client is working ...&quot;</span>);</span><br><span class="line">		Thread.sleep(Long.MAX_VALUE);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 1获取zk连接</span></span><br><span class="line">		DistributeClient client = <span class="keyword">new</span> DistributeClient();</span><br><span class="line">		client.getConnect();</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 2获取servers的子节点信息，从中获取服务器信息列表</span></span><br><span class="line">		client.getServerList();</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 3业务进程启动</span></span><br><span class="line">		client.business();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>分布式</category>
      </categories>
  </entry>
</search>
