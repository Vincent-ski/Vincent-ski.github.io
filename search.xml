<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Mybatis</title>
    <url>/Mybatis/</url>
    <content><![CDATA[<h1 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h1><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//com.vincent.pojo</span></span><br><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="meta">@NoArgsConstructor</span></span><br><span class="line"><span class="meta">@AllArgsConstructor</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Employee</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Integer empId;</span><br><span class="line">    <span class="keyword">private</span> String empName;</span><br><span class="line">    <span class="keyword">private</span> Double empSalary;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//com.vincent.dao</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">EmployeeMapper</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function">Employee <span class="title">selectEmployee</span><span class="params">(Integer empId)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 功能：根据主键值删除数据库记录</span></span><br><span class="line">    <span class="comment">// 入参：作为删除依据的主键值</span></span><br><span class="line">    <span class="comment">// 返回值：受影响的行数</span></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">deleteEmployeeById</span><span class="params">(Integer empId)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 功能：根据实体类对象中封装的数据执行更新操作</span></span><br><span class="line">    <span class="comment">// 入参：封装更新数据的实体类对象</span></span><br><span class="line">    <span class="comment">// 返回值：受影响的行数</span></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">updateEmployeeById</span><span class="params">(Employee employee)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">insertEmployee</span><span class="params">(Employee employee)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">updateEmployee</span><span class="params">(<span class="meta">@Param(&quot;empId&quot;)</span> Integer empId, <span class="meta">@Param(&quot;empSalary&quot;)</span> Double empSalary)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">updateEmployeeByMap</span><span class="params">(Map&lt;String, Object&gt; paramMap)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function">Map&lt;String,Object&gt; <span class="title">selectEmpNameAndMaxSalary</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function">List&lt;Employee&gt; <span class="title">selectAll</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="单个简单类型参数"><a href="#单个简单类型参数" class="headerlink" title="单个简单类型参数"></a>单个简单类型参数</h2><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Employee selectEmployee(Integer empId); --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">select</span> <span class="attr">id</span>=<span class="string">&quot;selectEmployee&quot;</span> <span class="attr">resultType</span>=<span class="string">&quot;com.vincent.pojo.Employee&quot;</span>&gt;</span></span><br><span class="line">  select emp_id empId,emp_name empName,emp_salary empSalary from t_emp where emp_id=#&#123;empId&#125;</span><br><span class="line"><span class="tag">&lt;/<span class="name">select</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="实体类类型参数"><a href="#实体类类型参数" class="headerlink" title="实体类类型参数"></a>实体类类型参数</h2><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- int insertEmployee(Employee employee); --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">insert</span> <span class="attr">id</span>=<span class="string">&quot;insertEmployee&quot;</span>&gt;</span></span><br><span class="line">  insert into t_emp(emp_name,emp_salary) values(#&#123;empName&#125;,#&#123;empSalary&#125;)</span><br><span class="line"><span class="tag">&lt;/<span class="name">insert</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>Mybatis会根据#{}中传入的数据，加工成getXxx()方法，通过反射在实体类对象中调用这个方法，从而获取到对应的数据。填充到#{}解析后的问号占位符这个位置。</p>
<h2 id="零散的简单类型数据"><a href="#零散的简单类型数据" class="headerlink" title="零散的简单类型数据"></a>零散的简单类型数据</h2><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- int updateEmployee(@Param(&quot;empId&quot;) Integer empId,@Param(&quot;empSalary&quot;) Double empSalary); --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">update</span> <span class="attr">id</span>=<span class="string">&quot;updateEmployee&quot;</span>&gt;</span></span><br><span class="line">  update t_emp set emp_salary=#&#123;empSalary&#125; where emp_id=#&#123;empId&#125;</span><br><span class="line"><span class="tag">&lt;/<span class="name">update</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="Map类型参数"><a href="#Map类型参数" class="headerlink" title="Map类型参数"></a>Map类型参数</h2><p>有很多零散的参数需要传递，但是没有对应的实体类类型可以使用。使用@Param注解一个一个传入又太麻烦了。所以都封装到Map中。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- int updateEmployeeByMap(Map&lt;String, Object&gt; paramMap); --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">update</span> <span class="attr">id</span>=<span class="string">&quot;updateEmployeeByMap&quot;</span>&gt;</span></span><br><span class="line">  update t_emp set emp_salary=#&#123;empSalaryKey&#125; where emp_id=#&#123;empIdKey&#125;</span><br><span class="line"><span class="tag">&lt;/<span class="name">update</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testUpdateEmpNameByMap</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  EmployeeMapper mapper = session.getMapper(EmployeeMapper.class);</span><br><span class="line">  Map&lt;String, Object&gt; paramMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">  paramMap.put(<span class="string">&quot;empSalaryKey&quot;</span>, <span class="number">999.99</span>);</span><br><span class="line">  paramMap.put(<span class="string">&quot;empIdKey&quot;</span>, <span class="number">5</span>);</span><br><span class="line">  <span class="keyword">int</span> result = mapper.updateEmployeeByMap(paramMap);</span><br><span class="line">  log.info(<span class="string">&quot;result = &quot;</span> + result);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>#{}中写Map中的key</p>
<h2 id="返回单个简单类型数据"><a href="#返回单个简单类型数据" class="headerlink" title="返回单个简单类型数据"></a>返回单个简单类型数据</h2><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- int selectEmpCount(); --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">select</span> <span class="attr">id</span>=<span class="string">&quot;selectEmpCount&quot;</span> <span class="attr">resultType</span>=<span class="string">&quot;int&quot;</span>&gt;</span></span><br><span class="line">  select count(*) from t_emp</span><br><span class="line"><span class="tag">&lt;/<span class="name">select</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>Mybatis 内部给常用的数据类型设定了很多别名。 以 int 类型为例，可以写的名称有：int、integer、Integer、java.lang.Integer、Int、INT、INTEGER 等等。</p>
<h2 id="返回实体类对象"><a href="#返回实体类对象" class="headerlink" title="返回实体类对象"></a>返回实体类对象</h2><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Employee selectEmployee(Integer empId); --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 编写具体的SQL语句，使用id属性唯一的标记一条SQL语句 --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- resultType属性：指定封装查询结果的Java实体类的全类名 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">select</span> <span class="attr">id</span>=<span class="string">&quot;selectEmployee&quot;</span> <span class="attr">resultType</span>=<span class="string">&quot;com.vincent.pojo.Employee&quot;</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- Mybatis负责把SQL语句中的#&#123;&#125;部分替换成“?”占位符 --&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 给每一个字段设置一个别名，让别名和Java实体类中属性名一致 --&gt;</span></span><br><span class="line">  select emp_id empId,emp_name empName,emp_salary empSalary from t_emp where emp_id=#&#123;maomi&#125;</span><br><span class="line"><span class="tag">&lt;/<span class="name">select</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="返回Map类型"><a href="#返回Map类型" class="headerlink" title="返回Map类型"></a>返回Map类型</h2><p>适用于SQL查询返回的各个字段综合起来并不和任何一个现有的实体类对应，没法封装到实体类对象中。能够封装成实体类类型的，就不使用Map类型。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Map&lt;String,Object&gt; selectEmpNameAndMaxSalary(); --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Map&lt;String,Object&gt; selectEmpNameAndMaxSalary(); --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 返回工资最高的员工的姓名和他的工资 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">select</span> <span class="attr">id</span>=<span class="string">&quot;selectEmpNameAndMaxSalary&quot;</span> <span class="attr">resultType</span>=<span class="string">&quot;map&quot;</span>&gt;</span></span><br><span class="line">  SELECT</span><br><span class="line">    emp_name 员工姓名,</span><br><span class="line">    emp_salary 员工工资,</span><br><span class="line">    (SELECT AVG(emp_salary) FROM t_emp) 部门平均工资</span><br><span class="line">  FROM t_emp WHERE emp_salary=(</span><br><span class="line">    SELECT MAX(emp_salary) FROM t_emp</span><br><span class="line">  )</span><br><span class="line"><span class="tag">&lt;/<span class="name">select</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testQueryEmpNameAndSalary</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  EmployeeMapper employeeMapper = session.getMapper(EmployeeMapper.class);</span><br><span class="line">  Map&lt;String, Object&gt; resultMap = employeeMapper.selectEmpNameAndMaxSalary();</span><br><span class="line">  Set&lt;Map.Entry&lt;String, Object&gt;&gt; entrySet = resultMap.entrySet();</span><br><span class="line">  <span class="keyword">for</span> (Map.Entry&lt;String, Object&gt; entry : entrySet) &#123;</span><br><span class="line">    String key = entry.getKey();</span><br><span class="line">    Object value = entry.getValue();</span><br><span class="line">    log.info(key + <span class="string">&quot;=&quot;</span> + value);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="返回List类型"><a href="#返回List类型" class="headerlink" title="返回List类型"></a>返回List类型</h2><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- List&lt;Employee&gt; selectAll(); --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">select</span> <span class="attr">id</span>=<span class="string">&quot;selectAll&quot;</span> <span class="attr">resultType</span>=<span class="string">&quot;com.vincent.pojo.Employee&quot;</span>&gt;</span></span><br><span class="line">  select emp_id empId,emp_name empName,emp_salary empSalary</span><br><span class="line">  from t_emp</span><br><span class="line"><span class="tag">&lt;/<span class="name">select</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="返回自增主键"><a href="#返回自增主键" class="headerlink" title="返回自增主键"></a>返回自增主键</h2><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- int insertEmployee(Employee employee); --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- useGeneratedKeys属性字面意思就是“使用生成的主键” --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- keyProperty属性可以指定主键在实体类对象中对应的属性名，Mybatis会将拿到的主键值存入这个属性 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">insert</span> <span class="attr">id</span>=<span class="string">&quot;insertEmployee&quot;</span> <span class="attr">useGeneratedKeys</span>=<span class="string">&quot;true&quot;</span> <span class="attr">keyProperty</span>=<span class="string">&quot;empId&quot;</span>&gt;</span></span><br><span class="line">  insert into t_emp(emp_name,emp_salary)</span><br><span class="line">  values(#&#123;empName&#125;,#&#123;empSalary&#125;)</span><br><span class="line"><span class="tag">&lt;/<span class="name">insert</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h1 id="关联关系"><a href="#关联关系" class="headerlink" title="关联关系"></a>关联关系</h1><table>
<thead>
<tr>
<th>关联关系</th>
<th>配置项关键词</th>
<th>所在配置文件和具体位置</th>
</tr>
</thead>
<tbody><tr>
<td>对一</td>
<td>association标签/javaType属性</td>
<td>Mapper配置文件中的resultMap标签内</td>
</tr>
<tr>
<td>对多</td>
<td>collection标签/ofType属性</td>
<td>Mapper配置文件中的resultMap标签内</td>
</tr>
<tr>
<td>对一分步</td>
<td>association标签/select属性/column属性</td>
<td>Mapper配置文件中的resultMap标签内</td>
</tr>
<tr>
<td>对多分步</td>
<td>collection标签/select属性/column属性</td>
<td>Mapper配置文件中的resultMap标签内</td>
</tr>
<tr>
<td>延迟加载<br>3.4.1版本前</td>
<td>lazyLoadingEnabled设置为true  <br>aggressiveLazyLoading设置为false</td>
<td>Mybatis全局配置文件中的settings标签内</td>
</tr>
<tr>
<td>延迟加载<br>3.4.1版本后</td>
<td>lazyLoadingEnabled设置为true</td>
<td>Mybatis全局配置文件中的settings标签内</td>
</tr>
</tbody></table>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//com.vincent.pojo</span></span><br><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="meta">@NoArgsConstructor</span></span><br><span class="line"><span class="meta">@AllArgsConstructor</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Customer</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Integer customerId;</span><br><span class="line">    <span class="keyword">private</span> String customerName;</span><br><span class="line">    <span class="keyword">private</span> List&lt;Order&gt; orderList;<span class="comment">// 体现的是对多的关系</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="meta">@NoArgsConstructor</span></span><br><span class="line"><span class="meta">@AllArgsConstructor</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Order</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Integer orderId;</span><br><span class="line">    <span class="keyword">private</span> String orderName;</span><br><span class="line">    <span class="keyword">private</span> Customer customer;<span class="comment">// 体现的是对一的关系</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//com.vincent.dao</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">CustomerMapper</span> </span>&#123;</span><br><span class="line">    <span class="function">Customer <span class="title">selectCustomerWithOrderList</span><span class="params">(Integer customerId)</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">OrderMapper</span> </span>&#123;</span><br><span class="line">    <span class="function">Order <span class="title">selectOrderWithCustomer</span><span class="params">(Integer orderId)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="对一"><a href="#对一" class="headerlink" title="对一"></a>对一</h2><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;</span></span><br><span class="line"><span class="meta">&lt;!DOCTYPE <span class="meta-keyword">mapper</span></span></span><br><span class="line"><span class="meta">        <span class="meta-keyword">PUBLIC</span> <span class="meta-string">&quot;-//mybatis.org//DTD Mapper 3.0//EN&quot;</span></span></span><br><span class="line"><span class="meta">        <span class="meta-string">&quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- mapper是根标签，namespace属性：在Mybatis全局范围内找到一个具体的Mapper配置 --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 引入接口后，为了方便通过接口全类名来找到Mapper配置文件，所以通常将namespace属性设置为接口全类名 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">mapper</span> <span class="attr">namespace</span>=<span class="string">&quot;com.vincent.dao.OrderMapper&quot;</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 创建resultMap实现“对一”关联关系映射 --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- id属性：通常设置为这个resultMap所服务的那条SQL语句的id加上“ResultMap” --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- type属性：要设置为这个resultMap所服务的那条SQL语句最终要返回的类型 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">resultMap</span> <span class="attr">id</span>=<span class="string">&quot;selectOrderWithCustomerResultMap&quot;</span> <span class="attr">type</span>=<span class="string">&quot;com.vincent.pojo.Order&quot;</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 先设置Order自身属性和字段的对应关系 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">id</span> <span class="attr">column</span>=<span class="string">&quot;order_id&quot;</span> <span class="attr">property</span>=<span class="string">&quot;orderId&quot;</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">result</span> <span class="attr">column</span>=<span class="string">&quot;order_name&quot;</span> <span class="attr">property</span>=<span class="string">&quot;orderName&quot;</span>/&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!-- 使用association标签配置“对一”关联关系 --&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- property属性：在Order类中对一的一端进行引用时使用的属性名 --&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- javaType属性：一的一端类的全类名 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">association</span> <span class="attr">property</span>=<span class="string">&quot;customer&quot;</span> <span class="attr">javaType</span>=<span class="string">&quot;com.vincent.pojo.Customer&quot;</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!-- 配置Customer类的属性和字段名之间的对应关系 --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">id</span> <span class="attr">column</span>=<span class="string">&quot;customer_id&quot;</span> <span class="attr">property</span>=<span class="string">&quot;customerId&quot;</span>/&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">result</span> <span class="attr">column</span>=<span class="string">&quot;customer_name&quot;</span> <span class="attr">property</span>=<span class="string">&quot;customerName&quot;</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">association</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">resultMap</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 编写具体的SQL语句，使用id属性唯一的标记一条SQL语句 --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- resultType属性：指定封装查询结果的Java实体类的全类名 --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- Order selectOrderWithCustomer(Integer orderId); --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">select</span> <span class="attr">id</span>=<span class="string">&quot;selectOrderWithCustomer&quot;</span> <span class="attr">resultMap</span>=<span class="string">&quot;selectOrderWithCustomerResultMap&quot;</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- Mybatis负责把SQL语句中的#&#123;&#125;部分替换成“?”占位符，在#&#123;&#125;内部还是要声明一个见名知意的名称 --&gt;</span></span><br><span class="line">        SELECT order_id,order_name,c.customer_id,customer_name</span><br><span class="line">        FROM t_order o</span><br><span class="line">        LEFT JOIN t_customer c</span><br><span class="line">        ON o.customer_id=c.customer_id</span><br><span class="line">        WHERE o.order_id=#&#123;orderId&#125;</span><br><span class="line">    <span class="tag">&lt;/<span class="name">select</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">mapper</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="对多"><a href="#对多" class="headerlink" title="对多"></a>对多</h2><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;</span></span><br><span class="line"><span class="meta">&lt;!DOCTYPE <span class="meta-keyword">mapper</span></span></span><br><span class="line"><span class="meta">        <span class="meta-keyword">PUBLIC</span> <span class="meta-string">&quot;-//mybatis.org//DTD Mapper 3.0//EN&quot;</span></span></span><br><span class="line"><span class="meta">        <span class="meta-string">&quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- mapper是根标签，namespace属性：在Mybatis全局范围内找到一个具体的Mapper配置 --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 引入接口后，为了方便通过接口全类名来找到Mapper配置文件，所以通常将namespace属性设置为接口全类名 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">mapper</span> <span class="attr">namespace</span>=<span class="string">&quot;com.vincent.dao.CustomerMapper&quot;</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 配置resultMap实现从Customer到OrderList的“对多”关联关系 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">resultMap</span> <span class="attr">id</span>=<span class="string">&quot;selectCustomerWithOrderListResultMap&quot;</span> <span class="attr">type</span>=<span class="string">&quot;com.vincent.pojo.Customer&quot;</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 映射Customer本身的属性 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">id</span> <span class="attr">column</span>=<span class="string">&quot;customer_id&quot;</span> <span class="attr">property</span>=<span class="string">&quot;customerId&quot;</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">result</span> <span class="attr">column</span>=<span class="string">&quot;customer_name&quot;</span> <span class="attr">property</span>=<span class="string">&quot;customerName&quot;</span>/&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!-- collection标签：映射“对多”的关联关系 --&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- property属性：在Customer类中，关联“多”的一端的属性名 --&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- ofType属性：集合属性中元素的类型 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">collection</span> <span class="attr">property</span>=<span class="string">&quot;orderList&quot;</span> <span class="attr">ofType</span>=<span class="string">&quot;com.vincent.pojo.Order&quot;</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!-- 映射Order的属性 --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">id</span> <span class="attr">column</span>=<span class="string">&quot;order_id&quot;</span> <span class="attr">property</span>=<span class="string">&quot;orderId&quot;</span>/&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">result</span> <span class="attr">column</span>=<span class="string">&quot;order_name&quot;</span> <span class="attr">property</span>=<span class="string">&quot;orderName&quot;</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">collection</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">resultMap</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- Customer selectCustomerWithOrderList(Integer customerId); --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">select</span> <span class="attr">id</span>=<span class="string">&quot;selectCustomerWithOrderList&quot;</span> <span class="attr">resultMap</span>=<span class="string">&quot;selectCustomerWithOrderListResultMap&quot;</span>&gt;</span></span><br><span class="line">      SELECT c.customer_id,c.customer_name,o.order_id,o.order_name</span><br><span class="line">      FROM t_customer c</span><br><span class="line">      LEFT JOIN t_order o</span><br><span class="line">      ON c.customer_id=o.customer_id</span><br><span class="line">      WHERE c.customer_id=#&#123;customerId&#125;</span><br><span class="line">    <span class="tag">&lt;/<span class="name">select</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">mapper</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="对多分步"><a href="#对多分步" class="headerlink" title="对多分步"></a>对多分步</h2><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;</span></span><br><span class="line"><span class="meta">&lt;!DOCTYPE <span class="meta-keyword">mapper</span></span></span><br><span class="line"><span class="meta">        <span class="meta-keyword">PUBLIC</span> <span class="meta-string">&quot;-//mybatis.org//DTD Mapper 3.0//EN&quot;</span></span></span><br><span class="line"><span class="meta">        <span class="meta-string">&quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- mapper是根标签，namespace属性：在Mybatis全局范围内找到一个具体的Mapper配置 --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 引入接口后，为了方便通过接口全类名来找到Mapper配置文件，所以通常将namespace属性设置为接口全类名 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">mapper</span> <span class="attr">namespace</span>=<span class="string">&quot;com.vincent.dao.CustomerMapper&quot;</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 专门指定一条SQL语句，用来查询Customer，而且是仅仅查询Customer本身，不携带Order --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">select</span> <span class="attr">id</span>=<span class="string">&quot;selectCustomerWithOrderList&quot;</span> <span class="attr">resultMap</span>=<span class="string">&quot;selectCustomerWithOrderListResultMap&quot;</span>&gt;</span></span><br><span class="line">        select customer_id,customer_name from t_customer</span><br><span class="line">        where customer_id=#&#123;customerId&#125;</span><br><span class="line">    <span class="tag">&lt;/<span class="name">select</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">select</span> <span class="attr">id</span>=<span class="string">&quot;selectOrderList&quot;</span> <span class="attr">resultType</span>=<span class="string">&quot;com.vincent.pojo.Order&quot;</span>&gt;</span></span><br><span class="line">        select order_id,order_name from t_order where customer_id=#&#123;customer_id&#125;</span><br><span class="line">    <span class="tag">&lt;/<span class="name">select</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">resultMap</span> <span class="attr">id</span>=<span class="string">&quot;selectCustomerWithOrderListResultMap&quot;</span> <span class="attr">type</span>=<span class="string">&quot;com.vincent.pojo.Customer&quot;</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- orderList集合属性的映射关系，使用分步查询 --&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 在collection标签中使用select属性指定要引用的SQL语句 --&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- select属性值的格式是：Mapper配置文件的名称空间.SQL语句id --&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- column属性：指定Customer和Order之间建立关联关系时所依赖的字段 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">collection</span></span></span><br><span class="line"><span class="tag">                <span class="attr">property</span>=<span class="string">&quot;orderList&quot;</span></span></span><br><span class="line"><span class="tag">                <span class="attr">select</span>=<span class="string">&quot;com.vincent.dao.CustomerMapper.selectOrderList&quot;</span></span></span><br><span class="line"><span class="tag">                <span class="attr">column</span>=<span class="string">&quot;customer_id&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">resultMap</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">mapper</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h1 id="动态SQL"><a href="#动态SQL" class="headerlink" title="动态SQL"></a>动态SQL</h1><h2 id="where"><a href="#where" class="headerlink" title="where"></a>where</h2><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- List&lt;Employee&gt; selectEmployeeByCondition(Employee employee); --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">select</span> <span class="attr">id</span>=<span class="string">&quot;selectEmployeeByCondition&quot;</span> <span class="attr">resultType</span>=<span class="string">&quot;com.vincent.pojo.Employee&quot;</span>&gt;</span></span><br><span class="line">    select emp_id,emp_name,emp_salary from t_emp</span><br><span class="line">    </span><br><span class="line">    <span class="comment">&lt;!-- where标签会自动去掉“标签体内前面多余的and/or” --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">where</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 使用if标签，让我们可以有选择的加入SQL语句的片段。这个SQL语句片段是否要加入整个SQL语句，就看if标签判断的结果是否为true --&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 在if标签的test属性中，可以访问实体类的属性，不可以访问数据库表的字段 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">if</span> <span class="attr">test</span>=<span class="string">&quot;empName != null&quot;</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!-- 在if标签内部，需要访问接口的参数时还是正常写#&#123;&#125; --&gt;</span></span><br><span class="line">            or emp_name=#&#123;empName&#125;</span><br><span class="line">        <span class="tag">&lt;/<span class="name">if</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">if</span> <span class="attr">test</span>=<span class="string">&quot;empSalary <span class="symbol">&amp;gt;</span> 2000&quot;</span>&gt;</span></span><br><span class="line">            or emp_salary&gt;#&#123;empSalary&#125;</span><br><span class="line">        <span class="tag">&lt;/<span class="name">if</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">         第一种情况：所有条件都满足 WHERE emp_name=? or emp_salary&gt;?</span></span><br><span class="line"><span class="comment">         第二种情况：部分条件满足 WHERE emp_salary&gt;?</span></span><br><span class="line"><span class="comment">         第三种情况：所有条件都不满足 没有where子句</span></span><br><span class="line"><span class="comment">         --&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">where</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">select</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="set标签"><a href="#set标签" class="headerlink" title="set标签"></a>set标签</h2><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- void updateEmployeeDynamic(Employee employee) --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">update</span> <span class="attr">id</span>=<span class="string">&quot;updateEmployeeDynamic&quot;</span>&gt;</span></span><br><span class="line">    update t_emp</span><br><span class="line">    <span class="comment">&lt;!-- set emp_name=#&#123;empName&#125;,emp_salary=#&#123;empSalary&#125; --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 使用set标签动态管理set子句，并且动态去掉两端多余的逗号 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">set</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">if</span> <span class="attr">test</span>=<span class="string">&quot;empName != null&quot;</span>&gt;</span></span><br><span class="line">            emp_name=#&#123;empName&#125;,</span><br><span class="line">        <span class="tag">&lt;/<span class="name">if</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">if</span> <span class="attr">test</span>=<span class="string">&quot;empSalary <span class="symbol">&amp;lt;</span> 3000&quot;</span>&gt;</span></span><br><span class="line">            emp_salary=#&#123;empSalary&#125;,</span><br><span class="line">        <span class="tag">&lt;/<span class="name">if</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">set</span>&gt;</span></span><br><span class="line">    where emp_id=#&#123;empId&#125;</span><br><span class="line">    <span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">         第一种情况：所有条件都满足 SET emp_name=?, emp_salary=?</span></span><br><span class="line"><span class="comment">         第二种情况：部分条件满足 SET emp_salary=?</span></span><br><span class="line"><span class="comment">         第三种情况：所有条件都不满足 update t_emp where emp_id=?</span></span><br><span class="line"><span class="comment">            没有set子句的update语句会导致SQL语法错误</span></span><br><span class="line"><span class="comment">     --&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">update</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="trim"><a href="#trim" class="headerlink" title="trim"></a>trim</h2><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- List&lt;Employee&gt; selectEmployeeByConditionByTrim(Employee employee) --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">select</span> <span class="attr">id</span>=<span class="string">&quot;selectEmployeeByConditionByTrim&quot;</span> <span class="attr">resultType</span>=<span class="string">&quot;com.vincent.pojo.Employee&quot;</span>&gt;</span></span><br><span class="line">    select emp_id,emp_name,emp_age,emp_salary,emp_gender</span><br><span class="line">    from t_emp</span><br><span class="line">    </span><br><span class="line">    <span class="comment">&lt;!-- prefix属性指定要动态添加的前缀 --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- suffix属性指定要动态添加的后缀 --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- prefixOverrides属性指定要动态去掉的前缀，使用“|”分隔有可能的多个值 --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- suffixOverrides属性指定要动态去掉的后缀，使用“|”分隔有可能的多个值 --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 当前例子用where标签实现更简洁，但是trim标签更灵活，可以用在任何有需要的地方 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">trim</span> <span class="attr">prefix</span>=<span class="string">&quot;where&quot;</span> <span class="attr">suffixOverrides</span>=<span class="string">&quot;and|or&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">if</span> <span class="attr">test</span>=<span class="string">&quot;empName != null&quot;</span>&gt;</span></span><br><span class="line">            emp_name=#&#123;empName&#125; and</span><br><span class="line">        <span class="tag">&lt;/<span class="name">if</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">if</span> <span class="attr">test</span>=<span class="string">&quot;empSalary <span class="symbol">&amp;gt;</span> 3000&quot;</span>&gt;</span></span><br><span class="line">            emp_salary&gt;#&#123;empSalary&#125; and</span><br><span class="line">        <span class="tag">&lt;/<span class="name">if</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">if</span> <span class="attr">test</span>=<span class="string">&quot;empAge <span class="symbol">&amp;lt;</span>= 20&quot;</span>&gt;</span></span><br><span class="line">            emp_age=#&#123;empAge&#125; or</span><br><span class="line">        <span class="tag">&lt;/<span class="name">if</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">if</span> <span class="attr">test</span>=<span class="string">&quot;empGender==&#x27;male&#x27;&quot;</span>&gt;</span></span><br><span class="line">            emp_gender=#&#123;empGender&#125;</span><br><span class="line">        <span class="tag">&lt;/<span class="name">if</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">trim</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">select</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="choose-when-otherwise"><a href="#choose-when-otherwise" class="headerlink" title="choose/when/otherwise"></a>choose/when/otherwise</h2><p>在多个分支条件中，仅执行一个。</p>
<ul>
<li>从上到下依次执行条件判断</li>
<li>遇到的第一个满足条件的分支会被采纳</li>
<li>被采纳分支后面的分支都将不被考虑</li>
<li>如果所有的when分支都不满足，那么就执行otherwise分支</li>
</ul>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- List&lt;Employee&gt; selectEmployeeByConditionByChoose(Employee employee) --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">select</span> <span class="attr">id</span>=<span class="string">&quot;selectEmployeeByConditionByChoose&quot;</span> <span class="attr">resultType</span>=<span class="string">&quot;com.vincent.pojo.Employee&quot;</span>&gt;</span></span><br><span class="line">    select emp_id,emp_name,emp_salary from t_emp</span><br><span class="line">    where</span><br><span class="line">    <span class="tag">&lt;<span class="name">choose</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">when</span> <span class="attr">test</span>=<span class="string">&quot;empName != null&quot;</span>&gt;</span>emp_name=#&#123;empName&#125;<span class="tag">&lt;/<span class="name">when</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">when</span> <span class="attr">test</span>=<span class="string">&quot;empSalary <span class="symbol">&amp;lt;</span> 3000&quot;</span>&gt;</span>emp_salary <span class="symbol">&amp;lt;</span> 3000<span class="tag">&lt;/<span class="name">when</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">otherwise</span>&gt;</span>1=1<span class="tag">&lt;/<span class="name">otherwise</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">choose</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">     第一种情况：第一个when满足条件 where emp_name=?</span></span><br><span class="line"><span class="comment">     第二种情况：第二个when满足条件 where emp_salary &lt; 3000</span></span><br><span class="line"><span class="comment">     第三种情况：两个when都不满足 where 1=1 执行了otherwise</span></span><br><span class="line"><span class="comment">     --&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">select</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="foreach"><a href="#foreach" class="headerlink" title="foreach"></a>foreach</h2><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">    collection属性：要遍历的集合</span></span><br><span class="line"><span class="comment">    item属性：遍历集合的过程中能得到每一个具体对象，在item属性中设置一个名字，将来通过这个名字引用遍历出来的对象</span></span><br><span class="line"><span class="comment">    separator属性：指定当foreach标签的标签体重复拼接字符串时，各个标签体字符串之间的分隔符</span></span><br><span class="line"><span class="comment">    open属性：指定整个循环把字符串拼好后，字符串整体的前面要添加的字符串</span></span><br><span class="line"><span class="comment">    close属性：指定整个循环把字符串拼好后，字符串整体的后面要添加的字符串</span></span><br><span class="line"><span class="comment">    index属性：这里起一个名字，便于后面引用</span></span><br><span class="line"><span class="comment">        遍历List集合，这里能够得到List集合的索引值</span></span><br><span class="line"><span class="comment">        遍历Map集合，这里能够得到Map集合的key</span></span><br><span class="line"><span class="comment"> --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">foreach</span> <span class="attr">collection</span>=<span class="string">&quot;empList&quot;</span> <span class="attr">item</span>=<span class="string">&quot;emp&quot;</span> <span class="attr">separator</span>=<span class="string">&quot;,&quot;</span> <span class="attr">open</span>=<span class="string">&quot;values&quot;</span> <span class="attr">index</span>=<span class="string">&quot;myIndex&quot;</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 在foreach标签内部如果需要引用遍历得到的具体的一个对象，需要使用item属性声明的名称 --&gt;</span></span><br><span class="line">    (#&#123;emp.empName&#125;,#&#123;myIndex&#125;,#&#123;emp.empSalary&#125;,#&#123;emp.empGender&#125;)</span><br><span class="line"><span class="tag">&lt;/<span class="name">foreach</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- int updateEmployeeBatch(@Param(&quot;empList&quot;) List&lt;Employee&gt; empList) --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">update</span> <span class="attr">id</span>=<span class="string">&quot;updateEmployeeBatch&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">foreach</span> <span class="attr">collection</span>=<span class="string">&quot;empList&quot;</span> <span class="attr">item</span>=<span class="string">&quot;emp&quot;</span> <span class="attr">separator</span>=<span class="string">&quot;;&quot;</span>&gt;</span></span><br><span class="line">        update t_emp set emp_name=#&#123;emp.empName&#125; where emp_id=#&#123;emp.empId&#125;</span><br><span class="line">    <span class="tag">&lt;/<span class="name">foreach</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">update</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="sql标签"><a href="#sql标签" class="headerlink" title="sql标签"></a>sql标签</h2><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 使用sql标签抽取重复出现的SQL片段 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">sql</span> <span class="attr">id</span>=<span class="string">&quot;mySelectSql&quot;</span>&gt;</span></span><br><span class="line">    select emp_id,emp_name,emp_age,emp_salary,emp_gender from t_emp</span><br><span class="line"><span class="tag">&lt;/<span class="name">sql</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 使用include标签引用声明的SQL片段 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">include</span> <span class="attr">refid</span>=<span class="string">&quot;mySelectSql&quot;</span>/&gt;</span></span><br></pre></td></tr></table></figure>

<h1 id="缓存"><a href="#缓存" class="headerlink" title="缓存"></a>缓存</h1><p>![截屏2022-09-07 16.43.58](/Users/vincent/Library/Application Support/typora-user-images/截屏2022-09-07 16.43.58.png)</p>
<p>查询的顺序是：</p>
<ul>
<li>先查询二级缓存，因为二级缓存中可能会有其他程序已经查出来的数据，可以拿来直接使用。</li>
<li>如果二级缓存没有命中，再查询一级缓存。</li>
<li>如果一级缓存也没有命中，则查询数据库。</li>
<li>SqlSession关闭之前，一级缓存中的数据会写入二级缓存。</li>
</ul>
<h2 id="一级缓存"><a href="#一级缓存" class="headerlink" title="一级缓存"></a>一级缓存</h2><p>失效情况：</p>
<ul>
<li>不是同一个SqlSession。</li>
<li>同一个SqlSession但是查询条件发生了变化。</li>
<li>同一个SqlSession两次查询期间执行了任何一次增删改操作。</li>
<li>同一个SqlSession两次查询期间手动清空了缓存。</li>
<li>同一个SqlSession两次查询期间提交了事务。</li>
</ul>
<h2 id="二级缓存"><a href="#二级缓存" class="headerlink" title="二级缓存"></a>二级缓存</h2><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">mapper</span> <span class="attr">namespace</span>=<span class="string">&quot;com.vincent.dao.EmployeeMapper&quot;</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">&lt;!-- 加入cache标签启用二级缓存功能 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">cache</span>/&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//序列化</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Employee</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</span><br></pre></td></tr></table></figure>

<h1 id="逆向工程"><a href="#逆向工程" class="headerlink" title="逆向工程"></a>逆向工程</h1><ul>
<li>正向工程：先创建Java实体类，由框架负责根据实体类生成数据库表。Hibernate是支持正向工程的。</li>
<li>逆向工程：先创建数据库表，由框架负责根据数据库表，反向生成如下资源：<ul>
<li>Java实体类</li>
<li>Mapper接口</li>
<li>Mapper配置文件</li>
</ul>
</li>
</ul>
<h2 id="配置pom"><a href="#配置pom" class="headerlink" title="配置pom"></a>配置pom</h2><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 依赖MyBatis核心包 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.mybatis<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mybatis<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.5.7<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line">  </span><br><span class="line"><span class="comment">&lt;!-- 控制Maven在构建过程中相关配置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">  <span class="comment">&lt;!-- 构建过程中用到的插件 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">&lt;!-- 具体插件，逆向工程的操作是以构建过程中插件形式出现的 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.mybatis.generator<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mybatis-generator-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.3.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">      <span class="comment">&lt;!-- 插件的依赖 --&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">&lt;!-- 逆向工程的核心依赖 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.mybatis.generator<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mybatis-generator-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.3.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">          </span><br><span class="line">        <span class="comment">&lt;!-- 数据库连接池 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.mchange<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>c3p0<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">version</span>&gt;</span>0.9.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">          </span><br><span class="line">        <span class="comment">&lt;!-- MySQL驱动 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.8<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="MBG配置文件"><a href="#MBG配置文件" class="headerlink" title="MBG配置文件"></a>MBG配置文件</h2><p>文件名必须是：generatorConfig.xml</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span></span><br><span class="line"><span class="meta">&lt;!DOCTYPE <span class="meta-keyword">generatorConfiguration</span></span></span><br><span class="line"><span class="meta">        <span class="meta-keyword">PUBLIC</span> <span class="meta-string">&quot;-//mybatis.org//DTD MyBatis Generator Configuration 1.0//EN&quot;</span></span></span><br><span class="line"><span class="meta">        <span class="meta-string">&quot;http://mybatis.org/dtd/mybatis-generator-config_1_0.dtd&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">generatorConfiguration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">            targetRuntime: 执行生成的逆向工程的版本</span></span><br><span class="line"><span class="comment">                    MyBatis3Simple: 生成基本的CRUD（清新简洁版）</span></span><br><span class="line"><span class="comment">                    MyBatis3: 生成带条件的CRUD（奢华尊享版）</span></span><br><span class="line"><span class="comment">     --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">context</span> <span class="attr">id</span>=<span class="string">&quot;DB2Tables&quot;</span> <span class="attr">targetRuntime</span>=<span class="string">&quot;MyBatis3&quot;</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 数据库的连接信息 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">jdbcConnection</span> <span class="attr">driverClass</span>=<span class="string">&quot;com.mysql.cj.jdbc.Driver&quot;</span></span></span><br><span class="line"><span class="tag">                        <span class="attr">connectionURL</span>=<span class="string">&quot;jdbc:mysql://localhost:3306/mybatis-example?serverTimezone=UTC&quot;</span></span></span><br><span class="line"><span class="tag">                        <span class="attr">userId</span>=<span class="string">&quot;root&quot;</span></span></span><br><span class="line"><span class="tag">                        <span class="attr">password</span>=<span class="string">&quot;xsh211819&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">jdbcConnection</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- javaBean的生成策略--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">javaModelGenerator</span> <span class="attr">targetPackage</span>=<span class="string">&quot;com.vincent.pojo&quot;</span> <span class="attr">targetProject</span>=<span class="string">&quot;./src/main/java&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;enableSubPackages&quot;</span> <span class="attr">value</span>=<span class="string">&quot;true&quot;</span> /&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;trimStrings&quot;</span> <span class="attr">value</span>=<span class="string">&quot;true&quot;</span> /&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">javaModelGenerator</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- SQL映射文件的生成策略 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">sqlMapGenerator</span> <span class="attr">targetPackage</span>=<span class="string">&quot;com.vincent.dao&quot;</span>  <span class="attr">targetProject</span>=<span class="string">&quot;./src/main/resources&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;enableSubPackages&quot;</span> <span class="attr">value</span>=<span class="string">&quot;true&quot;</span> /&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">sqlMapGenerator</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- Mapper接口的生成策略 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">javaClientGenerator</span> <span class="attr">type</span>=<span class="string">&quot;XMLMAPPER&quot;</span> <span class="attr">targetPackage</span>=<span class="string">&quot;com.vincent.dao&quot;</span>  <span class="attr">targetProject</span>=<span class="string">&quot;./src/main/java&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;enableSubPackages&quot;</span> <span class="attr">value</span>=<span class="string">&quot;true&quot;</span> /&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">javaClientGenerator</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 逆向分析的表 --&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- tableName设置为*号，可以对应所有表，此时不写domainObjectName --&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- domainObjectName属性指定生成出来的实体类的类名 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">table</span> <span class="attr">tableName</span>=<span class="string">&quot;t_emp&quot;</span> <span class="attr">domainObjectName</span>=<span class="string">&quot;Employee&quot;</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">table</span> <span class="attr">tableName</span>=<span class="string">&quot;t_customer&quot;</span> <span class="attr">domainObjectName</span>=<span class="string">&quot;Customer&quot;</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">table</span> <span class="attr">tableName</span>=<span class="string">&quot;t_order&quot;</span> <span class="attr">domainObjectName</span>=<span class="string">&quot;Order&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">context</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">generatorConfiguration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="QBC查询"><a href="#QBC查询" class="headerlink" title="QBC查询"></a>QBC查询</h2><p>Query By Criteria</p>
<p>QBC查询最大的特点就是将SQL语句中的WHERE子句进行了组件化的封装，让我们可以通过调用Criteria对象的方法自由的拼装查询条件。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 1.创建EmployeeExample对象</span></span><br><span class="line">EmployeeExample example = <span class="keyword">new</span> EmployeeExample();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2.通过example对象创建Criteria对象</span></span><br><span class="line">EmployeeExample.Criteria criteria01 = example.createCriteria();</span><br><span class="line">EmployeeExample.Criteria criteria02 = example.or();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3.在Criteria对象中封装查询条件</span></span><br><span class="line">criteria01</span><br><span class="line">    .andEmpAgeBetween(<span class="number">9</span>, <span class="number">99</span>)</span><br><span class="line">    .andEmpNameLike(<span class="string">&quot;%o%&quot;</span>)</span><br><span class="line">    .andEmpGenderEqualTo(<span class="string">&quot;male&quot;</span>)</span><br><span class="line">    .andEmpSalaryGreaterThan(<span class="number">500.55</span>);</span><br><span class="line"></span><br><span class="line">criteria02</span><br><span class="line">        .andEmpAgeBetween(<span class="number">9</span>, <span class="number">99</span>)</span><br><span class="line">        .andEmpNameLike(<span class="string">&quot;%o%&quot;</span>)</span><br><span class="line">        .andEmpGenderEqualTo(<span class="string">&quot;male&quot;</span>)</span><br><span class="line">        .andEmpSalaryGreaterThan(<span class="number">500.55</span>);</span><br><span class="line"></span><br><span class="line">SqlSession session = factory.openSession();</span><br><span class="line"></span><br><span class="line">EmployeeMapper mapper = session.getMapper(EmployeeMapper.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4.基于Criteria对象进行查询</span></span><br><span class="line">List&lt;Employee&gt; employeeList = mapper.selectByExample(example);</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (Employee employee : employeeList) &#123;</span><br><span class="line">    System.out.println(<span class="string">&quot;employee = &quot;</span> + employee);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">session.close();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 最终SQL的效果：</span></span><br><span class="line"><span class="comment">// WHERE ( emp_age between ? and ? and emp_name like ? and emp_gender = ? and emp_salary &gt; ? ) or( emp_age between ? and ? and emp_name like ? and emp_gender = ? and emp_salary &gt; ? )</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>工具</category>
      </categories>
  </entry>
  <entry>
    <title>Flink</title>
    <url>/Flink/</url>
    <content><![CDATA[<h2 id="Flink概述"><a href="#Flink概述" class="headerlink" title="Flink概述"></a>Flink概述</h2><h3 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h3><p>1、高吞吐和低延迟。每秒处理数百万个事件，毫秒级延迟。</p>
<p>2、结果的准确性。Flink 提供了事件时间(event-time)和处理时间(processing-time) 语义。对于乱序事件流，事件时间语义仍然能提供一致且准确的结果。</p>
<p>3、精确一次(exactly-once)的状态一致性保证。</p>
<p>4、可以连接到最常用的存储系统，如 Apache Kafka、Apache Cassandra、Elasticsearch、JDBC、Kinesis 和(分布式)文件系统，如 HDFS 和 S3。</p>
<p>5、高可用。本身高可用的设置，加上与K8s，YARN和Mesos的紧密集成，再加上从故障中快速恢复和动态扩展任务的能力，Flink 能做到以极少的停机时间 7×24 全天候运行。</p>
<p>6、能够更新应用程序代码并将作业(jobs)迁移到不同的 Flink 集群，而不会丢失应用程序的状态。</p>
<h3 id="分层API"><a href="#分层API" class="headerlink" title="分层API"></a>分层API</h3><p><img src="/Flink/%E6%88%AA%E5%B1%8F2022-03-22%20%E4%B8%8A%E5%8D%8811.27.50.png" alt="截屏2022-03-22 上午11.27.50"></p>
<h3 id="部署模式"><a href="#部署模式" class="headerlink" title="部署模式"></a>部署模式</h3><p>它们的区别主要在于：集群的生命周期以及资源的分配方式；以及应用的 main 方法到底在哪里执行——客户端(Client)还是 JobManager。</p>
<h4 id="会话模式-Session-Mode"><a href="#会话模式-Session-Mode" class="headerlink" title="会话模式(Session Mode)"></a>会话模式(Session Mode)</h4><p>集群启动时所有资源就都已经确定，所以所有提交的作业会竞争集群中的资源。会话模式比较适合于单个规模小、执行时间短的大量作业。</p>
<p><img src="/Flink/%E6%88%AA%E5%B1%8F2022-03-22%20%E4%B8%8A%E5%8D%8811.41.01.png" alt="截屏2022-03-22 上午11.41.01"></p>
<p>这样的好处很明显，我们只需要一个集群，就像一个大箱子，所有的作业提交之后都塞进去；集群的生命周期是超越于作业之上的，铁打的营盘流水的兵，作业结束了就释放资源，集群依然正常运行。当然缺点也是显而易见的：因为资源是共享的，所以资源不够了，提交新的作业就会失败。另外，同一个 TaskManager 上可能运行了很多作业，如果其中一个发生故障导致 TaskManager 宕机，那么所有作业都会受到影响。</p>
<h4 id="单作业模式-Per-Job-Mode"><a href="#单作业模式-Per-Job-Mode" class="headerlink" title="单作业模式(Per-Job Mode)"></a>单作业模式(Per-Job Mode)</h4><p>会话模式因为资源共享会导致很多问题，所以为了更好地隔离资源，我们可以考虑为每个提交的作业启动一个集群，这就是所谓的单作业(Per-Job)模式。</p>
<p><img src="/Flink/%E6%88%AA%E5%B1%8F2022-03-22%20%E4%B8%8A%E5%8D%8811.44.13.png" alt="截屏2022-03-22 上午11.44.13"></p>
<p>单作业模式也很好理解，就是严格的一对一，集群只为这个作业而生。同样由客户端运行应用程序，然后启动集群，作业被提交给 JobManager，进而分发给 TaskManager 执行。作业完成后，集群就会关闭，所有资源也会释放。这样一来，每个作业都有它自己的 JobManager 管理，占用独享的资源，即使发生故障，它的 TaskManager 宕机也不会影响其他作业。</p>
<p>这些特性使得单作业模式在生产环境运行更加稳定，所以是实际应用的首选模式。</p>
<p>需要注意的是，Flink 本身无法直接这样运行，所以单作业模式一般需要借助一些资源管理框架来启动集群，比如 YARN、Kubernetes。</p>
<h4 id="应用模式-Application-Mode"><a href="#应用模式-Application-Mode" class="headerlink" title="应用模式(Application Mode)"></a>应用模式(Application Mode)</h4><p>前面提到的两种模式下，应用代码都是在客户端上执行，然后由客户端提交给 JobManager 的。但是这种方式客户端需要占用大量网络带宽，去下载依赖和把二进制数据发送给 JobManager；加上很多情况下我们提交作业用的是同一个客户端，就会加重客户端所在节点的资源消耗。</p>
<p>所以解决办法就是，我们<strong>不要客户端</strong>了，直接把应用提交到 JobManger 上运行。而这也就代表着，我们需要为每一个提交的应用单独启动一个JobManager，也就是创建一个集群。这个 JobManager 只为执行这一个应用而存在，执行结束之后 JobManager 也就关闭了，这就是所谓的应用模式。</p>
<p><img src="/Flink/%E6%88%AA%E5%B1%8F2022-03-22%20%E4%B8%8A%E5%8D%8811.51.54.png" alt="截屏2022-03-22 上午11.51.54"></p>
<p>应用模式与单作业模式，都是提交作业之后才创建集群；单作业模式是通过客户端来提交的，客户端解析出的每一个作业对应一个集群；而应用模式下，是直接由 JobManager 执行应用程序的，并且即使应用包含了多个作业，也只创建一个集群。</p>
<p>总结一下，在会话模式下，集群的生命周期独立于集群上运行的任何作业的生命周期，并且提交的所有作业共享资源。而单作业模式为每个提交的作业创建一个集群，带来了更好的资源隔离，这时集群的生命周期与作业的生命周期绑定。最后，应用模式为每个应用程序创建一个会话集群，在 JobManager 上直接调用应用程序的 main() 方法。</p>
<h3 id="独立模式-Standalone"><a href="#独立模式-Standalone" class="headerlink" title="独立模式(Standalone)"></a>独立模式(Standalone)</h3><p>独立模式(Standalone)是部署 Flink 最基本也是最简单的方式：所需要的所有 Flink 组件，都只是操作系统上运行的一个 JVM 进程。</p>
<p>独立模式是独立运行的，不依赖任何外部的资源管理平台；当然独立也是有代价的：如果资源不足，或者出现故障，没有自动扩展或重分配资源的保证，必须手动处理。所以独立模式 一般只用在开发测试或作业非常少的场景下。</p>
<p>另外，我们也可以将独立模式的集群放在容器中运行。Flink 提供了独立模式的容器化部署方式，可以在 Docker 或者 Kubernetes 上进行部署。</p>
<h3 id="YARN-模式"><a href="#YARN-模式" class="headerlink" title="YARN 模式"></a>YARN 模式</h3><p>独立模式(Standalone)由 Flink 自身提供资源，无需其他框架，这种方式降低了和其他第三方资源框架的耦合性，独立性非常强。但我们知道，Flink 是大数据计算框架，不是资源调度框架，这并不是它的强项；所以还是应该让专业的框架做专业的事，和其他资源调度框架集成更靠谱。而在目前大数据生态中，国内应用最为广泛的资源管理平台就是 YARN 了。</p>
<p>整体来说，YARN 上部署的过程是：客户端把 Flink 应用提交给 Yarn 的 ResourceManager, Yarn 的 ResourceManager 会向 Yarn 的 NodeManager 申请容器。在这些容器上，Flink 会部署 JobManager 和 TaskManager 的实例，从而启动集群。Flink 会根据运行在 JobManger 上的作业所需要的 Slot 数量动态分配 TaskManager 资源。</p>
<h3 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h3><p>Flink 就是一个分布式的并行流处理系统。简单来说，它会由多个进程构成，这些进程一般会分布运行在不同的机器上。</p>
<p>Flink 可以配置为独立(Standalone)集群运行，也可以方便地跟一些集群资源管理工具集成使用，比如 YARN、Kubernetes 和 Mesos。Flink 也不会自己去提供持久化的分布式存储，而是直接利用了已有的分布式文件系统(比如HDFS)或者对象存储(比如S3)。而对于高可用的配置，Flink 是依靠 Apache ZooKeeper 来完成的。</p>
<p>Flink 的运行时架构中，最重要的就是两大组件：作业管理器(<strong>JobManger</strong>)和任务管理器 (<strong>TaskManager</strong>)。对于一个提交执行的作业，JobManager 是真正意义上的管理者(Master)， 负责管理调度，所以在不考虑高可用的情况下只能有一个；而 TaskManager 是工作者(Worker、Slave)，负责执行任务处理数据，所以可以有一个或多个。</p>
<p><img src="/Flink/%E6%88%AA%E5%B1%8F2022-03-22%20%E4%B8%8B%E5%8D%881.45.50.png" alt="截屏2022-03-22 下午1.45.50"></p>
<p>这里首先要说明一下”客户端”。其实客户端并不是处理系统的一部分，它<strong>只负责作业的提交</strong>。具体来说，就是调用程序的 main 方法，将代码转换成”数据流图”(<strong>Dataflow Graph</strong>)， 并最终生成作业图(<strong>JobGraph</strong>)，一并发送给 JobManager。提交之后，任务的执行其实就跟客户端没有关系了；我们可以在客户端选择断开与 JobManager 的连接，也可以继续保持连接。 之前我们在命令提交作业时，加上的-d 参数，就是表示分离模式(detached mode)，也就是断开连接。</p>
<p>当然，客户端可以随时连接到 JobManager，获取当前作业的状态和执行结果，也可以发送请求取消作业。不论通过 Web UI 还是命令行执行”flink run”的相关操作，都是通过客户端实现的。</p>
<p>TaskManager 启动之后，JobManager 会与它建立连接，并将作业图(<strong>JobGraph</strong>)转换成可执行的”执行图”(<strong>ExecutionGraph</strong>)分发给可用的 TaskManager，然后就由 TaskManager 具体执行任务。</p>
<h4 id="作业管理器-JobManager"><a href="#作业管理器-JobManager" class="headerlink" title="作业管理器(JobManager)"></a>作业管理器(JobManager)</h4><p>JobManager 是一个 Flink 集群中任务管理和调度的核心，是控制应用执行的主进程。也就是说，每个应用都应该被唯一的 JobManager 所控制执行。当然，在高可用(HA)的场景下，可能会出现多个 JobManager；这时只有一个是正在运行的领导节点(leader)，其他都是备用节点(standby)。</p>
<p>JobManger 又包含 3 个不同的组件：JobMaster，ResourceManager，Dispatcher</p>
<h5 id="JobMaster"><a href="#JobMaster" class="headerlink" title="JobMaster"></a>JobMaster</h5><p>JobMaster 是 JobManager 中最核心的组件，<strong>负责处理单独的作业</strong>(Job)。所以 JobMaster 和具体的 Job 是一一对应的，多个 Job 可以同时运行在一个 Flink 集群中，每个 Job 都有一个自己的 JobMaster。需要注意在早期版本的 Flink 中，没有 JobMaster 的概念；而 JobManager 的概念范围较小，实际指的就是现在所说的 JobMaster。</p>
<p>在作业提交时，JobMaster 会先接收到要执行的应用。这里所说”应用”一般是客户端提交来的，包括：Jar包、数据流图(dataflow graph)、作业图(JobGraph)。</p>
<p>JobMaster 会把 JobGraph 转换成一个物理层面的数据流图，这个图被叫作”执行图” (ExecutionGraph)，它包含了所有可以并发执行的任务。JobMaster 会向资源管理器 (ResourceManager)发出请求，申请执行任务必要的资源。一旦它获取到了足够的资源，就会将执行图分发到真正运行它们的 TaskManager 上。而在运行过程中，JobMaster 会负责所有需要中央协调的操作，比如说检查点(checkpoints)的协调。</p>
<h5 id="ResourceManager"><a href="#ResourceManager" class="headerlink" title="ResourceManager"></a>ResourceManager</h5><p>ResourceManager 主要<strong>负责资源的分配和管理</strong>，在 Flink 集群中只有一个。所谓资源，主要是指 TaskManager 的任务槽(task slots)。任务槽就是 Flink 集群中的资源调配单元，包含了机器用来执行计算的一组CPU和内存资源。每一个任务(Task)都需要分配到一个 slot 上执行。</p>
<p>这里注意要把 Flink 内置的 ResourceManager 和其他资源管理平台(比如 YARN)的 ResourceManager 区分开。</p>
<p>Flink 的 ResourceManager，针对不同的环境和资源管理平台(比如 Standalone 部署，或者 YARN)，有不同的具体实现。在 Standalone 部署时，因为 TaskManager 是单独启动的(没有 Per-Job 模式)，所以 ResourceManager 只能分发可用 TaskManager 的任务槽，不能单独启动新 TaskManager。</p>
<p>而在有资源管理平台时，就不受此限制。当新的作业申请资源时，ResourceManager 会将有空闲槽位的 TaskManager 分配给 JobMaster。如果 ResourceManager 没有足够的任务槽，它还可以向资源提供平台发起会话，请求提供启动 TaskManager 进程的容器。另外，ResourceManager 还负责停掉空闲的 TaskManager，释放计算资源。</p>
<h5 id="Dispatcher"><a href="#Dispatcher" class="headerlink" title="Dispatcher"></a>Dispatcher</h5><p>Dispatcher 主要负责提供一个 REST 接口，用来提交应用，并且负责为每一个新提交的作业启动一个新的 JobMaster 组件。Dispatcher 也会启动一个 Web UI，用来方便地展示和监控作业执行的信息。Dispatcher 在架构中并不是必需的，在不同的部署模式下可能会被忽略掉。</p>
<h4 id="任务管理器-TaskManager"><a href="#任务管理器-TaskManager" class="headerlink" title="任务管理器(TaskManager)"></a>任务管理器(TaskManager)</h4><p>TaskManager 是 Flink 中的工作进程，<strong>数据流的具体计算就是它来做的</strong>，所以也被称为 Worker。Flink 集群中必须至少有一个 TaskManager；当然由于分布式计算的考虑，通常会有多个 TaskManager 运行，每一个 TaskManager 都包含了一定数量的任务槽(task slots)。Slot 是资源调度的最小单位，slot 的数量限制了 TaskManager 能够并行处理的任务数量。</p>
<p>启动之后，TaskManager 会向 ResourceManager 注册它的 slots；收到 ResourceManager 的指令后，TaskManager 就会将一个或者多个槽位提供给 JobMaster 调用，JobMaster 就可以分配任务来执行了。在执行过程中，TaskManager 可以缓冲数据，还可以跟其他运行同一应用的 TaskManager 交换数据。</p>
<h4 id="作业提交流程"><a href="#作业提交流程" class="headerlink" title="作业提交流程"></a>作业提交流程</h4><p><img src="/Flink/%E6%88%AA%E5%B1%8F2022-03-22%20%E4%B8%8B%E5%8D%882.20.24.png" alt="截屏2022-03-22 下午2.20.24"></p>
<p>(1) 一般情况下，由客户端(App)通过分发器提供的 REST 接口，将作业提交给JobManager。</p>
<p>(2) 由分发器启动 JobMaster，并将作业(包含 JobGraph)提交给 JobMaster。</p>
<p>(3) JobMaster 将 JobGraph 解析为可执行的 ExecutionGraph，得到所需的资源数量，然后向资源管理器请求资源(slots)。</p>
<p>(4) 资源管理器判断当前是否由足够的可用资源；如果没有，启动新的 TaskManager。 </p>
<p>(5) TaskManager 启动之后，向 ResourceManager 注册自己的可用任务槽(slots)。</p>
<p>(6) 资源管理器通知 TaskManager 为新的作业提供 slots。</p>
<p>(7) TaskManager 连接到对应的 JobMaster，提供 slots。 </p>
<p>(8) JobMaster 将需要执行的任务分发给 TaskManager。 </p>
<p>(9) TaskManager 执行任务，互相之间可以交换数据。</p>
<p><strong>YARN集群 会话(Session)模式：</strong></p>
<p>在会话模式下，我们需要先启动一个 YARN session，这个会话会创建一个 Flink 集群。</p>
<p><img src="/Flink/%E6%88%AA%E5%B1%8F2022-03-22%20%E4%B8%8B%E5%8D%882.26.46.png" alt="截屏2022-03-22 下午2.26.46"></p>
<p>这里只启动了 JobManager，而 TaskManager 可以根据需要动态地启动。在 JobManager 内部，由于还没有提交作业，所以只有 ResourceManager 和 Dispatcher 在运行。</p>
<p><img src="/Flink/%E6%88%AA%E5%B1%8F2022-03-22%20%E4%B8%8B%E5%8D%882.28.07.png" alt="截屏2022-03-22 下午2.28.07"></p>
<p>接下来就是真正提交作业的流程：</p>
<p>(1) 客户端通过 REST 接口，将作业提交给分发器。</p>
<p>(2) 分发器启动 JobMaster，并将作业(包含 JobGraph)提交给 JobMaster。 (3)JobMaster 向资源管理器请求资源(slots)。</p>
<p>(4) 资源管理器向 YARN 的资源管理器请求 container 资源。</p>
<p>(5) YARN 启动新的 TaskManager 容器。</p>
<p>(6) TaskManager 启动之后，向 Flink 的资源管理器注册自己的可用任务槽。 </p>
<p>(7) 资源管理器通知 TaskManager 为新的作业提供 slots。 </p>
<p>(8) TaskManager 连接到对应的 JobMaster，提供 slots。</p>
<p>(9) JobMaster 将需要执行的任务分发给 TaskManager，执行任务。</p>
<p>注意：请求资源时要上报YARN的资源管理器。</p>
<p><strong>YARN集群 单作业(Per-Job)模式：</strong></p>
<p>在单作业模式下，Flink 集群不会预先启动，而是在提交作业时，才启动新的 JobManager。</p>
<p><img src="/Flink/%E6%88%AA%E5%B1%8F2022-03-22%20%E4%B8%8B%E5%8D%882.41.21.png" alt="截屏2022-03-22 下午2.41.21"></p>
<p>(1) 客户端将作业提交给 YARN 的资源管理器，这一步中会同时将 Flink 的 Jar 包和配置 上传到 HDFS，以便后续启动 Flink 相关组件的容器。</p>
<p>(2) YARN的资源管理器分配Container资源，启动Flink JobManager，并将作业提交给 JobMaster。这里省略了 Dispatcher 组件。</p>
<p>(3) JobMaster 向资源管理器请求资源(slots)。</p>
<p>(4) 资源管理器向 YARN 的资源管理器请求 container 资源。</p>
<p>(5) YARN 启动新的 TaskManager 容器。</p>
<p>(6) TaskManager 启动之后，向 Flink 的资源管理器注册自己的可用任务槽。 </p>
<p>(7) 资源管理器通知 TaskManager 为新的作业提供 slots。 </p>
<p>(8) TaskManager 连接到对应的 JobMaster，提供 slots。</p>
<p>(9) JobMaster 将需要执行的任务分发给 TaskManager，执行任务。</p>
<p>区别只在于 JobManager 的启动方式，以及省去了分发器。当第 2 步作业提交给 JobMaster，之后的流程就与会话模式完全一样了。</p>
<p><strong>YARN集群的应用(Application)模式：</strong></p>
<p>应用模式与单作业模式的提交流程非常相似，只是初始提交给 YARN 资源管理器的不再 是具体的作业，而是整个应用。一个应用中可能包含了多个作业，这些作业都将在 Flink 集群 中启动各自对应的 JobMaster。</p>
<h4 id="数据流图-Dataflow-Graph"><a href="#数据流图-Dataflow-Graph" class="headerlink" title="数据流图(Dataflow Graph)"></a>数据流图(Dataflow Graph)</h4><p>所有的 Flink 程序都可以归纳为由三部分构成：Source、Transformation 和 Sink。 </p>
<p>Source表示源算子，负责读取数据源。</p>
<p>Transformation表示转换算子，利用各种算子进行处理加工。</p>
<p>Sink表示下沉算子，负责数据的输出。</p>
<p>在运行时，Flink 程序会被映射成所有算子按照逻辑顺序连接在一起的一张图，这被称为逻辑数据流(logical dataflow)，或者叫数据流图(dataflow graph)。我们提交作业之后， 打开 Flink 自带的 Web UI，点击作业就能看到对应的 dataflow。</p>
<h4 id="并行度-Parallelism"><a href="#并行度-Parallelism" class="headerlink" title="并行度(Parallelism)"></a>并行度(Parallelism)</h4><p>在 Flink 执行过程中，每一个算子(operator)可以包含一个或多个子任务(operator subtask)，<strong>这些子任务在不同的线程、不同的物理机或不同的容器中完全独立地执行</strong>。</p>
<p><img src="/Flink/%E6%88%AA%E5%B1%8F2022-03-22%20%E4%B8%8B%E5%8D%884.36.32.png" alt="截屏2022-03-22 下午4.36.32"></p>
<p>**一个特定算子的子任务(subtask)的个数被称之为其并行度(parallelism)**。这样，包含并行子任务的数据流，就是并行数据流，它需要多个分区(stream partition)来分配并行任务。 一般情况下，一个流程序的并行度，可以认为就是其所有算子中最大的并行度。一个程序中， 不同的算子可能具有不同的并行度。</p>
<p>如图 4-8 所示，当前数据流中有 source、map、window、sink 四个算子，除最后 sink，其他算子的并行度都为 2。整个程序包含了 7 个子任务，至少需要 2 个分区来并行执行。我们可以说，这段流处理程序的并行度就是 2。</p>
<p><strong>并行度的设置：</strong></p>
<p>可以用不同的方法来设置并行度，它们的有效范围和优先级别也是不同的。</p>
<p>1、代码中设置</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//只针对当前算子有效</span></span><br><span class="line">stream.map(word -&gt; Tuple2.of(word, <span class="number">1L</span>)).setParallelism(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//全局设定并行度</span></span><br><span class="line"><span class="comment">//这样代码中所有算子，默认的并行度就都为2了</span></span><br><span class="line">env.setParallelism(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//keyBy不是算子，所以无法对keyBy设置并行度</span></span><br></pre></td></tr></table></figure>

<p>2、提交应用时设置</p>
<p>在使用flink run命令提交应用时，可以增加-p参数来指定当前应用程序执行的并行度，它的作用类似于执行环境的全局设置。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/flink run –p 2 –c com.vincent.wc.StreamWordCount ./FlinkTutorial-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure>

<p>3、配置文件中设置</p>
<p>我们还可以直接在集群的配置文件 flink-conf.yaml 中直接更改默认并行度。</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">parallelism.default:</span> <span class="number">2</span></span><br></pre></td></tr></table></figure>

<p>这个设置对于整个集群上提交的所有作业有效，初始值为 1。无论在代码中设置、还是提交时的 -p 参数，都不是必须的；所以在没有指定并行度的时候，就会采用配置文件中的集群 默认并行度。在开发环境中，没有配置文件，默认并行度就是当前机器的 CPU 核心数。</p>
<p><strong>优先级：</strong></p>
<p>(1) 对于一个算子，首先看在<strong>代码中是否单独指定了它的并行度</strong>，这个特定的设置优先级最高，会覆盖后面所有的设置。 </p>
<p>(2) 如果没有单独设置，那么采用当前<strong>代码中执行环境全局设置的并行度</strong>。 </p>
<p>(3) 如果代码中完全没有设置，那么采用<strong>提交时-p 参数指定的并行度</strong>。</p>
<p>(4) 如果提交时也未指定-p 参数，那么采用集群<strong>配置文件</strong>中的默认并行度。</p>
<h4 id="算子链-Operator-Chain"><a href="#算子链-Operator-Chain" class="headerlink" title="算子链(Operator Chain)"></a>算子链(Operator Chain)</h4><p>一个数据流在算子之间传输数据的形式可以是一对一(one-to-one)的直通 (forwarding)模式，也可以是打乱的重分区(redistributing)模式，具体是哪一种形式，取决于算子的种类。</p>
<p><img src="/Flink/%E6%88%AA%E5%B1%8F2022-03-22%20%E4%B8%8B%E5%8D%885.01.43.png" alt="截屏2022-03-22 下午5.01.43"></p>
<h5 id="一对一-one-to-one"><a href="#一对一-one-to-one" class="headerlink" title="一对一(one-to-one)"></a>一对一(one-to-one)</h5><p>这种模式下，<strong>数据流维护着分区以及元素的顺序</strong>。比如图中的 source 和 map 算子，source 算子读取数据之后，可以直接发送给 map 算子做处理，它们之间不需要重新分区，也不需要调整数据的顺序。这就意味着 map 算子的子任务，看到的元素个数和顺序跟 source 算子的子任务产生的完全一样，保证着一对一的关系。map、filter、flatMap 等算子都是这种 one-to-one 的对应关系。</p>
<h5 id="重分区-Redistributing"><a href="#重分区-Redistributing" class="headerlink" title="重分区(Redistributing)"></a>重分区(Redistributing)</h5><p>在这种模式下，<strong>数据流的分区会发生改变</strong>。比图中的 map 和后面的 keyBy/window 算子之间(这里的 keyBy 是数据传输算子，后面的 window、apply 方法共同构成了 window 算子)，以及 keyBy/window 算子和 Sink 算子之间，都是这样的关系。</p>
<p>每一个算子的子任务，会根据数据传输的策略，把数据发送到不同的下游目标任务。例如，keyBy()是分组操作，<strong>本质上基于键(key)的哈希值(hashCode)进行了重分区</strong>；而当并行度改变时，比如从并行度为 2 的 window 算子，要传递到并行度为 1 的 Sink 算子，这时的数据传输方式是再平衡(rebalance)，会把数据均匀地向下游子任务分发出去。这些传输方式都会引起重分区(redistribute)的过程，这一过程类似于 Spark 中的 shuffle。</p>
<h5 id="合并算子链"><a href="#合并算子链" class="headerlink" title="合并算子链"></a>合并算子链</h5><p>在 Flink 中，并行度相同的一对一(one to one)算子操作，可以直接链接在一起形成一个大的任务(task)，这样原来的算子就成为了真正任务里的一部分。每个 task 会被一个<strong>线程</strong>执行。这样的技术被称为“算子链”(Operator Chain)。</p>
<p><img src="/Flink/%E6%88%AA%E5%B1%8F2022-03-22%20%E4%B8%8B%E5%8D%885.28.05.png" alt="截屏2022-03-22 下午5.28.05"></p>
<p>比如在图 4-11 中的例子中，Source 和 map 之间满足了算子链的要求，所以可以直接合并在一起，形成了一个任务；因为并行度为 2，所以合并后的任务也有两个并行子任务。这样，这个数据流图所表示的作业最终会有 5 个任务，由 5 个线程并行执行。</p>
<p>Flink 为什么要有算子链这样一个设计呢？这是因为将算子链接成 task 是非常有效的优化：可以减少线程之间的切换和基于缓存区的数据交换，在减少时延的同时提升吞吐量。</p>
<p>Flink 默认会按照算子链的原则进行链接合并，如果我们想要禁止合并或者自行定义，也可以在代码中对算子做一些特定的设置：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 禁用算子链</span></span><br><span class="line">.map(word -&gt; Tuple2.of(word, <span class="number">1L</span>)).disableChaining(); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 从当前算子开始新链</span></span><br><span class="line">.map(word -&gt; Tuple2.of(word, <span class="number">1L</span>)).startNewChain();</span><br></pre></td></tr></table></figure>

<h4 id="作业图-JobGraph-与执行图-ExecutionGraph"><a href="#作业图-JobGraph-与执行图-ExecutionGraph" class="headerlink" title="作业图(JobGraph)与执行图(ExecutionGraph)"></a>作业图(JobGraph)与执行图(ExecutionGraph)</h4><p>逻辑流图(StreamGraph)→ 作业图(JobGraph)→ 执行图(ExecutionGraph)→ 物理图(Physical Graph)</p>
<p>处理 socket 文本流的 StreamWordCount 程序：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">env.socketTextStream().flatMap(...).keyBy(<span class="number">0</span>).sum(<span class="number">1</span>).print();</span><br></pre></td></tr></table></figure>

<p>提交时设置并行度为 2：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/flink run –p 2 –c com.vincent.wc.StreamWordCount ./FlinkTutorial-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure>

<p>除了 socketTextStream() 是非并行的 Source 算子，它的并行度始终 为 1，其他算子的并行度都为 2。</p>
<p><img src="/Flink/%E6%88%AA%E5%B1%8F2022-03-22%20%E4%B8%8B%E5%8D%889.46.48.png" alt="截屏2022-03-22 下午9.46.48"></p>
<ol>
<li>逻辑流图(StreamGraph)</li>
</ol>
<p>这是根据用户通过 DataStream API 编写的代码生成的最初的 DAG 图，用来表示程序的拓扑结构。这一步一般在<strong>客户端</strong>完成。我们可以看到，逻辑流图中的节点，完全对应着代码中的四步算子操作：源算子 Source(socketTextStream())→扁平映射算子 Flat Map(flatMap()) →分组聚合算子 Keyed Aggregation(keyBy/sum()) →输出算子 Sink(print())。</p>
<ol start="2">
<li>作业图(JobGraph)</li>
</ol>
<p>StreamGraph 经过优化后生成的就是作业图(JobGraph)，这是提交给 JobManager 的数据结构，确定了当前作业中所有任务的划分。主要的优化为：将多个符合条件的节点链接在一起合并成一个任务节点，形成<strong>算子链</strong>，这样可以减少数据交换的消耗。JobGraph 一般也是在<strong>客户端</strong>生成的，在作业提交时传递给 JobMaster。在图 4-12 中，分组聚合算子(Keyed Aggregation)和输出算子 Sink(print)并行度都为 2， 而且是一对一的关系，满足算子链的要求，所以会合并在一起，成为一个任务节点。</p>
<ol start="3">
<li>执行图(ExecutionGraph)</li>
</ol>
<p>JobMaster 收到 JobGraph 后，会根据它来生成执行图(ExecutionGraph)。ExecutionGraph 是 JobGraph 的并行化版本，是调度层最核心的数据结构。从图 4-12 中可以看到，与 JobGraph 最大的区别就是<strong>按照并行度对并行子任务进行了拆分</strong>， 并明确了任务间数据传输的方式。</p>
<ol start="4">
<li>物理图(Physical Graph)</li>
</ol>
<p>JobMaster 生成执行图后， 会将它分发给 TaskManager；各个 TaskManager 会根据执行图部署任务，最终的物理执行过程也会形成一张图，一般就叫作物理图(Physical Graph)。 这只是具体执行层面的图，并不是一个具体的数据结构。对应在上图 4-12 中，物理图主要就是在执行图的基础上，进一步<strong>确定数据存放的位置和收发的具体方式</strong>。有了物理图，TaskManager 就可以对传递来的数据进行处理计算了。所以我们可以看到，程序里定义了<strong>四个算子</strong>操作：源(Source)-&gt;转换(flatMap)-&gt;分组聚合(keyBy/sum)-&gt;输出(print)；合并算子链进行优化之后，就只有<strong>三个任务节点</strong>了；再考虑并行度后，一共有 <strong>5 个并行子任务</strong>，最终需要 <strong>5 个线程来</strong>执行。</p>
<h4 id="任务-Tasks-和任务槽-Task-Slots"><a href="#任务-Tasks-和任务槽-Task-Slots" class="headerlink" title="任务(Tasks)和任务槽(Task Slots)"></a>任务(Tasks)和任务槽(Task Slots)</h4><p>Flink 中每一个 worker(也就是 TaskManager)都是一个 <strong>JVM 进程</strong>，它可以启动<strong>多个独立的线程</strong>，来**并行执行多个子任务(subtask)**。</p>
<p>所以如果想要执行 5 个任务，并不一定非要 5 个 TaskManager，我们可以让 TaskManager 多线程执行任务。如果可以同时运行 5 个线程，那么只要一个 TaskManager 就可以满足我们之前程序的运行需求了。</p>
<p>很显然，TaskManager 的计算资源是有限的，并不是所有任务都可以放在一个 TaskManager 上并行执行。并行的任务越多，每个线程的资源就会越少。那一个 TaskManager 到底能并行处理多少个任务呢？为了控制并发量，我们需要在 TaskManager 上对每个任务运行所占用的资源做出明确的划分，这就是所谓的任务槽(task slots)。</p>
<p>每个任务槽(task slot)其实表示了<strong>TaskManager</strong>拥有计算资源的一个<strong>固定大小的子集</strong>。这些资源就是用来独立执行一个<strong>子任务</strong>的。</p>
<p><img src="/Flink/%E6%88%AA%E5%B1%8F2022-03-22%20%E4%B8%8B%E5%8D%8810.22.59.png" alt="截屏2022-03-22 下午10.22.59"></p>
<p>假如一个 TaskManager 有三个 slot，那么它会将管理的内存平均分成三份，每个 slot 独自占据一份。这样一来，我们在 slot 上执行一个子任务时，相当于划定了一块内存”专款专用”，就不需要跟来自其他作业的任务去竞争内存资源了。所以现在我们只要 2 个 TaskManager，就可以并行处理分配好的 5 个任务了。</p>
<p><strong>任务槽数量的设置</strong></p>
<p>我们可以通过集群的配置文件来设定 TaskManager 的 slot 数量：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">taskmanager.numberOfTaskSlots: 8</span><br></pre></td></tr></table></figure>

<p>通过调整 slot 的数量，我们就可以控制子任务之间的隔离级别。</p>
<p>具体来说，如果一个 TaskManager 只有一个 slot，那将意味着每个任务都会运行在独立的 JVM 中(当然，该 JVM 可能是通过一个特定的容器启动的)；而一个 TaskManager 设置多个 slot 则意味着多个子任务可以共享同一个 JVM。它们的区别在于：前者任务之间完全独立运行， 隔离级别更高、彼此间的影响可以降到最小；而后者在同一个 JVM 进程中运行的任务，将<strong>共享 TCP 连接和心跳消息</strong>，也可能<strong>共享数据集和数据结构</strong>，这就减少了每个任务的运行开销， 在降低隔离级别的同时提升了性能。</p>
<p>需要注意的是，<strong>slot 目前仅仅用来隔离内存</strong>，不会涉及 CPU 的隔离。在具体应用时，可以将 slot 数量配置为机器的 CPU 核心数，尽量避免不同任务之间对 CPU 的竞争。这也是开发环境默认并行度设为机器 CPU 数量的原因。</p>
<p><strong>任务对任务槽的共享</strong></p>
<p>这样看来，一共有多少任务，我们就需要有多少 slot 来并行处理它们。不过实际提交作业进行测试就会发现，我们之前的WordCount 程序设置并行度为 2 提交，一共有 5 个并行子任务，可集群即使只有 2 个 task slot 也是可以成功提交并运行的。这又是为什么呢？</p>
<p>我们可以基于之前的例子继续扩展。如果我们保持 sink 任务并行度为 1 不变，而作业提交时设置全局并行度为 6，那么前两个任务节点就会各自有 6 个并行子任务，整个流处理程序 则有 13 个子任务。那对于 2 个 TaskManager、每个有 3 个 slot 的集群配置来说，还能否正常运行呢?</p>
<p><img src="/Flink/%E6%88%AA%E5%B1%8F2022-03-22%20%E4%B8%8B%E5%8D%8810.35.04.png" alt="截屏2022-03-22 下午10.35.04"></p>
<p>完全没有问题。这是因为默认情况下，Flink 是允许子任务共享 slot 的。只要属于同一个作业，那么对于不同任务节点的并行子任务，就可以放到同一个 slot 上执行。 所以对于第一个任务节点 source→map，它的 6 个并行子任务必须分到不同的 slot 上(如果在 同一 slot 就没法数据并行了)，而第二个任务节点 keyBy/window/apply 的并行子任务却可以和第一个任务节点共享 slot。</p>
<p>这个特性看起来有点奇怪：我们不是希望并行处理、任务之间相互隔离吗，为什么这里又允许共享 slot 呢?</p>
<p>我们知道，一个 slot 对应了一组独立的计算资源。在之前不做共享的时候，每个任务都平等地占据了一个 slot，但其实<strong>不同的任务对资源的占用是不同的</strong>。例如这里的前两个任务， source/map 尽管是两个算子合并算子链得到的，但它只是基本的数据读取和简单转换，计算耗时极短，一般也不需要太大的内存空间；而 window 算子所做的窗口操作，往往会涉及大量的数据、状态存储和计算，我们一般把这类任务叫作<strong>资源密集型(intensive)任务</strong>。当它们被平等地分配到独立的 slot 上时，实际运行我们就会发现，大量数据到来时 source/map 和 sink 任务很快就可以完成，但 window 任务却耗时很久；于是<strong>下游的 sink 任务占据的 slot 就会等待闲置</strong>，而上游的 source/map 任务受限于下游的处理能力，也会在快速处理完一部分数据后阻塞对应的资源开始等待(相当于处理背压)。这样资源的利用就出现了极大的不平衡，忙的忙死，闲的闲死。</p>
<p>解决这一问题的思路就是允许 slot 共享。当我们<strong>将资源密集型和非密集型的任务同时放到 一个 slot 中</strong>，它们就可以自行分配对资源占用的比例，从而保证最重的活平均分配给所有的 TaskManager。</p>
<p>slot 共享另一个好处就是允许我们保存完整的作业管道。这样一来，即使某个 TaskManager 出现故障宕机，其他节点也可以完全不受影响，作业的任务可以继续执行。</p>
<p>另外，同一个任务节点的并行子任务是不能共享 slot 的。所以允许 slot 共享之后，运行作业所需的 slot 数量正好就是作业中所有算子并行度的最大值。这样一来，我们考虑当前集群需要配置多少 slot 资源时，就不需要再去详细计算一个作业总共包含多少个并行子任务了，只看<strong>最大的并行度</strong>就够了。</p>
<p>Flink 默认是允许 slot 共享的，如果希望某个算子对应的任务完全独占一个 slot，或者只有某一部分算子共享 slot，我们也可以通过设置 **slot 共享组(SlotSharingGroup)**手动指定：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">.map(word -&gt; Tuple2.of(word, <span class="number">1L</span>)).slotSharingGroup(<span class="string">&quot;1&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>这样，只有属于同一个 slot 共享组的子任务，才会开启 slot 共享；不同组之间的任务是完全隔离的，必须分配到不同的 slot 上。在这种场景下，总共需要的 slot 数量，就是各个 slot 共享组最大并行度的总和。</p>
<p><strong>任务槽和并行度的关系</strong></p>
<p>Slot 和并行度确实都跟程序的并行执行有关，但两者是完全不同的概念。简单来说，task slot 是静态的概念，是指 TaskManager 具有的并发执行能力，可以通过参数 taskmanager.numberOfTaskSlots 进行配置；而并行度(parallelism)是动态概念，也就是 TaskManager 运行程序时实际使用的并发能力，可以通过参数 parallelism.default 进行配置。换句话说，并行度如果小于等于集群中可用 slot 的总数，程序是可以正常执行的，因为 slot 不一定要全部占用，有十分力气可以只用八分；而如果并行度大于可用 slot 总数，导致超出了并行能力上限，那么心有余力不足，程序就只好等待资源管理器分配更多的资源了。</p>
<h2 id="Maven"><a href="#Maven" class="headerlink" title="Maven"></a>Maven</h2><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">flink.version</span>&gt;</span>1.12.0<span class="tag">&lt;/<span class="name">flink.version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">java.version</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">java.version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">scala.binary.version</span>&gt;</span>2.11<span class="tag">&lt;/<span class="name">scala.binary.version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">slf4j.version</span>&gt;</span>1.7.30<span class="tag">&lt;/<span class="name">slf4j.version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-java_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-clients_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-runtime-web_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-api<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;slf4j.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-log4j12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;slf4j.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.logging.log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j-to-slf4j<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.14.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.projectlombok<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>lombok<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.18.12<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.12.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.alibaba<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>fastjson<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.75<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-redis_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.1.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-elasticsearch6_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.12.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.49<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-jdbc_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.12.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-statebackend-rocksdb_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-cep_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-planner_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-planner-blink_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-scala_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-csv<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-json<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-hive_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- Hive Dependency --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="WordCount"><a href="#WordCount" class="headerlink" title="WordCount"></a>WordCount</h2><h3 id="batch"><a href="#batch" class="headerlink" title="batch"></a>batch</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//1.获取执行环境</span></span><br><span class="line">    ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.读取文件</span></span><br><span class="line">    DataSource&lt;String&gt; input = env.readTextFile(<span class="string">&quot;/Users/vincent/Documents/flink/input/word.txt&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.用flatmap压平 1行hello world 变成 1行hello 1行world</span></span><br><span class="line">    FlatMapOperator&lt;String, String&gt; stringObjectFlatMapOperator = input.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String s, Collector&lt;String&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="comment">//按照空格切割</span></span><br><span class="line">            String[] words = s.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">            <span class="keyword">for</span> (String word: words</span><br><span class="line">                 ) &#123;</span><br><span class="line">                collector.collect(word);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//4.单词变元组 hello -&gt; (hello,1)</span></span><br><span class="line">    MapOperator&lt;String, Tuple2&lt;String,Integer&gt;&gt; stringObjectMapOperator = stringObjectFlatMapOperator.map(<span class="keyword">new</span> MapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">map</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="comment">//return new Tuple2&lt;&gt;(s,1);</span></span><br><span class="line">            <span class="keyword">return</span> Tuple2.of(s,<span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//5.分组</span></span><br><span class="line">    UnsortedGrouping&lt;Tuple2&lt;String, Integer&gt;&gt; tuple2UnsortedGrouping = stringObjectMapOperator.groupBy(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//6.聚合</span></span><br><span class="line">    AggregateOperator&lt;Tuple2&lt;String, Integer&gt;&gt; tuple2AggregateOperator = tuple2UnsortedGrouping.sum(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//7.显示</span></span><br><span class="line">    tuple2AggregateOperator.print();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">// 1. 创建执行环境</span></span><br><span class="line">    ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    <span class="comment">// 2. 从文件读取数据  按行读取(存储的元素就是每行的文本)</span></span><br><span class="line">    DataSource&lt;String&gt; lineDS = env.readTextFile(<span class="string">&quot;/Users/vincent/Documents/flink/input/word.txt&quot;</span>);</span><br><span class="line">    <span class="comment">// 3. 转换数据格式</span></span><br><span class="line">    FlatMapOperator&lt;String, Tuple2&lt;String, Long&gt;&gt; wordAndOne = lineDS</span><br><span class="line">            .flatMap((String line, Collector&lt;Tuple2&lt;String, Long&gt;&gt; out) -&gt; &#123;</span><br><span class="line">                String[] words = line.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">                <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">                    out.collect(Tuple2.of(word, <span class="number">1L</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;)</span><br><span class="line">            .returns(Types.TUPLE(Types.STRING, Types.LONG));  <span class="comment">//当Lambda表达式使用 Java 泛型的时候, 由于泛型擦除的存在, 需要显示的声明类型信息</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4. 按照 word 进行分组</span></span><br><span class="line">    UnsortedGrouping&lt;Tuple2&lt;String, Long&gt;&gt; wordAndOneUG = wordAndOne.groupBy(<span class="number">0</span>);</span><br><span class="line">    <span class="comment">// 5. 分组内聚合统计</span></span><br><span class="line">    AggregateOperator&lt;Tuple2&lt;String, Long&gt;&gt; sum = wordAndOneUG.sum(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 6. 打印结果</span></span><br><span class="line">    sum.print();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="bounded-stream"><a href="#bounded-stream" class="headerlink" title="bounded_stream"></a>bounded_stream</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//1、创建执行环境，设置并行度为1</span></span><br><span class="line">    StreamExecutionEnvironment executionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    executionEnvironment.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2、读取文件流</span></span><br><span class="line">    DataStreamSource&lt;String&gt; stringDataStreamSource = executionEnvironment.readTextFile(<span class="string">&quot;/Users/vincent/Documents/flink/input/word.txt&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.hello world 转换为 (hello,1) (world,1)</span></span><br><span class="line">    SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; objectSingleOutputStreamOperator = stringDataStreamSource.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Tuple2&lt;String,Integer&gt;&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String s, Collector&lt;Tuple2&lt;String,Integer&gt;&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            String[] words = s.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">            <span class="keyword">for</span> (String word: words</span><br><span class="line">                 ) &#123;</span><br><span class="line">                collector.collect(Tuple2.of(word, <span class="number">1</span>));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//4.分组</span></span><br><span class="line">    KeyedStream&lt;Tuple2&lt;String, Integer&gt;, String&gt; tuple2TupleKeyedStream = objectSingleOutputStreamOperator.keyBy(<span class="keyword">new</span> KeySelector&lt;Tuple2&lt;String, Integer&gt;, String&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">getKey</span><span class="params">(Tuple2&lt;String, Integer&gt; stringIntegerTuple2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> stringIntegerTuple2.f0;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//5.聚合</span></span><br><span class="line">    SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; sum = tuple2TupleKeyedStream.sum(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//6.显示</span></span><br><span class="line">    sum.print();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//7.启动任务</span></span><br><span class="line">    JobExecutionResult execute = executionEnvironment.execute(<span class="string">&quot;start&quot;</span>);</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">// 1. 创建流式执行环境</span></span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    <span class="comment">// 2. 读取文件</span></span><br><span class="line">    DataStreamSource&lt;String&gt; lineDSS = env.readTextFile(<span class="string">&quot;/Users/vincent/Documents/flink/input/word.txt&quot;</span>);</span><br><span class="line">    <span class="comment">// 3. 转换数据格式</span></span><br><span class="line">    SingleOutputStreamOperator&lt;Tuple2&lt;String, Long&gt;&gt; wordAndOne = lineDSS</span><br><span class="line">            .flatMap((String line, Collector&lt;String&gt; words) -&gt; &#123;</span><br><span class="line">                Arrays.stream(line.split(<span class="string">&quot; &quot;</span>)).forEach(words::collect);</span><br><span class="line">            &#125;)</span><br><span class="line">            .returns(Types.STRING)</span><br><span class="line">            .map(word -&gt; Tuple2.of(word, <span class="number">1L</span>))</span><br><span class="line">            .returns(Types.TUPLE(Types.STRING, Types.LONG));</span><br><span class="line">    <span class="comment">// 4. 分组</span></span><br><span class="line">    KeyedStream&lt;Tuple2&lt;String, Long&gt;, String&gt; wordAndOneKS = wordAndOne</span><br><span class="line">            .keyBy(t -&gt; t.f0);</span><br><span class="line">    <span class="comment">// 5. 求和</span></span><br><span class="line">    SingleOutputStreamOperator&lt;Tuple2&lt;String, Long&gt;&gt; result = wordAndOneKS</span><br><span class="line">            .sum(<span class="number">1</span>);</span><br><span class="line">    <span class="comment">// 6. 打印</span></span><br><span class="line">    result.print();</span><br><span class="line">    <span class="comment">// 7. 执行</span></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="unbounded-stream"><a href="#unbounded-stream" class="headerlink" title="unbounded_stream"></a>unbounded_stream</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//1.获取执行环境</span></span><br><span class="line">    StreamExecutionEnvironment executionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    executionEnvironment.setParallelism(<span class="number">1</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//2.读取端口数据流</span></span><br><span class="line">    DataStreamSource&lt;String&gt; stringDataStreamSource = executionEnvironment.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">1234</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//3.转换为元组</span></span><br><span class="line">    SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; objectSingleOutputStreamOperator = stringDataStreamSource.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Tuple2&lt;String,Integer&gt;&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String s, Collector&lt;Tuple2&lt;String,Integer&gt;&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            String[] words = s.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">            <span class="keyword">for</span> (String word: words</span><br><span class="line">            ) &#123;</span><br><span class="line">                collector.collect(Tuple2.of(word, <span class="number">1</span>));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//4.分组</span></span><br><span class="line">    KeyedStream&lt;Tuple2&lt;String, Integer&gt;, String&gt; tuple2TupleKeyedStream = objectSingleOutputStreamOperator.keyBy(<span class="keyword">new</span> KeySelector&lt;Tuple2&lt;String, Integer&gt;, String&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">getKey</span><span class="params">(Tuple2&lt;String, Integer&gt; stringIntegerTuple2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> stringIntegerTuple2.f0;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//5.聚合</span></span><br><span class="line">    SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; sum = tuple2TupleKeyedStream.sum(<span class="number">1</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//6.显示</span></span><br><span class="line">    sum.print();</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//7.启动任务</span></span><br><span class="line">    executionEnvironment.execute();</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">// 1. 创建流式执行环境</span></span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    <span class="comment">// 2. 读取文本流</span></span><br><span class="line">    DataStreamSource&lt;String&gt; lineDSS = env.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">7777</span>);</span><br><span class="line">    <span class="comment">// 3. 转换数据格式</span></span><br><span class="line">    SingleOutputStreamOperator&lt;Tuple2&lt;String, Long&gt;&gt; wordAndOne = lineDSS</span><br><span class="line">            .flatMap((String line, Collector&lt;String&gt; words) -&gt; &#123;</span><br><span class="line">                Arrays.stream(line.split(<span class="string">&quot; &quot;</span>)).forEach(words::collect);</span><br><span class="line">            &#125;)</span><br><span class="line">            .returns(Types.STRING)</span><br><span class="line">            .map(word -&gt; Tuple2.of(word, <span class="number">1L</span>))</span><br><span class="line">            .returns(Types.TUPLE(Types.STRING, Types.LONG));</span><br><span class="line">    <span class="comment">// 4. 分组</span></span><br><span class="line">    KeyedStream&lt;Tuple2&lt;String, Long&gt;, String&gt; wordAndOneKS = wordAndOne</span><br><span class="line">            .keyBy(t -&gt; t.f0);</span><br><span class="line">    <span class="comment">// 5. 求和</span></span><br><span class="line">    SingleOutputStreamOperator&lt;Tuple2&lt;String, Long&gt;&gt; result = wordAndOneKS</span><br><span class="line">            .sum(<span class="number">1</span>);</span><br><span class="line">    <span class="comment">// 6. 打印</span></span><br><span class="line">    result.print();</span><br><span class="line">    <span class="comment">// 7. 执行</span></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Source"><a href="#Source" class="headerlink" title="Source"></a>Source</h2><h3 id="collection"><a href="#collection" class="headerlink" title="collection"></a>collection</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">StreamExecutionEnvironment executionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">executionEnvironment.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">List&lt;WaterSensor&gt; list = Arrays.asList(</span><br><span class="line">  <span class="keyword">new</span> WaterSensor(<span class="string">&quot;a&quot;</span>,<span class="number">12345678910L</span>,<span class="number">56</span>),</span><br><span class="line">  <span class="keyword">new</span> WaterSensor(<span class="string">&quot;b&quot;</span>,<span class="number">12354657829L</span>,<span class="number">57</span>),</span><br><span class="line">  <span class="keyword">new</span> WaterSensor(<span class="string">&quot;c&quot;</span>,<span class="number">12362457829L</span>,<span class="number">52</span>)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">DataStreamSource&lt;WaterSensor&gt; waterSensorDataStreamSource = executionEnvironment.fromCollection(list);</span><br><span class="line"></span><br><span class="line">waterSensorDataStreamSource.print();</span><br><span class="line"></span><br><span class="line">executionEnvironment.execute();</span><br></pre></td></tr></table></figure>

<h3 id="file"><a href="#file" class="headerlink" title="file"></a>file</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">StreamExecutionEnvironment executionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">executionEnvironment.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">DataStreamSource&lt;String&gt; stringDataStreamSource = executionEnvironment.readTextFile(<span class="string">&quot;input/sensor.txt&quot;</span>);</span><br><span class="line"></span><br><span class="line">SingleOutputStreamOperator&lt;WaterSensor&gt; map = stringDataStreamSource.map(<span class="keyword">new</span> MapFunction&lt;String, WaterSensor&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> WaterSensor <span class="title">map</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        String[] split = s.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> WaterSensor(split[<span class="number">0</span>],Long.parseLong(split[<span class="number">1</span>]),Integer.parseInt(split[<span class="number">2</span>]));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">map.print();</span><br><span class="line"></span><br><span class="line">executionEnvironment.execute();</span><br></pre></td></tr></table></figure>

<h3 id="socket"><a href="#socket" class="headerlink" title="socket"></a>socket</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">DataStreamSource&lt;String&gt; socketTextStream = env.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">1234</span>);</span><br><span class="line"></span><br><span class="line">socketTextStream.print();</span><br><span class="line"></span><br><span class="line">env.execute();</span><br></pre></td></tr></table></figure>

<h3 id="kafka"><a href="#kafka" class="headerlink" title="kafka"></a>kafka</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-console-producer.sh --broker-list linux1:9092 --topic clicks</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">    properties.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;linux1:9092&quot;</span>);</span><br><span class="line">    properties.setProperty(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;consumer-group&quot;</span>);</span><br><span class="line">    properties.setProperty(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">    properties.setProperty(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">    properties.setProperty(<span class="string">&quot;auto.offset.reset&quot;</span>, <span class="string">&quot;latest&quot;</span>);</span><br><span class="line"></span><br><span class="line">    DataStreamSource&lt;String&gt; stream = env.addSource(<span class="keyword">new</span> FlinkKafkaConsumer&lt;String&gt;(</span><br><span class="line">            <span class="string">&quot;clicks&quot;</span>,</span><br><span class="line">            <span class="keyword">new</span> SimpleStringSchema(),</span><br><span class="line">            properties</span><br><span class="line">    ));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    stream.print(<span class="string">&quot;Kafka&quot;</span>);</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="custom-parallel"><a href="#custom-parallel" class="headerlink" title="custom_parallel"></a>custom_parallel</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">    env.addSource(<span class="keyword">new</span> CustomSource()).setParallelism(<span class="number">2</span>).print();</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomSource</span> <span class="keyword">implements</span> <span class="title">ParallelSourceFunction</span>&lt;<span class="title">Integer</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">boolean</span> running = <span class="keyword">true</span>;</span><br><span class="line">    <span class="keyword">private</span> Random random = <span class="keyword">new</span> Random();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;Integer&gt; sourceContext)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">while</span> (running) &#123;</span><br><span class="line">            sourceContext.collect(random.nextInt());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        running = <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="custom-source"><a href="#custom-source" class="headerlink" title="custom_source"></a>custom_source</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    StreamExecutionEnvironment executionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    executionEnvironment.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    DataStreamSource&lt;WaterSensor&gt; objectDataStreamSource = executionEnvironment.addSource(<span class="keyword">new</span> MySource(<span class="string">&quot;localhost&quot;</span>,<span class="number">2345</span>));</span><br><span class="line"></span><br><span class="line">    objectDataStreamSource.print();</span><br><span class="line"></span><br><span class="line">    executionEnvironment.execute();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MySource</span> <span class="keyword">implements</span> <span class="title">SourceFunction</span>&lt;<span class="title">WaterSensor</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String host;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> port;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Boolean running = <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">    Socket socket = <span class="keyword">null</span>;</span><br><span class="line">    BufferedReader reader = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">MySource</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">MySource</span><span class="params">(String host, <span class="keyword">int</span> port)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.host = host;</span><br><span class="line">        <span class="keyword">this</span>.port = port;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        socket = <span class="keyword">new</span> Socket(host,port);</span><br><span class="line">        reader = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(socket.getInputStream(), UTF_8));</span><br><span class="line"></span><br><span class="line">        String s = reader.readLine();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(running &amp;&amp; s != <span class="keyword">null</span>)&#123;</span><br><span class="line"></span><br><span class="line">            String[] split = s.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">            WaterSensor waterSensor = <span class="keyword">new</span> WaterSensor(split[<span class="number">0</span>], Long.parseLong(split[<span class="number">1</span>]), Integer.parseInt(split[<span class="number">2</span>]));</span><br><span class="line">            ctx.collect(waterSensor);</span><br><span class="line">            s = reader.readLine();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        running = <span class="keyword">false</span>;</span><br><span class="line">        <span class="keyword">try</span>&#123;</span><br><span class="line">            reader.close();</span><br><span class="line">        &#125;<span class="keyword">catch</span> (Exception e)&#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            socket.close();</span><br><span class="line">        &#125;<span class="keyword">catch</span> (IOException e)&#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Transform"><a href="#Transform" class="headerlink" title="Transform"></a>Transform</h2><h3 id="map"><a href="#map" class="headerlink" title="map"></a>map</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    DataStreamSource&lt;Event&gt; stream = env.fromElements(</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Mary&quot;</span>, <span class="string">&quot;./home&quot;</span>, <span class="number">1000L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;./cart&quot;</span>, <span class="number">2000L</span>)</span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 传入匿名实现类的匿名对象，实现MapFunction</span></span><br><span class="line">    stream.map(<span class="keyword">new</span> MapFunction&lt;Event, String&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(Event e)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> e.user;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 传入MapFunction的实现类的匿名对象</span></span><br><span class="line">    stream.map(<span class="keyword">new</span> UserExtractor()).print();</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">UserExtractor</span> <span class="keyword">implements</span> <span class="title">MapFunction</span>&lt;<span class="title">Event</span>, <span class="title">String</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(Event e)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> e.user;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="richFunction"><a href="#richFunction" class="headerlink" title="richFunction"></a>richFunction</h3><p>RichFunction富有的地方在于：1、生命周期方法 2、可以获取上下文执行环境，做状态编程</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * An base interface for all rich user-defined functions. This class defines methods for the life</span></span><br><span class="line"><span class="comment"> * cycle of the functions, as well as methods to access the context in which the functions are</span></span><br><span class="line"><span class="comment"> * executed.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Public</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">RichFunction</span> <span class="keyword">extends</span> <span class="title">Function</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Initialization method for the function. It is called before the actual working methods (like</span></span><br><span class="line"><span class="comment">     * &lt;i&gt;map&lt;/i&gt; or &lt;i&gt;join&lt;/i&gt;) and thus suitable for one time setup work. For functions that are</span></span><br><span class="line"><span class="comment">     * part of an iteration, this method will be invoked at the beginning of each iteration</span></span><br><span class="line"><span class="comment">     * superstep.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;The configuration object passed to the function can be used for configuration and</span></span><br><span class="line"><span class="comment">     * initialization. The configuration contains all parameters that were configured on the</span></span><br><span class="line"><span class="comment">     * function in the program composition.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * &lt;pre&gt;&#123;<span class="doctag">@code</span></span></span><br><span class="line"><span class="comment">     * public class MyFilter extends RichFilterFunction&lt;String&gt; &#123;</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     *     private String searchString;</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     *     public void open(Configuration parameters) &#123;</span></span><br><span class="line"><span class="comment">     *         this.searchString = parameters.getString(&quot;foo&quot;);</span></span><br><span class="line"><span class="comment">     *     &#125;</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     *     public boolean filter(String value) &#123;</span></span><br><span class="line"><span class="comment">     *         return value.equals(searchString);</span></span><br><span class="line"><span class="comment">     *     &#125;</span></span><br><span class="line"><span class="comment">     * &#125;</span></span><br><span class="line"><span class="comment">     * &#125;&lt;/pre&gt;</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;By default, this method does nothing.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> parameters The configuration containing the parameters attached to the contract.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception Implementations may forward exceptions, which are caught by the runtime.</span></span><br><span class="line"><span class="comment">     *     When the runtime catches an exception, it aborts the task and lets the fail-over logic</span></span><br><span class="line"><span class="comment">     *     decide whether to retry the task execution.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@see</span> org.apache.flink.configuration.Configuration</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Tear-down method for the user code. It is called after the last call to the main working</span></span><br><span class="line"><span class="comment">     * methods (e.g. &lt;i&gt;map&lt;/i&gt; or &lt;i&gt;join&lt;/i&gt;). For functions that are part of an iteration, this</span></span><br><span class="line"><span class="comment">     * method will be invoked after each iteration superstep.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;This method can be used for clean up work.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception Implementations may forward exceptions, which are caught by the runtime.</span></span><br><span class="line"><span class="comment">     *     When the runtime catches an exception, it aborts the task and lets the fail-over logic</span></span><br><span class="line"><span class="comment">     *     decide whether to retry the task execution.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ------------------------------------------------------------------------</span></span><br><span class="line">    <span class="comment">//  Runtime context</span></span><br><span class="line">    <span class="comment">// ------------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Gets the context that contains information about the UDF&#x27;s runtime, such as the parallelism</span></span><br><span class="line"><span class="comment">     * of the function, the subtask index of the function, or the name of the task that executes the</span></span><br><span class="line"><span class="comment">     * function.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;The RuntimeContext also gives access to the &#123;<span class="doctag">@link</span></span></span><br><span class="line"><span class="comment">     * org.apache.flink.api.common.accumulators.Accumulator&#125;s and the &#123;<span class="doctag">@link</span></span></span><br><span class="line"><span class="comment">     * org.apache.flink.api.common.cache.DistributedCache&#125;.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> The UDF&#x27;s runtime context.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function">RuntimeContext <span class="title">getRuntimeContext</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Gets a specialized version of the &#123;<span class="doctag">@link</span> RuntimeContext&#125;, which has additional information</span></span><br><span class="line"><span class="comment">     * about the iteration in which the function is executed. This IterationRuntimeContext is only</span></span><br><span class="line"><span class="comment">     * available if the function is part of an iteration. Otherwise, this method throws an</span></span><br><span class="line"><span class="comment">     * exception.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> The IterationRuntimeContext.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> java.lang.IllegalStateException Thrown, if the function is not executed as part of an</span></span><br><span class="line"><span class="comment">     *     iteration.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function">IterationRuntimeContext <span class="title">getIterationRuntimeContext</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Sets the function&#x27;s runtime context. Called by the framework when creating a parallel</span></span><br><span class="line"><span class="comment">     * instance of the function.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> t The runtime context.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">setRuntimeContext</span><span class="params">(RuntimeContext t)</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">    DataStreamSource&lt;Event&gt; clicks = env.fromElements(</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Mary&quot;</span>, <span class="string">&quot;./home&quot;</span>, <span class="number">1000L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;./cart&quot;</span>, <span class="number">2000L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;./prod?id=1&quot;</span>, <span class="number">5</span> * <span class="number">1000L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Cary&quot;</span>, <span class="string">&quot;./home&quot;</span>, <span class="number">60</span> * <span class="number">1000L</span>)</span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将点击事件转换成长整型的时间戳输出</span></span><br><span class="line">    clicks.map(<span class="keyword">new</span> RichMapFunction&lt;Event, Long&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                    <span class="keyword">super</span>.open(parameters);</span><br><span class="line">                    System.out.println(<span class="string">&quot;索引为 &quot;</span> + getRuntimeContext().getIndexOfThisSubtask() + <span class="string">&quot; 的任务开始&quot;</span>);</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> Long <span class="title">map</span><span class="params">(Event value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                    <span class="keyword">return</span> value.timestamp;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                    <span class="keyword">super</span>.close();</span><br><span class="line">                    System.out.println(<span class="string">&quot;索引为 &quot;</span> + getRuntimeContext().getIndexOfThisSubtask() + <span class="string">&quot; 的任务结束&quot;</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;)</span><br><span class="line">            .print();</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap"></a>flatMap</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    DataStreamSource&lt;Event&gt; stream = env.fromElements(</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Mary&quot;</span>, <span class="string">&quot;./home&quot;</span>, <span class="number">1000L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;./cart&quot;</span>, <span class="number">2000L</span>)</span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">    stream.flatMap(<span class="keyword">new</span> MyFlatMap()).print();</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyFlatMap</span> <span class="keyword">implements</span> <span class="title">FlatMapFunction</span>&lt;<span class="title">Event</span>, <span class="title">String</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(Event value, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (value.user.equals(<span class="string">&quot;Mary&quot;</span>)) &#123;</span><br><span class="line">            out.collect(value.user);</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (value.user.equals(<span class="string">&quot;Bob&quot;</span>)) &#123;</span><br><span class="line">            out.collect(value.user);</span><br><span class="line">            out.collect(value.url);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    DataStreamSource&lt;Event&gt; stream = env.fromElements(</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Mary&quot;</span>, <span class="string">&quot;./home&quot;</span>, <span class="number">1000L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;./cart&quot;</span>, <span class="number">2000L</span>)</span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 传入匿名类实现FilterFunction</span></span><br><span class="line">    stream.filter(<span class="keyword">new</span> FilterFunction&lt;Event&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(Event e)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> e.user.equals(<span class="string">&quot;Mary&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 传入FilterFunction实现类</span></span><br><span class="line">    stream.filter(<span class="keyword">new</span> UserFilter()).print();</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">UserFilter</span> <span class="keyword">implements</span> <span class="title">FilterFunction</span>&lt;<span class="title">Event</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(Event e)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> e.user.equals(<span class="string">&quot;Mary&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="connect"><a href="#connect" class="headerlink" title="connect"></a>connect</h3><p><strong>说明：1、只能操作两个流    2、两个流可以是不同类型</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//1.获取执行环境</span></span><br><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//2.读取端口数据创建流</span></span><br><span class="line">DataStreamSource&lt;String&gt; stringDS = env.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">1234</span>);</span><br><span class="line">DataStreamSource&lt;String&gt; socketTextStream2 = env.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">4321</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//3.将socketTextStream2转换为Int类型</span></span><br><span class="line">SingleOutputStreamOperator&lt;Integer&gt; intDS = socketTextStream2.map(String::length);</span><br><span class="line"></span><br><span class="line"><span class="comment">//4.连接两个流</span></span><br><span class="line">ConnectedStreams&lt;String, Integer&gt; connectedStreams = stringDS.connect(intDS);</span><br><span class="line"></span><br><span class="line"><span class="comment">//5.处理连接之后的流</span></span><br><span class="line">SingleOutputStreamOperator&lt;Object&gt; result = connectedStreams.map(<span class="keyword">new</span> CoMapFunction&lt;String, Integer, Object&gt;() &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">map1</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> value;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">map2</span><span class="params">(Integer value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> value;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">//6.打印数据</span></span><br><span class="line">result.print();</span><br><span class="line"></span><br><span class="line"><span class="comment">//7.执行</span></span><br><span class="line">env.execute();</span><br></pre></td></tr></table></figure>

<h3 id="union"><a href="#union" class="headerlink" title="union"></a>union</h3><p><strong>说明：1、可以操作多个流    2、多个流必须是同类型</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//1.获取执行环境</span></span><br><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//2.读取端口数据创建流</span></span><br><span class="line">DataStreamSource&lt;String&gt; socketTextStream1 = env.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">1234</span>);</span><br><span class="line">DataStreamSource&lt;String&gt; socketTextStream2 = env.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">4321</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//3.连接两条流</span></span><br><span class="line">DataStream&lt;String&gt; union = socketTextStream1.union(socketTextStream2);</span><br><span class="line"></span><br><span class="line"><span class="comment">//4.打印</span></span><br><span class="line">union.print();</span><br><span class="line"></span><br><span class="line"><span class="comment">//5.执行任务</span></span><br><span class="line">env.execute();</span><br></pre></td></tr></table></figure>

<h3 id="max-maxBy"><a href="#max-maxBy" class="headerlink" title="max/maxBy"></a>max/maxBy</h3><p>max：取指定字段的当前的最大值，如果有多个字段，其他非比较字段，以第一条为准。</p>
<p>maxBy：取指定字段的当前的最大值，如果有多个字段，其他字段以最大值那条数据为准。另外，maxBy有第二个的参数，用来确定当比较字段出现相同时，返回之前的还是现在的，默认返回之前的，设置为false则返回新的，设置为true返回之前的。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//1.获取执行环境</span></span><br><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//2.读取端口数据并转换为JavaBean</span></span><br><span class="line">SingleOutputStreamOperator&lt;WaterSensor&gt; waterSensorDS = env.socketTextStream(<span class="string">&quot;linux1&quot;</span>, <span class="number">9999</span>)</span><br><span class="line">        .map(<span class="keyword">new</span> MapFunction&lt;String, WaterSensor&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> WaterSensor <span class="title">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String[] split = value.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> WaterSensor(split[<span class="number">0</span>],</span><br><span class="line">                        Long.parseLong(split[<span class="number">1</span>]),</span><br><span class="line">                        Integer.parseInt(split[<span class="number">2</span>]));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">//3.按照传感器ID分组</span></span><br><span class="line">KeyedStream&lt;WaterSensor, String&gt; keyedStream = waterSensorDS.keyBy(<span class="keyword">new</span> KeySelector&lt;WaterSensor, String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getKey</span><span class="params">(WaterSensor value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> value.getId();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">//4.计算最高水位线</span></span><br><span class="line"><span class="comment">//SingleOutputStreamOperator&lt;WaterSensor&gt; result = keyedStream.max(&quot;vc&quot;);</span></span><br><span class="line">SingleOutputStreamOperator&lt;WaterSensor&gt; result = keyedStream.maxBy(<span class="string">&quot;vc&quot;</span>, <span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//5.打印</span></span><br><span class="line">result.print();</span><br><span class="line"></span><br><span class="line"><span class="comment">//6.执行任务</span></span><br><span class="line">env.execute();</span><br></pre></td></tr></table></figure>

<h3 id="reduce"><a href="#reduce" class="headerlink" title="reduce"></a>reduce</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//获取PV最大的user数据</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 这里的使用了之前自定义数据源小节中的ClickSource()</span></span><br><span class="line">    env.addSource(<span class="keyword">new</span> ClickSource())</span><br><span class="line">            <span class="comment">// 将Event数据类型转换成元组类型</span></span><br><span class="line">            .map(<span class="keyword">new</span> MapFunction&lt;Event, Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Long&gt; <span class="title">map</span><span class="params">(Event e)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                    <span class="keyword">return</span> Tuple2.of(e.user, <span class="number">1L</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;)</span><br><span class="line">            .keyBy(r -&gt; r.f0) <span class="comment">// 使用用户名来进行分流</span></span><br><span class="line">            .reduce(<span class="keyword">new</span> ReduceFunction&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Long&gt; <span class="title">reduce</span><span class="params">(Tuple2&lt;String, Long&gt; value1, Tuple2&lt;String, Long&gt; value2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                    <span class="comment">// 每到一条数据，用户pv的统计值加1</span></span><br><span class="line">                    <span class="keyword">return</span> Tuple2.of(value1.f0, value1.f1 + value2.f1);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;)</span><br><span class="line">            .keyBy(r -&gt; <span class="keyword">true</span>) <span class="comment">// 为每一条数据分配同一个key，将聚合结果发送到一条流中去</span></span><br><span class="line">            .reduce(<span class="keyword">new</span> ReduceFunction&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Long&gt; <span class="title">reduce</span><span class="params">(Tuple2&lt;String, Long&gt; value1, Tuple2&lt;String, Long&gt; value2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                    <span class="comment">// 将累加器更新为当前最大的pv统计值，然后向下游发送累加器的值</span></span><br><span class="line">                    <span class="keyword">return</span> value1.f1 &gt; value2.f1 ? value1 : value2;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;)</span><br><span class="line">            .print();</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="returnType"><a href="#returnType" class="headerlink" title="returnType"></a>returnType</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    DataStreamSource&lt;Event&gt; clicks = env.fromElements(</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Mary&quot;</span>, <span class="string">&quot;./home&quot;</span>, <span class="number">1000L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;./cart&quot;</span>, <span class="number">2000L</span>)</span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 想要转换成二元组类型，需要进行以下处理</span></span><br><span class="line">    <span class="comment">// 1) 使用显式的 &quot;.returns(...)&quot;</span></span><br><span class="line">    DataStream&lt;Tuple2&lt;String, Long&gt;&gt; stream3 = clicks</span><br><span class="line">            .map( event -&gt; Tuple2.of(event.user, <span class="number">1L</span>) )</span><br><span class="line">            .returns(Types.TUPLE(Types.STRING, Types.LONG));</span><br><span class="line">    stream3.print();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2) 使用类来替代Lambda表达式</span></span><br><span class="line">    clicks.map(<span class="keyword">new</span> MyTuple2Mapper())</span><br><span class="line">            .print();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3) 使用匿名类来代替Lambda表达式</span></span><br><span class="line">    clicks.map(<span class="keyword">new</span> MapFunction&lt;Event, Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Long&gt; <span class="title">map</span><span class="params">(Event value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> Tuple2.of(value.user, <span class="number">1L</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;).print();</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 自定义MapFunction的实现类</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyTuple2Mapper</span> <span class="keyword">implements</span> <span class="title">MapFunction</span>&lt;<span class="title">Event</span>, <span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Long</span>&gt;&gt;</span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Long&gt; <span class="title">map</span><span class="params">(Event value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> Tuple2.of(value.user, <span class="number">1L</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="tupleAggreation"><a href="#tupleAggreation" class="headerlink" title="tupleAggreation"></a>tupleAggreation</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">		DataStreamSource&lt;Tuple2&lt;String, Integer&gt;&gt; stream = env.fromElements(</span><br><span class="line">                Tuple2.of(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>),</span><br><span class="line">                Tuple2.of(<span class="string">&quot;a&quot;</span>, <span class="number">3</span>),</span><br><span class="line">                Tuple2.of(<span class="string">&quot;a&quot;</span>, <span class="number">2</span>),</span><br><span class="line">                Tuple2.of(<span class="string">&quot;b&quot;</span>, <span class="number">3</span>),</span><br><span class="line">                Tuple2.of(<span class="string">&quot;b&quot;</span>, <span class="number">4</span>),</span><br><span class="line">                Tuple2.of(<span class="string">&quot;b&quot;</span>, <span class="number">1</span>)</span><br><span class="line">		);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//  stream.keyBy(r -&gt; r.f0).sum(1).print();</span></span><br><span class="line">    <span class="comment">//  stream.keyBy(r -&gt; r.f0).sum(&quot;f1&quot;).print();</span></span><br><span class="line">    <span class="comment">//  stream.keyBy(r -&gt; r.f0).max(1).print();</span></span><br><span class="line">    <span class="comment">//  stream.keyBy(r -&gt; r.f0).max(&quot;f1&quot;).print();</span></span><br><span class="line">    <span class="comment">//  stream.keyBy(r -&gt; r.f0).min(1).print();</span></span><br><span class="line">    <span class="comment">//  stream.keyBy(r -&gt; r.f0).min(&quot;f1&quot;).print();</span></span><br><span class="line">    <span class="comment">//  stream.keyBy(r -&gt; r.f0).maxBy(1).print();</span></span><br><span class="line">    <span class="comment">//  stream.keyBy(r -&gt; r.f0).maxBy(&quot;f1&quot;).print();</span></span><br><span class="line">    <span class="comment">//  stream.keyBy(r -&gt; r.f0).minBy(1).print();</span></span><br><span class="line">    stream.keyBy(r -&gt; r.f0).minBy(<span class="string">&quot;f1&quot;</span>).print();</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="process"><a href="#process" class="headerlink" title="process"></a>process</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//1.获取执行环境</span></span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.读取端口数据</span></span><br><span class="line">    DataStreamSource&lt;String&gt; socketTextStream = env.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">1234</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.使用Process实现压平功能</span></span><br><span class="line">    SingleOutputStreamOperator&lt;String&gt; wordDS = socketTextStream.process(<span class="keyword">new</span> ProcessFlatMapFunc());</span><br><span class="line"></span><br><span class="line">    <span class="comment">//4.使用Process实现Map功能</span></span><br><span class="line">    SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; wordToOneDS = wordDS.process(<span class="keyword">new</span> ProcessMapFunc());</span><br><span class="line"></span><br><span class="line">    <span class="comment">//5.按照单词分组</span></span><br><span class="line">    KeyedStream&lt;Tuple2&lt;String, Integer&gt;, String&gt; keyedStream = wordToOneDS.keyBy(data -&gt; data.f0);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//6.计算总和</span></span><br><span class="line">    SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; result = keyedStream.sum(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//7.打印</span></span><br><span class="line">    result.print();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//8.执行任务</span></span><br><span class="line">    env.execute();</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">ProcessFlatMapFunc</span> <span class="keyword">extends</span> <span class="title">ProcessFunction</span>&lt;<span class="title">String</span>, <span class="title">String</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//生命周期方法</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(String value, Context ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//运行时上下文,状态编程</span></span><br><span class="line">        RuntimeContext runtimeContext = getRuntimeContext();</span><br><span class="line"></span><br><span class="line">        String[] words = value.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">        <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">            out.collect(word);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//定时器</span></span><br><span class="line">        TimerService timerService = ctx.timerService();</span><br><span class="line">        timerService.registerProcessingTimeTimer(<span class="number">1245L</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取当前处理数据的时间</span></span><br><span class="line">        timerService.currentProcessingTime();</span><br><span class="line">        timerService.currentWatermark();  <span class="comment">//事件时间</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//侧输出流</span></span><br><span class="line">        <span class="comment">//ctx.output();</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">ProcessMapFunc</span> <span class="keyword">extends</span> <span class="title">ProcessFunction</span>&lt;<span class="title">String</span>, <span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Integer</span>&gt;&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(String value, Context ctx, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        out.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(value, <span class="number">1</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="physicalPartitioning"><a href="#physicalPartitioning" class="headerlink" title="physicalPartitioning"></a>physicalPartitioning</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">// 创建执行环境</span></span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    DataStreamSource&lt;Event&gt; stream = env.fromElements(<span class="keyword">new</span> Event(<span class="string">&quot;Mary&quot;</span>, <span class="string">&quot;./home&quot;</span>, <span class="number">1000L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;./cart&quot;</span>, <span class="number">2000L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;./prod?id=100&quot;</span>, <span class="number">3000L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;./prod?id=200&quot;</span>, <span class="number">3500L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;./prod?id=2&quot;</span>, <span class="number">2500L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;./prod?id=300&quot;</span>, <span class="number">3600L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;./home&quot;</span>, <span class="number">3000L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;./prod?id=1&quot;</span>, <span class="number">2300L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;./prod?id=3&quot;</span>, <span class="number">3300L</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1. 随机分区</span></span><br><span class="line">    stream.shuffle().print(<span class="string">&quot;shuffle&quot;</span>).setParallelism(<span class="number">4</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 轮询分区</span></span><br><span class="line">    stream.rebalance().print(<span class="string">&quot;rebalance&quot;</span>).setParallelism(<span class="number">4</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3. rescale重缩放分区</span></span><br><span class="line">    env.addSource(<span class="keyword">new</span> RichParallelSourceFunction&lt;Integer&gt;() &#123;  <span class="comment">// 这里使用了并行数据源的富函数版本</span></span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;Integer&gt; sourceContext)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">8</span>; i++) &#123;</span><br><span class="line">                        <span class="comment">// 将奇数发送到索引为1的并行子任务</span></span><br><span class="line">                        <span class="comment">// 将偶数发送到索引为0的并行子任务</span></span><br><span class="line">                        <span class="keyword">if</span> ( i % <span class="number">2</span> == getRuntimeContext().getIndexOfThisSubtask()) &#123;</span><br><span class="line">                            sourceContext.collect(i);</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;)</span><br><span class="line">            .setParallelism(<span class="number">2</span>)</span><br><span class="line">            .rescale()</span><br><span class="line">            .print().setParallelism(<span class="number">4</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4. 广播</span></span><br><span class="line">    stream.broadcast().print(<span class="string">&quot;broadcast&quot;</span>).setParallelism(<span class="number">4</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 5. 全局分区</span></span><br><span class="line">    stream.global().print(<span class="string">&quot;global&quot;</span>).setParallelism(<span class="number">4</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 6. 自定义重分区</span></span><br><span class="line">    <span class="comment">// 将自然数按照奇偶分区</span></span><br><span class="line">    env.fromElements(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>)</span><br><span class="line">            .partitionCustom(<span class="keyword">new</span> Partitioner&lt;Integer&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(Integer key, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">                    <span class="keyword">return</span> key % <span class="number">2</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;, <span class="keyword">new</span> KeySelector&lt;Integer, Integer&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> Integer <span class="title">getKey</span><span class="params">(Integer value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                    <span class="keyword">return</span> value;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;)</span><br><span class="line">            .print().setParallelism(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="PojoAggregation"><a href="#PojoAggregation" class="headerlink" title="PojoAggregation"></a>PojoAggregation</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    DataStreamSource&lt;Event&gt; stream = env.fromElements(</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Mary&quot;</span>, <span class="string">&quot;./home&quot;</span>, <span class="number">1000L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;./cart&quot;</span>, <span class="number">2000L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Mary&quot;</span>, <span class="string">&quot;./cart&quot;</span>, <span class="number">3000L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Mary&quot;</span>, <span class="string">&quot;./fav&quot;</span>, <span class="number">4000L</span>)</span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">    stream.keyBy(e -&gt; e.user)</span><br><span class="line">            <span class="comment">//.maxBy(&quot;timestamp&quot;)</span></span><br><span class="line">            .max(<span class="string">&quot;timestamp&quot;</span>)    <span class="comment">// 指定字段名称</span></span><br><span class="line">            .print();</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="udf"><a href="#udf" class="headerlink" title="udf"></a>udf</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    DataStreamSource&lt;Event&gt; clicks = env.fromElements(</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Mary&quot;</span>, <span class="string">&quot;./home&quot;</span>, <span class="number">1000L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;./cart&quot;</span>, <span class="number">2000L</span>)</span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1. 传入实现FilterFunction接口的自定义函数类</span></span><br><span class="line">    DataStream&lt;Event&gt; stream1 = clicks.filter(<span class="keyword">new</span> FlinkFilter());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 传入属性字段</span></span><br><span class="line">    DataStream&lt;Event&gt; stream2 = clicks.filter(<span class="keyword">new</span> KeyWordFilter(<span class="string">&quot;home&quot;</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 传入匿名类</span></span><br><span class="line">    DataStream&lt;Event&gt; stream3 = clicks.filter(<span class="keyword">new</span> FilterFunction&lt;Event&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(Event value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> value.url.contains(<span class="string">&quot;home&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3. 传入Lambda表达式</span></span><br><span class="line">    SingleOutputStreamOperator&lt;Event&gt; stream4 = clicks.filter(data -&gt; data.url.contains(<span class="string">&quot;home&quot;</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">//        stream1.print();</span></span><br><span class="line"><span class="comment">//        stream2.print();</span></span><br><span class="line"><span class="comment">//        stream3.print();</span></span><br><span class="line">    stream4.print();</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">FlinkFilter</span> <span class="keyword">implements</span> <span class="title">FilterFunction</span>&lt;<span class="title">Event</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(Event value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> value.url.contains(<span class="string">&quot;home&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">KeyWordFilter</span> <span class="keyword">implements</span> <span class="title">FilterFunction</span>&lt;<span class="title">Event</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String keyWord;</span><br><span class="line"></span><br><span class="line">    KeyWordFilter(String keyWord) &#123; <span class="keyword">this</span>.keyWord = keyWord; &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(Event value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> value.url.contains(<span class="keyword">this</span>.keyWord);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Sink"><a href="#Sink" class="headerlink" title="Sink"></a>Sink</h2><h3 id="kafka-1"><a href="#kafka-1" class="headerlink" title="kafka"></a>kafka</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">    properties.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;hadoop102:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">    DataStreamSource&lt;String&gt; stream = env.readTextFile(<span class="string">&quot;input/clicks.csv&quot;</span>);</span><br><span class="line"></span><br><span class="line">    stream</span><br><span class="line">            .addSink(<span class="keyword">new</span> FlinkKafkaProducer&lt;String&gt;(</span><br><span class="line">                    <span class="string">&quot;clicks&quot;</span>,</span><br><span class="line">                    <span class="keyword">new</span> SimpleStringSchema(),</span><br><span class="line">                    properties</span><br><span class="line">            ));</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="redis"><a href="#redis" class="headerlink" title="redis"></a>redis</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 自定义source</span></span><br><span class="line">    DataStreamSource&lt;Event&gt; stream = env.addSource(<span class="keyword">new</span> ClickSource());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建一个到redis连接的配置</span></span><br><span class="line">    FlinkJedisPoolConfig conf = <span class="keyword">new</span> FlinkJedisPoolConfig.Builder()</span><br><span class="line">            .setHost(<span class="string">&quot;hadoop102&quot;</span>)</span><br><span class="line">            .build();</span><br><span class="line"></span><br><span class="line">    stream.addSink(<span class="keyword">new</span> RedisSink&lt;Event&gt;(conf, <span class="keyword">new</span> MyRedisMapper()));</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyRedisMapper</span> <span class="keyword">implements</span> <span class="title">RedisMapper</span>&lt;<span class="title">Event</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> RedisCommandDescription <span class="title">getCommandDescription</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> RedisCommandDescription(RedisCommand.HSET, <span class="string">&quot;clicks&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getKeyFromData</span><span class="params">(Event data)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> data.user;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getValueFromData</span><span class="params">(Event data)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> data.url;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ClickSource</span> <span class="keyword">implements</span> <span class="title">SourceFunction</span>&lt;<span class="title">Event</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">// 声明一个布尔变量，作为控制数据生成的标识位</span></span><br><span class="line">    <span class="keyword">private</span> Boolean running = <span class="keyword">true</span>;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;Event&gt; ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Random random = <span class="keyword">new</span> Random();    <span class="comment">// 在指定的数据集中随机选取数据</span></span><br><span class="line">        String[] users = &#123;<span class="string">&quot;Mary&quot;</span>, <span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;Cary&quot;</span>&#125;;</span><br><span class="line">        String[] urls = &#123;<span class="string">&quot;./home&quot;</span>, <span class="string">&quot;./cart&quot;</span>, <span class="string">&quot;./fav&quot;</span>, <span class="string">&quot;./prod?id=1&quot;</span>, <span class="string">&quot;./prod?id=2&quot;</span>&#125;;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (running) &#123;</span><br><span class="line">            ctx.collect(<span class="keyword">new</span> Event(</span><br><span class="line">                    users[random.nextInt(users.length)],</span><br><span class="line">                    urls[random.nextInt(urls.length)],</span><br><span class="line">                    Calendar.getInstance().getTimeInMillis()</span><br><span class="line">            ));</span><br><span class="line">            <span class="comment">// 隔1秒生成一个点击事件，方便观测</span></span><br><span class="line">            Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        running = <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="elasticsearch"><a href="#elasticsearch" class="headerlink" title="elasticsearch"></a>elasticsearch</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    DataStreamSource&lt;Event&gt; stream = env.fromElements(</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Mary&quot;</span>, <span class="string">&quot;./home&quot;</span>, <span class="number">1000L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;./cart&quot;</span>, <span class="number">2000L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;./prod?id=100&quot;</span>, <span class="number">3000L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;./prod?id=200&quot;</span>, <span class="number">3500L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;./prod?id=2&quot;</span>, <span class="number">2500L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;./prod?id=300&quot;</span>, <span class="number">3600L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;./home&quot;</span>, <span class="number">3000L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;./prod?id=1&quot;</span>, <span class="number">2300L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;./prod?id=3&quot;</span>, <span class="number">3300L</span>));</span><br><span class="line"></span><br><span class="line">    ArrayList&lt;HttpHost&gt; httpHosts = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    httpHosts.add(<span class="keyword">new</span> HttpHost(<span class="string">&quot;hadoop102&quot;</span>, <span class="number">9200</span>, <span class="string">&quot;http&quot;</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建一个ElasticsearchSinkFunction</span></span><br><span class="line">    ElasticsearchSinkFunction&lt;Event&gt; elasticsearchSinkFunction = <span class="keyword">new</span> ElasticsearchSinkFunction&lt;Event&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Event element, RuntimeContext ctx, RequestIndexer indexer)</span> </span>&#123;</span><br><span class="line">            HashMap&lt;String, String&gt; data = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">            data.put(element.user, element.url);</span><br><span class="line"></span><br><span class="line">            IndexRequest request = Requests.indexRequest()</span><br><span class="line">                    .index(<span class="string">&quot;clicks&quot;</span>)</span><br><span class="line">                    .type(<span class="string">&quot;type&quot;</span>)    <span class="comment">// Es 6 必须定义 type</span></span><br><span class="line">                    .source(data);</span><br><span class="line"></span><br><span class="line">            indexer.add(request);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    stream.addSink(<span class="keyword">new</span> ElasticsearchSink.Builder&lt;Event&gt;(httpHosts, elasticsearchSinkFunction).build());</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="jdbc"><a href="#jdbc" class="headerlink" title="jdbc"></a>jdbc</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//1.获取执行环境</span></span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.读取端口数据并转换为JavaBean</span></span><br><span class="line">    SingleOutputStreamOperator&lt;WaterSensor&gt; waterSensorDS = env.socketTextStream(<span class="string">&quot;linux1&quot;</span>, <span class="number">9999</span>)</span><br><span class="line">            .map(<span class="keyword">new</span> MapFunction&lt;String, WaterSensor&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> WaterSensor <span class="title">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                    String[] split = value.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">new</span> WaterSensor(split[<span class="number">0</span>],</span><br><span class="line">                            Long.parseLong(split[<span class="number">1</span>]),</span><br><span class="line">                            Integer.parseInt(split[<span class="number">2</span>]));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.将数据写入MySQL</span></span><br><span class="line">    waterSensorDS.addSink(JdbcSink.sink(</span><br><span class="line">            <span class="string">&quot;INSERT INTO `sensor-0821` VALUES(?,?,?) ON DUPLICATE KEY UPDATE `ts`=?,`vc`=?&quot;</span>,</span><br><span class="line">            <span class="keyword">new</span> JdbcStatementBuilder&lt;WaterSensor&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accept</span><span class="params">(PreparedStatement preparedStatement, WaterSensor waterSensor)</span> <span class="keyword">throws</span> SQLException </span>&#123;</span><br><span class="line">                    <span class="comment">//给占位符赋值</span></span><br><span class="line">                    preparedStatement.setString(<span class="number">1</span>, waterSensor.getId());</span><br><span class="line">                    preparedStatement.setLong(<span class="number">2</span>, waterSensor.getTs());</span><br><span class="line">                    preparedStatement.setInt(<span class="number">3</span>, waterSensor.getVc());</span><br><span class="line">                    preparedStatement.setLong(<span class="number">4</span>, waterSensor.getTs());</span><br><span class="line">                    preparedStatement.setInt(<span class="number">5</span>, waterSensor.getVc());</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            JdbcExecutionOptions.builder()</span><br><span class="line">                    .withBatchSize(<span class="number">1</span>)</span><br><span class="line">                    .build(),</span><br><span class="line">            <span class="keyword">new</span> JdbcConnectionOptions.JdbcConnectionOptionsBuilder()</span><br><span class="line">                    .withUrl(<span class="string">&quot;jdbc:mysql://linux1:3306/test?useSSL=false&quot;</span>)</span><br><span class="line">                    .withDriverName(<span class="string">&quot;com.mysql.jdbc.Driver&quot;</span>)</span><br><span class="line">                    .withUsername(<span class="string">&quot;root&quot;</span>)</span><br><span class="line">                    .withPassword(<span class="string">&quot;000000&quot;</span>)</span><br><span class="line">                    .build()</span><br><span class="line">    ));</span><br><span class="line"></span><br><span class="line">    <span class="comment">//4.执行任务</span></span><br><span class="line">    env.execute();</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    DataStreamSource&lt;Event&gt; stream = env.fromElements(</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Mary&quot;</span>, <span class="string">&quot;./home&quot;</span>, <span class="number">1000L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;./cart&quot;</span>, <span class="number">2000L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;./prod?id=100&quot;</span>, <span class="number">3000L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;./prod?id=200&quot;</span>, <span class="number">3500L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;./prod?id=2&quot;</span>, <span class="number">2500L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;./prod?id=300&quot;</span>, <span class="number">3600L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;./home&quot;</span>, <span class="number">3000L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;./prod?id=1&quot;</span>, <span class="number">2300L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;./prod?id=3&quot;</span>, <span class="number">3300L</span>));</span><br><span class="line"></span><br><span class="line">    stream.addSink(</span><br><span class="line">            JdbcSink.sink(</span><br><span class="line">                    <span class="string">&quot;INSERT INTO clicks (user, url) VALUES (?, ?)&quot;</span>,</span><br><span class="line">                    (statement, r) -&gt; &#123;</span><br><span class="line">                        statement.setString(<span class="number">1</span>, r.user);</span><br><span class="line">                        statement.setString(<span class="number">2</span>, r.url);</span><br><span class="line">                    &#125;,</span><br><span class="line">                    <span class="keyword">new</span> JdbcConnectionOptions.JdbcConnectionOptionsBuilder()</span><br><span class="line">                            .withUrl(<span class="string">&quot;jdbc:mysql://localhost:3306/test&quot;</span>)</span><br><span class="line">                            .withDriverName(<span class="string">&quot;com.mysql.jdbc.Driver&quot;</span>)</span><br><span class="line">                            .withUsername(<span class="string">&quot;root&quot;</span>)</span><br><span class="line">                            .withPassword(<span class="string">&quot;root&quot;</span>)</span><br><span class="line">                            .build()</span><br><span class="line">            )</span><br><span class="line">    );</span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="file-1"><a href="#file-1" class="headerlink" title="file"></a>file</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">4</span>);</span><br><span class="line"></span><br><span class="line">    DataStream&lt;Event&gt; stream = env.fromElements(<span class="keyword">new</span> Event(<span class="string">&quot;Mary&quot;</span>, <span class="string">&quot;./home&quot;</span>, <span class="number">1000L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;./cart&quot;</span>, <span class="number">2000L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;./prod?id=100&quot;</span>, <span class="number">3000L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;./prod?id=200&quot;</span>, <span class="number">3500L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;./prod?id=2&quot;</span>, <span class="number">2500L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;./prod?id=300&quot;</span>, <span class="number">3600L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;./home&quot;</span>, <span class="number">3000L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;./prod?id=1&quot;</span>, <span class="number">2300L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;./prod?id=3&quot;</span>, <span class="number">3300L</span>));</span><br><span class="line"></span><br><span class="line">    StreamingFileSink&lt;String&gt; fileSink = StreamingFileSink</span><br><span class="line">            .&lt;String&gt;forRowFormat(<span class="keyword">new</span> Path(<span class="string">&quot;./output&quot;</span>),</span><br><span class="line">                    <span class="keyword">new</span> SimpleStringEncoder&lt;&gt;(<span class="string">&quot;UTF-8&quot;</span>))</span><br><span class="line">            .withRollingPolicy(</span><br><span class="line">                    DefaultRollingPolicy.builder()</span><br><span class="line">                            .withRolloverInterval(TimeUnit.MINUTES.toMillis(<span class="number">15</span>))</span><br><span class="line">                            .withInactivityInterval(TimeUnit.MINUTES.toMillis(<span class="number">5</span>))</span><br><span class="line">                            .withMaxPartSize(<span class="number">1024</span> * <span class="number">1024</span> * <span class="number">1024</span>)</span><br><span class="line">                            .build())</span><br><span class="line">            .build();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将Event转换成String写入文件</span></span><br><span class="line">    stream.map(Event::toString).addSink(fileSink);</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="custome-sink"><a href="#custome-sink" class="headerlink" title="custome_sink"></a>custome_sink</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//1.获取执行环境</span></span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.读取端口数据并转换为JavaBean</span></span><br><span class="line">    SingleOutputStreamOperator&lt;WaterSensor&gt; waterSensorDS = env.socketTextStream(<span class="string">&quot;linux1&quot;</span>, <span class="number">9999</span>)</span><br><span class="line">            .map(<span class="keyword">new</span> MapFunction&lt;String, WaterSensor&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> WaterSensor <span class="title">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                    String[] split = value.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">new</span> WaterSensor(split[<span class="number">0</span>],</span><br><span class="line">                            Long.parseLong(split[<span class="number">1</span>]),</span><br><span class="line">                            Integer.parseInt(split[<span class="number">2</span>]));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.将数据写入Mysql</span></span><br><span class="line">    waterSensorDS.addSink(<span class="keyword">new</span> MySink());</span><br><span class="line"></span><br><span class="line">    <span class="comment">//4.执行任务</span></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MySink</span> <span class="keyword">extends</span> <span class="title">RichSinkFunction</span>&lt;<span class="title">WaterSensor</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//声明连接</span></span><br><span class="line">    <span class="keyword">private</span> Connection connection;</span><br><span class="line">    <span class="keyword">private</span> PreparedStatement preparedStatement;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//生命周期方法,用于创建连接</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        connection = DriverManager.getConnection(<span class="string">&quot;jdbc:mysql://linux1:3306/test?useSSL=false&quot;</span>, <span class="string">&quot;root&quot;</span>, <span class="string">&quot;000000&quot;</span>);</span><br><span class="line">        preparedStatement = connection.prepareStatement(<span class="string">&quot;INSERT INTO `sensor-0821` VALUES(?,?,?) ON DUPLICATE KEY UPDATE `ts`=?,`vc`=?&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">invoke</span><span class="params">(WaterSensor value, Context context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//给占位符赋值</span></span><br><span class="line">        preparedStatement.setString(<span class="number">1</span>, value.getId());</span><br><span class="line">        preparedStatement.setLong(<span class="number">2</span>, value.getTs());</span><br><span class="line">        preparedStatement.setInt(<span class="number">3</span>, value.getVc());</span><br><span class="line">        preparedStatement.setLong(<span class="number">4</span>, value.getTs());</span><br><span class="line">        preparedStatement.setInt(<span class="number">5</span>, value.getVc());</span><br><span class="line"></span><br><span class="line">        <span class="comment">//执行操作</span></span><br><span class="line">        preparedStatement.execute();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//生命周期方法,用于关闭连接</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        preparedStatement.close();</span><br><span class="line">        connection.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Window"><a href="#Window" class="headerlink" title="Window"></a>Window</h2><h3 id="时间窗口-Time-Window"><a href="#时间窗口-Time-Window" class="headerlink" title="时间窗口(Time Window)"></a>时间窗口(Time Window)</h3><p>时间窗口以时间点来定义窗口的开始(start)和结束(end)，所以截取出的就是某一时间段的数据。到达结束时间时，窗口不再收集数据，触发计算输出结果，并将窗口关闭销毁。</p>
<p>Flink 中有一个专门的类来表示时间窗口，名称就叫作 TimeWindow。这个类只有两个私有属性：start 和 end，表示窗口的开始和结束的时间戳，单位为<strong>毫秒</strong>。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> start;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> end;</span><br></pre></td></tr></table></figure>

<p>我们可以调用公有的 getStart() 和 getEnd() 方法直接获取这两个时间戳。另外，TimeWindow 还提供了一个 maxTimestamp()方法，用来获取窗口中能够包含数据的最大时间戳。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">maxTimestamp</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> end - <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>很明显，窗口中的数据，最大允许的时间戳就是end - 1，这也就代表了我们定义的窗口时间范围都是左闭右开的区间[start，end)。</p>
<h4 id="TimeTumbling"><a href="#TimeTumbling" class="headerlink" title="TimeTumbling"></a>TimeTumbling</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//处理时间</span></span><br><span class="line">stream.keyBy(...)</span><br><span class="line">      .window(TumblingProcessingTimeWindows.of(Time.seconds(<span class="number">5</span>)))</span><br><span class="line">      .aggregate(...)</span><br><span class="line"><span class="comment">//北京时间0点</span></span><br><span class="line">.window(TumblingProcessingTimeWindows.of(Time.days(<span class="number">1</span>), Time.hours(-<span class="number">8</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment">//事件时间</span></span><br><span class="line">stream.keyBy(...)</span><br><span class="line">      .window(TumblingEventTimeWindows.of(Time.seconds(<span class="number">5</span>)))</span><br><span class="line">      .aggregate(...)</span><br></pre></td></tr></table></figure>

<h4 id="TimeSliding"><a href="#TimeSliding" class="headerlink" title="TimeSliding"></a>TimeSliding</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//一个长度为 10 秒、滑动步长为 5 秒</span></span><br><span class="line"><span class="comment">//处理时间</span></span><br><span class="line">stream.keyBy(...)</span><br><span class="line">      .window(SlidingProcessingTimeWindows.of(Time.seconds(<span class="number">10</span>), Time.seconds(<span class="number">5</span>)))</span><br><span class="line">      .aggregate(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">//事件时间</span></span><br><span class="line">stream.keyBy(...)</span><br><span class="line">      .window(SlidingEventTimeWindows.of(Time.seconds(<span class="number">10</span>), Time.seconds(<span class="number">5</span>)))</span><br><span class="line">      .aggregate(...)</span><br></pre></td></tr></table></figure>

<h4 id="TimeSession"><a href="#TimeSession" class="headerlink" title="TimeSession"></a>TimeSession</h4><p>窗口分配器由类 ProcessingTimeSessionWindows 提供，需要调用它的静态方法.withGap()或者.withDynamicGap()。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//静态会话超时时间为 10 秒的会话窗口</span></span><br><span class="line"><span class="comment">//处理时间</span></span><br><span class="line">stream.keyBy(...)</span><br><span class="line">      .window(ProcessingTimeSessionWindows.withGap(Time.seconds(<span class="number">10</span>)))</span><br><span class="line">      .aggregate(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">//事件时间</span></span><br><span class="line">stream.keyBy(...)</span><br><span class="line">      .window(EventTimeSessionWindows.withGap(Time.seconds(<span class="number">10</span>)))</span><br><span class="line">      .aggregate(...)</span><br></pre></td></tr></table></figure>

<p>这里.withDynamicGap()方法需要传入一个 SessionWindowTimeGapExtractor 作为参数，用来<strong>定义session gap的动态提取逻辑</strong>。在这里，我们提取了数据元素的第一个字段，用它的长度乘以 1000 作为会话超时的间隔。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">stream.keyBy(...)</span><br><span class="line">      .window(ProcessingTimeSessionWindows.withDynamicGap(</span><br><span class="line">        <span class="keyword">new</span> SessionWindowTimeGapExtractor&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extract</span><span class="params">(Tuple2&lt;String, Long&gt; element)</span> </span>&#123;</span><br><span class="line">                    <span class="comment">// 提取 session gap 值返回, 单位毫秒 </span></span><br><span class="line">                    <span class="keyword">return</span> element.f0.length() * <span class="number">1000</span>;</span><br><span class="line">                &#125; </span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line">      ).aggregate(...)</span><br></pre></td></tr></table></figure>

<h3 id="计数窗口-Count-Window"><a href="#计数窗口-Count-Window" class="headerlink" title="计数窗口(Count Window)"></a>计数窗口(<strong>Count Window</strong>)</h3><p>计数窗口基于元素的个数来截取数据，到达固定的个数时就触发计算并关闭窗口。这相当于座位有限、人满就发车，是否发车与时间无关。每个窗口截取数据的个数，就是窗口的大小。</p>
<h4 id="CountTumbling"><a href="#CountTumbling" class="headerlink" title="CountTumbling"></a>CountTumbling</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//当窗口中元素数量达到 10 的时候，就会触发计算执行并关闭窗口</span></span><br><span class="line">stream.keyBy(...)</span><br><span class="line">      .countWindow(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<h4 id="CountSliding"><a href="#CountSliding" class="headerlink" title="CountSliding"></a>CountSliding</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//长度为 10、滑动步长为 3 的滑动计数窗口。每个窗口统计 10 个数据，每隔 3 个数据就统计输出一次结果</span></span><br><span class="line">stream.keyBy(...)</span><br><span class="line">      .countWindow(<span class="number">10</span>,<span class="number">3</span>)</span><br></pre></td></tr></table></figure>

<h3 id="全局窗口-Global-Windows"><a href="#全局窗口-Global-Windows" class="headerlink" title="全局窗口(Global Windows)"></a>全局窗口(<strong>Global Windows</strong>)</h3><p>还有一类比较通用的窗口，就是全局窗口。这种窗口全局有效，会<strong>把相同 key 的所有数据都分配到同一个窗口中</strong>；说直白一点，就跟没分窗口一样。无界流的数据永无止尽，所以这种窗口也没有结束的时候，默认是不会做触发计算的。如果希望它能对数据进行计算处理， 还需要**自定义触发器(Trigger)**。Flink 中的计数窗口(Count Window)，底层就是用全局窗口实现的。</p>
<figure class="highlight reasonml"><table><tr><td class="code"><pre><span class="line">stream.key<span class="constructor">By(<span class="operator">...</span>)</span></span><br><span class="line">      .window(<span class="module-access"><span class="module"><span class="identifier">GlobalWindows</span>.</span></span>create<span class="literal">()</span>);</span><br></pre></td></tr></table></figure>

<h3 id="窗口函数-Window-Functions"><a href="#窗口函数-Window-Functions" class="headerlink" title="窗口函数(Window Functions)"></a>窗口函数(Window Functions)</h3><p><strong>1、按键分区(Keyed)和非按键分区(Non-Keyed)</strong></p>
<p>按键分区窗口(Keyed Windows)：</p>
<p>经过按键分区 keyBy 操作后，数据流会按照 key 被分为多条逻辑流(logical streams)，这就是 KeyedStream。基于 KeyedStream 进行窗口操作时，窗口计算会在多个并行子任务上同时执行。相同 key 的数据会被发送到同一个并行子任务，而窗口操作会基于每个 key 进行单独的处理。所以可以认为，每个 key 上都定义了一组窗口，各自独立地进行统计计算。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">stream.keyBy(...)</span><br><span class="line">      .window(...)</span><br></pre></td></tr></table></figure>

<p>非按键分区(Non-Keyed Windows)：</p>
<p>如果没有进行 keyBy，那么原始的 DataStream 就不会分成多条逻辑流。这时窗口逻辑只能在一个任务(task)上执行，就相当于并行度变成了 1。所以在实际应用中一般不推荐使用这种方式。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">stream.windowAll(...)</span><br></pre></td></tr></table></figure>

<p><strong>2、代码中窗口 API 的调用</strong></p>
<p>窗口操作主要有两个部分：窗口分配器(Window Assigners)和窗口函数(Window Functions)。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">stream.keyBy(&lt;key selector&gt;)</span><br><span class="line">      .window(&lt;window assigner&gt;)      <span class="comment">//窗口分配器</span></span><br><span class="line">      .aggregate(&lt;window function&gt;)   <span class="comment">//窗口函数</span></span><br></pre></td></tr></table></figure>

<p>其中.window() 方法需要传入一个窗口分配器，它指明了窗口的类型；而后面的.aggregate()方法传入一个窗口函数作为参数，它用来定义窗口具体的处理逻辑。窗口分配器有各种形式， 而窗口函数的调用方法也不只.aggregate()一种，我们接下来就详细展开讲解。</p>
<p>另外，在实际应用中，一般都需要并行执行任务，非按键分区很少用到，所以我们之后都以按键分区窗口为例；如果想要实现非按键分区窗口，只要前面不做 keyBy，后面调用.window()时直接换成.windowAll()就可以了。</p>
<p>经<strong>窗口分配器</strong>处理之后，数据可以分配到对应的窗口中，而数据流经过转换得到的数据类型是 WindowedStream。这个类型并不是 DataStream，所以并不能直接进行其他转换，而必须进一步调用<strong>窗口函数</strong>，对收集到的数据进行处理计算之后，才能最终再次得到 DataStream。</p>
<p><img src="/Flink/%E6%88%AA%E5%B1%8F2022-03-23%20%E4%B8%8B%E5%8D%883.31.54.png" alt="截屏2022-03-23 下午3.31.54"></p>
<p>窗口函数定义了要对窗口中收集的数据做的计算操作，根据处理的方式可以分为两类：<strong>增量聚合函数</strong>和<strong>全窗口函数</strong>。</p>
<h4 id="增量聚合函数-incremental-aggregation-functions"><a href="#增量聚合函数-incremental-aggregation-functions" class="headerlink" title="增量聚合函数(incremental aggregation functions)"></a>增量聚合函数(incremental aggregation functions)</h4><p>就像 DataStream 的简单聚合一样，每来一条数据就立即进行计算，中间只要保持一个简单的聚合状态就可以了；区别只是在于不立即输出结果，而是要等到窗口结束时间。等到窗口到了结束时间需要输出计算结果的时候，我们只需要拿出之前聚合的状态直接输出，这无疑就大大提高了程序运行的效率和实时性。</p>
<p>典型的增量聚合函数有两个：ReduceFunction 和 AggregateFunction。</p>
<p><strong>ReduceFunction：</strong></p>
<p>将窗口中收集到的数据两两进行归约，当我们进行流处理时，就是要保存一个状态；每来一个新的数据，就和之前的聚合状态做归约，这样就实现了增量式的聚合。</p>
<p>代码中我们对每个用户的行为数据进行了开窗统计。与 word count 逻辑类似，首先将数据转换成(user, count)的二元组形式(类型为 Tuple2&lt;String, Long&gt;)，每条数据对应的初始 count 值都是 1；然后按照用户 id 分组，在处理时间下开滚动窗口，统计每 5 秒内的用户行为数量。 对于窗口的计算，我们用 ReduceFunction 对 count 值做了增量聚合：窗口中会将当前的总 count 值保存成一个归约状态，每来一条数据，就会调用内部的 reduce 方法，将新数据中的 count 值叠加到状态上，并得到新的状态保存起来。等到了 5 秒窗口的结束时间，就把归约好的状态直接输出。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 从自定义数据源读取数据，并提取时间戳、生成水位线</span></span><br><span class="line">    SingleOutputStreamOperator&lt;Event&gt; stream = env.addSource(<span class="keyword">new</span> ClickSource())</span><br><span class="line">                    .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Event&gt;forBoundedOutOfOrderness(Duration.ZERO)</span><br><span class="line">                    .withTimestampAssigner(<span class="keyword">new</span> SerializableTimestampAssigner&lt;Event&gt;() &#123;</span><br><span class="line">                        <span class="meta">@Override</span></span><br><span class="line">                        <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Event element, <span class="keyword">long</span> recordTimestamp)</span> </span>&#123;</span><br><span class="line">                            <span class="keyword">return</span> element.timestamp;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;));          </span><br><span class="line"></span><br><span class="line">    stream.map( <span class="keyword">new</span> MapFunction&lt;Event, Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">                        <span class="meta">@Override</span></span><br><span class="line">                        <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Long&gt; <span class="title">map</span><span class="params">(Event value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                            <span class="comment">// 将数据转换成二元组，方便计算</span></span><br><span class="line">                            <span class="keyword">return</span> Tuple2.of(value.user, <span class="number">1L</span>);</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">          )</span><br><span class="line">          .keyBy(r -&gt; r.f0)</span><br><span class="line">          <span class="comment">// 设置滚动事件时间窗口</span></span><br><span class="line">          .window(TumblingEventTimeWindows.of(Time.seconds(<span class="number">5</span>)))</span><br><span class="line">          .reduce(<span class="keyword">new</span> ReduceFunction&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">              <span class="meta">@Override</span></span><br><span class="line">              <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Long&gt; <span class="title">reduce</span><span class="params">(Tuple2&lt;String, Long&gt; value1, Tuple2&lt;String, Long&gt; value2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                  <span class="comment">// 定义累加规则，窗口闭合时，向下游发送累加结果</span></span><br><span class="line">                  <span class="keyword">return</span> Tuple2.of(value1.f0, value1.f1 + value2.f1);</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;)</span><br><span class="line">          .print();</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>AggregateFunction：</strong></p>
<p><strong>ReduceFunction 可以解决大多数归约聚合的问题，但是这个接口有一个限制，就是聚合状态的类型、输出结果的类型都必须和输入数据类型一样。</strong>这就迫使我们必须在聚合前，先将数据转换(map)成预期结果类型；而在有些情况下，还需要对状态进行进一步处理才能得到输出结果，这时它们的类型可能不同，使用 ReduceFunction 就会非常麻烦。</p>
<p>如果我们希望计算一组数据的平均值，应该怎样做聚合呢？很明显，这时我们需要计算两个状态量：数据的总和(sum)，以及数据的个数(count)，而最终输出结果是两者的商 (sum/count)。如果用 ReduceFunction，那么我们应该先把数据转换成二元组(sum, count)的形式，然后进行归约聚合，最后再将元组的两个元素相除转换得到最后的平均值。本来应该只是一个任务，可我们却需要 map-reduce-map 三步操作，这显然不够高效。应该允许让输入数据、中间状态、输出结果三者类型不同。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">AggregateFunction</span>&lt;<span class="title">IN</span>, <span class="title">ACC</span>, <span class="title">OUT</span>&gt; <span class="keyword">extends</span> <span class="title">Function</span>, <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">   <span class="function">ACC <span class="title">createAccumulator</span><span class="params">()</span></span>;</span><br><span class="line">   <span class="function">ACC <span class="title">add</span><span class="params">(IN value, ACC accumulator)</span></span>;</span><br><span class="line">   <span class="function">OUT <span class="title">getResult</span><span class="params">(ACC accumulator)</span></span>;</span><br><span class="line">   <span class="function">ACC <span class="title">merge</span><span class="params">(ACC a, ACC b)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>AggregateFunction 可以看作是 ReduceFunction 的通用版本，这里有三种类型：输入类型 (IN)、累加器类型(ACC)和输出类型(OUT)。输入类型 IN 就是输入流中元素的数据类型；累加器类型 ACC 则是我们进行聚合的中间状态类型；而输出类型当然就是最终计算结果的类型了。</p>
<p>createAccumulator()：创建一个累加器，这就是为聚合创建了一个<strong>初始状态</strong>，每个聚合任务只会<strong>调用一次</strong>。</p>
<p>add()：将输入的元素添加到累加器中。这就是基于聚合状态，对新来的数据进行进一步聚合的过程。方法传入两个参数：当前新到的数据 value，和当前的累加器 accumulator；返回一个新的累加器值，也就是<strong>对聚合状态进行更新</strong>。每条数据到来之后都会调用这个方法。</p>
<p>getResult()：从累加器中提取聚合的输出结果。也就是说，我们可以定义多个状态，然后再基于这些聚合的状态计算出一个结果进行输出。比如之前我们提到的计算平均值，就可以把 sum 和 count 作为状态放入累加器，而在调用这个方法时相除得到最终结果。这个方法只在窗口要输出结果时调用。</p>
<p>merge()：<strong>合并两个累加器</strong>，并将合并后的状态作为一个累加器返回。这个方法只在需要合并窗口的场景下才会被调用；最常见的合并窗口(Merging Window)的场景就是会话窗口(Session Windows)。</p>
<p>AggregateFunction 的工作原理是：首先调用 createAccumulator()为任务初始化一个状态(累加器)；而后每来一个数据就调用一次 add() 方法，对数据进行聚合，得到的结果保存在状态中；等到了窗口需要输出时，再调用 getResult() 方法得到计算结果。很明显，与 ReduceFunction 相同 AggregateFunction 也是增量式的聚合；而由于<strong>输入、中间状态、输出的类型可以不同</strong>，使得应用更加灵活方便。</p>
<p>在电商网站中，PV(页面浏览量) 和 UV(独立访客数) 是非常重要的两个流量指标。一般来说，PV 统计的是所有的点击量；而对用户 id 进行去重之后，得到的就是 UV。所以有时我们会用 PV/UV 这个比值，来表示人均重复访问量，也就是平均每个用户会访问多少次页面，这在一定程度上代表了用户的粘度。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    SingleOutputStreamOperator&lt;Event&gt; stream = env.addSource(<span class="keyword">new</span> ClickSource())</span><br><span class="line">            .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Event&gt;forMonotonousTimestamps()</span><br><span class="line">                    .withTimestampAssigner(<span class="keyword">new</span> SerializableTimestampAssigner&lt;Event&gt;() &#123;</span><br><span class="line">                        <span class="meta">@Override</span></span><br><span class="line">                        <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Event element, <span class="keyword">long</span> recordTimestamp)</span> </span>&#123;</span><br><span class="line">                            <span class="keyword">return</span> element.timestamp;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 所有数据设置相同的key，发送到同一个分区统计PV和UV，再相除</span></span><br><span class="line">    stream.keyBy(data -&gt; <span class="keyword">true</span>)</span><br><span class="line">            .window(SlidingEventTimeWindows.of(Time.seconds(<span class="number">10</span>), Time.seconds(<span class="number">2</span>)))</span><br><span class="line">            .aggregate(<span class="keyword">new</span> AvgPv())</span><br><span class="line">            .print();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">AvgPv</span> <span class="keyword">implements</span> <span class="title">AggregateFunction</span>&lt;<span class="title">Event</span>, <span class="title">Tuple2</span>&lt;<span class="title">HashSet</span>&lt;<span class="title">String</span>&gt;, <span class="title">Long</span>&gt;, <span class="title">Double</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Tuple2&lt;HashSet&lt;String&gt;, Long&gt; createAccumulator() &#123;</span><br><span class="line">        <span class="comment">// 创建累加器</span></span><br><span class="line">        <span class="keyword">return</span> Tuple2.of(<span class="keyword">new</span> HashSet&lt;String&gt;(), <span class="number">0L</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Tuple2&lt;HashSet&lt;String&gt;, Long&gt; add(Event value, Tuple2&lt;HashSet&lt;String&gt;, Long&gt; accumulator) &#123;</span><br><span class="line">        <span class="comment">// 属于本窗口的数据来一条累加一次，并返回累加器</span></span><br><span class="line">        accumulator.f0.add(value.user);</span><br><span class="line">        <span class="keyword">return</span> Tuple2.of(accumulator.f0, accumulator.f1 + <span class="number">1L</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Double <span class="title">getResult</span><span class="params">(Tuple2&lt;HashSet&lt;String&gt;, Long&gt; accumulator)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 窗口闭合时，增量聚合结束，将计算结果发送到下游</span></span><br><span class="line">        <span class="keyword">return</span> (<span class="keyword">double</span>) accumulator.f1 / accumulator.f0.size();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Tuple2&lt;HashSet&lt;String&gt;, Long&gt; merge(Tuple2&lt;HashSet&lt;String&gt;, Long&gt; a, Tuple2&lt;HashSet&lt;String&gt;, Long&gt; b) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="全窗口函数-full-window-functions"><a href="#全窗口函数-full-window-functions" class="headerlink" title="全窗口函数(full window functions)"></a>全窗口函数(full window functions)</h4><p>窗口操作中的另一大类就是全窗口函数。与增量聚合函数不同，全窗口函数需要先收集窗口中的数据，并在内部缓存起来，等到窗口要输出结果的时候再取出数据进行计算。</p>
<p>为什么还需要有全窗口函数呢？这是因为有些场景下，我们要做的计算必须基于全部的数据才有效，这时做增量聚合就没什么意义了；另外，输出的结果有可能要包含上下文中的一些信息(比如窗口的起始时间)，这是增量聚合函数做不到的。所以，我们还需要有更丰富的窗口计算方式，这就可以用全窗口函数来实现。在 Flink 中，全窗口函数也有两种：<strong>WindowFunction</strong> 和 <strong>ProcessWindowFunction</strong>。</p>
<p><strong>窗口函数(WindowFunction)</strong></p>
<p>WindowFunction 字面上就是窗口函数，它其实是老版本的通用窗口函数接口。我们可以基于 WindowedStream 调用.apply()方法，传入一个WindowFunction 的实现类。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">stream</span><br><span class="line">   .keyBy(&lt;key selector&gt;)</span><br><span class="line">   .window(&lt;window assigner&gt;)</span><br><span class="line">   .apply(<span class="keyword">new</span> MyWindowFunction());</span><br></pre></td></tr></table></figure>

<p>这个类中可以获取到包含<strong>窗口所有数据的可迭代集合(Iterable)<strong>，还可以拿到</strong>窗口 (Window)本身的信息</strong>。WindowFunction 接口在源码中实现如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">WindowFunction</span>&lt;<span class="title">IN</span>, <span class="title">OUT</span>, <span class="title">KEY</span>, <span class="title">W</span> <span class="keyword">extends</span> <span class="title">Window</span>&gt; <span class="keyword">extends</span> <span class="title">Function</span>, <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">		<span class="function"><span class="keyword">void</span> <span class="title">apply</span><span class="params">(KEY key, W window, Iterable&lt;IN&gt; input, Collector&lt;OUT&gt; out)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>WindowFunction 能提供的上下文信息较少，也没有更高级的功能。 事实上，它的作用可以被 ProcessWindowFunction 全覆盖，所以之后可能会逐渐弃用。一般在实际应用，直接使用 ProcessWindowFunction 就可以了。</p>
<p><strong>处理窗口函数(ProcessWindowFunction)</strong></p>
<p>ProcessWindowFunction 是 Window API 中最底层的通用窗口函数接口。之所以说它最底层，是因为除了可以拿到窗口中的所有数据之外ProcessWindowFunction 还可以获取到一个<strong>上下文对象(Context)<strong>。这个上下文对象非常强大，不仅能够获取窗口信息，还可以访问当前的</strong>时间</strong>和<strong>状态</strong>信息。这里的时间就包括了处理时间(processing time)和事件时间水位线(event time watermark)。这就使得 ProcessWindowFunction 更加灵活、功能更加丰富。</p>
<p>作为一个全窗口函数， ProcessWindowFunction 同样需要将所有数据缓存下来、等到窗口触发计算时才使用。它其实就是一个增强版的WindowFunction。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">ProcessWindowFunction</span>&lt;<span class="title">IN</span>, <span class="title">OUT</span>, <span class="title">KEY</span>, <span class="title">W</span> <span class="keyword">extends</span> <span class="title">Window</span>&gt; <span class="keyword">extends</span> <span class="title">AbstractRichFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Evaluates the window and outputs none or several elements.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key The key for which this window is evaluated.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context The context in which the window is being evaluated.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> elements The elements in the window being evaluated.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> out A collector for emitting elements.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception The function may throw exceptions to fail the program and trigger recovery.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">            KEY key, Context context, Iterable&lt;IN&gt; elements, Collector&lt;OUT&gt; out)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Deletes any state in the &#123;<span class="doctag">@code</span> Context&#125; when the Window expires (the watermark passes its</span></span><br><span class="line"><span class="comment">     * &#123;<span class="doctag">@code</span> maxTimestamp&#125; + &#123;<span class="doctag">@code</span> allowedLateness&#125;).</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context The context to which the window is being evaluated</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception The function may throw exceptions to fail the program and trigger recovery.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">clear</span><span class="params">(Context context)</span> <span class="keyword">throws</span> Exception </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** The context holding window metadata. */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Context</span> <span class="keyword">implements</span> <span class="title">java</span>.<span class="title">io</span>.<span class="title">Serializable</span> </span>&#123;</span><br><span class="line">        <span class="comment">/** Returns the window that is being evaluated. */</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> W <span class="title">window</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/** Returns the current processing time. */</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">long</span> <span class="title">currentProcessingTime</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/** Returns the current event-time watermark. */</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">long</span> <span class="title">currentWatermark</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * State accessor for per-key and per-window state.</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * &lt;p&gt;&lt;b&gt;<span class="doctag">NOTE:</span>&lt;/b&gt;If you use per-window state you have to ensure that you clean it up by</span></span><br><span class="line"><span class="comment">         * implementing &#123;<span class="doctag">@link</span> ProcessWindowFunction#clear(Context)&#125;.</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> KeyedStateStore <span class="title">windowState</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/** State accessor for per-key global state. */</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> KeyedStateStore <span class="title">globalState</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * Emits a record to the side output identified by the &#123;<span class="doctag">@link</span> OutputTag&#125;.</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> outputTag the &#123;<span class="doctag">@code</span> OutputTag&#125; that identifies the side output to emit to.</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> value The record to emit.</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">abstract</span> &lt;X&gt; <span class="function"><span class="keyword">void</span> <span class="title">output</span><span class="params">(OutputTag&lt;X&gt; outputTag, X value)</span></span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    SingleOutputStreamOperator&lt;Event&gt; stream = env.addSource(<span class="keyword">new</span> ClickSource())</span><br><span class="line">            .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Event&gt;forBoundedOutOfOrderness(Duration.ZERO)</span><br><span class="line">                    .withTimestampAssigner(<span class="keyword">new</span> SerializableTimestampAssigner&lt;Event&gt;() &#123;</span><br><span class="line">                        <span class="meta">@Override</span></span><br><span class="line">                        <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Event element, <span class="keyword">long</span> recordTimestamp)</span> </span>&#123;</span><br><span class="line">                            <span class="keyword">return</span> element.timestamp;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将数据全部发往同一分区，按窗口统计UV</span></span><br><span class="line">    stream.keyBy(data -&gt; <span class="keyword">true</span>)</span><br><span class="line">            .window(TumblingEventTimeWindows.of(Time.seconds(<span class="number">10</span>)))</span><br><span class="line">            .process(<span class="keyword">new</span> UvCountByWindow())</span><br><span class="line">            .print();</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 自定义窗口处理函数</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">UvCountByWindow</span> <span class="keyword">extends</span> <span class="title">ProcessWindowFunction</span>&lt;<span class="title">Event</span>, <span class="title">String</span>, <span class="title">Boolean</span>, <span class="title">TimeWindow</span>&gt;</span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Boolean aBoolean, Context context, Iterable&lt;Event&gt; elements, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        HashSet&lt;String&gt; userSet = <span class="keyword">new</span> HashSet&lt;&gt;();</span><br><span class="line">        <span class="comment">// 遍历所有数据，放到Set里去重</span></span><br><span class="line">        <span class="keyword">for</span> (Event event: elements)&#123;</span><br><span class="line">            userSet.add(event.user);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 结合窗口信息，包装输出内容</span></span><br><span class="line">        Long start = context.window().getStart();</span><br><span class="line">        Long end = context.window().getEnd();</span><br><span class="line">        out.collect(<span class="string">&quot;窗口: &quot;</span> + <span class="keyword">new</span> Timestamp(start) + <span class="string">&quot; ~ &quot;</span> + <span class="keyword">new</span> Timestamp(end)</span><br><span class="line">                + <span class="string">&quot; 的独立访客数量是：&quot;</span> + userSet.size());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>全窗口函数因为运行效率较低，很少直接单独使用，往往会和增量聚合函数结合在一起，共同实现窗口的处理计算。</p>
<h4 id="增量聚合和全窗口函数的结合使用"><a href="#增量聚合和全窗口函数的结合使用" class="headerlink" title="增量聚合和全窗口函数的结合使用"></a>增量聚合和全窗口函数的结合使用</h4><p>我们之前在调用 WindowedStream 的.reduce()和.aggregate()方法时，只是简单地直接传入了一个 ReduceFunction 或 AggregateFunction 进行增量聚合。除此之外，其实还可以传入第二个参数：一个全窗口函数，可以是 WindowFunction 或者 ProcessWindowFunction。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// ReduceFunction 与 WindowFunction 结合</span></span><br><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">SingleOutputStreamOperator&lt;R&gt; <span class="title">reduce</span><span class="params">(ReduceFunction&lt;T&gt; reduceFunction, WindowFunction&lt;T, R, K, W&gt; function)</span></span></span><br><span class="line"><span class="function"><span class="comment">// ReduceFunction 与 ProcessWindowFunction 结合</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> &lt;R&gt; SingleOutputStreamOperator&lt;R&gt; <span class="title">reduce</span><span class="params">(ReduceFunction&lt;T&gt; reduceFunction, ProcessWindowFunction&lt;T, R, K, W&gt; function)</span></span></span><br><span class="line"><span class="function"><span class="comment">// AggregateFunction 与 WindowFunction 结合</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> &lt;ACC, V, R&gt; SingleOutputStreamOperator&lt;R&gt; <span class="title">aggregate</span><span class="params">(AggregateFunction&lt;T, ACC, V&gt; aggFunction, WindowFunction&lt;V, R, K, W&gt; windowFunction)</span></span></span><br><span class="line"><span class="function"><span class="comment">// AggregateFunction 与 ProcessWindowFunction 结合</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> &lt;ACC, V, R&gt; SingleOutputStreamOperator&lt;R&gt; <span class="title">aggregate</span><span class="params">(AggregateFunction&lt;T, ACC, V&gt; aggFunction, ProcessWindowFunction&lt;V, R, K, W&gt; windowFunction)</span></span></span><br></pre></td></tr></table></figure>

<p>这样调用的处理机制是：基于第一个参数(增量聚合函数)来处理窗口数据，每来一个数据就做一次聚合；等到窗口需要触发计算时，则调用第二个参数(全窗口函数)的处理逻辑输出结果。需要注意的是，这里的<strong>全窗口函数就不再缓存所有数据</strong>了，而是<strong>直接将增量聚合函数的结果拿来当作了 Iterable 类型的输入</strong>。一般情况下，这时的<strong>可迭代集合中就只有一个元素</strong>了。</p>
<p>下面我们举一个具体的实例来说明。在网站的各种统计指标中，一个很重要的统计指标就是热门的链接；想要得到热门的url，前提是得到每个链接的热门度。一般情况下，可以用 url 的浏览量(点击量)表示热门度。我们这里统计 10 秒钟的 url 浏览量，每 5 秒钟更新一次；另外为了更加清晰地展示，还应该把窗口的起始结束时间一起输出。我们可以定义滑动窗口，并结合增量聚合函数和全窗口函数来得到统计结果。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//POJO 类</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UrlViewCount</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> String url;</span><br><span class="line">    <span class="keyword">public</span> Long count;</span><br><span class="line">    <span class="keyword">public</span> Long windowStart;</span><br><span class="line">    <span class="keyword">public</span> Long windowEnd;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">UrlViewCount</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">UrlViewCount</span><span class="params">(String url, Long count, Long windowStart, Long windowEnd)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.url = url;</span><br><span class="line">        <span class="keyword">this</span>.count = count;</span><br><span class="line">        <span class="keyword">this</span>.windowStart = windowStart;</span><br><span class="line">        <span class="keyword">this</span>.windowEnd = windowEnd;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;UrlViewCount&#123;&quot;</span> +</span><br><span class="line">                <span class="string">&quot;url=&#x27;&quot;</span> + url + <span class="string">&#x27;\&#x27;&#x27;</span> +</span><br><span class="line">                <span class="string">&quot;, count=&quot;</span> + count +</span><br><span class="line">                <span class="string">&quot;, windowStart=&quot;</span> + <span class="keyword">new</span> Timestamp(windowStart) +</span><br><span class="line">                <span class="string">&quot;, windowEnd=&quot;</span> + <span class="keyword">new</span> Timestamp(windowEnd) +</span><br><span class="line">                <span class="string">&#x27;&#125;&#x27;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    SingleOutputStreamOperator&lt;Event&gt; stream = env.addSource(<span class="keyword">new</span> ClickSource())</span><br><span class="line">            .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Event&gt;forMonotonousTimestamps()</span><br><span class="line">                    .withTimestampAssigner(<span class="keyword">new</span> SerializableTimestampAssigner&lt;Event&gt;() &#123;</span><br><span class="line">                        <span class="meta">@Override</span></span><br><span class="line">                        <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Event element, <span class="keyword">long</span> recordTimestamp)</span> </span>&#123;</span><br><span class="line">                            <span class="keyword">return</span> element.timestamp;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 需要按照url分组，开滑动窗口统计</span></span><br><span class="line">    stream.keyBy(data -&gt; data.url)</span><br><span class="line">            .window(SlidingEventTimeWindows.of(Time.seconds(<span class="number">10</span>), Time.seconds(<span class="number">5</span>)))</span><br><span class="line">            <span class="comment">// 同时传入增量聚合函数和全窗口函数</span></span><br><span class="line">            .aggregate(<span class="keyword">new</span> UrlViewCountAgg(), <span class="keyword">new</span> UrlViewCountResult())</span><br><span class="line">            .print();</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 自定义增量聚合函数，来一条数据就加一</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">UrlViewCountAgg</span> <span class="keyword">implements</span> <span class="title">AggregateFunction</span>&lt;<span class="title">Event</span>, <span class="title">Long</span>, <span class="title">Long</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">createAccumulator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0L</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">add</span><span class="params">(Event value, Long accumulator)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> accumulator + <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">getResult</span><span class="params">(Long accumulator)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> accumulator;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">merge</span><span class="params">(Long a, Long b)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 自定义窗口处理函数，只需要包装窗口信息</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">UrlViewCountResult</span> <span class="keyword">extends</span> <span class="title">ProcessWindowFunction</span>&lt;<span class="title">Long</span>, <span class="title">UrlViewCount</span>, <span class="title">String</span>, <span class="title">TimeWindow</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(String url, Context context, Iterable&lt;Long&gt; elements, Collector&lt;UrlViewCount&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// 结合窗口信息，包装输出内容</span></span><br><span class="line">        Long start = context.window().getStart();</span><br><span class="line">        Long end = context.window().getEnd();</span><br><span class="line">        <span class="comment">// 迭代器中只有一个元素，就是增量聚合函数的计算结果</span></span><br><span class="line">        out.collect(<span class="keyword">new</span> UrlViewCount(url, elements.iterator().next(), start, end));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>代码中用一个AggregateFunction来实现增量聚合，每来一个数据就计数加一；得到的结果交给ProcessWindowFunction，结合窗口信息包装成我们想要的 UrlViewCount，最终输出统计结果。</p>
<h3 id="其他API"><a href="#其他API" class="headerlink" title="其他API"></a>其他API</h3><h4 id="触发器-Trigger"><a href="#触发器-Trigger" class="headerlink" title="触发器(Trigger)"></a>触发器(Trigger)</h4><p>触发器主要是用来控制窗口什么时候触发计算。所谓的触发计算，本质上就是执行窗口函数，所以可以认为是计算得到结果并输出的过程。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">stream.keyBy(...)</span><br><span class="line">      .window(...)</span><br><span class="line">      .trigger(<span class="keyword">new</span> MyTrigger())</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * A &#123;<span class="doctag">@code</span> Trigger&#125; determines when a pane of a window should be evaluated to emit the results for</span></span><br><span class="line"><span class="comment"> * that part of the window.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;A pane is the bucket of elements that have the same key (assigned by the &#123;<span class="doctag">@link</span></span></span><br><span class="line"><span class="comment"> * org.apache.flink.api.java.functions.KeySelector&#125;) and same &#123;<span class="doctag">@link</span> Window&#125;. An element can be in</span></span><br><span class="line"><span class="comment"> * multiple panes if it was assigned to multiple windows by the &#123;<span class="doctag">@link</span></span></span><br><span class="line"><span class="comment"> * org.apache.flink.streaming.api.windowing.assigners.WindowAssigner&#125;. These panes all have their</span></span><br><span class="line"><span class="comment"> * own instance of the &#123;<span class="doctag">@code</span> Trigger&#125;.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;Triggers must not maintain state internally since they can be re-created or reused for</span></span><br><span class="line"><span class="comment"> * different keys. All necessary state should be persisted using the state abstraction available on</span></span><br><span class="line"><span class="comment"> * the &#123;<span class="doctag">@link</span> TriggerContext&#125;.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;When used with a &#123;<span class="doctag">@link</span></span></span><br><span class="line"><span class="comment"> * org.apache.flink.streaming.api.windowing.assigners.MergingWindowAssigner&#125; the &#123;<span class="doctag">@code</span> Trigger&#125;</span></span><br><span class="line"><span class="comment"> * must return &#123;<span class="doctag">@code</span> true&#125; from &#123;<span class="doctag">@link</span> #canMerge()&#125; and &#123;<span class="doctag">@link</span> #onMerge(Window, OnMergeContext)&#125;</span></span><br><span class="line"><span class="comment"> * most be properly implemented.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> &lt;T&gt; The type of elements on which this &#123;<span class="doctag">@code</span> Trigger&#125; works.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> &lt;W&gt; The type of &#123;<span class="doctag">@link</span> Window Windows&#125; on which this &#123;<span class="doctag">@code</span> Trigger&#125; can operate.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@PublicEvolving</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Trigger</span>&lt;<span class="title">T</span>, <span class="title">W</span> <span class="keyword">extends</span> <span class="title">Window</span>&gt; <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = -<span class="number">4104633972991191369L</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Called for every element that gets added to a pane. The result of this will determine whether</span></span><br><span class="line"><span class="comment">     * the pane is evaluated to emit results.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> element The element that arrived.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> timestamp The timestamp of the element that arrived.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> window The window to which the element is being added.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> ctx A context object that can be used to register timer callbacks.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> TriggerResult <span class="title">onElement</span><span class="params">(T element, <span class="keyword">long</span> timestamp, W window, TriggerContext ctx)</span></span></span><br><span class="line"><span class="function">            <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Called when a processing-time timer that was set using the trigger context fires.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> time The timestamp at which the timer fired.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> window The window for which the timer fired.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> ctx A context object that can be used to register timer callbacks.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> TriggerResult <span class="title">onProcessingTime</span><span class="params">(<span class="keyword">long</span> time, W window, TriggerContext ctx)</span></span></span><br><span class="line"><span class="function">            <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Called when an event-time timer that was set using the trigger context fires.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> time The timestamp at which the timer fired.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> window The window for which the timer fired.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> ctx A context object that can be used to register timer callbacks.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> TriggerResult <span class="title">onEventTime</span><span class="params">(<span class="keyword">long</span> time, W window, TriggerContext ctx)</span></span></span><br><span class="line"><span class="function">            <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Returns true if this trigger supports merging of trigger state and can therefore be used with</span></span><br><span class="line"><span class="comment">     * a &#123;<span class="doctag">@link</span> org.apache.flink.streaming.api.windowing.assigners.MergingWindowAssigner&#125;.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;If this returns &#123;<span class="doctag">@code</span> true&#125; you must properly implement &#123;<span class="doctag">@link</span> #onMerge(Window,</span></span><br><span class="line"><span class="comment">     * OnMergeContext)&#125;</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">canMerge</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Called when several windows have been merged into one window by the &#123;<span class="doctag">@link</span></span></span><br><span class="line"><span class="comment">     * org.apache.flink.streaming.api.windowing.assigners.WindowAssigner&#125;.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> window The new window that results from the merge.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> ctx A context object that can be used to register timer callbacks and access state.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onMerge</span><span class="params">(W window, OnMergeContext ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException(<span class="string">&quot;This trigger does not support merging.&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Clears any state that the trigger might still hold for the given window. This is called when</span></span><br><span class="line"><span class="comment">     * a window is purged. Timers set using &#123;<span class="doctag">@link</span> TriggerContext#registerEventTimeTimer(long)&#125; and</span></span><br><span class="line"><span class="comment">     * &#123;<span class="doctag">@link</span> TriggerContext#registerProcessingTimeTimer(long)&#125; should be deleted here as well as</span></span><br><span class="line"><span class="comment">     * state acquired using &#123;<span class="doctag">@link</span> TriggerContext#getPartitionedState(StateDescriptor)&#125;.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">clear</span><span class="params">(W window, TriggerContext ctx)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ------------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * A context object that is given to &#123;<span class="doctag">@link</span> Trigger&#125; methods to allow them to register timer</span></span><br><span class="line"><span class="comment">     * callbacks and deal with state.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">TriggerContext</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/** Returns the current processing time. */</span></span><br><span class="line">        <span class="function"><span class="keyword">long</span> <span class="title">getCurrentProcessingTime</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * Returns the metric group for this &#123;<span class="doctag">@link</span> Trigger&#125;. This is the same metric group that</span></span><br><span class="line"><span class="comment">         * would be returned from &#123;<span class="doctag">@link</span> RuntimeContext#getMetricGroup()&#125; in a user function.</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * &lt;p&gt;You must not call methods that create metric objects (such as &#123;<span class="doctag">@link</span></span></span><br><span class="line"><span class="comment">         * MetricGroup#counter(int)&#125; multiple times but instead call once and store the metric</span></span><br><span class="line"><span class="comment">         * object in a field.</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="function">MetricGroup <span class="title">getMetricGroup</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/** Returns the current watermark time. */</span></span><br><span class="line">        <span class="function"><span class="keyword">long</span> <span class="title">getCurrentWatermark</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * Register a system time callback. When the current system time passes the specified time</span></span><br><span class="line"><span class="comment">         * &#123;<span class="doctag">@link</span> Trigger#onProcessingTime(long, Window, TriggerContext)&#125; is called with the time</span></span><br><span class="line"><span class="comment">         * specified here.</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> time The time at which to invoke &#123;<span class="doctag">@link</span> Trigger#onProcessingTime(long, Window,</span></span><br><span class="line"><span class="comment">         *     TriggerContext)&#125;</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="function"><span class="keyword">void</span> <span class="title">registerProcessingTimeTimer</span><span class="params">(<span class="keyword">long</span> time)</span></span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * Register an event-time callback. When the current watermark passes the specified time</span></span><br><span class="line"><span class="comment">         * &#123;<span class="doctag">@link</span> Trigger#onEventTime(long, Window, TriggerContext)&#125; is called with the time</span></span><br><span class="line"><span class="comment">         * specified here.</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> time The watermark at which to invoke &#123;<span class="doctag">@link</span> Trigger#onEventTime(long, Window,</span></span><br><span class="line"><span class="comment">         *     TriggerContext)&#125;</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@see</span> org.apache.flink.streaming.api.watermark.Watermark</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="function"><span class="keyword">void</span> <span class="title">registerEventTimeTimer</span><span class="params">(<span class="keyword">long</span> time)</span></span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/** Delete the processing time trigger for the given time. */</span></span><br><span class="line">        <span class="function"><span class="keyword">void</span> <span class="title">deleteProcessingTimeTimer</span><span class="params">(<span class="keyword">long</span> time)</span></span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/** Delete the event-time trigger for the given time. */</span></span><br><span class="line">        <span class="function"><span class="keyword">void</span> <span class="title">deleteEventTimeTimer</span><span class="params">(<span class="keyword">long</span> time)</span></span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * Retrieves a &#123;<span class="doctag">@link</span> State&#125; object that can be used to interact with fault-tolerant state</span></span><br><span class="line"><span class="comment">         * that is scoped to the window and key of the current trigger invocation.</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> stateDescriptor The StateDescriptor that contains the name and type of the state</span></span><br><span class="line"><span class="comment">         *     that is being accessed.</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> &lt;S&gt; The type of the state.</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@return</span> The partitioned state object.</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@throws</span> UnsupportedOperationException Thrown, if no partitioned state is available for</span></span><br><span class="line"><span class="comment">         *     the function (function is not part os a KeyedStream).</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        &lt;S extends State&gt; <span class="function">S <span class="title">getPartitionedState</span><span class="params">(StateDescriptor&lt;S, ?&gt; stateDescriptor)</span></span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * Retrieves a &#123;<span class="doctag">@link</span> ValueState&#125; object that can be used to interact with fault-tolerant</span></span><br><span class="line"><span class="comment">         * state that is scoped to the window and key of the current trigger invocation.</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> name The name of the key/value state.</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> stateType The class of the type that is stored in the state. Used to generate</span></span><br><span class="line"><span class="comment">         *     serializers for managed memory and checkpointing.</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> defaultState The default state value, returned when the state is accessed and no</span></span><br><span class="line"><span class="comment">         *     value has yet been set for the key. May be null.</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> &lt;S&gt; The type of the state.</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@return</span> The partitioned state object.</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@throws</span> UnsupportedOperationException Thrown, if no partitioned state is available for</span></span><br><span class="line"><span class="comment">         *     the function (function is not part os a KeyedStream).</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@deprecated</span> Use &#123;<span class="doctag">@link</span> #getPartitionedState(StateDescriptor)&#125;.</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="meta">@Deprecated</span></span><br><span class="line">        &lt;S extends Serializable&gt; <span class="function">ValueState&lt;S&gt; <span class="title">getKeyValueState</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">                String name, Class&lt;S&gt; stateType, S defaultState)</span></span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * Retrieves a &#123;<span class="doctag">@link</span> ValueState&#125; object that can be used to interact with fault-tolerant</span></span><br><span class="line"><span class="comment">         * state that is scoped to the window and key of the current trigger invocation.</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> name The name of the key/value state.</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> stateType The type information for the type that is stored in the state. Used to</span></span><br><span class="line"><span class="comment">         *     create serializers for managed memory and checkpoints.</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> defaultState The default state value, returned when the state is accessed and no</span></span><br><span class="line"><span class="comment">         *     value has yet been set for the key. May be null.</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> &lt;S&gt; The type of the state.</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@return</span> The partitioned state object.</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@throws</span> UnsupportedOperationException Thrown, if no partitioned state is available for</span></span><br><span class="line"><span class="comment">         *     the function (function is not part os a KeyedStream).</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@deprecated</span> Use &#123;<span class="doctag">@link</span> #getPartitionedState(StateDescriptor)&#125;.</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="meta">@Deprecated</span></span><br><span class="line">        &lt;S extends Serializable&gt; <span class="function">ValueState&lt;S&gt; <span class="title">getKeyValueState</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">                String name, TypeInformation&lt;S&gt; stateType, S defaultState)</span></span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Extension of &#123;<span class="doctag">@link</span> TriggerContext&#125; that is given to &#123;<span class="doctag">@link</span> Trigger#onMerge(Window,</span></span><br><span class="line"><span class="comment">     * OnMergeContext)&#125;.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">OnMergeContext</span> <span class="keyword">extends</span> <span class="title">TriggerContext</span> </span>&#123;</span><br><span class="line">        &lt;S extends MergingState&lt;?, ?&gt;&gt; <span class="function"><span class="keyword">void</span> <span class="title">mergePartitionedState</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">                StateDescriptor&lt;S, ?&gt; stateDescriptor)</span></span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Trigger 是窗口算子的内部属性，每个窗口分配器(WindowAssigner)都会对应一个默认的触发器；对于 Flink 内置的窗口类型，它们的触发器都已经做了实现。例如，所有事件时间窗口，默认的触发器都是 EventTimeTrigger；类似还有 ProcessingTimeTrigger 和 CountTrigger。 所以一般情况下是不需要自定义触发器的，不过我们依然有必要了解它的原理。</p>
<p>Trigger 是一个抽象类，自定义时必须实现下面四个抽象方法：</p>
<p>onElement()：窗口中每到来一个元素，都会调用这个方法。</p>
<p>onEventTime()：当注册的事件时间定时器触发时，将调用这个方法。</p>
<p>onProcessingTime ()：当注册的处理时间定时器触发时，将调用这个方法。</p>
<p>clear()：当窗口关闭销毁时，调用这个方法。一般用来清除自定义的状态。</p>
<p>除了 clear() 比较像生命周期方法，其他三个方法其实都是对某种事件的响应。 onElement() 是对流中数据元素到来的响应；而另两个则是对时间的响应。这几个方法的参数中都有一个触发器上下文(TriggerContext)对象，可以用来注册定时器回调(callback)。这里提到的定时器(Timer)，其实就是我们设定的一个闹钟，代表未来某个时间点会执行的事件；当时间进展到设定的值时，就会执行定义好的操作。</p>
<p>上面的前三个方法可以响应事件，那它们又是怎样跟窗口操作联系起来的呢？这就需要了解一下它们的返回值。这<strong>三个方法返回类型都是 TriggerResult</strong>，这是一个枚举类型(enum)， 其中定义了对窗口进行操作的四种类型。</p>
<p>CONTINUE(继续)：什么都不做</p>
<p>FIRE(触发)：触发计算，输出结果</p>
<p>PURGE(清除)：清空窗口中的所有数据，销毁窗口</p>
<p>FIRE_AND_PURGE(触发并清除)：触发计算输出结果，并清除窗口</p>
<p>在日常业务场景中，我们经常会开比较大的窗口来计算每个窗口的 pv 或者 uv 等数据。但窗口开的太大，会使我们看到计算结果的时间间隔变长。所以我们可以使用触发器，来隔一段时间触发一次窗口计算。我们在代码中计算了每个 url 在 10 秒滚动窗口的 pv 指标，然后设置了触发器，每隔 1 秒钟触发一次窗口的计算。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    env</span><br><span class="line">            .addSource(<span class="keyword">new</span> ClickSource())</span><br><span class="line">            .assignTimestampsAndWatermarks(</span><br><span class="line">                    WatermarkStrategy.&lt;Event&gt;forMonotonousTimestamps()</span><br><span class="line">                            .withTimestampAssigner(<span class="keyword">new</span> SerializableTimestampAssigner&lt;Event&gt;() &#123;</span><br><span class="line">                                <span class="meta">@Override</span></span><br><span class="line">                                <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Event event, <span class="keyword">long</span> l)</span> </span>&#123;</span><br><span class="line">                                    <span class="keyword">return</span> event.timestamp;</span><br><span class="line">                                &#125;</span><br><span class="line">                            &#125;)</span><br><span class="line">            )</span><br><span class="line">            .keyBy(r -&gt; r.url)</span><br><span class="line">            .window(TumblingEventTimeWindows.of(Time.seconds(<span class="number">10</span>)))</span><br><span class="line">            .trigger(<span class="keyword">new</span> MyTrigger())</span><br><span class="line">            .process(<span class="keyword">new</span> WindowResult())</span><br><span class="line">            .print();</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">WindowResult</span> <span class="keyword">extends</span> <span class="title">ProcessWindowFunction</span>&lt;<span class="title">Event</span>, <span class="title">UrlViewCount</span>, <span class="title">String</span>, <span class="title">TimeWindow</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(String s, Context context, Iterable&lt;Event&gt; iterable, Collector&lt;UrlViewCount&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        collector.collect(</span><br><span class="line">                <span class="keyword">new</span> UrlViewCount(</span><br><span class="line">                        s,</span><br><span class="line">                        <span class="comment">// 获取迭代器中的元素个数</span></span><br><span class="line">                        iterable.spliterator().getExactSizeIfKnown(),</span><br><span class="line">                        context.window().getStart(),</span><br><span class="line">                        context.window().getEnd()</span><br><span class="line">                )</span><br><span class="line">        );</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyTrigger</span> <span class="keyword">extends</span> <span class="title">Trigger</span>&lt;<span class="title">Event</span>, <span class="title">TimeWindow</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> TriggerResult <span class="title">onElement</span><span class="params">(Event event, <span class="keyword">long</span> l, TimeWindow timeWindow, TriggerContext triggerContext)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        ValueState&lt;Boolean&gt; isFirstEvent = triggerContext.getPartitionedState(</span><br><span class="line">                <span class="keyword">new</span> ValueStateDescriptor&lt;Boolean&gt;(<span class="string">&quot;first-event&quot;</span>, Types.BOOLEAN)</span><br><span class="line">        );</span><br><span class="line">        <span class="keyword">if</span> (isFirstEvent.value() == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">long</span> i = timeWindow.getStart(); i &lt; timeWindow.getEnd(); i = i + <span class="number">1000L</span>) &#123;</span><br><span class="line">                triggerContext.registerEventTimeTimer(i);</span><br><span class="line">            &#125;</span><br><span class="line">            isFirstEvent.update(<span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> TriggerResult.CONTINUE;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> TriggerResult <span class="title">onEventTime</span><span class="params">(<span class="keyword">long</span> l, TimeWindow timeWindow, TriggerContext triggerContext)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> TriggerResult.FIRE;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> TriggerResult <span class="title">onProcessingTime</span><span class="params">(<span class="keyword">long</span> l, TimeWindow timeWindow, TriggerContext triggerContext)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> TriggerResult.CONTINUE;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">clear</span><span class="params">(TimeWindow timeWindow, TriggerContext triggerContext)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        ValueState&lt;Boolean&gt; isFirstEvent = triggerContext.getPartitionedState(</span><br><span class="line">                <span class="keyword">new</span> ValueStateDescriptor&lt;Boolean&gt;(<span class="string">&quot;first-event&quot;</span>, Types.BOOLEAN)</span><br><span class="line">        );</span><br><span class="line">        isFirstEvent.clear();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="移除器-Evictor"><a href="#移除器-Evictor" class="headerlink" title="移除器(Evictor)"></a>移除器(Evictor)</h4><p>移除器主要用来定义移除某些数据的逻辑。基于 WindowedStream 调用.evictor()方法，就可以传入一个自定义的移除器(Evictor)。Evictor 是一个接口，不同的窗口类型都有各自预实现的移除器。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">stream.keyBy(...)</span><br><span class="line">      .window(...)</span><br><span class="line">      .evictor(<span class="keyword">new</span> MyEvictor())</span><br></pre></td></tr></table></figure>

<p>Evictor 接口定义了两个方法:</p>
<p>evictBefore()：定义执行窗口函数之前的移除数据操作。</p>
<p>evictAfter()：定义执行窗口函数之后的以处数据操作默认情况下，预实现的移除器都是在执行窗口函数(window fucntions)之前移除数据的。</p>
<h4 id="允许延迟-Allowed-Lateness"><a href="#允许延迟-Allowed-Lateness" class="headerlink" title="允许延迟(Allowed Lateness)"></a>允许延迟(Allowed Lateness)</h4><p>在事件时间语义下，窗口中可能会出现数据迟到的情况。这是因为在乱序流中，水位线 (watermark)并不一定能保证时间戳更早的所有数据不会再来。当水位线已经到达窗口结束时间时，窗口会触发计算并输出结果，这时一般也就要销毁窗口了；如果窗口关闭之后，又有本属于窗口内的数据姗姗来迟，默认情况下就会被丢弃。这也很好理解：窗口触发计算就像发车，如果要赶的车已经开走了，又不能坐其他的车(保证分配窗口的正确性)，那就只好放弃坐班了。</p>
<p>不过在多数情况下，直接丢弃数据也会导致统计结果不准确，我们还是希望该上车的人都能上来。为了解决迟到数据的问题，Flink 提供了一个特殊的接口，可以为窗口算子设置一个允许的最大延迟(Allowed Lateness)。也就是说，我们可以设定允许延迟一段时间，在这段时间内，窗口不会销毁，<strong>继续到来的数据依然可以进入窗口中并触发计算</strong>。直到水位线推进到了窗口结束时间 + 延迟时间，才真正将窗口的内容清空，正式关闭窗口。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">stream.keyBy(...)</span><br><span class="line">      .window(TumblingEventTimeWindows.of(Time.hours(<span class="number">1</span>)))</span><br><span class="line">      .allowedLateness(Time.minutes(<span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<h4 id="侧输出流"><a href="#侧输出流" class="headerlink" title="侧输出流"></a>侧输出流</h4><p>Flink 还提供了另外一种方式处理迟到数据。我们可以将未收入窗口的迟到数据，放入侧输出流(side output)进行另外的处理。所谓的侧输出流，相当于是数据流的一个分支，这个流中单独放置那些错过了该上的车、本该被丢弃的数据。</p>
<p>基于 WindowedStream 调用.sideOutputLateData() 方法，就可以实现这个功能。方法需要传入一个输出标签(OutputTag)，用来标记分支的迟到数据流。因为保存的就是流中的原始数据，所以 OutputTag 的类型与流中数据<strong>类型相同</strong>。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 读取socket文本流</span></span><br><span class="line">    SingleOutputStreamOperator&lt;Event&gt; stream =</span><br><span class="line">            env.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">7777</span>)</span><br><span class="line">                    .map(<span class="keyword">new</span> MapFunction&lt;String, Event&gt;() &#123;</span><br><span class="line">                        <span class="meta">@Override</span></span><br><span class="line">                        <span class="function"><span class="keyword">public</span> Event <span class="title">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                            String[] fields = value.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">                            <span class="keyword">return</span> <span class="keyword">new</span> Event(fields[<span class="number">0</span>].trim(), fields[<span class="number">1</span>].trim(), Long.valueOf(fields[<span class="number">2</span>].trim()));</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;)</span><br><span class="line">                    <span class="comment">// 方式一：设置watermark延迟时间，2秒钟</span></span><br><span class="line">                    .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Event&gt;forBoundedOutOfOrderness(Duration.ofSeconds(<span class="number">2</span>))</span><br><span class="line">                            .withTimestampAssigner(<span class="keyword">new</span> SerializableTimestampAssigner&lt;Event&gt;() &#123;</span><br><span class="line">                                <span class="meta">@Override</span></span><br><span class="line">                                <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Event element, <span class="keyword">long</span> recordTimestamp)</span> </span>&#123;</span><br><span class="line">                                    <span class="keyword">return</span> element.timestamp;</span><br><span class="line">                                &#125;</span><br><span class="line">                            &#125;));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 定义侧输出流标签</span></span><br><span class="line">    OutputTag&lt;Event&gt; outputTag = <span class="keyword">new</span> OutputTag&lt;Event&gt;(<span class="string">&quot;late&quot;</span>)&#123;&#125;;</span><br><span class="line"></span><br><span class="line">    SingleOutputStreamOperator&lt;UrlViewCount&gt; result = stream.keyBy(data -&gt; data.url)</span><br><span class="line">            .window(TumblingEventTimeWindows.of(Time.seconds(<span class="number">10</span>)))</span><br><span class="line">            <span class="comment">// 方式二：允许窗口处理迟到数据，设置1分钟的等待时间</span></span><br><span class="line">            .allowedLateness(Time.minutes(<span class="number">1</span>))</span><br><span class="line">            <span class="comment">// 方式三：将最后的迟到数据输出到侧输出流</span></span><br><span class="line">            .sideOutputLateData(outputTag)</span><br><span class="line">            .aggregate(<span class="keyword">new</span> UrlViewCountAgg(), <span class="keyword">new</span> UrlViewCountResult());</span><br><span class="line"></span><br><span class="line">    result.print(<span class="string">&quot;result&quot;</span>);</span><br><span class="line">    result.getSideOutput(outputTag).print(<span class="string">&quot;late&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 为方便观察，可以将原始数据也输出</span></span><br><span class="line">    stream.print(<span class="string">&quot;input&quot;</span>);</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">UrlViewCountAgg</span> <span class="keyword">implements</span> <span class="title">AggregateFunction</span>&lt;<span class="title">Event</span>, <span class="title">Long</span>, <span class="title">Long</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">createAccumulator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0L</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">add</span><span class="params">(Event value, Long accumulator)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> accumulator + <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">getResult</span><span class="params">(Long accumulator)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> accumulator;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">merge</span><span class="params">(Long a, Long b)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">UrlViewCountResult</span> <span class="keyword">extends</span> <span class="title">ProcessWindowFunction</span>&lt;<span class="title">Long</span>, <span class="title">UrlViewCount</span>, <span class="title">String</span>, <span class="title">TimeWindow</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(String url, Context context, Iterable&lt;Long&gt; elements, Collector&lt;UrlViewCount&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// 结合窗口信息，包装输出内容</span></span><br><span class="line">        Long start = context.window().getStart();</span><br><span class="line">        Long end = context.window().getEnd();</span><br><span class="line">        out.collect(<span class="keyword">new</span> UrlViewCount(url, elements.iterator().next(), start, end));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这里注意，getSideOutput() 是 SingleOutputStreamOperator 的方法，获取到的侧输出流数据类型应该和 OutputTag 指定的类型一致，与窗口聚合之后流中的数据类型<strong>可以不同</strong>。</p>
<h3 id="窗口的生命周期"><a href="#窗口的生命周期" class="headerlink" title="窗口的生命周期"></a>窗口的生命周期</h3><p><strong>1、窗口的创建</strong></p>
<p>窗口的类型和基本信息由窗口分配器(window assigners)指定，但窗口不会预先创建好，而是由数据驱动创建。当第一个应该属于这个窗口的数据元素到达时，就会创建对应的窗口。</p>
<p><strong>2、窗口计算的触发</strong></p>
<p>除了窗口分配器，每个窗口还会有自己的<strong>窗口函数(window functions)<strong>和</strong>触发器(trigger)<strong>。 窗口函数可以分为</strong>增量聚合函数</strong>和<strong>全窗口函数</strong>，主要定义了窗口中计算的逻辑；而触发器则是指定调用窗口函数的条件。</p>
<p>对于不同的窗口类型，触发计算的条件也会不同。例如，一个滚动事件时间窗口，应该在水位线到达窗口结束时间的时候触发计算，属于定点发车；而一个计数窗口，会在窗口中元素数量达到定义大小时触发计算，属于人满就发车。所以 Flink 预定义的窗口类型都有对应内置的触发器。</p>
<p>对于事件时间窗口而言，除去到达结束时间的定点发车，还有另一种情形。当我们设置了允许延迟，那么如果水位线超过了窗口结束时间、但还没有到达设定的最大延迟时间，这期间内到达的迟到数据也会触发窗口计算。这类似于没有准时赶上班车的人又追上了车，这时车要再次停靠、开门，将新的数据整合统计进来。</p>
<p><strong>3、窗口的销毁</strong></p>
<p>一般情况下，当时间达到了结束点，就会直接触发计算输出结果、进而清除状态销毁窗口。 这时窗口的销毁可以认为和触发计算是同一时刻。这里需要注意，Flink 中只对时间窗口(TimeWindow)有销毁机制；由于计数窗口(CountWindow)是基于全局窗口(GlobalWindw) 实现的，而全局窗口不会清除状态，所以就不会被销毁。</p>
<p>在特殊的场景下，窗口的销毁和触发计算会有所不同。事件时间语义下，如果设置了允许延迟，那么在水位线到达窗口结束时间时，仍然不会销毁窗口；窗口真正被完全删除的时间点， 是窗口的结束时间加上用户指定的允许延迟时间。</p>
<p><strong>4、窗口 API 调用总结</strong></p>
<p><img src="/Flink/%E6%88%AA%E5%B1%8F2022-03-23%20%E4%B8%8B%E5%8D%888.50.24.png" alt="截屏2022-03-23 下午8.50.24"></p>
<h2 id="WaterMark"><a href="#WaterMark" class="headerlink" title="WaterMark"></a>WaterMark</h2><p>水位线 = 观察到的最大事件时间 – 最大延迟时间 – 1 毫秒</p>
<p>Flink<strong>内置</strong>了两个WaterMark生成器：</p>
<p>1、Monotonously Increasing Timestamps(时间戳单调增长：其实就是允许的延迟为0)</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//有序流</span></span><br><span class="line">WatermarkStrategy.forMonotonousTimestamps();</span><br></pre></td></tr></table></figure>

<p>2、Fixed Amount of Lateness(允许固定时间的延迟)</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//乱序流</span></span><br><span class="line">WatermarkStrategy.forBoundedOutOfOrderness(Duration.ofSeconds(<span class="number">10</span>));</span><br></pre></td></tr></table></figure>

<h3 id="WatermarkTest"><a href="#WatermarkTest" class="headerlink" title="WatermarkTest"></a>WatermarkTest</h3><p><strong>Flink 内置水位线生成器</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将数据源改为socket文本流，并转换成Event类型</span></span><br><span class="line">    env.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">7777</span>)</span><br><span class="line">            .map(<span class="keyword">new</span> MapFunction&lt;String, Event&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> Event <span class="title">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                    String[] fields = value.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">new</span> Event(fields[<span class="number">0</span>].trim(), fields[<span class="number">1</span>].trim(), Long.valueOf(fields[<span class="number">2</span>].trim()));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;)</span><br><span class="line">            <span class="comment">// 插入水位线的逻辑</span></span><br><span class="line">            .assignTimestampsAndWatermarks(</span><br><span class="line">                    <span class="comment">// 针对乱序流插入水位线，延迟时间设置为5s</span></span><br><span class="line">                    WatermarkStrategy.&lt;Event&gt;forBoundedOutOfOrderness(Duration.ofSeconds(<span class="number">5</span>))</span><br><span class="line">                            .withTimestampAssigner(<span class="keyword">new</span> SerializableTimestampAssigner&lt;Event&gt;() &#123;</span><br><span class="line">                                <span class="comment">// 抽取时间戳的逻辑</span></span><br><span class="line">                                <span class="meta">@Override</span></span><br><span class="line">                                <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Event element, <span class="keyword">long</span> recordTimestamp)</span> </span>&#123;</span><br><span class="line">                                    <span class="keyword">return</span> element.timestamp;</span><br><span class="line">                                &#125;</span><br><span class="line">                            &#125;)</span><br><span class="line">            )</span><br><span class="line">            <span class="comment">// 根据user分组，开窗统计</span></span><br><span class="line">            .keyBy(data -&gt; data.user)</span><br><span class="line">            .window(TumblingEventTimeWindows.of(Time.seconds(<span class="number">10</span>)))</span><br><span class="line">            .process(<span class="keyword">new</span> WatermarkTestResult())</span><br><span class="line">            .print();</span><br><span class="line"></span><br><span class="line">    env.execute(); </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 自定义处理窗口函数，输出当前的水位线和窗口信息</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">WatermarkTestResult</span> <span class="keyword">extends</span> <span class="title">ProcessWindowFunction</span>&lt;<span class="title">Event</span>, <span class="title">String</span>, <span class="title">String</span>, <span class="title">TimeWindow</span>&gt;</span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(String s, Context context, Iterable&lt;Event&gt; elements, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Long start = context.window().getStart();</span><br><span class="line">        Long end = context.window().getEnd();</span><br><span class="line">        Long currentWatermark = context.currentWatermark();</span><br><span class="line">        Long count = elements.spliterator().getExactSizeIfKnown();</span><br><span class="line">        out.collect(<span class="string">&quot;窗口&quot;</span> + start + <span class="string">&quot; ~ &quot;</span> + end + <span class="string">&quot;中共有&quot;</span> + count + <span class="string">&quot;个元素，窗口闭合计算时，水位线处于：&quot;</span> + currentWatermark);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>自定义WatermarkStrategy：</strong></p>
<p>有2种风格的WaterMark生产方式：periodic(周期性) and punctuated(间歇性)，都需要继承接口<strong>WatermarkGenerator</strong>。</p>
<p>onEvent()和 onPeriodicEmit()，前者是在<strong>每个事件到来时调用</strong>，而后者<strong>由框架周期性调用</strong>。周期性调用的方法中发出水位线，自然就是周期性生成水位线；而在事件触发的方法中发出水位线，自然就是断点式生成了。两种方式的不同就集中体现在这两个方法的实现上。</p>
<h3 id="CustomWatermarkTest"><a href="#CustomWatermarkTest" class="headerlink" title="CustomWatermarkTest"></a>CustomWatermarkTest</h3><p>自定义水位线策略</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    env</span><br><span class="line">            .addSource(<span class="keyword">new</span> ClickSource())</span><br><span class="line">            .assignTimestampsAndWatermarks(<span class="keyword">new</span> CustomWatermarkStrategy())</span><br><span class="line">            .print();</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomWatermarkStrategy</span> <span class="keyword">implements</span> <span class="title">WatermarkStrategy</span>&lt;<span class="title">Event</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> TimestampAssigner&lt;Event&gt; <span class="title">createTimestampAssigner</span><span class="params">(TimestampAssignerSupplier.Context context)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> SerializableTimestampAssigner&lt;Event&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Event element, <span class="keyword">long</span> recordTimestamp)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> element.timestamp; <span class="comment">// 告诉程序数据源里的时间戳是哪一个字段</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> WatermarkGenerator&lt;Event&gt; <span class="title">createWatermarkGenerator</span><span class="params">(WatermarkGeneratorSupplier.Context context)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> CustomPeriodicGenerator();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//periodic</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomPeriodicGenerator</span> <span class="keyword">implements</span> <span class="title">WatermarkGenerator</span>&lt;<span class="title">Event</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Long delayTime = <span class="number">5000L</span>; <span class="comment">// 延迟时间</span></span><br><span class="line">    <span class="keyword">private</span> Long maxTs = Long.MIN_VALUE + delayTime + <span class="number">1L</span>; <span class="comment">// 观察到的最大时间戳</span></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onEvent</span><span class="params">(Event event, <span class="keyword">long</span> eventTimestamp, WatermarkOutput output)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 每来一条数据就调用一次</span></span><br><span class="line">        maxTs = Math.max(event.timestamp, maxTs); <span class="comment">// 更新最大时间戳</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPeriodicEmit</span><span class="params">(WatermarkOutput output)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 发射水位线，默认200ms调用一次</span></span><br><span class="line">        output.emitWatermark(<span class="keyword">new</span> Watermark(maxTs - delayTime - <span class="number">1L</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//punctuated</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomPunctuatedGenerator</span> <span class="keyword">implements</span> <span class="title">WatermarkGenerator</span>&lt;<span class="title">Event</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onEvent</span><span class="params">(Event r, <span class="keyword">long</span> eventTimestamp, WatermarkOutput output)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 只有在遇到特定的 itemId 时，才发出水位线 </span></span><br><span class="line">        <span class="keyword">if</span> (r.user.equals(<span class="string">&quot;Mary&quot;</span>)) &#123;</span><br><span class="line">            output.emitWatermark(<span class="keyword">new</span> Watermark(r.timestamp - <span class="number">1</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPeriodicEmit</span><span class="params">(WatermarkOutput output)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 不需要做任何事情，因为我们在 onEvent 方法中发射了水位线</span></span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="EmitWatermarkInSourceFunction"><a href="#EmitWatermarkInSourceFunction" class="headerlink" title="EmitWatermarkInSourceFunction"></a>EmitWatermarkInSourceFunction</h3><p>我们也可以在自定义的数据源中抽取事件时间，然后发送水位线。这里要注意的是，在自定义数据源中发送了水位线以后，就不能再在程序中使用 assignTimestampsAndWatermarks 方法来生成水位线了。在自定义数据源中生成水位线和在程序中使用 assignTimestampsAndWatermarks 方法生成水位线<strong>二者只能取其一</strong>。</p>
<p>在自定义水位线中生成水位线相比 assignTimestampsAndWatermarks 方法更加灵活，可以任意的产生周期性的、非周期性的水位线，以及水位线的大小也完全由我们自定义。所以非常适合用来编写 Flink 的测试程序，测试 Flink 的各种各样的特性。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    env.addSource(<span class="keyword">new</span> ClickSourceWithWatermark()).print();</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">ClickSourceWithWatermark</span> <span class="keyword">implements</span> <span class="title">SourceFunction</span>&lt;<span class="title">Event</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">boolean</span> running = <span class="keyword">true</span>;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;Event&gt; sourceContext)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Random random = <span class="keyword">new</span> Random();</span><br><span class="line">        String[] userArr = &#123;<span class="string">&quot;Mary&quot;</span>, <span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;Alice&quot;</span>&#125;;</span><br><span class="line">        String[] urlArr  = &#123;<span class="string">&quot;./home&quot;</span>, <span class="string">&quot;./cart&quot;</span>, <span class="string">&quot;./prod?id=1&quot;</span>&#125;;</span><br><span class="line">        <span class="keyword">while</span> (running) &#123;</span><br><span class="line">            <span class="keyword">long</span> currTs = Calendar.getInstance().getTimeInMillis(); <span class="comment">// 毫秒时间戳</span></span><br><span class="line">            String username = userArr[random.nextInt(userArr.length)];</span><br><span class="line">            String url      = urlArr[random.nextInt(urlArr.length)];</span><br><span class="line">            Event event = <span class="keyword">new</span> Event(username, url, currTs);</span><br><span class="line">            <span class="comment">// 使用collectWithTimestamp方法将数据发送出去，并指明数据中的时间戳的字段</span></span><br><span class="line">            sourceContext.collectWithTimestamp(event, event.timestamp);</span><br><span class="line">            <span class="comment">// 发送水位线</span></span><br><span class="line">            sourceContext.emitWatermark(<span class="keyword">new</span> Watermark(event.timestamp - <span class="number">1L</span>));</span><br><span class="line">            Thread.sleep(<span class="number">1000L</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        running = <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="ProcessFunction-API"><a href="#ProcessFunction-API" class="headerlink" title="ProcessFunction API"></a>ProcessFunction API</h2><p>处理函数提供了一个定时服务 (TimerService)，我们可以通过它访问流中的事件(event)、时间戳(timestamp)、水位线 (watermark)，甚至可以注册定时事件。而且处理函数继承了 AbstractRichFunction 抽象类，所以拥有富函数类的所有特性，同样可以访问状态(state)和其他运行时信息。此外，处理函数还可以直接将数据输出到侧输出流(side output)中。所以，处理函数是最为灵活的处理方法，可以实现各种自定义的业务逻辑；同时也是整个 DataStream API 的底层基础。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * A function that processes elements of a stream.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;For every element in the input stream &#123;<span class="doctag">@link</span> #processElement(Object, Context, Collector)&#125; is</span></span><br><span class="line"><span class="comment"> * invoked. This can produce zero or more elements as output. Implementations can also query the</span></span><br><span class="line"><span class="comment"> * time and set timers through the provided &#123;<span class="doctag">@link</span> Context&#125;. For firing timers &#123;<span class="doctag">@link</span> #onTimer(long,</span></span><br><span class="line"><span class="comment"> * OnTimerContext, Collector)&#125; will be invoked. This can again produce zero or more elements as</span></span><br><span class="line"><span class="comment"> * output and register further timers.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;&lt;b&gt;<span class="doctag">NOTE:</span>&lt;/b&gt; Access to keyed state and timers (which are also scoped to a key) is only</span></span><br><span class="line"><span class="comment"> * available if the &#123;<span class="doctag">@code</span> ProcessFunction&#125; is applied on a &#123;<span class="doctag">@code</span> KeyedStream&#125;.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;&lt;b&gt;<span class="doctag">NOTE:</span>&lt;/b&gt; A &#123;<span class="doctag">@code</span> ProcessFunction&#125; is always a &#123;<span class="doctag">@link</span></span></span><br><span class="line"><span class="comment"> * org.apache.flink.api.common.functions.RichFunction&#125;. Therefore, access to the &#123;<span class="doctag">@link</span></span></span><br><span class="line"><span class="comment"> * org.apache.flink.api.common.functions.RuntimeContext&#125; is always available and setup and teardown</span></span><br><span class="line"><span class="comment"> * methods can be implemented. See &#123;<span class="doctag">@link</span></span></span><br><span class="line"><span class="comment"> * org.apache.flink.api.common.functions.RichFunction#open(org.apache.flink.configuration.Configuration)&#125;</span></span><br><span class="line"><span class="comment"> * and &#123;<span class="doctag">@link</span> org.apache.flink.api.common.functions.RichFunction#close()&#125;.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> &lt;I&gt; Type of the input elements.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> &lt;O&gt; Type of the output elements.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@PublicEvolving</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">ProcessFunction</span>&lt;<span class="title">I</span>, <span class="title">O</span>&gt; <span class="keyword">extends</span> <span class="title">AbstractRichFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Process one element from the input stream.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;This function can output zero or more elements using the &#123;<span class="doctag">@link</span> Collector&#125; parameter and</span></span><br><span class="line"><span class="comment">     * also update internal state or set timers using the &#123;<span class="doctag">@link</span> Context&#125; parameter.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value The input value.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> ctx A &#123;<span class="doctag">@link</span> Context&#125; that allows querying the timestamp of the element and getting a</span></span><br><span class="line"><span class="comment">     *     &#123;<span class="doctag">@link</span> TimerService&#125; for registering timers and querying the time. The context is only</span></span><br><span class="line"><span class="comment">     *     valid during the invocation of this method, do not store it.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> out The collector for returning result values.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception This method may throw exceptions. Throwing an exception will cause the</span></span><br><span class="line"><span class="comment">     *     operation to fail and may trigger recovery.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(I value, Context ctx, Collector&lt;O&gt; out)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Called when a timer set using &#123;<span class="doctag">@link</span> TimerService&#125; fires.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> timestamp The timestamp of the firing timer.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> ctx An &#123;<span class="doctag">@link</span> OnTimerContext&#125; that allows querying the timestamp of the firing timer,</span></span><br><span class="line"><span class="comment">     *     querying the &#123;<span class="doctag">@link</span> TimeDomain&#125; of the firing timer and getting a &#123;<span class="doctag">@link</span> TimerService&#125;</span></span><br><span class="line"><span class="comment">     *     for registering timers and querying the time. The context is only valid during the</span></span><br><span class="line"><span class="comment">     *     invocation of this method, do not store it.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> out The collector for returning result values.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception This method may throw exceptions. Throwing an exception will cause the</span></span><br><span class="line"><span class="comment">     *     operation to fail and may trigger recovery.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onTimer</span><span class="params">(<span class="keyword">long</span> timestamp, OnTimerContext ctx, Collector&lt;O&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Information available in an invocation of &#123;<span class="doctag">@link</span> #processElement(Object, Context, Collector)&#125;</span></span><br><span class="line"><span class="comment">     * or &#123;<span class="doctag">@link</span> #onTimer(long, OnTimerContext, Collector)&#125;.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Context</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * Timestamp of the element currently being processed or timestamp of a firing timer.</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * &lt;p&gt;This might be &#123;<span class="doctag">@code</span> null&#125;, for example if the time characteristic of your program is</span></span><br><span class="line"><span class="comment">         * set to &#123;<span class="doctag">@link</span> org.apache.flink.streaming.api.TimeCharacteristic#ProcessingTime&#125;.</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> Long <span class="title">timestamp</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/** A &#123;<span class="doctag">@link</span> TimerService&#125; for querying time and registering timers. */</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> TimerService <span class="title">timerService</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * Emits a record to the side output identified by the &#123;<span class="doctag">@link</span> OutputTag&#125;.</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> outputTag the &#123;<span class="doctag">@code</span> OutputTag&#125; that identifies the side output to emit to.</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> value The record to emit.</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">abstract</span> &lt;X&gt; <span class="function"><span class="keyword">void</span> <span class="title">output</span><span class="params">(OutputTag&lt;X&gt; outputTag, X value)</span></span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Information available in an invocation of &#123;<span class="doctag">@link</span> #onTimer(long, OnTimerContext, Collector)&#125;.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">OnTimerContext</span> <span class="keyword">extends</span> <span class="title">Context</span> </span>&#123;</span><br><span class="line">        <span class="comment">/** The &#123;<span class="doctag">@link</span> TimeDomain&#125; of the firing timer. */</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> TimeDomain <span class="title">timeDomain</span><span class="params">()</span></span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    env</span><br><span class="line">            .addSource(<span class="keyword">new</span> ClickSource())</span><br><span class="line">            .assignTimestampsAndWatermarks(</span><br><span class="line">                    WatermarkStrategy.&lt;Event&gt;forMonotonousTimestamps()</span><br><span class="line">                            .withTimestampAssigner(<span class="keyword">new</span> SerializableTimestampAssigner&lt;Event&gt;() &#123;</span><br><span class="line">                                <span class="meta">@Override</span></span><br><span class="line">                                <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Event event, <span class="keyword">long</span> l)</span> </span>&#123;</span><br><span class="line">                                    <span class="keyword">return</span> event.timestamp;</span><br><span class="line">                                &#125;</span><br><span class="line">                            &#125;)</span><br><span class="line">            )</span><br><span class="line">            .process(<span class="keyword">new</span> ProcessFunction&lt;Event, String&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(Event value, Context ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                    <span class="keyword">if</span> (value.user.equals(<span class="string">&quot;Mary&quot;</span>)) &#123;</span><br><span class="line">                        out.collect(value.user);</span><br><span class="line">                    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (value.user.equals(<span class="string">&quot;Bob&quot;</span>)) &#123;</span><br><span class="line">                        out.collect(value.user);</span><br><span class="line">                        out.collect(value.user);</span><br><span class="line">                    &#125;</span><br><span class="line">                    System.out.println(ctx.timerService().currentWatermark());</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;)</span><br><span class="line">            .print();</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>抽象类 ProcessFunction 继承了 AbstractRichFunction，有两个泛型类型参数：I 表示 Input，也就是输入的数据类型；O 表示 Output，也就是处理完成之后输出的数据类型。</p>
<p>内部单独定义了两个方法：一个是必须要实现的抽象方法.processElement()；另一个是非抽象方法.onTimer()。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">ProcessFunction</span>&lt;<span class="title">I</span>, <span class="title">O</span>&gt; <span class="keyword">extends</span> <span class="title">AbstractRichFunction</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(I value, Context ctx, Collector&lt;O&gt; out)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onTimer</span><span class="params">(<span class="keyword">long</span> timestamp, OnTimerContext ctx, Collector&lt;O&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;&#125;</span><br><span class="line">    </span><br><span class="line">    ... </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>processElement()：</strong></p>
<p>value：当前流中的输入元素，也就是正在处理的数据，类型与流中数据类型一致。</p>
<p>ctx：类型是ProcessFunction中定义的内部抽象类Context，表示当前运行的上下文，可以获取到当前的时间戳，并提供了用于查询时间和注册定时器的定时服务(TimerService)，以及可以将数据发送到侧输出流(side output)的方法.output()。 Context 抽象类定义如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Context</span> </span>&#123;</span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> Long <span class="title">timestamp</span><span class="params">()</span></span>;</span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> TimerService <span class="title">timerService</span><span class="params">()</span></span>;</span><br><span class="line">   <span class="keyword">public</span> <span class="keyword">abstract</span> &lt;X&gt; <span class="function"><span class="keyword">void</span> <span class="title">output</span><span class="params">(OutputTag&lt;X&gt; outputTag, X value)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>out：收集器(类型为Collector)，用于返回输出数据。使用方式与 flatMap 算子中的收集器完全一样，直接调用 out.collect()方法就可以向下游发出一个数据。 这个方法可以多次调用，也可以不调用。</p>
<p><strong>onTimer()：</strong></p>
<p>用于定义定时触发的操作，这是一个非常强大、也非常有趣的功能。<strong>这个方法只有在注册好的定时器触发的时候才会调用，而定时器是通过定时服务TimerService来注册的</strong>。打个比方，注册定时器(timer)就是设了一个闹钟，到了设定时间就会响；而.onTimer()中定义的， 就是闹钟响的时候要做的事。所以它本质上是一个基于时间的回调(callback)方法，通过时间的进展来触发；在事件时间语义下就是由水位线(watermark)来触发了。</p>
<p>与.processElement()类似，定时方法.onTimer()也有三个参数：时间戳(timestamp)，上下文(ctx)，以及收集器(out)。这里的 timestamp 是指设定好的触发时间，事件时间语义下当然就是水位线了。另外这里同样有上下文和收集器，所以也可以调用定时服务(TimerService)， 以及任意输出处理之后的数据。</p>
<p>既然有.onTimer()方法做定时触发，我们用 ProcessFunction 也可以自定义数据按照时间分组、定时触发计算输出结果；这其实就实现了窗口(window)的功能。所以说 ProcessFunction 是真正意义上的终极奥义，用它可以实现一切功能。</p>
<p>我们也可以看到，处理函数都是基于事件触发的。水位线就如同插入流中的一条数据一样；只不过处理真正的数据事件调用的是.processElement()方法，而处理水位线事件调用的是.onTimer()。</p>
<p>这里需要注意的是，上面的.onTimer()方法只是定时器触发时的操作，而定时器(timer) 真正的设置需要用到上下文 ctx 中的定时服务。在 Flink 中，只有按键分区流 KeyedStream 才支持设置定时器的操作。</p>
<p><strong>Flink 提供了 8 个不同的处理函数：</strong></p>
<h3 id="ProcessFunction"><a href="#ProcessFunction" class="headerlink" title="ProcessFunction"></a>ProcessFunction</h3><p>最基本的处理函数，基于 DataStream 直接调用.process()时作为参数传入。</p>
<h3 id="KeyedProcessFunction"><a href="#KeyedProcessFunction" class="headerlink" title="KeyedProcessFunction"></a>KeyedProcessFunction</h3><p>对流按键分区后的处理函数，基于 KeyedStream 调用.process()时作为参数传入。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * A keyed function that processes elements of a stream.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;For every element in the input stream &#123;<span class="doctag">@link</span> #processElement(Object, Context, Collector)&#125; is</span></span><br><span class="line"><span class="comment"> * invoked. This can produce zero or more elements as output. Implementations can also query the</span></span><br><span class="line"><span class="comment"> * time and set timers through the provided &#123;<span class="doctag">@link</span> Context&#125;. For firing timers &#123;<span class="doctag">@link</span> #onTimer(long,</span></span><br><span class="line"><span class="comment"> * OnTimerContext, Collector)&#125; will be invoked. This can again produce zero or more elements as</span></span><br><span class="line"><span class="comment"> * output and register further timers.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;&lt;b&gt;<span class="doctag">NOTE:</span>&lt;/b&gt; Access to keyed state and timers (which are also scoped to a key) is only</span></span><br><span class="line"><span class="comment"> * available if the &#123;<span class="doctag">@code</span> KeyedProcessFunction&#125; is applied on a &#123;<span class="doctag">@code</span> KeyedStream&#125;.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;&lt;b&gt;<span class="doctag">NOTE:</span>&lt;/b&gt; A &#123;<span class="doctag">@code</span> KeyedProcessFunction&#125; is always a &#123;<span class="doctag">@link</span></span></span><br><span class="line"><span class="comment"> * org.apache.flink.api.common.functions.RichFunction&#125;. Therefore, access to the &#123;<span class="doctag">@link</span></span></span><br><span class="line"><span class="comment"> * org.apache.flink.api.common.functions.RuntimeContext&#125; is always available and setup and teardown</span></span><br><span class="line"><span class="comment"> * methods can be implemented. See &#123;<span class="doctag">@link</span></span></span><br><span class="line"><span class="comment"> * org.apache.flink.api.common.functions.RichFunction#open(org.apache.flink.configuration.Configuration)&#125;</span></span><br><span class="line"><span class="comment"> * and &#123;<span class="doctag">@link</span> org.apache.flink.api.common.functions.RichFunction#close()&#125;.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> &lt;K&gt; Type of the key.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> &lt;I&gt; Type of the input elements.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> &lt;O&gt; Type of the output elements.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@PublicEvolving</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">KeyedProcessFunction</span>&lt;<span class="title">K</span>, <span class="title">I</span>, <span class="title">O</span>&gt; <span class="keyword">extends</span> <span class="title">AbstractRichFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Process one element from the input stream.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;This function can output zero or more elements using the &#123;<span class="doctag">@link</span> Collector&#125; parameter and</span></span><br><span class="line"><span class="comment">     * also update internal state or set timers using the &#123;<span class="doctag">@link</span> Context&#125; parameter.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value The input value.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> ctx A &#123;<span class="doctag">@link</span> Context&#125; that allows querying the timestamp of the element and getting a</span></span><br><span class="line"><span class="comment">     *     &#123;<span class="doctag">@link</span> TimerService&#125; for registering timers and querying the time. The context is only</span></span><br><span class="line"><span class="comment">     *     valid during the invocation of this method, do not store it.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> out The collector for returning result values.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception This method may throw exceptions. Throwing an exception will cause the</span></span><br><span class="line"><span class="comment">     *     operation to fail and may trigger recovery.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(I value, Context ctx, Collector&lt;O&gt; out)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Called when a timer set using &#123;<span class="doctag">@link</span> TimerService&#125; fires.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> timestamp The timestamp of the firing timer.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> ctx An &#123;<span class="doctag">@link</span> OnTimerContext&#125; that allows querying the timestamp, the &#123;<span class="doctag">@link</span></span></span><br><span class="line"><span class="comment">     *     TimeDomain&#125;, and the key of the firing timer and getting a &#123;<span class="doctag">@link</span> TimerService&#125; for</span></span><br><span class="line"><span class="comment">     *     registering timers and querying the time. The context is only valid during the invocation</span></span><br><span class="line"><span class="comment">     *     of this method, do not store it.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> out The collector for returning result values.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception This method may throw exceptions. Throwing an exception will cause the</span></span><br><span class="line"><span class="comment">     *     operation to fail and may trigger recovery.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onTimer</span><span class="params">(<span class="keyword">long</span> timestamp, OnTimerContext ctx, Collector&lt;O&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Information available in an invocation of &#123;<span class="doctag">@link</span> #processElement(Object, Context, Collector)&#125;</span></span><br><span class="line"><span class="comment">     * or &#123;<span class="doctag">@link</span> #onTimer(long, OnTimerContext, Collector)&#125;.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Context</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * Timestamp of the element currently being processed or timestamp of a firing timer.</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * &lt;p&gt;This might be &#123;<span class="doctag">@code</span> null&#125;, for example if the time characteristic of your program is</span></span><br><span class="line"><span class="comment">         * set to &#123;<span class="doctag">@link</span> org.apache.flink.streaming.api.TimeCharacteristic#ProcessingTime&#125;.</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> Long <span class="title">timestamp</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/** A &#123;<span class="doctag">@link</span> TimerService&#125; for querying time and registering timers. */</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> TimerService <span class="title">timerService</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * Emits a record to the side output identified by the &#123;<span class="doctag">@link</span> OutputTag&#125;.</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> outputTag the &#123;<span class="doctag">@code</span> OutputTag&#125; that identifies the side output to emit to.</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> value The record to emit.</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">abstract</span> &lt;X&gt; <span class="function"><span class="keyword">void</span> <span class="title">output</span><span class="params">(OutputTag&lt;X&gt; outputTag, X value)</span></span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/** Get key of the element being processed. */</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> K <span class="title">getCurrentKey</span><span class="params">()</span></span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Information available in an invocation of &#123;<span class="doctag">@link</span> #onTimer(long, OnTimerContext, Collector)&#125;.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">OnTimerContext</span> <span class="keyword">extends</span> <span class="title">Context</span> </span>&#123;</span><br><span class="line">        <span class="comment">/** The &#123;<span class="doctag">@link</span> TimeDomain&#125; of the firing timer. */</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> TimeDomain <span class="title">timeDomain</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/** Get key of the firing timer. */</span></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> K <span class="title">getCurrentKey</span><span class="params">()</span></span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="定时器-Timer-和定时服务-TimerService"><a href="#定时器-Timer-和定时服务-TimerService" class="headerlink" title="定时器(Timer)和定时服务(TimerService)"></a>定时器(Timer)和定时服务(TimerService)</h4><p>定时器(timers)是处理函数中进行时间相关操作的主要机制。在.onTimer()方法中可以实现定时处理的逻辑，而它能触发的前提，就是之前曾经注册过定时器、并且现在已经到了触发时间。注册定时器的功能，是通过上下文中提供的定时服务(TimerService)来实现的。</p>
<p>TimerService 是 Flink 关于时间和定时器的基础服务接口，包含以下六个方法：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取当前的处理时间</span></span><br><span class="line"><span class="function"><span class="keyword">long</span> <span class="title">currentProcessingTime</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取当前的水位线(事件时间) </span></span><br><span class="line"><span class="function"><span class="keyword">long</span> <span class="title">currentWatermark</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册处理时间定时器，当处理时间超过 time 时触发 </span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">registerProcessingTimeTimer</span><span class="params">(<span class="keyword">long</span> time)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册事件时间定时器，当水位线超过 time 时触发 </span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">registerEventTimeTimer</span><span class="params">(<span class="keyword">long</span> time)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 删除触发时间为 time 的处理时间定时器</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">deleteProcessingTimeTimer</span><span class="params">(<span class="keyword">long</span> time)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 删除触发时间为 time 的处理时间定时器 </span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">deleteEventTimeTimer</span><span class="params">(<span class="keyword">long</span> time)</span></span>;</span><br></pre></td></tr></table></figure>

<p><strong>需要注意，尽管处理函数中都可以直接访问 TimerService，不过只有基于 KeyedStream 的处理函数，才能去调用注册和删除定时器的方法；未作按键分区的 DataStream 不支持定时器操作，只能获取当前时间。</strong></p>
<p>基于 KeyedStream 注册定时器时，会传入一个定时器触发的时间戳，这个时间戳的定时器对于每个 key 都是有效的。这样，我们的代码并不需要做额外的处理，底层就可以直接对不同 key 进行独立的处理操作了。</p>
<p>利用这个特性，有时我们可以故意降低时间戳的精度，来减少定时器的数量，从而提高处理性能。比如我们可以在设置定时器时只保留整秒数，那么定时器的触发频率就是最多 1 秒一次。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">long</span> coalescedTime = time / <span class="number">1000</span> * <span class="number">1000</span>;</span><br><span class="line"></span><br><span class="line">ctx.timerService().registerProcessingTimeTimer(coalescedTime);</span><br></pre></td></tr></table></figure>

<p>这里注意<strong>定时器的时间戳必须是毫秒数</strong>，所以我们得到整秒之后还要乘以1000。定时器默认的区分精度是毫秒。</p>
<p><strong>EventTimeTimerTest：</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    SingleOutputStreamOperator&lt;Event&gt; stream = env.addSource(<span class="keyword">new</span> CustomSource())</span><br><span class="line">            .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Event&gt;forMonotonousTimestamps()</span><br><span class="line">                    .withTimestampAssigner(<span class="keyword">new</span> SerializableTimestampAssigner&lt;Event&gt;() &#123;</span><br><span class="line">                        <span class="meta">@Override</span></span><br><span class="line">                        <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Event element, <span class="keyword">long</span> recordTimestamp)</span> </span>&#123;</span><br><span class="line">                            <span class="keyword">return</span> element.timestamp;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 基于KeyedStream定义事件时间定时器</span></span><br><span class="line">    stream.keyBy(data -&gt; <span class="keyword">true</span>)</span><br><span class="line">            .process(<span class="keyword">new</span> KeyedProcessFunction&lt;Boolean, Event, String&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(Event value, Context ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                    out.collect(<span class="string">&quot;数据到达，时间戳为：&quot;</span> + ctx.timestamp());</span><br><span class="line">                    out.collect(<span class="string">&quot;数据到达，水位线为：&quot;</span> + ctx.timerService().currentWatermark() + <span class="string">&quot;\n -------分割线-------&quot;</span>);</span><br><span class="line">                    <span class="comment">// 注册一个10秒后的定时器</span></span><br><span class="line">                    ctx.timerService().registerEventTimeTimer(ctx.timestamp() + <span class="number">10</span> * <span class="number">1000L</span>);</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onTimer</span><span class="params">(<span class="keyword">long</span> timestamp, OnTimerContext ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                    out.collect(<span class="string">&quot;定时器触发，触发时间：&quot;</span> + timestamp);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;)</span><br><span class="line">            .print();</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 自定义测试数据源</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomSource</span> <span class="keyword">implements</span> <span class="title">SourceFunction</span>&lt;<span class="title">Event</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;Event&gt; ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// 直接发出测试数据</span></span><br><span class="line">        ctx.collect(<span class="keyword">new</span> Event(<span class="string">&quot;Mary&quot;</span>, <span class="string">&quot;./home&quot;</span>, <span class="number">1000L</span>));</span><br><span class="line">        <span class="comment">// 为了更加明显，中间停顿5秒钟</span></span><br><span class="line">        Thread.sleep(<span class="number">5000L</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 发出10秒后的数据</span></span><br><span class="line">        ctx.collect(<span class="keyword">new</span> Event(<span class="string">&quot;Mary&quot;</span>, <span class="string">&quot;./home&quot;</span>, <span class="number">11000L</span>));</span><br><span class="line">        Thread.sleep(<span class="number">5000L</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 发出10秒+1ms后的数据</span></span><br><span class="line">        ctx.collect(<span class="keyword">new</span> Event(<span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;./cart&quot;</span>, <span class="number">11001L</span>));</span><br><span class="line">        Thread.sleep(<span class="number">5000L</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123; &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="ProcessWindowFunction"><a href="#ProcessWindowFunction" class="headerlink" title="ProcessWindowFunction"></a>ProcessWindowFunction</h3><p>开窗之后的处理函数，也是全窗口函数的代表。基于 WindowedStream 调用.process()时作为参数传入。</p>
<p>ProcessWindowFunction 既是处理函数又是全窗口函数。从名字上也可以推测出，它的本质似乎更倾向于窗口函数一些。事实上它的用法也确实跟其他处理函数有很大不同。我们可以从源码中的定义看到这一点：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">ProcessWindowFunction</span>&lt;<span class="title">IN</span>, <span class="title">OUT</span>, <span class="title">KEY</span>, <span class="title">W</span> <span class="keyword">extends</span> <span class="title">Window</span>&gt; <span class="keyword">extends</span> <span class="title">AbstractRichFunction</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(KEY key, Context context, Iterable&lt;IN&gt; elements, Collector&lt;OUT&gt; out)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">    <span class="comment">//key:窗口做统计计算基于的键，也就是之前keyBy用来分区的字段</span></span><br><span class="line">		<span class="comment">//context:当前窗口进行计算的上下文，它的类型就是ProcessWindowFunction内部定义的抽象类Context</span></span><br><span class="line">		<span class="comment">//elements:窗口收集到用来计算的所有数据，这是一个可迭代的集合类型</span></span><br><span class="line">		<span class="comment">//out:用来发送数据输出计算结果的收集器，类型为Collector</span></span><br><span class="line">  </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">clear</span><span class="params">(Context context)</span> <span class="keyword">throws</span> Exception </span>&#123;&#125;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">//上下文context所包含的内容也跟其他处理函数有所差别</span></span><br><span class="line">  	<span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Context</span> <span class="keyword">implements</span> <span class="title">java</span>.<span class="title">io</span>.<span class="title">Serializable</span> </span>&#123;</span><br><span class="line">		    <span class="comment">/** Returns the window that is being evaluated. */</span></span><br><span class="line">		    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> W <span class="title">window</span><span class="params">()</span></span>;</span><br><span class="line">		</span><br><span class="line">		    <span class="comment">/** Returns the current processing time. */</span></span><br><span class="line">		    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">long</span> <span class="title">currentProcessingTime</span><span class="params">()</span></span>;</span><br><span class="line">		</span><br><span class="line">		    <span class="comment">/** Returns the current event-time watermark. */</span></span><br><span class="line">		    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">long</span> <span class="title">currentWatermark</span><span class="params">()</span></span>;</span><br><span class="line">		</span><br><span class="line">		    <span class="comment">/**</span></span><br><span class="line"><span class="comment">		     * State accessor for per-key and per-window state.</span></span><br><span class="line"><span class="comment">		     *</span></span><br><span class="line"><span class="comment">		     * &lt;p&gt;&lt;b&gt;<span class="doctag">NOTE:</span>&lt;/b&gt;If you use per-window state you have to ensure that you clean it up by</span></span><br><span class="line"><span class="comment">		     * implementing &#123;<span class="doctag">@link</span> ProcessWindowFunction#clear(Context)&#125;.</span></span><br><span class="line"><span class="comment">		     */</span></span><br><span class="line">		    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> KeyedStateStore <span class="title">windowState</span><span class="params">()</span></span>;</span><br><span class="line">		</span><br><span class="line">		    <span class="comment">/** State accessor for per-key global state. */</span></span><br><span class="line">		    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> KeyedStateStore <span class="title">globalState</span><span class="params">()</span></span>;</span><br><span class="line">		</span><br><span class="line">		    <span class="comment">/**</span></span><br><span class="line"><span class="comment">		     * Emits a record to the side output identified by the &#123;<span class="doctag">@link</span> OutputTag&#125;.</span></span><br><span class="line"><span class="comment">		     *</span></span><br><span class="line"><span class="comment">		     * <span class="doctag">@param</span> outputTag the &#123;<span class="doctag">@code</span> OutputTag&#125; that identifies the side output to emit to.</span></span><br><span class="line"><span class="comment">		     * <span class="doctag">@param</span> value The record to emit.</span></span><br><span class="line"><span class="comment">		     */</span></span><br><span class="line">		    <span class="keyword">public</span> <span class="keyword">abstract</span> &lt;X&gt; <span class="function"><span class="keyword">void</span> <span class="title">output</span><span class="params">(OutputTag&lt;X&gt; outputTag, X value)</span></span>;</span><br><span class="line">		&#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="KeyedProcessTopN"><a href="#KeyedProcessTopN" class="headerlink" title="KeyedProcessTopN"></a>KeyedProcessTopN</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//KeyedProcessTopN</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 从自定义数据源读取数据</span></span><br><span class="line">    SingleOutputStreamOperator&lt;Event&gt; eventStream = env.addSource(<span class="keyword">new</span> ClickSource())</span><br><span class="line">            .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Event&gt;forMonotonousTimestamps()</span><br><span class="line">                    .withTimestampAssigner(<span class="keyword">new</span> SerializableTimestampAssigner&lt;Event&gt;() &#123;</span><br><span class="line">                        <span class="meta">@Override</span></span><br><span class="line">                        <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Event element, <span class="keyword">long</span> recordTimestamp)</span> </span>&#123;</span><br><span class="line">                            <span class="keyword">return</span> element.timestamp;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 需要按照url分组，求出每个url的访问量</span></span><br><span class="line">    SingleOutputStreamOperator&lt;UrlViewCount&gt; urlCountStream =</span><br><span class="line">            eventStream.keyBy(data -&gt; data.url)</span><br><span class="line">                    .window(SlidingEventTimeWindows.of(Time.seconds(<span class="number">10</span>), Time.seconds(<span class="number">5</span>)))</span><br><span class="line">                    .aggregate(<span class="keyword">new</span> UrlViewCountAgg(),</span><br><span class="line">                            <span class="keyword">new</span> UrlViewCountResult());</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 对结果中同一个窗口的统计数据，进行排序处理</span></span><br><span class="line">    SingleOutputStreamOperator&lt;String&gt; result = urlCountStream.keyBy(data -&gt; data.windowEnd)</span><br><span class="line">            .process(<span class="keyword">new</span> TopN(<span class="number">2</span>));</span><br><span class="line"></span><br><span class="line">    result.print(<span class="string">&quot;result&quot;</span>);</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 自定义增量聚合</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">UrlViewCountAgg</span> <span class="keyword">implements</span> <span class="title">AggregateFunction</span>&lt;<span class="title">Event</span>, <span class="title">Long</span>, <span class="title">Long</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">createAccumulator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0L</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">add</span><span class="params">(Event value, Long accumulator)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> accumulator + <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">getResult</span><span class="params">(Long accumulator)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> accumulator;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">merge</span><span class="params">(Long a, Long b)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 自定义全窗口函数，只需要包装窗口信息</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">UrlViewCountResult</span> <span class="keyword">extends</span> <span class="title">ProcessWindowFunction</span>&lt;<span class="title">Long</span>, <span class="title">UrlViewCount</span>, <span class="title">String</span>, <span class="title">TimeWindow</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(String url, Context context, Iterable&lt;Long&gt; elements, Collector&lt;UrlViewCount&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// 结合窗口信息，包装输出内容</span></span><br><span class="line">        Long start = context.window().getStart();</span><br><span class="line">        Long end = context.window().getEnd();</span><br><span class="line">        out.collect(<span class="keyword">new</span> UrlViewCount(url, elements.iterator().next(), start, end));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 自定义处理函数，排序取top n</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">TopN</span> <span class="keyword">extends</span> <span class="title">KeyedProcessFunction</span>&lt;<span class="title">Long</span>, <span class="title">UrlViewCount</span>, <span class="title">String</span>&gt;</span>&#123;</span><br><span class="line">    <span class="comment">// 将n作为属性</span></span><br><span class="line">    <span class="keyword">private</span> Integer n;</span><br><span class="line">    <span class="comment">// 定义一个列表状态</span></span><br><span class="line">    <span class="keyword">private</span> ListState&lt;UrlViewCount&gt; urlViewCountListState;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">TopN</span><span class="params">(Integer n)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.n = n;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// 从环境中获取列表状态</span></span><br><span class="line">        urlViewCountListState = getRuntimeContext().getListState(</span><br><span class="line">                <span class="keyword">new</span> ListStateDescriptor&lt;UrlViewCount&gt;(<span class="string">&quot;url-view-count-list&quot;</span>,</span><br><span class="line">                        Types.POJO(UrlViewCount.class)));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(UrlViewCount value, Context ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// 将count数据添加到列表状态中，保存起来</span></span><br><span class="line">        urlViewCountListState.add(value);</span><br><span class="line">        <span class="comment">// 注册 window end + 1ms后的定时器，等待所有数据到齐开始排序</span></span><br><span class="line">        ctx.timerService().registerEventTimeTimer(ctx.getCurrentKey() + <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onTimer</span><span class="params">(<span class="keyword">long</span> timestamp, OnTimerContext ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// 将数据从列表状态变量中取出，放入ArrayList，方便排序</span></span><br><span class="line">        ArrayList&lt;UrlViewCount&gt; urlViewCountArrayList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span> (UrlViewCount urlViewCount : urlViewCountListState.get()) &#123;</span><br><span class="line">            urlViewCountArrayList.add(urlViewCount);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 清空状态，释放资源</span></span><br><span class="line">        urlViewCountListState.clear();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 排序</span></span><br><span class="line">        urlViewCountArrayList.sort(<span class="keyword">new</span> Comparator&lt;UrlViewCount&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(UrlViewCount o1, UrlViewCount o2)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> o2.count.intValue() - o1.count.intValue();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 取前两名，构建输出结果</span></span><br><span class="line">        StringBuilder result = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">        result.append(<span class="string">&quot;========================================\n&quot;</span>);</span><br><span class="line">        result.append(<span class="string">&quot;窗口结束时间：&quot;</span> + <span class="keyword">new</span> Timestamp(timestamp - <span class="number">1</span>) + <span class="string">&quot;\n&quot;</span>);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="keyword">this</span>.n; i++) &#123;</span><br><span class="line">            UrlViewCount UrlViewCount = urlViewCountArrayList.get(i);</span><br><span class="line">            String info = <span class="string">&quot;No.&quot;</span> + (i + <span class="number">1</span>) + <span class="string">&quot; &quot;</span></span><br><span class="line">                    + <span class="string">&quot;url：&quot;</span> + UrlViewCount.url + <span class="string">&quot; &quot;</span></span><br><span class="line">                    + <span class="string">&quot;浏览量：&quot;</span> + UrlViewCount.count + <span class="string">&quot;\n&quot;</span>;</span><br><span class="line">            result.append(info);</span><br><span class="line">        &#125;</span><br><span class="line">        result.append(<span class="string">&quot;========================================\n&quot;</span>);</span><br><span class="line">        out.collect(result.toString());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="ProcessAllWindowFunction"><a href="#ProcessAllWindowFunction" class="headerlink" title="ProcessAllWindowFunction"></a>ProcessAllWindowFunction</h3><p>同样是开窗之后的处理函数，基于 AllWindowedStream 调用.process()时作为参数传入。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Base abstract class for functions that are evaluated over non-keyed windows using a context for</span></span><br><span class="line"><span class="comment"> * retrieving extra information.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> &lt;IN&gt; The type of the input value.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> &lt;OUT&gt; The type of the output value.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> &lt;W&gt; The type of &#123;<span class="doctag">@code</span> Window&#125; that this window function can be applied on.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@PublicEvolving</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">ProcessAllWindowFunction</span>&lt;<span class="title">IN</span>, <span class="title">OUT</span>, <span class="title">W</span> <span class="keyword">extends</span> <span class="title">Window</span>&gt;</span></span><br><span class="line"><span class="class">        <span class="keyword">extends</span> <span class="title">AbstractRichFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Evaluates the window and outputs none or several elements.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context The context in which the window is being evaluated.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> elements The elements in the window being evaluated.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> out A collector for emitting elements.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception The function may throw exceptions to fail the program and trigger recovery.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Context context, Iterable&lt;IN&gt; elements, Collector&lt;OUT&gt; out)</span></span></span><br><span class="line"><span class="function">            <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Deletes any state in the &#123;<span class="doctag">@code</span> Context&#125; when the Window expires (the watermark passes its</span></span><br><span class="line"><span class="comment">     * &#123;<span class="doctag">@code</span> maxTimestamp&#125; + &#123;<span class="doctag">@code</span> allowedLateness&#125;).</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context The context to which the window is being evaluated</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception The function may throw exceptions to fail the program and trigger recovery.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">clear</span><span class="params">(Context context)</span> <span class="keyword">throws</span> Exception </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** The context holding window metadata. */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Context</span> </span>&#123;</span><br><span class="line">        <span class="comment">/** <span class="doctag">@return</span> The window that is being evaluated. */</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> W <span class="title">window</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * State accessor for per-key and per-window state.</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * &lt;p&gt;&lt;b&gt;<span class="doctag">NOTE:</span>&lt;/b&gt;If you use per-window state you have to ensure that you clean it up by</span></span><br><span class="line"><span class="comment">         * implementing &#123;<span class="doctag">@link</span> ProcessWindowFunction#clear(ProcessWindowFunction.Context)&#125;.</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> KeyedStateStore <span class="title">windowState</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/** State accessor for per-key global state. */</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> KeyedStateStore <span class="title">globalState</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * Emits a record to the side output identified by the &#123;<span class="doctag">@link</span> OutputTag&#125;.</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> outputTag the &#123;<span class="doctag">@code</span> OutputTag&#125; that identifies the side output to emit to.</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> value The record to emit.</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">abstract</span> &lt;X&gt; <span class="function"><span class="keyword">void</span> <span class="title">output</span><span class="params">(OutputTag&lt;X&gt; outputTag, X value)</span></span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="ProcessAllWindowTopN"><a href="#ProcessAllWindowTopN" class="headerlink" title="ProcessAllWindowTopN"></a>ProcessAllWindowTopN</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//ProcessAllWindowTopN</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    SingleOutputStreamOperator&lt;Event&gt; eventStream = env.addSource(<span class="keyword">new</span> ClickSource())</span><br><span class="line">            .assignTimestampsAndWatermarks(</span><br><span class="line">                    WatermarkStrategy.&lt;Event&gt;forMonotonousTimestamps()</span><br><span class="line">                            .withTimestampAssigner(<span class="keyword">new</span> SerializableTimestampAssigner&lt;Event&gt;() &#123;</span><br><span class="line">                                <span class="meta">@Override</span></span><br><span class="line">                                <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Event element, <span class="keyword">long</span> recordTimestamp)</span> </span>&#123;</span><br><span class="line">                                    <span class="keyword">return</span> element.timestamp;</span><br><span class="line">                                &#125;</span><br><span class="line">                            &#125;)</span><br><span class="line">            );</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 只需要url就可以统计数量，所以转换成String直接开窗统计</span></span><br><span class="line">    SingleOutputStreamOperator&lt;String&gt; result = eventStream</span><br><span class="line">            .map(<span class="keyword">new</span> MapFunction&lt;Event, String&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(Event value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                    <span class="keyword">return</span> value.url;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;)</span><br><span class="line">            .windowAll(SlidingEventTimeWindows.of(Time.seconds(<span class="number">10</span>), Time.seconds(<span class="number">5</span>)))    <span class="comment">// 开滑动窗口</span></span><br><span class="line">            .process(<span class="keyword">new</span> ProcessAllWindowFunction&lt;String, String, TimeWindow&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Context context, Iterable&lt;String&gt; elements, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                    HashMap&lt;String, Long&gt; urlCountMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">                    <span class="comment">// 遍历窗口中数据，将浏览量保存到一个 HashMap 中</span></span><br><span class="line">                    <span class="keyword">for</span> (String url : elements) &#123;</span><br><span class="line">                        <span class="keyword">if</span> (urlCountMap.containsKey(url)) &#123;</span><br><span class="line">                            <span class="keyword">long</span> count = urlCountMap.get(url);</span><br><span class="line">                            urlCountMap.put(url, count + <span class="number">1L</span>);</span><br><span class="line">                        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                            urlCountMap.put(url, <span class="number">1L</span>);</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                    ArrayList&lt;Tuple2&lt;String, Long&gt;&gt; mapList = <span class="keyword">new</span> ArrayList&lt;Tuple2&lt;String, Long&gt;&gt;();</span><br><span class="line">                    <span class="comment">// 将浏览量数据放入ArrayList，进行排序</span></span><br><span class="line">                    <span class="keyword">for</span> (String key : urlCountMap.keySet()) &#123;</span><br><span class="line">                        mapList.add(Tuple2.of(key, urlCountMap.get(key)));</span><br><span class="line">                    &#125;</span><br><span class="line">                    mapList.sort(<span class="keyword">new</span> Comparator&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">                        <span class="meta">@Override</span></span><br><span class="line">                        <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(Tuple2&lt;String, Long&gt; o1, Tuple2&lt;String, Long&gt; o2)</span> </span>&#123;</span><br><span class="line">                            <span class="keyword">return</span> o2.f1.intValue() - o1.f1.intValue();</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;);</span><br><span class="line">                    <span class="comment">// 取排序后的前两名，构建输出结果</span></span><br><span class="line">                    StringBuilder result = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">                    result.append(<span class="string">&quot;========================================\n&quot;</span>);</span><br><span class="line">                    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">2</span>; i++) &#123;</span><br><span class="line">                        Tuple2&lt;String, Long&gt; temp = mapList.get(i);</span><br><span class="line">                        String info = <span class="string">&quot;浏览量No.&quot;</span> + (i + <span class="number">1</span>) +</span><br><span class="line">                                <span class="string">&quot; url：&quot;</span> + temp.f0 +</span><br><span class="line">                                <span class="string">&quot; 浏览量：&quot;</span> + temp.f1 +</span><br><span class="line">                                <span class="string">&quot; 窗口结束时间：&quot;</span> + <span class="keyword">new</span> Timestamp(context.window().getEnd()) + <span class="string">&quot;\n&quot;</span>;</span><br><span class="line"></span><br><span class="line">                        result.append(info);</span><br><span class="line">                    &#125;</span><br><span class="line">                    result.append(<span class="string">&quot;========================================\n&quot;</span>);</span><br><span class="line">                    out.collect(result.toString());</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line"></span><br><span class="line">    result.print();</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="CoProcessFunction"><a href="#CoProcessFunction" class="headerlink" title="CoProcessFunction"></a>CoProcessFunction</h3><p>合并(connect)两条流之后的处理函数，基于 ConnectedStreams 调用.process()时作为参数传入。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">CoProcessFunction</span>&lt;<span class="title">IN1</span>, <span class="title">IN2</span>, <span class="title">OUT</span>&gt; <span class="keyword">extends</span> <span class="title">AbstractRichFunction</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">processElement1</span><span class="params">(IN1 value, Context ctx, Collector&lt;OUT&gt; out)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">processElement2</span><span class="params">(IN2 value, Context ctx, Collector&lt;OUT&gt; out)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onTimer</span><span class="params">(<span class="keyword">long</span> timestamp, OnTimerContext ctx, Collector&lt;OUT&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Context</span> </span>&#123;...&#125;</span><br><span class="line">    </span><br><span class="line">    ... </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>它需要实现的就是 processElement1()、processElement2()两个方法，在每个数据到来时，会根据来源的流调用其中的一个方法进行处理。CoProcessFunction同样可以通过上下文ctx来访问 timestamp、水位线，并通过 TimerService 注册定时器；另外也提供了.onTimer()方法，用于定义定时触发的处理操作。</p>
<h4 id="实时对账"><a href="#实时对账" class="headerlink" title="实时对账"></a>实时对账</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BillCheckExample</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 来自app的支付日志</span></span><br><span class="line">        SingleOutputStreamOperator&lt;Tuple3&lt;String, String, Long&gt;&gt; appStream = env.fromElements(</span><br><span class="line">                Tuple3.of(<span class="string">&quot;order-1&quot;</span>, <span class="string">&quot;app&quot;</span>, <span class="number">1000L</span>),</span><br><span class="line">                Tuple3.of(<span class="string">&quot;order-2&quot;</span>, <span class="string">&quot;app&quot;</span>, <span class="number">2000L</span>)</span><br><span class="line">        ).assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Tuple3&lt;String, String, Long&gt;&gt;forMonotonousTimestamps()</span><br><span class="line">                .withTimestampAssigner(<span class="keyword">new</span> SerializableTimestampAssigner&lt;Tuple3&lt;String, String, Long&gt;&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Tuple3&lt;String, String, Long&gt; element, <span class="keyword">long</span> recordTimestamp)</span> </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> element.f2;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 来自第三方支付平台的支付日志</span></span><br><span class="line">        SingleOutputStreamOperator&lt;Tuple4&lt;String, String, String, Long&gt;&gt; thirdpartStream = env.fromElements(</span><br><span class="line">                Tuple4.of(<span class="string">&quot;order-1&quot;</span>, <span class="string">&quot;third-party&quot;</span>, <span class="string">&quot;success&quot;</span>, <span class="number">3000L</span>),</span><br><span class="line">                Tuple4.of(<span class="string">&quot;order-3&quot;</span>, <span class="string">&quot;third-party&quot;</span>, <span class="string">&quot;success&quot;</span>, <span class="number">4000L</span>)</span><br><span class="line">        ).assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Tuple4&lt;String, String, String, Long&gt;&gt;forMonotonousTimestamps()</span><br><span class="line">                .withTimestampAssigner(<span class="keyword">new</span> SerializableTimestampAssigner&lt;Tuple4&lt;String, String, String, Long&gt;&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Tuple4&lt;String, String, String, Long&gt; element, <span class="keyword">long</span> recordTimestamp)</span> </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> element.f3;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 检测同一支付单在两条流中是否匹配，不匹配就报警</span></span><br><span class="line">        appStream.connect(thirdpartStream)</span><br><span class="line">                .keyBy(data -&gt; data.f0, data -&gt; data.f0)</span><br><span class="line">                .process(<span class="keyword">new</span> OrderMatchResult())</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 自定义实现CoProcessFunction</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderMatchResult</span> <span class="keyword">extends</span> <span class="title">CoProcessFunction</span>&lt;<span class="title">Tuple3</span>&lt;<span class="title">String</span>, <span class="title">String</span>, <span class="title">Long</span>&gt;, <span class="title">Tuple4</span>&lt;<span class="title">String</span>, <span class="title">String</span>, <span class="title">String</span>, <span class="title">Long</span>&gt;, <span class="title">String</span>&gt;</span>&#123;</span><br><span class="line">        <span class="comment">// 定义状态变量，用来保存已经到达的事件</span></span><br><span class="line">        <span class="keyword">private</span> ValueState&lt;Tuple3&lt;String, String, Long&gt;&gt; appEventState;</span><br><span class="line">        <span class="keyword">private</span> ValueState&lt;Tuple4&lt;String, String, String, Long&gt;&gt; thirdPartyEventState;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            appEventState = getRuntimeContext().getState(</span><br><span class="line">                    <span class="keyword">new</span> ValueStateDescriptor&lt;Tuple3&lt;String, String, Long&gt;&gt;(<span class="string">&quot;app-event&quot;</span>, Types.TUPLE(Types.STRING, Types.STRING, Types.LONG))</span><br><span class="line">            );</span><br><span class="line"></span><br><span class="line">            thirdPartyEventState = getRuntimeContext().getState(</span><br><span class="line">                    <span class="keyword">new</span> ValueStateDescriptor&lt;Tuple4&lt;String, String, String, Long&gt;&gt;(<span class="string">&quot;thirdparty-event&quot;</span>, Types.TUPLE(Types.STRING, Types.STRING, Types.STRING,Types.LONG))</span><br><span class="line">            );</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement1</span><span class="params">(Tuple3&lt;String, String, Long&gt; value, Context ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="comment">// 看另一条流中事件是否来过</span></span><br><span class="line">            <span class="keyword">if</span> (thirdPartyEventState.value() != <span class="keyword">null</span>)&#123;</span><br><span class="line">                out.collect(<span class="string">&quot;对账成功：&quot;</span> + value + <span class="string">&quot;  &quot;</span> + thirdPartyEventState.value());</span><br><span class="line">                <span class="comment">// 清空状态</span></span><br><span class="line">                thirdPartyEventState.clear();</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// 更新状态</span></span><br><span class="line">                appEventState.update(value);</span><br><span class="line">                <span class="comment">// 注册一个5秒后的定时器，开始等待另一条流的事件</span></span><br><span class="line">                ctx.timerService().registerEventTimeTimer(value.f2 + <span class="number">5000L</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement2</span><span class="params">(Tuple4&lt;String, String, String, Long&gt; value, Context ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">if</span> (appEventState.value() != <span class="keyword">null</span>)&#123;</span><br><span class="line">                out.collect(<span class="string">&quot;对账成功：&quot;</span> + appEventState.value() + <span class="string">&quot;  &quot;</span> + value);</span><br><span class="line">                <span class="comment">// 清空状态</span></span><br><span class="line">                appEventState.clear();</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// 更新状态</span></span><br><span class="line">                thirdPartyEventState.update(value);</span><br><span class="line">                <span class="comment">// 注册一个5秒后的定时器，开始等待另一条流的事件</span></span><br><span class="line">                ctx.timerService().registerEventTimeTimer(value.f3 + <span class="number">5000L</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onTimer</span><span class="params">(<span class="keyword">long</span> timestamp, OnTimerContext ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="comment">// 定时器触发，判断状态，如果某个状态不为空，说明另一条流中事件没来</span></span><br><span class="line">            <span class="keyword">if</span> (appEventState.value() != <span class="keyword">null</span>) &#123;</span><br><span class="line">                out.collect(<span class="string">&quot;对账失败：&quot;</span> + appEventState.value() + <span class="string">&quot;  &quot;</span> + <span class="string">&quot;第三方支付平台信息未到&quot;</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (thirdPartyEventState.value() != <span class="keyword">null</span>) &#123;</span><br><span class="line">                out.collect(<span class="string">&quot;对账失败：&quot;</span> + thirdPartyEventState.value() + <span class="string">&quot;  &quot;</span> + <span class="string">&quot;app信息未到&quot;</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            appEventState.clear();</span><br><span class="line">            thirdPartyEventState.clear();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="ProcessJoinFunction"><a href="#ProcessJoinFunction" class="headerlink" title="ProcessJoinFunction"></a>ProcessJoinFunction</h3><p>间隔连接(interval join)两条流之后的处理函数，基于 IntervalJoinedStream 调用.process()时作为参数传入。</p>
<h3 id="BroadcastProcessFunction"><a href="#BroadcastProcessFunction" class="headerlink" title="BroadcastProcessFunction"></a>BroadcastProcessFunction</h3><p>广播连接流处理函数，基于 BroadcastConnectedStream 调用.process()时作为参数传入。这里的广播连接流BroadcastConnectedStream，是一个未 keyBy 的普通 DataStream 与一个广播流(BroadcastStream)做连接(conncet)之后的产物。</p>
<p>这种连接方式往往用在需要动态定义某些规则或配置的场景。因为规则是实时变动的，所以我们可以用一个单独的流来获取规则数据；而这些规则或配置是对整个应用全局有效的，所以不能只把这数据传递给一个下游并行子任务处理，而是要广播(broadcast)给所有的并行子任务。而下游子任务收到广播出来的规则，会把它保存成一个状态，这就是所谓的广播状态(broadcast state)。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">BroadcastProcessFunction</span>&lt;<span class="title">IN1</span>, <span class="title">IN2</span>, <span class="title">OUT</span>&gt; <span class="keyword">extends</span> <span class="title">BaseBroadcastProcessFunction</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(IN1 value, ReadOnlyContext ctx, Collector&lt;OUT&gt; out)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">processBroadcastElement</span><span class="params">(IN2 value, Context ctx, Collector&lt;OUT&gt; out)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">    </span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="KeyedBroadcastProcessFunction"><a href="#KeyedBroadcastProcessFunction" class="headerlink" title="KeyedBroadcastProcessFunction"></a>KeyedBroadcastProcessFunction</h3><p>按键分区的广播连接流处理函数，同样是基于 BroadcastConnectedStream 调用.process()时作为参数传入。与 BroadcastProcessFunction 不同的是，这时的广播连接流，是一个 KeyedStream 与广播流(BroadcastStream)做连接之后的产物。</p>
<h3 id="侧输出流-Side-Output"><a href="#侧输出流-Side-Output" class="headerlink" title="侧输出流(Side Output)"></a>侧输出流(Side Output)</h3><p>处理函数还有另外一个特有功能，就是将自定义的数据放入侧输出流(side output)输出。具体应用时，只要在处理函数的.processElement()或者.onTimer()方法中，调用上下文的.output()方法就可以了。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataStream&lt;Integer&gt; stream = env.addSource(...);</span><br><span class="line"></span><br><span class="line">SingleOutputStreamOperator&lt;Long&gt; longStream = stream.process(<span class="keyword">new</span> ProcessFunction&lt;Integer, Long&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">( Integer value, Context ctx, Collector&lt;Integer&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// 转换成 Long，输出到主流中</span></span><br><span class="line">        out.collect(Long.valueOf(value));</span><br><span class="line">        <span class="comment">// 转换成 String，输出到侧输出流中</span></span><br><span class="line">        ctx.output(outputTag, <span class="string">&quot;side-output: &quot;</span> + String.valueOf(value));</span><br><span class="line">    &#125; </span><br><span class="line">  &#125;</span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<p>这里 output()方法需要传入两个参数，第一个是一个输出标签OutputTag，用来标识侧输出流，一般会在外部统一声明；第二个就是要输出的数据。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//部先将 OutputTag 声明出来</span></span><br><span class="line">OutputTag&lt;String&gt; outputTag = <span class="keyword">new</span> OutputTag&lt;String&gt;(<span class="string">&quot;side-output&quot;</span>) &#123;&#125;;</span><br></pre></td></tr></table></figure>

<p>如果想要获取这个侧输出流，可以基于处理之后的 DataStream 直接调用.getSideOutput() 方法，传入对应的 OutputTag。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataStream&lt;String&gt; stringStream = longStream.getSideOutput(outputTag);</span><br></pre></td></tr></table></figure>

<h2 id="多流转换"><a href="#多流转换" class="headerlink" title="多流转换"></a>多流转换</h2><p>多流转换可以分为分流和<strong>合流</strong>两大类。目前分流的操作一般是通过侧输出流(side output)来实现，而合流的算子比较丰富，根据不同的需求可以调用 union、 connect、join 以及 coGroup 等接口进行连接合并操作。</p>
<h3 id="分流"><a href="#分流" class="headerlink" title="分流"></a>分流</h3><p>分流就是将一条数据流拆分成完全独立的两条、甚至多条流。也就是基于一个 DataStream，得到完全平等的多个子 DataStream。</p>
<h4 id="filter实现"><a href="#filter实现" class="headerlink" title="filter实现"></a>filter实现</h4><p>只要针对同一条流多次独立调用.filter()方法进行筛选，就可以得到拆分之后的流了。</p>
<p>例如，我们可以将电商网站收集到的用户行为数据进行一个拆分，根据类型(type)的不同，分为Mary的浏览数据、Bob的浏览数据等等。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    SingleOutputStreamOperator&lt;Event&gt; stream = env</span><br><span class="line">            .addSource(<span class="keyword">new</span> ClickSource());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 筛选Mary的浏览行为放入MaryStream流中</span></span><br><span class="line">    DataStream&lt;Event&gt; MaryStream = stream.filter(<span class="keyword">new</span> FilterFunction&lt;Event&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(Event value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> value.user.equals(<span class="string">&quot;Mary&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 筛选Bob的购买行为放入BobStream流中</span></span><br><span class="line">    DataStream&lt;Event&gt; BobStream = stream.filter(<span class="keyword">new</span> FilterFunction&lt;Event&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(Event value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> value.user.equals(<span class="string">&quot;Bob&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 筛选其他人的浏览行为放入elseStream流中</span></span><br><span class="line">    DataStream&lt;Event&gt; elseStream = stream.filter(<span class="keyword">new</span> FilterFunction&lt;Event&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(Event value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> !value.user.equals(<span class="string">&quot;Mary&quot;</span>) &amp;&amp; !value.user.equals(<span class="string">&quot;Bob&quot;</span>) ;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    MaryStream.print(<span class="string">&quot;Mary pv&quot;</span>);</span><br><span class="line">    BobStream.print(<span class="string">&quot;Bob pv&quot;</span>);</span><br><span class="line">    elseStream.print(<span class="string">&quot;else pv&quot;</span>);</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这段代码背后的含义，是将原始数据流 stream 复制三份，然后对每一份分别做筛选；这明显是不够高效的。我们自然想到，能不能不用复制流，直接用一个算子就把它们都拆分开呢?</p>
<h4 id="侧输出流-1"><a href="#侧输出流-1" class="headerlink" title="侧输出流"></a>侧输出流</h4><p>处理函数本身可以认为是一个转换算子，它的输出类型是单一的，处理之后得到的仍然是一个 DataStream；而侧输出流则不受限制，可以任意自定义输出数据，它们就像从主流上分叉出的支流。尽管看起来主流和支流有所区别，不过实际上它们都是某种类型的 DataStream，所以本质上还是平等的。利用侧输出流就可以很方便地实现分流操作，而且得到的多条 DataStream <strong>类型可以不同</strong>，这就给我们的应用带来了极大的便利。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SplitStreamByOutputTag</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 定义输出标签，侧输出流的数据类型为三元组(user, url, timestamp)</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> OutputTag&lt;Tuple3&lt;String, String, Long&gt;&gt; MaryTag = <span class="keyword">new</span> OutputTag&lt;Tuple3&lt;String, String, Long&gt;&gt;(<span class="string">&quot;Mary-pv&quot;</span>)&#123;&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> OutputTag&lt;Tuple3&lt;String, String, Long&gt;&gt; BobTag = <span class="keyword">new</span> OutputTag&lt;Tuple3&lt;String, String, Long&gt;&gt;(<span class="string">&quot;Bob-pv&quot;</span>)&#123;&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;Event&gt; stream = env</span><br><span class="line">                .addSource(<span class="keyword">new</span> ClickSource());</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;Event&gt; processedStream = stream.process(<span class="keyword">new</span> ProcessFunction&lt;Event, Event&gt;() &#123;</span><br><span class="line">          </span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(Event value, Context ctx, Collector&lt;Event&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">if</span> (value.user.equals(<span class="string">&quot;Mary&quot;</span>))&#123;</span><br><span class="line">                    ctx.output(MaryTag, <span class="keyword">new</span> Tuple3&lt;&gt;(value.user, value.url, value.timestamp));</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (value.user.equals(<span class="string">&quot;Bob&quot;</span>))&#123;</span><br><span class="line">                    ctx.output(BobTag, <span class="keyword">new</span> Tuple3&lt;&gt;(value.user, value.url, value.timestamp));</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    out.collect(value);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        processedStream.getSideOutput(MaryTag).print(<span class="string">&quot;Mary pv&quot;</span>);</span><br><span class="line">        processedStream.getSideOutput(BobTag).print(<span class="string">&quot;Bob pv&quot;</span>);</span><br><span class="line">        processedStream.print(<span class="string">&quot;else&quot;</span>);</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="合流"><a href="#合流" class="headerlink" title="合流"></a>合流</h3><h4 id="联合-Union"><a href="#联合-Union" class="headerlink" title="联合(Union)"></a>联合(Union)</h4><p>联合操作要求必须流中的数据类型必须相同，合并之后的新流会包括所有流中的元素，数据类型不变。这种合流方式非常简单粗暴，就像公路上多个车道汇在一起一样。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//union()的参数可以是多个 DataStream，所以联合操作可以实现多条流的合并</span></span><br><span class="line">stream1.union(stream2, stream3, ...)</span><br></pre></td></tr></table></figure>

<p>注意，对于合流之后的水位线，也是要以最小的那个为准，这样才可以保证所有流都不会再传来之前的数据。换句话说，多流合并时处理的时效性是以最慢的那个流为准的。我们自然可以想到，这与之前介绍的并行任务水位线传递的规则是完全一致的；多条流的合并，某种意义上也可以看作是多个并行任务向同一个下游任务汇合的过程。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    SingleOutputStreamOperator&lt;Event&gt; stream1 = env.socketTextStream(<span class="string">&quot;hadoop102&quot;</span>, <span class="number">7777</span>)</span><br><span class="line">            .map(data -&gt; &#123;</span><br><span class="line">                String[] field = data.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Event(field[<span class="number">0</span>].trim(), field[<span class="number">1</span>].trim(), Long.valueOf(field[<span class="number">2</span>].trim()));</span><br><span class="line">            &#125;)</span><br><span class="line">            .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Event&gt;forBoundedOutOfOrderness(Duration.ofSeconds(<span class="number">2</span>))</span><br><span class="line">                    .withTimestampAssigner(<span class="keyword">new</span> SerializableTimestampAssigner&lt;Event&gt;() &#123;</span><br><span class="line">                        <span class="meta">@Override</span></span><br><span class="line">                        <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Event element, <span class="keyword">long</span> recordTimestamp)</span> </span>&#123;</span><br><span class="line">                            <span class="keyword">return</span> element.timestamp;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;)</span><br><span class="line">            );</span><br><span class="line"></span><br><span class="line">    stream1.print(<span class="string">&quot;stream1&quot;</span>);</span><br><span class="line"></span><br><span class="line">    SingleOutputStreamOperator&lt;Event&gt; stream2 = env.socketTextStream(<span class="string">&quot;hadoop103&quot;</span>, <span class="number">7777</span>)</span><br><span class="line">            .map(data -&gt; &#123;</span><br><span class="line">                String[] field = data.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Event(field[<span class="number">0</span>].trim(), field[<span class="number">1</span>].trim(), Long.valueOf(field[<span class="number">2</span>].trim()));</span><br><span class="line">            &#125;)</span><br><span class="line">            .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Event&gt;forBoundedOutOfOrderness(Duration.ofSeconds(<span class="number">5</span>))</span><br><span class="line">                    .withTimestampAssigner(<span class="keyword">new</span> SerializableTimestampAssigner&lt;Event&gt;() &#123;</span><br><span class="line">                        <span class="meta">@Override</span></span><br><span class="line">                        <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Event element, <span class="keyword">long</span> recordTimestamp)</span> </span>&#123;</span><br><span class="line">                            <span class="keyword">return</span> element.timestamp;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;)</span><br><span class="line">            );</span><br><span class="line"></span><br><span class="line">    stream2.print(<span class="string">&quot;stream2&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 合并两条流</span></span><br><span class="line">    stream1.union(stream2)</span><br><span class="line">            .process(<span class="keyword">new</span> ProcessFunction&lt;Event, String&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(Event value, Context ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                    out.collect(<span class="string">&quot;水位线：&quot;</span> + ctx.timerService().currentWatermark());</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;)</span><br><span class="line">            .print();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="连接-Connect"><a href="#连接-Connect" class="headerlink" title="连接(Connect)"></a>连接(Connect)</h4><p><img src="/Flink/%E6%88%AA%E5%B1%8F2022-03-25%20%E4%B8%8A%E5%8D%8812.08.27.png" alt="截屏2022-03-25 上午12.08.27"></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    </span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    DataStream&lt;Integer&gt; stream1 = env.fromElements(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>);</span><br><span class="line">    DataStream&lt;Long&gt; stream2 = env.fromElements(<span class="number">1L</span>,<span class="number">2L</span>,<span class="number">3L</span>);</span><br><span class="line"></span><br><span class="line">    ConnectedStreams&lt;Integer, Long&gt; connectedStreams = stream1.connect(stream2);</span><br><span class="line">    </span><br><span class="line">    SingleOutputStreamOperator&lt;String&gt; result = connectedStreams.map(<span class="keyword">new</span> CoMapFunction&lt;Integer, Long, String&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">map1</span><span class="params">(Integer value)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&quot;Integer: &quot;</span> + value;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">map2</span><span class="params">(Long value)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&quot;Long: &quot;</span> + value;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    result.print();</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这里我们将一条 Integer 流和一条 Long 流合并，转换成 String 输出。所以当遇到第一条流输入的整型值时，调用.map1()；而遇到第二条流输入的长整型数据时，调用.map2()：最终都转换为字符串输出，合并成了一条字符串流。</p>
<p>值得一提的是，ConnectedStreams 也可以直接调用.keyBy()进行按键分区的操作，得到的还是一个 ConnectedStreams：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">connectedStreams.keyBy(keySelector1, keySelector2);</span><br></pre></td></tr></table></figure>

<p>这里传入两个参数 keySelector1 和 keySelector2，是两条流中各自的键选择器；当然也可以直接传入键的位置值(keyPosition)，或者键的字段名(field)，这与普通的 keyBy 用法完全 一致。ConnectedStreams 进行 keyBy 操作，其实就是把两条流中 key 相同的数据放到了一起，然后针对来源的流再做各自处理，这在一些场景下非常有用。另外，我们也可以在合并之前就将两条流分别进行 keyBy，得到的 KeyedStream 再进行连接(connect)操作，效果是一样的。 要注意两条流定义的键的类型必须相同，否则会抛出异常。</p>
<h4 id="双流联结-Join"><a href="#双流联结-Join" class="headerlink" title="双流联结(Join)"></a>双流联结(Join)</h4><h5 id="窗口联结-Window-Join"><a href="#窗口联结-Window-Join" class="headerlink" title="窗口联结(Window Join)"></a>窗口联结(Window Join)</h5><p>可以定义时间窗口，并将两条流中共享一个公共键(key)的数据放在窗口中进行配对处理。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">stream1.join(stream2)</span><br><span class="line">       .where(&lt;KeySelector&gt;)</span><br><span class="line">       .equalTo(&lt;KeySelector&gt;)</span><br><span class="line">       .window(&lt;WindowAssigner&gt;)</span><br><span class="line">       .apply(&lt;JoinFunction&gt;)</span><br></pre></td></tr></table></figure>

<p>上面代码中.where()的参数是键选择器(KeySelector)，用来指定第一条流中的 key；而.equalTo()传入的 KeySelector 则指定了第二条流中的 key。两者相同的元素，如果在同一窗口中，就可以匹配起来，并通过一个联结函数(JoinFunction)进行处理了。</p>
<p>传入的 JoinFunction 也是一个函数类接口，使用时需要实现内部的.join()方法。这个方法有两个参数，分别表示两条流中成对匹配的数据。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">JoinFunction</span>&lt;<span class="title">IN1</span>, <span class="title">IN2</span>, <span class="title">OUT</span>&gt; <span class="keyword">extends</span> <span class="title">Function</span>, <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">   <span class="function">OUT <span class="title">join</span><span class="params">(IN1 first, IN2 second)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="/Flink/%E6%88%AA%E5%B1%8F2022-03-25%20%E4%B8%8A%E5%8D%889.56.19.png" alt="截屏2022-03-25 上午9.56.19"></p>
<p>除了 JoinFunction，在.apply()方法中还可以传入 FlatJoinFunction，用法非常类似，只是内部需要实现的.join()方法没有返回值。结果的输出是通过收集器(Collector)来实现的，所以对于一对匹配数据可以输出任意条结果。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    DataStream&lt;Tuple2&lt;String, Long&gt;&gt; stream1 = env</span><br><span class="line">            .fromElements(</span><br><span class="line">                    Tuple2.of(<span class="string">&quot;a&quot;</span>, <span class="number">1000L</span>),</span><br><span class="line">                    Tuple2.of(<span class="string">&quot;b&quot;</span>, <span class="number">1000L</span>),</span><br><span class="line">                    Tuple2.of(<span class="string">&quot;a&quot;</span>, <span class="number">2000L</span>),</span><br><span class="line">                    Tuple2.of(<span class="string">&quot;b&quot;</span>, <span class="number">2000L</span>)</span><br><span class="line">            )</span><br><span class="line">            .assignTimestampsAndWatermarks(</span><br><span class="line">                    WatermarkStrategy</span><br><span class="line">                            .&lt;Tuple2&lt;String, Long&gt;&gt;forMonotonousTimestamps()</span><br><span class="line">                            .withTimestampAssigner(</span><br><span class="line">                                    <span class="keyword">new</span> SerializableTimestampAssigner&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">                                        <span class="meta">@Override</span></span><br><span class="line">                                        <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Tuple2&lt;String, Long&gt; stringLongTuple2, <span class="keyword">long</span> l)</span> </span>&#123;</span><br><span class="line">                                            <span class="keyword">return</span> stringLongTuple2.f1;</span><br><span class="line">                                        &#125;</span><br><span class="line">                                    &#125;</span><br><span class="line">                            )</span><br><span class="line">            );</span><br><span class="line"></span><br><span class="line">    DataStream&lt;Tuple2&lt;String, Long&gt;&gt; stream2 = env</span><br><span class="line">            .fromElements(</span><br><span class="line">                    Tuple2.of(<span class="string">&quot;a&quot;</span>, <span class="number">3000L</span>),</span><br><span class="line">                    Tuple2.of(<span class="string">&quot;b&quot;</span>, <span class="number">3000L</span>),</span><br><span class="line">                    Tuple2.of(<span class="string">&quot;a&quot;</span>, <span class="number">4000L</span>),</span><br><span class="line">                    Tuple2.of(<span class="string">&quot;b&quot;</span>, <span class="number">4000L</span>)</span><br><span class="line">            )</span><br><span class="line">            .assignTimestampsAndWatermarks(</span><br><span class="line">                    WatermarkStrategy</span><br><span class="line">                            .&lt;Tuple2&lt;String, Long&gt;&gt;forMonotonousTimestamps()</span><br><span class="line">                            .withTimestampAssigner(</span><br><span class="line">                                    <span class="keyword">new</span> SerializableTimestampAssigner&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">                                        <span class="meta">@Override</span></span><br><span class="line">                                        <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Tuple2&lt;String, Long&gt; stringLongTuple2, <span class="keyword">long</span> l)</span> </span>&#123;</span><br><span class="line">                                            <span class="keyword">return</span> stringLongTuple2.f1;</span><br><span class="line">                                        &#125;</span><br><span class="line">                                    &#125;</span><br><span class="line">                            )</span><br><span class="line">            );</span><br><span class="line"></span><br><span class="line">    stream1</span><br><span class="line">            .join(stream2)</span><br><span class="line">            .where(r -&gt; r.f0)</span><br><span class="line">            .equalTo(r -&gt; r.f0)</span><br><span class="line">            .window(TumblingEventTimeWindows.of(Time.seconds(<span class="number">5</span>)))</span><br><span class="line">            .apply(<span class="keyword">new</span> JoinFunction&lt;Tuple2&lt;String, Long&gt;, Tuple2&lt;String, Long&gt;, String&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> String <span class="title">join</span><span class="params">(Tuple2&lt;String, Long&gt; left, Tuple2&lt;String, Long&gt; right)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                    <span class="keyword">return</span> left + <span class="string">&quot;=&gt;&quot;</span> + right;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;)</span><br><span class="line">            .print();</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="间隔联结-Interval-Join"><a href="#间隔联结-Interval-Join" class="headerlink" title="间隔联结(Interval Join)"></a>间隔联结(Interval Join)</h5><p>在有些场景下，我们要处理的时间间隔可能并不是固定的。比如，在交易系统中，需要实时地对每一笔交易进行核验，保证两个账户转入转出数额相等，也就是所谓的实时对账。 两次转账的数据可能写入了不同的日志流，它们的时间戳应该相差不大，所以我们可以考虑只统计一段时间内是否有出账入账的数据匹配。这时显然不应该用滚动窗口或滑动窗口来处理——因为匹配的两个数据有可能刚好卡在窗口边缘两侧，于是窗口内就都没有匹配了；会话窗口虽然时间不固定，但也明显不适合这个场景。 基于时间的窗口联结已经无能为力了。</p>
<p>为了应对这样的需求，Flink 提供了一种叫作间隔联结(interval join)的合流操作。顾名思义，间隔联结的思路就是针对一条流的每个数据，<strong>开辟出其时间戳前后的一段时间间隔</strong>，看这期间是否有来自另一条流的数据匹配。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">stream1</span><br><span class="line">   .keyBy(&lt;KeySelector&gt;)</span><br><span class="line">   .intervalJoin(stream2.keyBy(&lt;KeySelector&gt;))</span><br><span class="line">   .between(Time.milliseconds(-<span class="number">2</span>), Time.milliseconds(<span class="number">1</span>))</span><br><span class="line">   .process (<span class="keyword">new</span> ProcessJoinFunction&lt;Integer, Integer, String()&#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(Integer left, Integer right, Context ctx, Collector&lt;String&gt; out)</span> </span>&#123;</span><br><span class="line">                              out.collect(left + <span class="string">&quot;,&quot;</span> + right);</span><br><span class="line">                           </span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">    );</span><br></pre></td></tr></table></figure>

<p>抽象类 ProcessJoinFunction 就像是 ProcessFunction 和 JoinFunction 的结合，内部同样有一个抽象方法.processElement()。与其他处理函数不同的是，它多了一个参数，这自然是因为有来自两条流的数据。参数中 left 指的就是第一条流中的数据，right 则是第二条流中与它匹配的数据。每当检测到一组匹配，就会调用这里的.processElement()方法，经处理转换之后输出结果。</p>
<p>举一个例子，我们有两条流，一条是下订单的流，一条是浏览数据的流。我们可以针对同一个用户，来做这样一个联结。也就是使用一个用户的下订单的事件和这个用户的最近十分钟的浏览数据进行一个联结查询：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    SingleOutputStreamOperator&lt;Tuple3&lt;String, String, Long&gt;&gt; orderStream = env.fromElements(</span><br><span class="line">            Tuple3.of(<span class="string">&quot;Mary&quot;</span>, <span class="string">&quot;order-1&quot;</span>, <span class="number">5000L</span>),</span><br><span class="line">            Tuple3.of(<span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;order-2&quot;</span>, <span class="number">5000L</span>),</span><br><span class="line">            Tuple3.of(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;order-3&quot;</span>, <span class="number">20000L</span>),</span><br><span class="line">            Tuple3.of(<span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;order-4&quot;</span>, <span class="number">20000L</span>),</span><br><span class="line">            Tuple3.of(<span class="string">&quot;Cary&quot;</span>, <span class="string">&quot;order-5&quot;</span>, <span class="number">51000L</span>)</span><br><span class="line">    ).assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Tuple3&lt;String, String, Long&gt;&gt;forMonotonousTimestamps()</span><br><span class="line">            .withTimestampAssigner(<span class="keyword">new</span> SerializableTimestampAssigner&lt;Tuple3&lt;String, String, Long&gt;&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Tuple3&lt;String, String, Long&gt; element, <span class="keyword">long</span> recordTimestamp)</span> </span>&#123;</span><br><span class="line">                    <span class="keyword">return</span> element.f2;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;)</span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">    SingleOutputStreamOperator&lt;Event&gt; clickStream = env.fromElements(</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;./cart&quot;</span>, <span class="number">2000L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;./prod?id=100&quot;</span>, <span class="number">3000L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;./prod?id=200&quot;</span>, <span class="number">3500L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;./prod?id=2&quot;</span>, <span class="number">2500L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;./prod?id=300&quot;</span>, <span class="number">36000L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;./home&quot;</span>, <span class="number">30000L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;./prod?id=1&quot;</span>, <span class="number">23000L</span>),</span><br><span class="line">            <span class="keyword">new</span> Event(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;./prod?id=3&quot;</span>, <span class="number">33000L</span>)</span><br><span class="line">    ).assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Event&gt;forMonotonousTimestamps()</span><br><span class="line">            .withTimestampAssigner(<span class="keyword">new</span> SerializableTimestampAssigner&lt;Event&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Event element, <span class="keyword">long</span> recordTimestamp)</span> </span>&#123;</span><br><span class="line">                    <span class="keyword">return</span> element.timestamp;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;)</span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">    orderStream.keyBy(data -&gt; data.f0)</span><br><span class="line">            .intervalJoin(clickStream.keyBy(data -&gt; data.user))</span><br><span class="line">            .between(Time.seconds(-<span class="number">5</span>), Time.seconds(<span class="number">10</span>))</span><br><span class="line">            .process(<span class="keyword">new</span> ProcessJoinFunction&lt;Tuple3&lt;String, String, Long&gt;, Event, String&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(Tuple3&lt;String, String, Long&gt; left, Event right, Context ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                    out.collect(right + <span class="string">&quot; =&gt; &quot;</span> + left);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;)</span><br><span class="line">            .print();</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="窗口同组联结-Window-CoGroup"><a href="#窗口同组联结-Window-CoGroup" class="headerlink" title="窗口同组联结(Window CoGroup)"></a>窗口同组联结(Window CoGroup)</h5><p>除窗口联结和间隔联结之外，Flink 还提供了一个窗口同组联结(window coGroup)操作。它的用法跟 window join 非常类似，也是将两条流合并之后开窗处理匹配的元素，调用时只需要将.join()换为.coGroup()就可以了。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">stream1.coGroup(stream2)</span><br><span class="line">   .where(&lt;KeySelector&gt;)</span><br><span class="line">   .equalTo(&lt;KeySelector&gt;)</span><br><span class="line">   .window(TumblingEventTimeWindows.of(Time.hours(<span class="number">1</span>)))</span><br><span class="line">   .apply(&lt;CoGroupFunction&gt;)</span><br></pre></td></tr></table></figure>

<p>与 window join 的区别在于，调用.apply()方法定义具体操作时，传入的是一个 CoGroupFunction。这也是一个函数类接口，源码中定义如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">CoGroupFunction</span>&lt;<span class="title">IN1</span>, <span class="title">IN2</span>, <span class="title">O</span>&gt; <span class="keyword">extends</span> <span class="title">Function</span>, <span class="title">Serializable</span> </span>&#123; </span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">coGroup</span><span class="params">(Iterable&lt;IN1&gt; first, Iterable&lt;IN2&gt; second, Collector&lt;O&gt; out)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>.coGroup()方法，有些类似于 FlatJoinFunction 中.join()的形式，同样有三个参数，分别代表两条流中的数据以及用于输出的收集器(Collector)。<strong>不同的是，这里的前两个参数不再是单独的每一组配对数据了，而是传入了可遍历的数据集合。也就是说，现在不会再去计算窗口中两条流数据集的笛卡尔积，而是直接把收集到的所有数据一次性传入，至于要怎样配对完全是自定义的。</strong>这样.coGroup()方法只会被调用一次，而且即使一条流的数据没有任何另一条流的数据匹配，也可以出现在集合中、当然也可以定义输出结果了。</p>
<p><strong>coGroup 操作比窗口的 join 更加通用，不仅可以实现类似 SQL 中的内连接(inner join)，也可以实现左外连接(left outer join)、右外连接(right outer join)和全外连接(full outer join)。事实上，窗口 join 的底层，也是通过 coGroup 来实现的。</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    DataStream&lt;Tuple2&lt;String, Long&gt;&gt; stream1 = env</span><br><span class="line">            .fromElements(</span><br><span class="line">                    Tuple2.of(<span class="string">&quot;a&quot;</span>, <span class="number">1000L</span>),</span><br><span class="line">                    Tuple2.of(<span class="string">&quot;b&quot;</span>, <span class="number">1000L</span>),</span><br><span class="line">                    Tuple2.of(<span class="string">&quot;a&quot;</span>, <span class="number">2000L</span>),</span><br><span class="line">                    Tuple2.of(<span class="string">&quot;b&quot;</span>, <span class="number">2000L</span>)</span><br><span class="line">            )</span><br><span class="line">            .assignTimestampsAndWatermarks(</span><br><span class="line">                    WatermarkStrategy</span><br><span class="line">                            .&lt;Tuple2&lt;String, Long&gt;&gt;forMonotonousTimestamps()</span><br><span class="line">                            .withTimestampAssigner(</span><br><span class="line">                                    <span class="keyword">new</span> SerializableTimestampAssigner&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">                                        <span class="meta">@Override</span></span><br><span class="line">                                        <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Tuple2&lt;String, Long&gt; stringLongTuple2, <span class="keyword">long</span> l)</span> </span>&#123;</span><br><span class="line">                                            <span class="keyword">return</span> stringLongTuple2.f1;</span><br><span class="line">                                        &#125;</span><br><span class="line">                                    &#125;</span><br><span class="line">                            )</span><br><span class="line">            );</span><br><span class="line"></span><br><span class="line">    DataStream&lt;Tuple2&lt;String, Long&gt;&gt; stream2 = env</span><br><span class="line">            .fromElements(</span><br><span class="line">                    Tuple2.of(<span class="string">&quot;a&quot;</span>, <span class="number">3000L</span>),</span><br><span class="line">                    Tuple2.of(<span class="string">&quot;b&quot;</span>, <span class="number">3000L</span>),</span><br><span class="line">                    Tuple2.of(<span class="string">&quot;a&quot;</span>, <span class="number">4000L</span>),</span><br><span class="line">                    Tuple2.of(<span class="string">&quot;b&quot;</span>, <span class="number">4000L</span>)</span><br><span class="line">            )</span><br><span class="line">            .assignTimestampsAndWatermarks(</span><br><span class="line">                    WatermarkStrategy</span><br><span class="line">                            .&lt;Tuple2&lt;String, Long&gt;&gt;forMonotonousTimestamps()</span><br><span class="line">                            .withTimestampAssigner(</span><br><span class="line">                                    <span class="keyword">new</span> SerializableTimestampAssigner&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">                                        <span class="meta">@Override</span></span><br><span class="line">                                        <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Tuple2&lt;String, Long&gt; stringLongTuple2, <span class="keyword">long</span> l)</span> </span>&#123;</span><br><span class="line">                                            <span class="keyword">return</span> stringLongTuple2.f1;</span><br><span class="line">                                        &#125;</span><br><span class="line">                                    &#125;</span><br><span class="line">                            )</span><br><span class="line">            );</span><br><span class="line"></span><br><span class="line">    stream1</span><br><span class="line">            .coGroup(stream2)</span><br><span class="line">            .where(r -&gt; r.f0)</span><br><span class="line">            .equalTo(r -&gt; r.f0)</span><br><span class="line">            .window(TumblingEventTimeWindows.of(Time.seconds(<span class="number">5</span>)))</span><br><span class="line">            .apply(<span class="keyword">new</span> CoGroupFunction&lt;Tuple2&lt;String, Long&gt;, Tuple2&lt;String, Long&gt;, String&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">coGroup</span><span class="params">(Iterable&lt;Tuple2&lt;String, Long&gt;&gt; iter1, Iterable&lt;Tuple2&lt;String, Long&gt;&gt; iter2, Collector&lt;String&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                    collector.collect(iter1 + <span class="string">&quot;=&gt;&quot;</span> + iter2);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;)</span><br><span class="line">            .print();</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight gcode"><table><tr><td class="code"><pre><span class="line">[<span class="comment">(a,1000)</span>, <span class="comment">(a,2000)</span>]=&gt;[<span class="comment">(a,3000)</span>, <span class="comment">(a,4000)</span>]</span><br><span class="line">[<span class="comment">(b,1000)</span>, <span class="comment">(b,2000)</span>]=&gt;[<span class="comment">(b,3000)</span>, <span class="comment">(b,4000)</span>]</span><br></pre></td></tr></table></figure>

<h2 id="State"><a href="#State" class="headerlink" title="State"></a>State</h2><p>有状态的算子任务，则除当前数据之外，还需要一些其他数据来得到计算结果。这里的其他数据，就是所谓的状态(state)，最常见的就是之前到达的数据，或者由之前数据计算出的某个结果。比如，做求和(sum)计算时，需要保存之前所有数据的和，这就是状态；窗口算子中会保存已经到达的所有数据，这些也都是它的状态。另外，如果我们希望检索到某种事件模式(event pattern)，比如先有下单行为，后有支付行为，那么也应该把之前的行为保存下来，这同样属于状态。容易发现，之前讲过的聚合算子、窗口算子都属于有状态的算子。</p>
<p>有状态算子的一般处理流程：(1)算子任务接收到上游发来的数据；(2)获取当前状态；(3)根据业务逻辑进行计算，更新状态；(4)得到计算结果，输出发送到下游任务。</p>
<p><img src="/Flink/%E6%88%AA%E5%B1%8F2022-03-25%20%E4%B8%8A%E5%8D%8811.24.56.png" alt="截屏2022-03-25 上午11.24.56"></p>
<table>
<thead>
<tr>
<th></th>
<th>ManagedState</th>
<th>RawState</th>
</tr>
</thead>
<tbody><tr>
<td>状态管理方式</td>
<td>Flink Runtime托管, 自动存储, 自动恢复, 自动伸缩</td>
<td>用户自己管理</td>
</tr>
<tr>
<td>状态数据结构</td>
<td>Flink提供多种常用数据结构, 例如:ListState, MapState等</td>
<td>字节数组: byte[]</td>
</tr>
<tr>
<td>使用场景</td>
<td>绝大数Flink算子</td>
<td>所有算子</td>
</tr>
</tbody></table>
<p>对Managed State继续细分，它又有两种类型：</p>
<p>a) Operator State(算子状态)</p>
<p>状态作用范围限定为当前的算子任务实例，也就是只对当前并行子任务实例有效。这就意味着对于一个并行子任务，占据了一个分区，它所处理的所有数据都会访问到相同的状态，状态对于同一任务而言是共享的。</p>
<p><img src="/Flink/%E6%88%AA%E5%B1%8F2022-03-25%20%E4%B8%8B%E5%8D%883.47.50.png" alt="截屏2022-03-25 下午3.47.50"></p>
<p>算子状态可以用在所有算子上，使用的时候其实就跟一个本地变量没什么区别——因为本地变量的作用域也是当前任务实例。在使用时，我们还需进一步实现 CheckpointedFunction 接口。</p>
<p>b) Keyed State(键控状态)</p>
<p>状态是根据输入流中定义的键(key)来维护和访问的，所以只能定义在按键分区流(KeyedStream)中，也就 keyBy 之后才可以使用。</p>
<p><img src="/Flink/%E6%88%AA%E5%B1%8F2022-03-25%20%E4%B8%8B%E5%8D%883.48.55.png" alt="截屏2022-03-25 下午3.48.55"></p>
<p>按键分区状态应用非常广泛。之前讲到的聚合算子必须在 keyBy 之后才能使用，就是因为聚合的结果是以Keyed State的形式保存的。另外，也可以通过富函数类(Rich Function) 来自定义 Keyed State，所以只要提供了富函数类接口的算子，也都可以使用 Keyed State。</p>
<p>所以即使是 map、filter 这样无状态的基本转换算子，我们也可以通过富函数类给它们追加 Keyed State，或者实现 CheckpointedFunction 接口来定义 Operator State；从这个角度讲， Flink 中所有的算子都可以是有状态的，不愧是有状态的流处理。</p>
<p>无论是 Keyed State 还是 Operator State，它们都是在本地实例上维护的，也就是说<strong>每个并行子任务维护着对应的状态</strong>，算子的<strong>子任务之间状态不共享</strong>。关于状态的具体使用。</p>
<table>
<thead>
<tr>
<th></th>
<th>OperatorState</th>
<th>KeyedState</th>
</tr>
</thead>
<tbody><tr>
<td>适用用算子类型</td>
<td>可用于所有算子: 常用于source, 例如 FlinkKafkaConsumer</td>
<td>只适用于KeyedStream上的算子</td>
</tr>
<tr>
<td>状态分配</td>
<td>一个算子的子任务对应一个状态</td>
<td>一个Key对应一个State: 一个算子会处理多个Key, 则访问相应的多个State</td>
</tr>
<tr>
<td>创建和访问方式</td>
<td>实现CheckpointedFunction或ListCheckpointed(已经过时)接口</td>
<td>重写RichFunction, 通过里面的RuntimeContext访问</td>
</tr>
<tr>
<td>横向扩展</td>
<td>并发改变时有多重重写分配方式可选: 均匀分配和合并后每个得到全量</td>
<td>并发改变, State随着Key在实例间迁移</td>
</tr>
<tr>
<td>支持的数据结构</td>
<td>ListState、UnionListState和BroadCastState</td>
<td>ValueState, ListState,MapState ReduceState, AggregatingState</td>
</tr>
</tbody></table>
<h3 id="按键分区状态-Keyed-State"><a href="#按键分区状态-Keyed-State" class="headerlink" title="按键分区状态(Keyed State)"></a>按键分区状态(Keyed State)</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//stateTest</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    SingleOutputStreamOperator&lt;Event&gt; stream = env.addSource(<span class="keyword">new</span> ClickSource())</span><br><span class="line">            .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Event&gt;forMonotonousTimestamps()</span><br><span class="line">                    .withTimestampAssigner(<span class="keyword">new</span> SerializableTimestampAssigner&lt;Event&gt;() &#123;</span><br><span class="line">                        <span class="meta">@Override</span></span><br><span class="line">                        <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Event element, <span class="keyword">long</span> recordTimestamp)</span> </span>&#123;</span><br><span class="line">                            <span class="keyword">return</span> element.timestamp;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;)</span><br><span class="line">            );</span><br><span class="line"></span><br><span class="line">    stream.keyBy(data -&gt; data.user)</span><br><span class="line">                    .flatMap(<span class="keyword">new</span> MyFlatMap())</span><br><span class="line">                            .print();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 实现自定义的FlatMapFunction，用于Keyed State测试</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyFlatMap</span> <span class="keyword">extends</span> <span class="title">RichFlatMapFunction</span>&lt;<span class="title">Event</span>, <span class="title">String</span>&gt;</span>&#123;</span><br><span class="line">    <span class="comment">// 定义状态</span></span><br><span class="line">    ValueState&lt;Event&gt; myValueState;</span><br><span class="line">    ListState&lt;Event&gt; myListState;</span><br><span class="line">    MapState&lt;String, Long&gt; myMapState;</span><br><span class="line">    ReducingState&lt;Event&gt; myReducingState;</span><br><span class="line">    AggregatingState&lt;Event, String&gt; myAggregatingState;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 增加一个本地变量进行对比</span></span><br><span class="line">    Long count = <span class="number">0L</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        ValueStateDescriptor&lt;Event&gt; valueStateDescriptor = <span class="keyword">new</span> ValueStateDescriptor&lt;&gt;(<span class="string">&quot;my-state&quot;</span>, Event.class);</span><br><span class="line">        myValueState = getRuntimeContext().getState(valueStateDescriptor);</span><br><span class="line"></span><br><span class="line">        myListState = getRuntimeContext().getListState(<span class="keyword">new</span> ListStateDescriptor&lt;Event&gt;(<span class="string">&quot;my-list&quot;</span>, Event.class));</span><br><span class="line">        myMapState = getRuntimeContext().getMapState(<span class="keyword">new</span> MapStateDescriptor&lt;String, Long&gt;(<span class="string">&quot;my-map&quot;</span>, String.class, Long.class));</span><br><span class="line"></span><br><span class="line">        myReducingState = getRuntimeContext().getReducingState(<span class="keyword">new</span> ReducingStateDescriptor&lt;Event&gt;(<span class="string">&quot;my-reduce&quot;</span>,</span><br><span class="line">                <span class="keyword">new</span> ReduceFunction&lt;Event&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> Event <span class="title">reduce</span><span class="params">(Event value1, Event value2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> <span class="keyword">new</span> Event(value1.user, value1.url, value2.timestamp);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                , Event.class));</span><br><span class="line"></span><br><span class="line">        myAggregatingState = getRuntimeContext().getAggregatingState(<span class="keyword">new</span> AggregatingStateDescriptor&lt;Event, Long, String&gt;(<span class="string">&quot;my-agg&quot;</span>,</span><br><span class="line">                <span class="keyword">new</span> AggregateFunction&lt;Event, Long, String&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> Long <span class="title">createAccumulator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> <span class="number">0L</span>;</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> Long <span class="title">add</span><span class="params">(Event value, Long accmulator)</span> </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> accmulator + <span class="number">1</span>;</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> String <span class="title">getResult</span><span class="params">(Long accumulator)</span> </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> <span class="string">&quot;count: &quot;</span> + accumulator;</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> Long <span class="title">merge</span><span class="params">(Long a, Long b)</span> </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> a + b;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                , Long.class));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置状态的TTL</span></span><br><span class="line">        StateTtlConfig ttlConfig = StateTtlConfig.newBuilder(Time.hours(<span class="number">1</span>))</span><br><span class="line">                .setUpdateType(StateTtlConfig.UpdateType.OnReadAndWrite)</span><br><span class="line">                .setStateVisibility(StateTtlConfig.StateVisibility.ReturnExpiredIfNotCleanedUp)</span><br><span class="line">                .build();</span><br><span class="line"></span><br><span class="line">        valueStateDescriptor.enableTimeToLive(ttlConfig);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(Event value, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// 访问和更新状态</span></span><br><span class="line">        System.out.println(myValueState.value());</span><br><span class="line">        myValueState.update(value);</span><br><span class="line">        System.out.println( <span class="string">&quot;my value: &quot;</span> + myValueState.value() );</span><br><span class="line"></span><br><span class="line">        myListState.add(value);</span><br><span class="line"></span><br><span class="line">        myMapState.put(value.user, myMapState.get(value.user) == <span class="keyword">null</span>? <span class="number">1</span>: myMapState.get(value.user) + <span class="number">1</span>);</span><br><span class="line">        System.out.println( <span class="string">&quot;my map value: &quot;</span> + myMapState.get(value.user) );</span><br><span class="line"></span><br><span class="line">        myReducingState.add(value);</span><br><span class="line">        System.out.println( <span class="string">&quot;my reducing value: &quot;</span> + myReducingState.get() );</span><br><span class="line"></span><br><span class="line">        myAggregatingState.add(value);</span><br><span class="line">        System.out.println( <span class="string">&quot;my agg value: &quot;</span> + myAggregatingState.get() );</span><br><span class="line"></span><br><span class="line">        count ++;</span><br><span class="line">        System.out.println(<span class="string">&quot;count: &quot;</span> + count);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="值状态-ValueState"><a href="#值状态-ValueState" class="headerlink" title="值状态(ValueState)"></a>值状态(ValueState)</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">ValueState</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">State</span> </span>&#123;</span><br><span class="line">   <span class="function">T <span class="title">value</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">   <span class="function"><span class="keyword">void</span> <span class="title">update</span><span class="params">(T value)</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>T value()：获取当前状态的值。<br>update(Tvalue)：对状态进行更新，传入的参数value就是要覆写的状态值。</p>
<p>在具体使用时，为了让运行时上下文清楚到底是哪个状态，我们还需要创建一个状态描述器(StateDescriptor)来提供状态的基本信息。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ValueStateDescriptor</span><span class="params">(String name, Class&lt;T&gt; typeClass)</span> </span>&#123;</span><br><span class="line">   <span class="keyword">super</span>(name, typeClass, <span class="keyword">null</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这里需要传入状态的名称和类型。</p>
<p>例子：我们这里会使用用户 id 来进行分流，然后分别统计每个用户的 pv 数据，由于我们并不想每次 pv 加一，就将统计结果发送到下游去，所以这里我们注册了一个定时器，用来隔一段时间发送 pv 的统计结果，这样对下游算子的压力不至于太大。具体实现方式是定义一个用来保存定时器时间戳的值状态变量。当定时器触发并向下游发送数据以后，便清空储存定时器时间戳的状态变量，这样当新的数据到来时，发现并没有定时器存在，就可以注册新的定时器了， 注册完定时器之后将定时器的时间戳继续保存在状态变量中。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//PeriodicPvExample</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    SingleOutputStreamOperator&lt;Event&gt; stream = env.addSource(<span class="keyword">new</span> ClickSource())</span><br><span class="line">            .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Event&gt;forMonotonousTimestamps()</span><br><span class="line">                    .withTimestampAssigner(<span class="keyword">new</span> SerializableTimestampAssigner&lt;Event&gt;() &#123;</span><br><span class="line">                        <span class="meta">@Override</span></span><br><span class="line">                        <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Event element, <span class="keyword">long</span> recordTimestamp)</span> </span>&#123;</span><br><span class="line">                            <span class="keyword">return</span> element.timestamp;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;)</span><br><span class="line">            );</span><br><span class="line"></span><br><span class="line">    stream.print(<span class="string">&quot;input&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 统计每个用户的pv，隔一段时间（10s）输出一次结果</span></span><br><span class="line">    stream.keyBy(data -&gt; data.user)</span><br><span class="line">            .process(<span class="keyword">new</span> PeriodicPvResult())</span><br><span class="line">            .print();</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册定时器，周期性输出pv</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">PeriodicPvResult</span> <span class="keyword">extends</span> <span class="title">KeyedProcessFunction</span>&lt;<span class="title">String</span> ,<span class="title">Event</span>, <span class="title">String</span>&gt;</span>&#123;</span><br><span class="line">    <span class="comment">// 定义两个状态，保存当前pv值，以及定时器时间戳</span></span><br><span class="line">    ValueState&lt;Long&gt; countState;</span><br><span class="line">    ValueState&lt;Long&gt; timerTsState;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        countState = getRuntimeContext().getState(<span class="keyword">new</span> ValueStateDescriptor&lt;Long&gt;(<span class="string">&quot;count&quot;</span>, Long.class));</span><br><span class="line">        timerTsState = getRuntimeContext().getState(<span class="keyword">new</span> ValueStateDescriptor&lt;Long&gt;(<span class="string">&quot;timerTs&quot;</span>, Long.class));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(Event value, Context ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// 更新count值</span></span><br><span class="line">        Long count = countState.value();</span><br><span class="line">        <span class="keyword">if</span> (count == <span class="keyword">null</span>)&#123;</span><br><span class="line">            countState.update(<span class="number">1L</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            countState.update(count + <span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 注册定时器</span></span><br><span class="line">        <span class="keyword">if</span> (timerTsState.value() == <span class="keyword">null</span>)&#123;</span><br><span class="line">            ctx.timerService().registerEventTimeTimer(value.timestamp + <span class="number">10</span> * <span class="number">1000L</span>);</span><br><span class="line">            timerTsState.update(value.timestamp + <span class="number">10</span> * <span class="number">1000L</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onTimer</span><span class="params">(<span class="keyword">long</span> timestamp, OnTimerContext ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        out.collect(ctx.getCurrentKey() + <span class="string">&quot; pv: &quot;</span> + countState.value());</span><br><span class="line">        <span class="comment">// 清空状态</span></span><br><span class="line">        timerTsState.clear();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="列表状态-ListState"><a href="#列表状态-ListState" class="headerlink" title="列表状态(ListState)"></a>列表状态(ListState)</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">ListState</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">MergingState</span>&lt;<span class="title">T</span>, <span class="title">Iterable</span>&lt;<span class="title">T</span>&gt;&gt; </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">update</span><span class="params">(List&lt;T&gt; values)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">addAll</span><span class="params">(List&lt;T&gt; values)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Iterable<T>get()：获取当前的列表状态，返回的是一个可迭代类型Iterable<T>。</T></T></p>
<p>update(List<T>values)：传入一个列表values，直接对状态进行覆盖。</T></p>
<p>add(Tvalue)：在状态列表中添加一个元素value。</p>
<p>addAll(List<T>values)：向列表中添加多个元素，以列表values形式传入。</T></p>
<p>在 Flink SQL 中，支持两条流的全量 Join，语法如下：SELECT * FROM A INNER JOIN B WHERE A.id = B.id；这样一条 SQL 语句要慎用，因为 Flink 会将 A 流和 B 流的所有数据都保存下来，然后进行 Join。不过在这里我们可以用列表状态变量来实现一下这个 SQL 语句的功能。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//TwoStreamFullJoinExample</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    SingleOutputStreamOperator&lt;Tuple3&lt;String, String, Long&gt;&gt; stream1 = env</span><br><span class="line">            .fromElements(</span><br><span class="line">                    Tuple3.of(<span class="string">&quot;a&quot;</span>, <span class="string">&quot;stream-1&quot;</span>, <span class="number">1000L</span>),</span><br><span class="line">                    Tuple3.of(<span class="string">&quot;b&quot;</span>, <span class="string">&quot;stream-1&quot;</span>, <span class="number">2000L</span>)</span><br><span class="line">            )</span><br><span class="line">            .assignTimestampsAndWatermarks(</span><br><span class="line">                    WatermarkStrategy.&lt;Tuple3&lt;String, String, Long&gt;&gt;forMonotonousTimestamps()</span><br><span class="line">                            .withTimestampAssigner(<span class="keyword">new</span> SerializableTimestampAssigner&lt;Tuple3&lt;String, String, Long&gt;&gt;() &#123;</span><br><span class="line">                                <span class="meta">@Override</span></span><br><span class="line">                                <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Tuple3&lt;String, String, Long&gt; t, <span class="keyword">long</span> l)</span> </span>&#123;</span><br><span class="line">                                    <span class="keyword">return</span> t.f2;</span><br><span class="line">                                &#125;</span><br><span class="line">                            &#125;)</span><br><span class="line">            );</span><br><span class="line"></span><br><span class="line">    SingleOutputStreamOperator&lt;Tuple3&lt;String, String, Long&gt;&gt; stream2 = env</span><br><span class="line">            .fromElements(</span><br><span class="line">                    Tuple3.of(<span class="string">&quot;a&quot;</span>, <span class="string">&quot;stream-2&quot;</span>, <span class="number">3000L</span>),</span><br><span class="line">                    Tuple3.of(<span class="string">&quot;b&quot;</span>, <span class="string">&quot;stream-2&quot;</span>, <span class="number">4000L</span>)</span><br><span class="line">            )</span><br><span class="line">            .assignTimestampsAndWatermarks(</span><br><span class="line">                    WatermarkStrategy.&lt;Tuple3&lt;String, String, Long&gt;&gt;forMonotonousTimestamps()</span><br><span class="line">                            .withTimestampAssigner(<span class="keyword">new</span> SerializableTimestampAssigner&lt;Tuple3&lt;String, String, Long&gt;&gt;() &#123;</span><br><span class="line">                                <span class="meta">@Override</span></span><br><span class="line">                                <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Tuple3&lt;String, String, Long&gt; t, <span class="keyword">long</span> l)</span> </span>&#123;</span><br><span class="line">                                    <span class="keyword">return</span> t.f2;</span><br><span class="line">                                &#125;</span><br><span class="line">                            &#125;)</span><br><span class="line">            );</span><br><span class="line"></span><br><span class="line">    stream1.keyBy(r -&gt; r.f0)</span><br><span class="line">            .connect(stream2.keyBy(r -&gt; r.f0))</span><br><span class="line">            .process(<span class="keyword">new</span> CoProcessFunction&lt;Tuple3&lt;String, String, Long&gt;, Tuple3&lt;String, String, Long&gt;, String&gt;() &#123;</span><br><span class="line">                <span class="keyword">private</span> ListState&lt;Tuple3&lt;String, String, Long&gt;&gt; stream1ListState;</span><br><span class="line">                <span class="keyword">private</span> ListState&lt;Tuple3&lt;String, String, Long&gt;&gt; stream2ListState;</span><br><span class="line"></span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                    <span class="keyword">super</span>.open(parameters);</span><br><span class="line">                    stream1ListState = getRuntimeContext().getListState(</span><br><span class="line">                            <span class="keyword">new</span> ListStateDescriptor&lt;Tuple3&lt;String, String, Long&gt;&gt;(<span class="string">&quot;stream1-list&quot;</span>, Types.TUPLE(Types.STRING, Types.STRING))</span><br><span class="line">                    );</span><br><span class="line">                    stream2ListState = getRuntimeContext().getListState(</span><br><span class="line">                            <span class="keyword">new</span> ListStateDescriptor&lt;Tuple3&lt;String, String, Long&gt;&gt;(<span class="string">&quot;stream2-list&quot;</span>, Types.TUPLE(Types.STRING, Types.STRING))</span><br><span class="line">                    );</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement1</span><span class="params">(Tuple3&lt;String, String, Long&gt; left, Context context, Collector&lt;String&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                    stream1ListState.add(left);</span><br><span class="line">                    <span class="keyword">for</span> (Tuple3&lt;String, String, Long&gt; right : stream2ListState.get()) &#123;</span><br><span class="line">                        collector.collect(left + <span class="string">&quot; =&gt; &quot;</span> + right);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement2</span><span class="params">(Tuple3&lt;String, String, Long&gt; right, Context context, Collector&lt;String&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                    stream2ListState.add(right);</span><br><span class="line">                    <span class="keyword">for</span> (Tuple3&lt;String, String, Long&gt; left : stream1ListState.get()) &#123;</span><br><span class="line">                        collector.collect(left + <span class="string">&quot; =&gt; &quot;</span> + right);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;)</span><br><span class="line">            .print();</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="映射状态-MapState"><a href="#映射状态-MapState" class="headerlink" title="映射状态(MapState)"></a>映射状态(MapState)</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">MapState</span>&lt;<span class="title">UK</span>, <span class="title">UV</span>&gt; <span class="keyword">extends</span> <span class="title">State</span> </span>&#123;</span><br><span class="line">    <span class="function">UV <span class="title">get</span><span class="params">(UK key)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">put</span><span class="params">(UK key, UV value)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">putAll</span><span class="params">(Map&lt;UK, UV&gt; map)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">remove</span><span class="params">(UK key)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">    <span class="function"><span class="keyword">boolean</span> <span class="title">contains</span><span class="params">(UK key)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">    Iterable&lt;Map.Entry&lt;UK, UV&gt;&gt; entries() <span class="keyword">throws</span> Exception;</span><br><span class="line">    <span class="function">Iterable&lt;UK&gt; <span class="title">keys</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">    <span class="function">Iterable&lt;UV&gt; <span class="title">values</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">    Iterator&lt;Map.Entry&lt;UK, UV&gt;&gt; iterator() <span class="keyword">throws</span> Exception;</span><br><span class="line">    <span class="function"><span class="keyword">boolean</span> <span class="title">isEmpty</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>get(UKkey)：传入一个key作为参数，查询对应的value值。</p>
<p>put(UKkey,UVvalue)：传入一个键值对，更新key对应的value值。</p>
<p>putAll(Map&lt;UK,UV&gt;map)：将传入的映射map中所有的键值对，全部添加到映射状态中。</p>
<p>remove(UKkey)：将指定key对应的键值对删除。</p>
<p>contains(UK key)：判断是否存在指定的 key，返回一个 boolean 值。 另外，MapState 也提供了获取整个映射相关信息的方法。</p>
<p>entries()：获取映射状态中所有的键值对。</p>
<p>keys()：获取映射状态中所有的键(key)，返回一个可迭代 Iterable 类型。</p>
<p>values()：获取映射状态中所有的值(value)，返回一个可迭代 Iterable 类型。</p>
<p>booleanisEmpty()：判断映射是否为空，返回一个boolean值。</p>
<p>映射状态的用法和 Java 中的 HashMap 很相似。在这里我们可以通过 MapState 的使用来探索一下窗口的底层实现，也就是我们要用映射状态来完整模拟窗口的功能。这里我们模拟一个滚动窗口。我们要计算的是每一个 url 在每一个窗口中的 pv 数据。我们之前使用增量聚合和全窗口聚合结合的方式实现过这个需求。这里我们用 MapState 再来实现一下。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//使用KeyedProcessFunction模拟滚动窗口</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    SingleOutputStreamOperator&lt;Event&gt; stream = env.addSource(<span class="keyword">new</span> ClickSource())</span><br><span class="line">            .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Event&gt;forMonotonousTimestamps()</span><br><span class="line">                    .withTimestampAssigner(<span class="keyword">new</span> SerializableTimestampAssigner&lt;Event&gt;() &#123;</span><br><span class="line">                        <span class="meta">@Override</span></span><br><span class="line">                        <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Event element, <span class="keyword">long</span> recordTimestamp)</span> </span>&#123;</span><br><span class="line">                            <span class="keyword">return</span> element.timestamp;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;)</span><br><span class="line">            );</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 统计每10s窗口内，每个url的pv</span></span><br><span class="line">    stream.keyBy(data -&gt; data.url)</span><br><span class="line">            .process(<span class="keyword">new</span> FakeWindowResult(<span class="number">10000L</span>))</span><br><span class="line">            .print();</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">FakeWindowResult</span> <span class="keyword">extends</span> <span class="title">KeyedProcessFunction</span>&lt;<span class="title">String</span>, <span class="title">Event</span>, <span class="title">String</span>&gt;</span>&#123;</span><br><span class="line">    <span class="comment">// 定义属性，窗口长度</span></span><br><span class="line">    <span class="keyword">private</span> Long windowSize;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">FakeWindowResult</span><span class="params">(Long windowSize)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.windowSize = windowSize;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 声明状态，用map保存pv值（窗口start，count）</span></span><br><span class="line">    MapState&lt;Long, Long&gt; windowPvMapState;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        windowPvMapState = getRuntimeContext().getMapState(<span class="keyword">new</span> MapStateDescriptor&lt;Long, Long&gt;(<span class="string">&quot;window-pv&quot;</span>, Long.class, Long.class));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(Event value, Context ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// 每来一条数据，就根据时间戳判断属于哪个窗口</span></span><br><span class="line">        Long windowStart = value.timestamp / windowSize * windowSize;</span><br><span class="line">        Long windowEnd = windowStart + windowSize;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 注册 end -1 的定时器，窗口触发计算</span></span><br><span class="line">        ctx.timerService().registerEventTimeTimer(windowEnd - <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 更新状态中的pv值</span></span><br><span class="line">        <span class="keyword">if</span> (windowPvMapState.contains(windowStart))&#123;</span><br><span class="line">            Long pv = windowPvMapState.get(windowStart);</span><br><span class="line">            windowPvMapState.put(windowStart, pv + <span class="number">1</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            windowPvMapState.put(windowStart, <span class="number">1L</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 定时器触发，直接输出统计的pv结果</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onTimer</span><span class="params">(<span class="keyword">long</span> timestamp, OnTimerContext ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Long windowEnd = timestamp + <span class="number">1</span>;</span><br><span class="line">        Long windowStart = windowEnd - windowSize;</span><br><span class="line">        Long pv = windowPvMapState.get(windowStart);</span><br><span class="line">        out.collect( <span class="string">&quot;url: &quot;</span> + ctx.getCurrentKey()</span><br><span class="line">                + <span class="string">&quot; 访问量: &quot;</span> + pv</span><br><span class="line">                + <span class="string">&quot; 窗口：&quot;</span> + <span class="keyword">new</span> Timestamp(windowStart) + <span class="string">&quot; ~ &quot;</span> + <span class="keyword">new</span> Timestamp(windowEnd));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 模拟窗口的销毁，清除map中的key</span></span><br><span class="line">        windowPvMapState.remove(windowStart);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="归约状态-ReducingState"><a href="#归约状态-ReducingState" class="headerlink" title="归约状态(ReducingState)"></a>归约状态(ReducingState)</h4><p>似于值状态(Value)，不过需要对添加进来的所有数据进行归约，将归约聚合之后的值作为状态保存下来。ReducintState<T>这个接口调用的方法类似于 ListState，只不过它保存的只是一个聚合值，所以调用.add()方法时，不是在状态列表里添加元素，而是直接把新数据和之前的状态进行归约，并用得到的结果更新状态。</T></p>
<p>归约逻辑的定义，是在归约状态描述器(ReducingStateDescriptor)中，通过传入一个归 约函数(ReduceFunction)来实现的。这里的归约函数，就是我们之前介绍 reduce 聚合算子时讲到的 ReduceFunction，所以状态类型跟输入的数据类型是一样的。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ReducingStateDescriptor</span><span class="params">( String name, ReduceFunction&lt;T&gt; reduceFunction, Class&lt;T&gt; typeClass)</span> </span>&#123;...&#125;</span><br></pre></td></tr></table></figure>

<p>这里的描述器有三个参数，其中第二个参数就是定义了归约聚合逻辑的 ReduceFunction， 另外两个参数则是状态的名称和类型。</p>
<h4 id="聚合状态-AggregatingState"><a href="#聚合状态-AggregatingState" class="headerlink" title="聚合状态(AggregatingState)"></a>聚合状态(AggregatingState)</h4><p>与归约状态非常类似，聚合状态也是一个值，用来保存添加进来的所有数据的聚合结果。与 ReducingState 不同的是，它的聚合逻辑是由在描述器中传入一个更加一般化的聚合函数(AggregateFunction)来定义的；这也就是之前我们讲过的 AggregateFunction，里面通过一个累加器(Accumulator)来表示状态，所以聚合的状态类型可以跟添加进来的数据类型完全不同，使用更加灵活。</p>
<p>同样地，AggregatingState 接口调用方法也与 ReducingState 相同，调用.add()方法添加元素时，会直接使用指定的 AggregateFunction 进行聚合并更新状态。</p>
<p>们举一个简单的例子，对用户点击事件流每 5 个数据统计一次平均时间戳。这是一个类似计数窗口(CountWindow)求平均值的计算，这里我们可以使用一个有聚合状态的 RichFlatMapFunction 来实现。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//AverageTimestampExample</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    SingleOutputStreamOperator&lt;Event&gt; stream = env.addSource(<span class="keyword">new</span> ClickSource())</span><br><span class="line">            .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Event&gt;forMonotonousTimestamps()</span><br><span class="line">                    .withTimestampAssigner(<span class="keyword">new</span> SerializableTimestampAssigner&lt;Event&gt;() &#123;</span><br><span class="line">                        <span class="meta">@Override</span></span><br><span class="line">                        <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Event element, <span class="keyword">long</span> recordTimestamp)</span> </span>&#123;</span><br><span class="line">                            <span class="keyword">return</span> element.timestamp;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;)</span><br><span class="line">            );</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 统计每个用户的点击频次，到达5次就输出统计结果</span></span><br><span class="line">    stream.keyBy(data -&gt; data.user)</span><br><span class="line">            .flatMap(<span class="keyword">new</span> AvgTsResult())</span><br><span class="line">            .print();</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">AvgTsResult</span> <span class="keyword">extends</span> <span class="title">RichFlatMapFunction</span>&lt;<span class="title">Event</span>, <span class="title">String</span>&gt;</span>&#123;</span><br><span class="line">    <span class="comment">// 定义聚合状态，用来计算平均时间戳</span></span><br><span class="line">    AggregatingState&lt;Event, Long&gt; avgTsAggState;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 定义一个值状态，用来保存当前用户访问频次</span></span><br><span class="line">    ValueState&lt;Long&gt; countState;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        avgTsAggState = getRuntimeContext().getAggregatingState(<span class="keyword">new</span> AggregatingStateDescriptor&lt;Event, Tuple2&lt;Long, Long&gt;, Long&gt;(</span><br><span class="line">                <span class="string">&quot;avg-ts&quot;</span>,</span><br><span class="line">                <span class="keyword">new</span> AggregateFunction&lt;Event, Tuple2&lt;Long, Long&gt;, Long&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> Tuple2&lt;Long, Long&gt; <span class="title">createAccumulator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> Tuple2.of(<span class="number">0L</span>, <span class="number">0L</span>);</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> Tuple2&lt;Long, Long&gt; <span class="title">add</span><span class="params">(Event value, Tuple2&lt;Long, Long&gt; accumulator)</span> </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> Tuple2.of(accumulator.f0 + value.timestamp, accumulator.f1 + <span class="number">1</span>);</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> Long <span class="title">getResult</span><span class="params">(Tuple2&lt;Long, Long&gt; accumulator)</span> </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> accumulator.f0 / accumulator.f1;</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> Tuple2&lt;Long, Long&gt; <span class="title">merge</span><span class="params">(Tuple2&lt;Long, Long&gt; a, Tuple2&lt;Long, Long&gt; b)</span> </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;,</span><br><span class="line">                Types.TUPLE(Types.LONG, Types.LONG)</span><br><span class="line">        ));</span><br><span class="line"></span><br><span class="line">        countState = getRuntimeContext().getState(<span class="keyword">new</span> ValueStateDescriptor&lt;Long&gt;(<span class="string">&quot;count&quot;</span>, Long.class));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(Event value, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Long count = countState.value();</span><br><span class="line">        <span class="keyword">if</span> (count == <span class="keyword">null</span>)&#123;</span><br><span class="line">            count = <span class="number">1L</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            count ++;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        countState.update(count);</span><br><span class="line">        avgTsAggState.add(value);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 达到5次就输出结果，并清空状态</span></span><br><span class="line">        <span class="keyword">if</span> (count == <span class="number">5</span>)&#123;</span><br><span class="line">            out.collect(value.user + <span class="string">&quot; 平均时间戳：&quot;</span> + <span class="keyword">new</span> Timestamp(avgTsAggState.get()));</span><br><span class="line">            countState.clear();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="状态生存时间-TTL"><a href="#状态生存时间-TTL" class="headerlink" title="状态生存时间(TTL)"></a>状态生存时间(TTL)</h4><p>在实际应用中，很多状态会随着时间的推移逐渐增长，如果不加以限制，最终就会导致存储空间的耗尽。一个优化的思路是直接在代码中调用.clear()方法去清除状态，但是有时候我们的逻辑要求不能直接清除。这时就需要配置一个状态的生存时间(time-to-live，TTL)，当状态在内存中存在的时间超出这个值时，就将它清除。</p>
<p>具体实现上，如果用一个进程不停地扫描所有状态看是否过期，显然会占用大量资源做无用功。状态的失效其实不需要立即删除，所以我们可以给状态附加一个属性，也就是状态的失效时间。状态创建的时候，设置 失效时间 = 当前时间 + TTL；之后如果有对状态的访问和修改，我们可以再对失效时间进行更新；当设置的清除条件被触发时(比如，状态被访问的时候、或者每隔一段时间扫描一次失效状态)，就可以判断状态是否失效、从而进行清除了。</p>
<p>配置状态的 TTL 时，需要创建一个 StateTtlConfig 配置对象，然后调用状态描述器的.enableTimeToLive()方法启动 TTL 功能。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">StateTtlConfig ttlConfig = StateTtlConfig</span><br><span class="line">   .newBuilder(Time.seconds(<span class="number">10</span>))</span><br><span class="line">   .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite)</span><br><span class="line">   .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired)</span><br><span class="line">   .build();</span><br><span class="line">   </span><br><span class="line">ValueStateDescriptor&lt;String&gt; stateDescriptor = <span class="keyword">new</span> ValueStateDescriptor&lt;&gt;(<span class="string">&quot;my state&quot;</span>, String.class);</span><br><span class="line">stateDescriptor.enableTimeToLive(ttlConfig);</span><br></pre></td></tr></table></figure>

<p><strong>.newBuilder()</strong></p>
<p>状态 TTL 配置的构造器方法，必须调用，返回一个 Builder 之后再调用.build()方法就可以得到 StateTtlConfig 了。方法需要传入一个 Time 作为参数，这就是设定的状态生存时间。</p>
<p><strong>.setUpdateType()</strong></p>
<p>设置更新类型。更新类型指定了什么时候更新状态失效时间，这里的 OnCreateAndWrite 表示只有创建状态和更改状态(写操作)时更新失效时间。另一种类型 OnReadAndWrite 则表示无论读写操作都会更新失效时间，也就是只要对状态进行了访问，就表明它是活跃的，从而延长生存时间。这个配置默认为 OnCreateAndWrite。</p>
<p><strong>.setStateVisibility()</strong></p>
<p>设置状态的可见性。所谓的状态可见性，是指因为清除操作并不是实时的，所以当状态过期之后还有可能继续存在，这时如果对它进行访问，能否正常读取到就是一个问题了。这里设置的 NeverReturnExpired 是默认行为，表示从不返回过期值，也就是只要过期就认为它已经被清除了，应用不能继续读取；这在处理会话或者隐私数据时比较重要。对应的另一种配置是 ReturnExpireDefNotCleanedUp，就是如果过期状态还存在，就返回它的值。</p>
<p>除此之外，TTL 配置还可以设置在保存检查点(checkpoint)时触发清除操作，或者配置增量的清理(incremental cleanup)，还可以针对 RocksDB 状态后端使用压缩过滤器(compaction filter)进行后台清理。</p>
<p>这里需要注意，目前的 TTL 设置只支持处理时间。另外，所有集合类型的状态(例如 ListState、MapState)在设置 TTL 时，都是针对每一项(per-entry)元素的。也就是说，一个列表状态中的每一个元素，都会以自己的失效时间来进行清理，而不是整个列表一起清理。</p>
<h3 id="算子状态-Operator-State"><a href="#算子状态-Operator-State" class="headerlink" title="算子状态(Operator State)"></a>算子状态(Operator State)</h3><p>算子状态(Operator State)就是一个算子并行实例上定义的状态，作用范围被限定为当前算子任务。<strong>算子状态跟数据的 key 无关</strong>，所以不同 key 的数据只要被分发到同一个并行子任务， 就会访问到同一个 Operator State。</p>
<p>算子状态的实际应用场景不如 Keyed State 多，一般用在 Source 或 Sink 等与外部系统连接的算子上，或者完全没有 key 定义的场景。比如 Flink 的 Kafka 连接器中，就用到了算子状态。 在我们给 Source 算子设置并行度后，Kafka 消费者的每一个并行实例，都会为对应的主题topic)分区维护一个偏移量， 作为算子状态保存起来。这在保证 Flink 应用精确一次(exactly-once)状态一致性时非常有用。 当算子的并行度发生变化时，算子状态也支持在并行的算子任务实例之间做重组分配。根据状态的类型不同，重组分配的方案也会不同。</p>
<p>算子状态也支持不同的结构类型，主要有三种：ListState、UnionListState 和 BroadcastState。</p>
<h4 id="列表状态-ListState-1"><a href="#列表状态-ListState-1" class="headerlink" title="列表状态(ListState)"></a>列表状态(ListState)</h4><p>与 Keyed State 中的列表状态的区别是：在算子状态的上下文中，不会按键(key)分别处理状态，所以每一个并行子任务上只会保留一个列表(list)，也就是当前并行子任务上所有状态项的集合。列表中的状态项就是可以重新分配的最细粒度，彼此之间完全独立。</p>
<p>当算子并行度进行缩放调整时，算子的列表状态中的所有元素项会被统一收集起来，相当于把多个分区的列表合并成了一个大列表，然后再均匀地分配给所有并行任务。这种均匀分配的具体方法就是轮询(round-robin)，与之前介绍的 rebanlance 数据传输方式类似， 是通过逐一发牌的方式将状态项平均分配的。这种方式也叫作平均分割重组(even-split redistribution)。</p>
<p>算子状态中不会存在键组(key group)这样的结构，所以为了方便重组分配，就把它直接定义成了列表(list)。这也就解释了，为什么算子状态中没有最简单的值状态(ValueState)。</p>
<h4 id="联合列表状态-UnionListState"><a href="#联合列表状态-UnionListState" class="headerlink" title="联合列表状态(UnionListState)"></a>联合列表状态(UnionListState)</h4><p>与 ListState 类似，联合列表状态也会将状态表示为一个列表。它与常规列表状态的区别在于，算子并行度进行缩放调整时对于状态的分配方式不同。</p>
<p>UnionListState 的重点就在于联合(union)。在并行度调整时，<strong>常规列表状态是轮询分配状态项</strong>，而<strong>联合列表状态的算子则会直接广播状态的完整列表</strong>。这样，并行度缩放之后的并行子任务就获取到了联合后完整的大列表，可以自行选择要使用的状态项和要丢弃的状态项。这种分配也叫作联合重组(union redistribution)。如果列表中状态项数量太多，为资源和效率考虑一般不建议使用联合重组的方式。</p>
<h4 id="广播状态-BroadcastState"><a href="#广播状态-BroadcastState" class="headerlink" title="广播状态(BroadcastState)"></a>广播状态(BroadcastState)</h4><p>有时我们希望<strong>算子并行子任务都保持同一份全局状态</strong>，用来做统一的配置和规则设定。 这时所有分区的所有数据都会访问到同一个状态，状态就像被广播到所有分区一样，这种特殊的算子状态，就叫作广播状态(BroadcastState)。</p>
<p>因为广播状态在每个并行子任务上的实例都一样，所以在并行度调整的时候就比较简单， 只要复制一份到新的并行任务就可以实现扩展；而对于并行度缩小的情况，可以将多余的并行子任务连同状态直接砍掉——因为状态都是复制出来的，并不会丢失。</p>
<p>在底层，广播状态是以类似映射结构(map)的键值对(key-value)来保存的，必须基于一个广播流(BroadcastStream)来创建。</p>
<p>我们举一个广播状态的应用案例。考虑在电商应用中，往往需要判断用户先后发生的行为的组合模式，比如登录-下单或者登录-支付，检测出这些连续的行为进行统计，就可以了解平台的运用状况以及用户的行为习惯。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 读取用户行为事件流</span></span><br><span class="line">    DataStreamSource&lt;Action&gt; actionStream = env.fromElements(</span><br><span class="line">            <span class="keyword">new</span> Action(<span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;login&quot;</span>),</span><br><span class="line">            <span class="keyword">new</span> Action(<span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;pay&quot;</span>),</span><br><span class="line">            <span class="keyword">new</span> Action(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;login&quot;</span>),</span><br><span class="line">            <span class="keyword">new</span> Action(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;buy&quot;</span>)</span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 定义行为模式流，代表了要检测的标准</span></span><br><span class="line">    DataStreamSource&lt;Pattern&gt; patternStream = env</span><br><span class="line">            .fromElements(</span><br><span class="line">                    <span class="keyword">new</span> Pattern(<span class="string">&quot;login&quot;</span>, <span class="string">&quot;pay&quot;</span>),</span><br><span class="line">                    <span class="keyword">new</span> Pattern(<span class="string">&quot;login&quot;</span>, <span class="string">&quot;buy&quot;</span>)</span><br><span class="line">            );</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 定义广播状态的描述器，创建广播流</span></span><br><span class="line">    MapStateDescriptor&lt;Void, Pattern&gt; bcStateDescriptor = <span class="keyword">new</span> MapStateDescriptor&lt;&gt;(</span><br><span class="line">            <span class="string">&quot;patterns&quot;</span>, Types.VOID, Types.POJO(Pattern.class));</span><br><span class="line">    BroadcastStream&lt;Pattern&gt; bcPatterns = patternStream.broadcast(bcStateDescriptor);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将事件流和广播流连接起来，进行处理</span></span><br><span class="line">    DataStream&lt;Tuple2&lt;String, Pattern&gt;&gt; matches = actionStream</span><br><span class="line">            .keyBy(data -&gt; data.userId)</span><br><span class="line">            .connect(bcPatterns)</span><br><span class="line">            .process(<span class="keyword">new</span> PatternEvaluator());</span><br><span class="line"></span><br><span class="line">    matches.print();</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">PatternEvaluator</span> <span class="keyword">extends</span> <span class="title">KeyedBroadcastProcessFunction</span>&lt;<span class="title">String</span>, <span class="title">Action</span>, <span class="title">Pattern</span>, <span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Pattern</span>&gt;&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 定义一个值状态，保存上一次用户行为</span></span><br><span class="line">    ValueState&lt;String&gt; prevActionState;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration conf)</span> </span>&#123;</span><br><span class="line">        prevActionState = getRuntimeContext().getState(</span><br><span class="line">                <span class="keyword">new</span> ValueStateDescriptor&lt;&gt;(<span class="string">&quot;lastAction&quot;</span>, Types.STRING));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processBroadcastElement</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">            Pattern pattern,</span></span></span><br><span class="line"><span class="params"><span class="function">            Context ctx,</span></span></span><br><span class="line"><span class="params"><span class="function">            Collector&lt;Tuple2&lt;String, Pattern&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        BroadcastState&lt;Void, Pattern&gt; bcState = ctx.getBroadcastState(</span><br><span class="line">                <span class="keyword">new</span> MapStateDescriptor&lt;&gt;(<span class="string">&quot;patterns&quot;</span>, Types.VOID, Types.POJO(Pattern.class)));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将广播状态更新为当前的pattern</span></span><br><span class="line">        bcState.put(<span class="keyword">null</span>, pattern);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(Action action, ReadOnlyContext ctx,</span></span></span><br><span class="line"><span class="params"><span class="function">                               Collector&lt;Tuple2&lt;String, Pattern&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Pattern pattern = ctx.getBroadcastState(</span><br><span class="line">                <span class="keyword">new</span> MapStateDescriptor&lt;&gt;(<span class="string">&quot;patterns&quot;</span>, Types.VOID, Types.POJO(Pattern.class))).get(<span class="keyword">null</span>);</span><br><span class="line"></span><br><span class="line">        String prevAction = prevActionState.value();</span><br><span class="line">        <span class="keyword">if</span> (pattern != <span class="keyword">null</span> &amp;&amp; prevAction != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="comment">// 如果前后两次行为都符合模式定义，输出一组匹配</span></span><br><span class="line">            <span class="keyword">if</span> (pattern.action1.equals(prevAction) &amp;&amp; pattern.action2.equals(action.action)) &#123;</span><br><span class="line">                out.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(ctx.getCurrentKey(), pattern));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 更新状态</span></span><br><span class="line">        prevActionState.update(action.action);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义用户行为事件POJO类</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Action</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> String userId;</span><br><span class="line">    <span class="keyword">public</span> String action;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Action</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Action</span><span class="params">(String userId, String action)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.userId = userId;</span><br><span class="line">        <span class="keyword">this</span>.action = action;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;Action&#123;&quot;</span> +</span><br><span class="line">                <span class="string">&quot;userId=&quot;</span> + userId +</span><br><span class="line">                <span class="string">&quot;, action=&#x27;&quot;</span> + action + <span class="string">&#x27;\&#x27;&#x27;</span> +</span><br><span class="line">                <span class="string">&#x27;&#125;&#x27;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义行为模式POJO类，包含先后发生的两个行为</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Pattern</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> String action1;</span><br><span class="line">    <span class="keyword">public</span> String action2;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Pattern</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Pattern</span><span class="params">(String action1, String action2)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.action1 = action1;</span><br><span class="line">        <span class="keyword">this</span>.action2 = action2;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;Pattern&#123;&quot;</span> +</span><br><span class="line">                <span class="string">&quot;action1=&#x27;&quot;</span> + action1 + <span class="string">&#x27;\&#x27;&#x27;</span> +</span><br><span class="line">                <span class="string">&quot;, action2=&#x27;&quot;</span> + action2 + <span class="string">&#x27;\&#x27;&#x27;</span> +</span><br><span class="line">                <span class="string">&#x27;&#125;&#x27;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="状态持久化和状态后端"><a href="#状态持久化和状态后端" class="headerlink" title="状态持久化和状态后端"></a>状态持久化和状态后端</h3><h4 id="检查点-Checkpoint"><a href="#检查点-Checkpoint" class="headerlink" title="检查点(Checkpoint)"></a>检查点(Checkpoint)</h4><p>有状态流应用中的检查点(checkpoint)，其实就是所有任务的状态在某个时间点的一个快照(一份拷贝)。</p>
<p>我们对状态进行持久化保存的目的是为了故障恢复；在发生故障、重启应用后，数据还会被发往之前分配的分区吗？显然不是，因为并行度可能发生了调整，不论是按键(key)的哈希值分区，还是直接轮询(round-robin)分区，数据分配到的分区都会发生变化。这很好理解，当打牌的人数 3 个增加到 4 个时，即使牌的次序不变，轮流发到每个人手里的牌也会不同。数据分区发生变化，带来的问题就是，怎么保证原先的状态跟故障恢复后数据的对应关系呢？</p>
<p>对于Keyed State这个问题很好解决：状态都是跟key相关的，而相同key的数据不管发往哪个分区，总是会全部进入一个分区的；于是只要将状态也按照 key 的哈希值计算出对应的分区，进行重组分配就可以了。恢复状态后继续处理数据，就总能按照 key 找到对应之前的状态，就保证了结果的一致性。所以 Flink 对 Keyed State 进行了非常完善的包装，我们不需实现 任何接口就可以直接使用。</p>
<p>而对于 Operator State 来说就会有所不同。因为不存在 key，所有数据发往哪个分区是不可预测的；也就是说，当发生故障重启之后，我们不能保证某个数据跟之前一样，进入到同一个并行子任务、访问同一个状态。所以 Flink 无法直接判断该怎样保存和恢复状态，而是提供了接口，让我们根据业务需求自行设计状态的快照保存(snapshot)和恢复(restore)逻辑。</p>
<p>在 Flink 中，对状态进行持久化保存的快照机制叫作检查点(Checkpoint)。于是使用算子状态时，就需要对检查点的相关操作进行定义，实现一个 CheckpointedFunction 接口。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">CheckpointedFunction</span> </span>&#123;</span><br><span class="line">		<span class="comment">// 保存状态快照到检查点时(外部持久化)，调用这个方法</span></span><br><span class="line">   	<span class="function"><span class="keyword">void</span> <span class="title">snapshotState</span><span class="params">(FunctionSnapshotContext context)</span> <span class="keyword">throws</span> Exception</span></span><br><span class="line"><span class="function">		<span class="comment">// 初始化状态时调用这个方法，也会在恢复状态时调用</span></span></span><br><span class="line"><span class="function">		<span class="keyword">void</span> <span class="title">initializeState</span><span class="params">(FunctionInitializationContext context)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//BufferingSinkExample</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    env.enableCheckpointing(<span class="number">10000L</span>);</span><br><span class="line">      <span class="comment">//env.setStateBackend(new EmbeddedRocksDBStateBackend());</span></span><br><span class="line"></span><br><span class="line">      <span class="comment">//env.getCheckpointConfig().setCheckpointStorage(new FileSystemCheckpointStorage(&quot;&quot;));</span></span><br><span class="line"></span><br><span class="line">    CheckpointConfig checkpointConfig = env.getCheckpointConfig();</span><br><span class="line">    checkpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);</span><br><span class="line">    checkpointConfig.setMinPauseBetweenCheckpoints(<span class="number">500</span>);</span><br><span class="line">    checkpointConfig.setCheckpointTimeout(<span class="number">60000</span>);</span><br><span class="line">    checkpointConfig.setMaxConcurrentCheckpoints(<span class="number">1</span>);</span><br><span class="line">    checkpointConfig.enableExternalizedCheckpoints(</span><br><span class="line">            CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);</span><br><span class="line">    checkpointConfig.enableUnalignedCheckpoints();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    SingleOutputStreamOperator&lt;Event&gt; stream = env.addSource(<span class="keyword">new</span> ClickSource())</span><br><span class="line">            .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Event&gt;forMonotonousTimestamps()</span><br><span class="line">                    .withTimestampAssigner(<span class="keyword">new</span> SerializableTimestampAssigner&lt;Event&gt;() &#123;</span><br><span class="line">                        <span class="meta">@Override</span></span><br><span class="line">                        <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Event element, <span class="keyword">long</span> recordTimestamp)</span> </span>&#123;</span><br><span class="line">                            <span class="keyword">return</span> element.timestamp;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;)</span><br><span class="line">            );</span><br><span class="line"></span><br><span class="line">    stream.print(<span class="string">&quot;input&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 批量缓存输出</span></span><br><span class="line">    stream.addSink(<span class="keyword">new</span> BufferingSink(<span class="number">10</span>));</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">BufferingSink</span> <span class="keyword">implements</span> <span class="title">SinkFunction</span>&lt;<span class="title">Event</span>&gt;, <span class="title">CheckpointedFunction</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> threshold;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">transient</span> ListState&lt;Event&gt; checkpointedState;</span><br><span class="line">    <span class="keyword">private</span> List&lt;Event&gt; bufferedElements;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">BufferingSink</span><span class="params">(<span class="keyword">int</span> threshold)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.threshold = threshold;</span><br><span class="line">        <span class="keyword">this</span>.bufferedElements = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">invoke</span><span class="params">(Event value, Context context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        bufferedElements.add(value);</span><br><span class="line">        <span class="keyword">if</span> (bufferedElements.size() == threshold) &#123;</span><br><span class="line">            <span class="keyword">for</span> (Event element: bufferedElements) &#123;</span><br><span class="line">                <span class="comment">// 输出到外部系统，这里用控制台打印模拟</span></span><br><span class="line">                System.out.println(element);</span><br><span class="line">            &#125;</span><br><span class="line">            System.out.println(<span class="string">&quot;==========输出完毕=========&quot;</span>);</span><br><span class="line">            bufferedElements.clear();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">snapshotState</span><span class="params">(FunctionSnapshotContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        checkpointedState.clear();</span><br><span class="line">        <span class="comment">// 把当前局部变量中的所有元素写入到检查点中</span></span><br><span class="line">        <span class="keyword">for</span> (Event element : bufferedElements) &#123;</span><br><span class="line">            checkpointedState.add(element);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initializeState</span><span class="params">(FunctionInitializationContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        ListStateDescriptor&lt;Event&gt; descriptor = <span class="keyword">new</span> ListStateDescriptor&lt;&gt;(</span><br><span class="line">                <span class="string">&quot;buffered-elements&quot;</span>,</span><br><span class="line">                Types.POJO(Event.class));</span><br><span class="line"></span><br><span class="line">        checkpointedState = context.getOperatorStateStore().getListState(descriptor);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 如果是从故障中恢复，就将ListState中的所有元素添加到局部变量中</span></span><br><span class="line">        <span class="keyword">if</span> (context.isRestored()) &#123;</span><br><span class="line">            <span class="keyword">for</span> (Event element : checkpointedState.get()) &#123;</span><br><span class="line">                bufferedElements.add(element);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>如果保存检查点之后又处理了一些数据，然后发生了故障，那么重启恢复状态之后这些数据带来的状态改变会丢失。为了让最终处理结果正确，我们还需要让源(Source)算子重新读取这些数据，再次处理一遍。这就需要流的数据源具有<strong>数据重放</strong>的能力，一个典型的例子就是 Kafka，我们可以通过保存消费数据的偏移量、故障重启后重新提交来实现数据的重放。 这是对**至少一次(at least once)**状态一致性的保证，如果希望实现精确一次(exactly once) 的一致性，还需要数据写入外部系统时的相关保证。</p>
<p>默认情况下，检查点是被禁用的，需要在代码中手动开启。直接调用执行环境 的.enableCheckpointing()方法就可以开启检查点。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getEnvironment();</span><br><span class="line"><span class="comment">//这里传入的参数是检查点的间隔时间，单位为毫秒</span></span><br><span class="line">env.enableCheckpointing(<span class="number">1000</span>);</span><br></pre></td></tr></table></figure>

<p>除了检查点之外，Flink 还提供了保存点(savepoint)的功能。保存点在原理和形式上跟检查点完全一样，也是状态持久化保存的一个快照；区别在于，保存点是自定义的镜像保存，所以不会由 Flink 自动创建，而需要用户手动触发。这在有计划地停止、重启应用时非常有用。</p>
<h4 id="状态后端-State-Backends"><a href="#状态后端-State-Backends" class="headerlink" title="状态后端(State Backends)"></a>状态后端(State Backends)</h4><p>检查点的保存离不开 JobManager 和 TaskManager，以及外部存储系统的协调。在应用进行检查点保存时，首先会由 JobManager 向所有 TaskManager 发出触发检查点的命令；TaskManger 收到之后，将当前任务的所有状态进行快照保存，持久化到远程的存储介质中；完成之后向 JobManager 返回确认信息。这个过程是分布式的，当 JobManger 收到所有 TaskManager 的返回信息后，就会确认当前检查点成功保存，如图 9-5 所示。而这一切工作的协调，就需要一个专职人员来完成。</p>
<p><img src="/Flink/%E6%88%AA%E5%B1%8F2022-03-28%20%E4%B8%8B%E5%8D%883.40.58.png" alt="截屏2022-03-28 下午3.40.58"></p>
<p>在 Flink 中，状态的存储、访问以及维护，都是由一个可插拔的组件决定的，这个组件就叫作状态后端(state backend)。状态后端主要负责两件事：一是本地的状态管理，二是将检查点(checkpoint)写入远程的持久化存储。</p>
<h5 id="状态后端的分类"><a href="#状态后端的分类" class="headerlink" title="状态后端的分类"></a><strong>状态后端的分类</strong></h5><p>状态后端是一个开箱即用的组件，可以在不改变应用程序逻辑的情况下独立配置。 Flink 中提供了两类不同的状态后端，一种是哈希表状态后端(HashMapStateBackend)，另一种是内嵌 RocksDB 状态后端(EmbeddedRocksDBStateBackend)。如果没有特别配置，系统默认的状态后端是 HashMapStateBackend。</p>
<p><strong>哈希表状态后端(HashMapStateBackend)</strong></p>
<p>这种方式就是我们之前所说的，把状态存放在<strong>内存</strong>里。具体实现上，哈希表状态后端在内部会直接把状态当作对象(objects)，保存在 Taskmanager 的 JVM 堆(heap)上。普通的<strong>状态</strong>，以及<strong>窗口中收集的数据</strong>和**触发器(triggers)**，都会以键值对(key-value)的形式存储起来，所以底层是一个哈希表(HashMap)，这种状态后端也因此得名。</p>
<p>对于检查点的保存，一般是放在持久化的分布式文件系统(file system)中，也可以通过配置检查点存储(CheckpointStorage)来另外指定。</p>
<p>HashMapStateBackend 是将本地状态全部放入内存的，这样可以获得最快的读写速度，使计算性能达到最佳；代价则是内存的占用。它适用于具有大状态、长窗口、大键值状态的作业，对所有高可用性设置也是有效的。</p>
<p><strong>内嵌 RocksDB 状态后端(EmbeddedRocksDBStateBackend)</strong></p>
<p>RocksDB 是一种内嵌的 key-value 存储介质，可以把数据持久化到<strong>本地硬盘</strong>。配置 EmbeddedRocksDBStateBackend 后，会将处理中的数据全部放入 RocksDB 数据库中，RocksDB 默认存储在 TaskManager 的本地数据目录里。</p>
<p>与 HashMapStateBackend 直接在堆内存中存储对象不同，这种方式下状态主要是放在 RocksDB 中的。数据被存储为**序列化的字节数组(Byte Arrays)**，读写操作需要序列化/反序列化，因此状态的访问性能要差一些。另外，因为做了序列化，key 的比较也会按照字节进行，而不是直接调用.hashCode()和.equals()方法。</p>
<p>对于检查点，同样会写入到远程的持久化文件系统中。</p>
<p>EmbeddedRocksDBStateBackend 始终执行的是异步快照，也就是不会因为保存检查点而阻塞数据的处理；而且它还提供了增量式保存检查点的机制，这在很多情况下可以大大提升保存效率。</p>
<p>由于它会把状态数据落盘，而且支持增量化的检查点，所以在状态非常大、窗口非常长、 键/值状态很大的应用场景中是一个好选择，同样对所有高可用性设置有效。</p>
<h5 id="如何选择正确的状态后端"><a href="#如何选择正确的状态后端" class="headerlink" title="如何选择正确的状态后端"></a>如何选择正确的状态后端</h5><p>HashMap 和 RocksDB 两种状态后端最大的区别，就在于本地状态存放在哪里：前者是内存，后者是 RocksDB。在实际应用中，选择那种状态后端，主要是需要根据业务需求在处理性能和应用的扩展性上做一个选择。</p>
<p>HashMapStateBackend 是内存计算，读写速度非常快；但是，状态的大小会受到集群可用内存的限制，如果应用的状态随着时间不停地增长，就会耗尽内存资源。</p>
<p>而 RocksDB 是硬盘存储，所以可以根据可用的磁盘空间进行扩展，而且是唯一支持增量检查点的状态后端，所以它非常适合于超级海量状态的存储。不过由于每个状态的读写都需要做序列化/反序列化，而且可能需要直接从磁盘读取数据，这就会导致性能的降低，平均读写性能要比 HashMapStateBackend 慢一个数量级。</p>
<h5 id="状态后端的配置"><a href="#状态后端的配置" class="headerlink" title="状态后端的配置"></a>状态后端的配置</h5><p>在不做配置的时候，应用程序使用的默认状态后端是由集群配置文件 flink-conf.yaml 中指定的，配置的键名称为 state.backend。这个默认配置对集群上运行的所有作业都有效，我们可以通过更改配置值来改变默认的状态后端。另外，我们还可以在代码中为当前作业单独配置状态后端，这个配置会覆盖掉集群配置文件的默认值。</p>
<p>配置默认的状态后端：</p>
<p>在 flink-conf.yaml 中，可以使用 state.backend 来配置默认状态后端。配置项的可能值为 hashmap，这样配置的就是 HashMapStateBackend；也可以是 rocksdb，这样配置的就是 EmbeddedRocksDBStateBackend。另外，也可以是一个实现了状态后端工厂 StateBackendFactory 的类的完全限定类名。</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 默认状态后端</span></span><br><span class="line"><span class="attr">state.backend:</span> <span class="string">hashmap</span></span><br><span class="line"><span class="comment"># 存放检查点的文件路径</span></span><br><span class="line"><span class="attr">state.checkpoints.dir:</span> <span class="string">hdfs://namenode:40010/flink/checkpoints</span></span><br></pre></td></tr></table></figure>

<p>这里的 state.checkpoints.dir 配置项，定义了状态后端将检查点和元数据写入的目录。</p>
<p>为每个作业(Per-job)单独配置状态后端：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setStateBackend(<span class="keyword">new</span> HashMapStateBackend());</span><br><span class="line"><span class="comment">//env.setStateBackend(new EmbeddedRocksDBStateBackend());</span></span><br></pre></td></tr></table></figure>

<p>需添加依赖：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-statebackend-rocksdb_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">version</span>&gt;</span>1.13.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="容错机制"><a href="#容错机制" class="headerlink" title="容错机制"></a>容错机制</h2><h3 id="检查点的保存"><a href="#检查点的保存" class="headerlink" title="检查点的保存"></a>检查点的保存</h3><h4 id="周期性的触发保存"><a href="#周期性的触发保存" class="headerlink" title="周期性的触发保存"></a>周期性的触发保存</h4><p>随时存档确实恢复起来方便，可是需要我们不停地做存档操作。如果每处理一条数据就进行检查点的保存，当大量数据同时到来时，就会耗费很多资源来频繁做检查点，数据处理的速度就会受到影响。所以更好的方式是，每隔一段时间去做一次存档，这样既不会影响数据的正常处理，也不会有太大的延迟——毕竟故障恢复的情况不是随时发生的。在 Flink 中，检查点的保存是周期性触发的，间隔时间可以进行设置。</p>
<p>所以检查点作为应用状态的一份存档，其实就是所有任务状态在同一时间点的一个快照(snapshot)，它的触发是周期性的。具体来说，当每隔一段时间检查点保存操作被触发时， 就把每个任务当前的状态复制一份，按照一定的逻辑结构放在一起持久化保存起来，就构成了检查点。</p>
<h4 id="保存的时间点"><a href="#保存的时间点" class="headerlink" title="保存的时间点"></a>保存的时间点</h4><p>这里有一个关键问题：当检查点的保存被触发时，任务有可能正在处理某个数据，这时该怎么办呢？最简单的想法是，可以在某个时刻按下暂停键，让所有任务停止处理数据。这样状态就不再更改，大家可以一起复制保存；保存完毕之后，再同时恢复数据处理就可以了。 然而仔细思考就会发现这有很多问题。这种想法其实是粗暴地停止一切来拍照，在保存检查点的过程中，任务完全中断了，这会造成很大的延迟；我们之前为了实时性做出的所有设计就毁在了做快照上。另一方面，我们做快照的目的是为了故障恢复；现在的快照中，有些任务正在处理数据，那它保存的到底是处理到什么程度的状态呢？举个例子，我们在程序中某一步操作中自定义了一个 ValueState，处理的逻辑是：当遇到一个数据时，状态先加 1；而后经过一些其他步骤后再加 1。现在停止处理数据，状态到底是被加了 1 还是加了 2 呢？这很重要，因为状态恢复之后，我们需要知道当前数据从哪里开始继续处理。要满足这个要求，就必须将暂停时的所有环境信息都保存下来——而这显然是很麻烦的。 为了解决这个问题，我们不应该一刀切把所有任务同时停掉，而是至少得先把手头正在处理的数据弄完。这样的话，我们在检查点中就不需要保存所有上下文信息，只要知道当前处理到哪个数据就可以了。</p>
<p>但这样依然会有问题：分布式系统的节点之间需要通过网络通信来传递数据，如果我们保存检查点的时候刚好有数据在网络传输的路上，那么下游任务是没法将数据保存起来的；故障重启之后，我们只能期待上游任务重新发送这个数据。然而上游任务是无法知道下游任务是否收到数据的，只能盲目地重发，这可能导致下游将数据处理两次，结果就会出现错误。</p>
<p>所以我们最终的选择是；当所有任务都恰好处理完一个相同的输入数据的时候，将它们的状态保存下来。首先，这样避免了除状态之外其他额外信息的存储，提高了检查点保存的效率。 其次，一个数据要么就是被所有任务完整地处理完，状态得到了保存；要么就是没处理完，状态全部没保存：这就相当于构建了一个事务(transaction)。如果出现故障，我们恢复到之前保存的状态，故障时正在处理的所有数据都需要重新处理；所以我们只需要让源(source) 任务向数据源重新提交偏移量、请求重放数据就可以了。这<strong>需要源任务可以把偏移量作为算子状态保存下来</strong>，而且<strong>外部数据源能够重置偏移量</strong>；Kafka 就是满足这些要求的一个最好的例子。</p>
<h4 id="保存的具体流程"><a href="#保存的具体流程" class="headerlink" title="保存的具体流程"></a>保存的具体流程</h4><p>检查点的保存，最关键的就是要等所有任务将同一个数据处理完毕。下面我们通过一个具体的例子，来详细描述一下检查点具体的保存过程。回忆一下我们最初实现的统计词频的程序——WordCount。这里为了方便，我们直接从数据源读入已经分开的一个个单词，例如这里输入的就是：hello world hello flink hello world hello flink …… 对应的代码就可以简化为：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;Tuple2&lt;String, Long&gt;&gt; wordCountStream =</span><br><span class="line">   env.addSource(...)</span><br><span class="line">       .map(word -&gt; Tuple2.of(word, <span class="number">1L</span>))</span><br><span class="line">       .returns(Types.TUPLE(Types.STRING, Types.LONG))</span><br><span class="line">       .keyBy(t -&gt; t.f0)</span><br><span class="line">       .sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure>

<p>源(Source)任务从外部数据源读取数据，并<strong>记录当前的偏移量</strong>，作为<strong>算子状态(Operator State)<strong>保存下来。然后将数据发给下游的 Map 任务，它会将一个单词转换成(word, count)二元组，初始 count 都是 1，也就是(“hello”, 1)、(“world”, 1)这样的形式；这是一个无状态的算子任务。进而以 word 作为键(key)进行分区，调用.sum()方法就可以对 count 值进行求和统计了；Sum 算子会把当前</strong>求和的结果</strong>作为**按键分区状态(Keyed State)**保存下来。最后得到的就是当前单词的频次统计(word, count)，如图 10-2 所示。</p>
<p><img src="/Flink/%E6%88%AA%E5%B1%8F2022-07-11%20%E4%B8%8B%E5%8D%883.02.09.png" alt="截屏2022-07-11 下午3.02.09"></p>
<p>当我们需要保存检查点(checkpoint)时，就是在所有任务处理完同一条数据后，对状态做个快照保存下来。例如上图中，已经处理了 3 条数据：”hello” “world” “hello”，所以我们会看到 Source 算子的偏移量为 3；后面的 Sum 算子处理完第三条数据”hello”之后，此时已经有 2 个”hello”和 1 个”world”，所以对应的状态为”hello”-&gt; 2，”world”-&gt; 1(这里 KeyedState 底层会以 key-value 形式存储)。此时所有任务都已经处理完了前三个数据，所以我们可以把当前的状态保存成一个检查点，写入外部存储中。至于具体保存到哪里，这是由状态后端的配置项检查点存储(CheckpointStorage)来决定的，可以有作业管理器的堆内存(JobManagerCheckpointStorage)和文件系统(FileSystemCheckpointStorage)两种选择。一般情况下，我们会将检查点写入持久化的分布式文件系统。</p>
<h3 id="从检查点恢复状态"><a href="#从检查点恢复状态" class="headerlink" title="从检查点恢复状态"></a>从检查点恢复状态</h3><p>在运行流处理程序时，Flink 会周期性地保存检查点。当发生故障时，就需要找到最近一次成功保存的检查点来恢复状态。例如在上节的 word count 示例中，我们处理完三个数据后保存了一个检查点。之后继续运行，又正常处理了一个数据”flink”，在处理第五个数据”hello”时发生了故障，如图 10-3 所示。</p>
<p><img src="/Flink/%E6%88%AA%E5%B1%8F2022-07-11%20%E4%B8%8B%E5%8D%883.09.55.png" alt="截屏2022-07-11 下午3.09.55"></p>
<p>这里 Source 任务已经处理完毕，所以偏移量为 5；Map 任务也处理完成了。而 Sum 任务在处理中发生了故障，此时状态并未保存。</p>
<p>接下来就需要从检查点来恢复状态了。具体的步骤为：</p>
<h4 id="重启应用"><a href="#重启应用" class="headerlink" title="重启应用"></a>重启应用</h4><p>遇到故障之后，第一步当然就是重启。我们将应用重新启动后，<strong>所有任务的状态会清空</strong>， 如图 10-4 所示。</p>
<p><img src="/Flink/%E6%88%AA%E5%B1%8F2022-07-11%20%E4%B8%8B%E5%8D%883.12.40.png" alt="截屏2022-07-11 下午3.12.40"></p>
<h4 id="读取检查点，重置状态"><a href="#读取检查点，重置状态" class="headerlink" title="读取检查点，重置状态"></a>读取检查点，重置状态</h4><p>找到最近一次保存的检查点，从中读出每个算子任务状态的快照，分别填充到对应的状态中。这样，Flink 内部所有任务的状态，就恢复到了保存检查点的那一时刻，也就是刚好处理完第三个数据的时候，如图 10-5 所示。这里 key 为”flink”并没有数据到来，所以初始为 0。</p>
<p><img src="/Flink/%E6%88%AA%E5%B1%8F2022-07-11%20%E4%B8%8B%E5%8D%883.32.10.png" alt="截屏2022-07-11 下午3.32.10"></p>
<h4 id="重放数据"><a href="#重放数据" class="headerlink" title="重放数据"></a>重放数据</h4><p>从检查点恢复状态后还有一个问题：如果直接继续处理数据，那么保存检查点之后、到发生故障这段时间内的数据，也就是第 4、5 个数据(“flink” “hello”)就相当于丢掉了；这会造成计算结果的错误。为了不丢数据，我们应该从保存检查点后开始重新读取数据，这可以通过 Source 任务**向外部数据源重新提交偏移量(offset)**来实现，如图 10-6 所示。</p>
<p><img src="/Flink/%E6%88%AA%E5%B1%8F2022-07-11%20%E4%B8%8B%E5%8D%883.35.06.png" alt="截屏2022-07-11 下午3.35.06"></p>
<h4 id="继续处理数据"><a href="#继续处理数据" class="headerlink" title="继续处理数据"></a>继续处理数据</h4><p>接下来，我们就可以正常处理数据了。首先是重放第 4、5 个数据，然后继续读取后面的数据，如图 10-7 所示。</p>
<p><img src="/Flink/%E6%88%AA%E5%B1%8F2022-07-11%20%E4%B8%8B%E5%8D%883.35.40.png" alt="截屏2022-07-11 下午3.35.40"></p>
<p>当处理到第 5 个数据时，就已经追上了发生故障时的系统状态。之后继续处理，就好像没有发生过故障一样；我们既没有丢掉数据也没有重复计算数据，这就保证了计算结果的正确性。 在分布式系统中，这叫作实现了”精确一次”(exactly-once)的状态一致性保证。</p>
<p>这里我们也可以发现，想要正确地从检查点中读取并恢复状态，必须知道**每个算子任务状态的类型和它们的先后顺序(拓扑结构)**；因此为了可以从之前的检查点中恢复状态，我们在改动程序、修复 bug 时要保证状态的拓扑顺序和类型不变。状态的拓扑结构在 JobManager 上可以由 JobGraph 分析得到，而检查点保存的定期触发也是由 JobManager 控制的；所以故障恢复的过程需要 JobManager 的参与。</p>
<h3 id="检查点算法"><a href="#检查点算法" class="headerlink" title="检查点算法"></a>检查点算法</h3><p>略</p>
<h3 id="检查点配置"><a href="#检查点配置" class="headerlink" title="检查点配置"></a>检查点配置</h3><h4 id="启用检查点"><a href="#启用检查点" class="headerlink" title="启用检查点"></a>启用检查点</h4><p>默认情况下，Flink 程序是禁用检查点的。如果想要为 Flink 应用开启自动保存快照的功能，需要在代码中显式地调用执行环境的.enableCheckpointing()方法：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); </span><br><span class="line"><span class="comment">// 每隔1秒启动一次检查点保存 </span></span><br><span class="line">env.enableCheckpointing(<span class="number">1000</span>);</span><br></pre></td></tr></table></figure>

<p>这里需要传入一个长整型的<strong>毫秒数</strong>，表示周期性保存检查点的间隔时间。如果不传参数直接启用检查点，默认的间隔周期为 500 毫秒，这种方式已经被弃用。检查点的间隔时间是对处理性能和故障恢复速度的一个权衡。如果我们希望对性能的影响更小，可以调大间隔时间；而如果希望故障重启后迅速赶上实时的数据处理，就需要将间隔时间设小一些。</p>
<h4 id="检查点存储-Checkpoint-Storage"><a href="#检查点存储-Checkpoint-Storage" class="headerlink" title="检查点存储(Checkpoint Storage)"></a>检查点存储(Checkpoint Storage)</h4><p>检查点具体的持久化存储位置，取决于”检查点存储”(CheckpointStorage)的设置。默认情况下，检查点存储在 JobManager 的堆(heap)内存中。而对于大状态的持久化保存，Flink 也提供了在其他存储位置进行保存的接口，这就是 CheckpointStorage。具体可以通过调用检查点配置的.setCheckpointStorage()来配置，需要传入一个 CheckpointStorage 的实现类。Flink 主要提供了两种 CheckpointStorage：**作业管理器的堆内存(JobManagerCheckpointStorage)<strong>和</strong>文件系统(FileSystemCheckpointStorage)**。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//配置存储检查点到JobManager堆内存 </span></span><br><span class="line">env.getCheckpointConfig().setCheckpointStorage(<span class="keyword">new</span> JobManagerCheckpointStorage());</span><br><span class="line"><span class="comment">//配置存储检查点到文件系统 </span></span><br><span class="line">env.getCheckpointConfig().setCheckpointStorage(<span class="keyword">new</span> FileSystemCheckpointStorage(<span class="string">&quot;hdfs://namenode:40010/flink/checkpoints&quot;</span>));</span><br></pre></td></tr></table></figure>

<p>对于实际生产应用，我们一般会将 CheckpointStorage 配置为高可用的分布式文件系统 (HDFS，S3 等)。</p>
<h4 id="其他高级配置"><a href="#其他高级配置" class="headerlink" title="其他高级配置"></a><strong>其他高级配置</strong></h4><p>检查点还有很多可以配置的选项，可以通过获取检查点配置(CheckpointConfig)来进行设置。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">CheckpointConfig checkpointConfig = env.getCheckpointConfig();</span><br></pre></td></tr></table></figure>

<p>我们这里做一个简单的列举说明：</p>
<h5 id="检查点模式-CheckpointingMode"><a href="#检查点模式-CheckpointingMode" class="headerlink" title="检查点模式(CheckpointingMode)"></a>检查点模式(CheckpointingMode)</h5><p>设置检查点一致性的保证级别，有”精确一次”(exactly-once)和”至少一次”(at-least-once)两个选项。默认级别为 exactly-once，而对于大多数低延迟的流处理程序，at-least-once 就够用了，而且处理效率会更高。</p>
<h5 id="超时时间-checkpointTimeout"><a href="#超时时间-checkpointTimeout" class="headerlink" title="超时时间(checkpointTimeout)"></a>超时时间(checkpointTimeout)</h5><p>用于指定检查点保存的超时时间，超时没完成就会被丢弃掉。传入一个长整型毫秒数作为参数，表示超时时间。</p>
<h5 id="最小间隔时间-minPauseBetweenCheckpoints"><a href="#最小间隔时间-minPauseBetweenCheckpoints" class="headerlink" title="最小间隔时间(minPauseBetweenCheckpoints)"></a>最小间隔时间(minPauseBetweenCheckpoints)</h5><p>用于指定在上一个检查点完成之后，检查点协调器(checkpoint coordinator)最快等多久可以出发保存下一个检查点的指令。这就意味着即使已经达到了周期触发的时间点，只要距离上一个检查点完成的间隔不够，就依然不能开启下一次检查点的保存。这就为正常处理数据留下了充足的间隙。当指定这个参数时，maxConcurrentCheckpoints 的值强制为 1。</p>
<h5 id="最大并发检查点数量-maxConcurrentCheckpoints"><a href="#最大并发检查点数量-maxConcurrentCheckpoints" class="headerlink" title="最大并发检查点数量(maxConcurrentCheckpoints)"></a>最大并发检查点数量(maxConcurrentCheckpoints)</h5><p>用于指定运行中的检查点最多可以有多少个。由于每个任务的处理进度不同，完全可能出现后面的任务还没完成前一个检查点的保存、前面任务已经开始保存下一个检查点了。这个参数就是限制同时进行的最大数量。如果前面设置了 minPauseBetweenCheckpoints，则 maxConcurrentCheckpoints 这个参数就不起作用了。</p>
<h5 id="开启外部持久化存储-enableExternalizedCheckpoints"><a href="#开启外部持久化存储-enableExternalizedCheckpoints" class="headerlink" title="开启外部持久化存储(enableExternalizedCheckpoints)"></a>开启外部持久化存储(enableExternalizedCheckpoints)</h5><p>用于开启检查点的外部持久化，而且默认在作业失败的时候不会自动清理，如果想释放空间需要自己手工清理。里面传入的参数ExternalizedCheckpointCleanup 指定了当作业取消的时候外部的检查点该如何清理。</p>
<p>DELETE_ON_CANCELLATION：在作业取消的时候会自动删除外部检查点，但是如果是作业失败退出，则会保留检查点。</p>
<p>RETAIN_ON_CANCELLATION：作业取消的时候也会保留外部检查点。 </p>
<p><strong>检查点异常时是否让整个任务失败(failOnCheckpointingErrors)</strong></p>
<p>用于指定在检查点发生异常的时候，是否应该让任务直接失败退出。默认为 true，如果设置为 false，则任务会丢弃掉检查点然后继续运行。 </p>
<p><strong>不对齐检查点(enableUnalignedCheckpoints)</strong></p>
<p>不再执行检查点的分界线对齐操作，启用之后可以大大减少产生背压时的检查点保存时间。这个设置要求检查点模式(CheckpointingMode)必须为 exctly-once，并且并发的检查点个数为 1。</p>
<p>代码中具体设置如下:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="comment">//启用检查点，间隔时间1秒</span></span><br><span class="line">env.enableCheckpointing(<span class="number">1000</span>);</span><br><span class="line"></span><br><span class="line">CheckpointConfig checkpointConfig = env.getCheckpointConfig();</span><br><span class="line"><span class="comment">//设置精确一次模式</span></span><br><span class="line">checkpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE); </span><br><span class="line"><span class="comment">//最小间隔时间500毫秒</span></span><br><span class="line">checkpointConfig.setMinPauseBetweenCheckpoints(<span class="number">500</span>);</span><br><span class="line"><span class="comment">//超时时间1分钟</span></span><br><span class="line">checkpointConfig.setCheckpointTimeout(<span class="number">60000</span>);</span><br><span class="line"><span class="comment">//同时只能有一个检查点</span></span><br><span class="line">checkpointConfig.setMaxConcurrentCheckpoints(<span class="number">1</span>);</span><br><span class="line"><span class="comment">//开启检查点的外部持久化保存，作业取消后依然保留</span></span><br><span class="line">checkpointConfig.enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);</span><br><span class="line"><span class="comment">//启用不对齐的检查点保存方式</span></span><br><span class="line">checkpointConfig.enableUnalignedCheckpoints();</span><br><span class="line"><span class="comment">//设置检查点存储，可以直接传入一个String，指定文件系统的路径</span></span><br><span class="line">checkpointConfig.setCheckpointStorage(<span class="string">&quot;hdfs://my/checkpoint/dir&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="保存点-Savepoint"><a href="#保存点-Savepoint" class="headerlink" title="保存点(Savepoint)"></a>保存点(Savepoint)</h3><p>除了检查点(checkpoint)外，Flink 还提供了另一个非常独特的镜像保存功能——保存点 (Savepoint)。</p>
<p>从名称就可以看出，这也是一个存盘的备份，它的原理和算法与检查点完全相同，只是<strong>多了一些额外的元数据</strong>。事实上，保存点就是通过检查点的机制来创建流式作业状态的一致性镜像(consistent image)的。</p>
<p>保存点中的状态快照，是以算子 ID 和状态名称组织起来的，相当于一个键值对。从保存点启动应用程序时，Flink 会将保存点的状态数据重新分配给相应的算子任务。</p>
<h4 id="保存点的用途"><a href="#保存点的用途" class="headerlink" title="保存点的用途"></a>保存点的用途</h4><p>保存点与检查点最大的区别，就是<strong>触发的时机</strong>。检查点是由 Flink 自动管理的，定期创建， 发生故障之后自动读取进行恢复，这是一个”自动存盘”的功能；而<strong>保存点不会自动创建</strong>，必须由用户明确地<strong>手动触发</strong>保存操作，所以就是”手动存盘”。因此两者尽管原理一致，但用途就有所差别了：检查点主要用来做故障恢复，是容错机制的核心；保存点则更加灵活，可以用来做有计划的手动备份和恢复。</p>
<p>保存点可以当作一个强大的运维工具来使用。我们可以在需要的时候创建一个保存点，然后停止应用，做一些处理调整之后再从保存点重启。它适用的具体场景有：</p>
<p><strong>版本管理和归档存储</strong></p>
<p>对重要的节点进行手动备份，设置为某一版本，归档(archive)存储应用程序的状态。 </p>
<p><strong>更新Flink版本</strong></p>
<p>目前 Flink 的底层架构已经非常稳定，所以当 Flink 版本升级时，程序本身一般是兼容的。</p>
<p>这时不需要重新执行所有的计算，只要创建一个保存点，停掉应用、升级 Flink 后，从保存点重启就可以继续处理了。</p>
<p><strong>更新应用程序</strong></p>
<p>我们不仅可以在应用程序不变的时候，更新 Flink 版本；还可以直接更新应用程序。前提是程序必须是兼容的，也就是说更改之后的程序，状态的拓扑结构和数据类型都是不变的，这样才能正常从之前的保存点去加载。这个功能非常有用。我们可以及时修复应用程序中的逻辑 bug，更新之后接着继续处理；也可以用于有不同业务逻辑的场景，比如 A/B 测试等等。</p>
<p><strong>调整并行度</strong></p>
<p>如果应用运行的过程中，发现需要的资源不足或已经有了大量剩余，也可以通过从保存点重启的方式，将应用程序的并行度增大或减小。</p>
<p><strong>暂停应用程序</strong></p>
<p>有时候我们不需要调整集群或者更新程序，只是单纯地希望把应用暂停、释放一些资源来处理更重要的应用程序。使用保存点就可以灵活实现应用的暂停和重启，可以对有限的集群资源做最好的优化配置。 </p>
<p>需要注意的是，保存点能够在程序更改的时候依然兼容，前提是状态的拓扑结构和数据类型不变。我们知道保存点中状态都是以<strong>算子ID-状态名称</strong>这样的 key-value 组织起来的，算子ID可以在代码中直接调用 SingleOutputStreamOperator 的.uid() 方法来进行指定：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataStream&lt;String&gt; stream = env</span><br><span class="line">  .addSource(<span class="keyword">new</span> StatefulSource())</span><br><span class="line">  .uid(<span class="string">&quot;source-id&quot;</span>)</span><br><span class="line">  .map(<span class="keyword">new</span> StatefulMapper())</span><br><span class="line">  .uid(<span class="string">&quot;mapper-id&quot;</span>)</span><br><span class="line">  .print();</span><br></pre></td></tr></table></figure>

<p>对于没有设置 ID 的算子，Flink 默认会自动进行设置，所以在重新启动应用后可能会导致 ID 不同而无法兼容以前的状态。所以为了方便后续的维护，强烈建议在程序中为每一个算子手动指定 ID。</p>
<h4 id="使用保存点"><a href="#使用保存点" class="headerlink" title="使用保存点"></a>使用保存点</h4><p>保存点的使用非常简单，我们可以使用命令行工具来创建保存点，也可以从保存点恢复作业。</p>
<p><strong>创建保存点</strong></p>
<p>要在命令行中为运行的作业创建一个保存点镜像，只需要执行:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/flink savepoint :jobId [:targetDirectory]</span><br></pre></td></tr></table></figure>

<p>这里 jobId 需要填充要做镜像保存的作业 ID，目标路径 targetDirectory 可选，表示保存点存储的路径。</p>
<p>对于保存点的默认路径，可以通过配置文件 flink-conf.yaml 中的 state.savepoints.dir 项来设定：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">state.savepoints.dir: hdfs:///flink/savepoints</span><br></pre></td></tr></table></figure>

<p>当然对于单独的作业，我们也可以在程序代码中通过执行环境来设置：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">env.setDefaultSavepointDir(<span class="string">&quot;hdfs:///flink/savepoints&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>由于创建保存点一般都是希望更改环境之后重启，所以创建之后往往紧接着就是停掉作业的操作。除了对运行的作业创建保存点，我们也可以在停掉一个作业时直接创建保存点：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/flink stop --savepointPath [:targetDirectory] :jobId</span><br></pre></td></tr></table></figure>

<p><strong>从保存点重启应用</strong><br> 我们已经知道，提交启动一个Flink作业，使用的命令是flink run；现在要从保存点重启一个应用，其实本质是一样的：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/flink run -s :savepointPath [:runArgs]</span><br></pre></td></tr></table></figure>

<p>这里只要增加一个-s 参数，指定保存点的路径就可以了，其他启动时的参数还是完全一样的。</p>
<h3 id="状态一致性"><a href="#状态一致性" class="headerlink" title="状态一致性"></a>状态一致性</h3><p>完整的流处理应用，应该包括了<strong>数据源</strong>、<strong>流处理器</strong>和<strong>外部存储系统</strong>三个部分。这个完整应用的一致性，就叫作端到端(end-to-end)的状态一致性，它取决于三个组件中最弱的那一环。一般来说，能否达到 at-least-once 一致性级别，主要看数据源能够重放数据；而能否达到 exactly-once 级别，流处理器内部、数据源、外部存储都要有相应的保证机制。</p>
<h4 id="输入端保障"><a href="#输入端保障" class="headerlink" title="输入端保障"></a>输入端保障</h4><p>输入端主要指的就是 Flink 读取的<strong>外部数据源</strong>。对于一些数据源来说，并不提供数据的缓冲或是持久化保存，数据被消费之后就彻底不存在了。例如 socket 文本流就是这样，socket 服务器是不负责存储数据的，发送一条数据之后，我们只能消费一次，是一锤子买卖。对于这样的数据源，故障后我们即使通过检查点恢复之前的状态，可保存检查点之后到发生故障期间的数据已经不能重发了，这就会导致数据丢失。所以就只能保证 at-most-once 的一致性语 义，相当于没有保证。</p>
<p>想要在故障恢复后不丢数据，外部数据源就必须拥有重放数据的能力。常见的做法就是<strong>对数据进行持久化保存</strong>，并且可以<strong>重设数据的读取位置</strong>。一个最经典的应用就是 Kafka。在 Flink 的 Source 任务中将数据读取的偏移量保存为状态，这样就可以在故障恢复时从检查点中读取出来，对数据源重置偏移量，重新获取数据。</p>
<p>数据源可重放数据，或者说可重置读取数据偏移量，加上 Flink 的 Source 算子将偏移量作为状态保存进检查点，就可以保证数据不丢。这是达到 at-least-once 一致性语义的基本要求， 当然也是实现端到端 exactly-once 的基本要求。</p>
<h4 id="输出端保障"><a href="#输出端保障" class="headerlink" title="输出端保障"></a>输出端保障</h4><p>有了 Flink 的检查点机制，以及可重放数据的外部数据源，我们已经能做到 at-least-once 了。但是想要实现 exactly-once 却有更大的困难：数据有可能重复写入外部系统。因为检查点保存之后，继续到来的数据也会一一处理，任务的状态也会更新，最终通过 Sink 任务将计算结果输出到外部系统；只是状态改变还没有存到下一个检查点中。这时如果出现故障，这些数据都会重新来一遍，就计算了两次。我们知道对 Flink 内部状态来说，重复计算的动作是没有影响的，因为状态已经回滚，最终改变只会发生一次；但对于外部系统来说， 已经写入的结果就是泼出去的水，已经无法收回了，再次执行写入就会把同一个数据写入两次。所以这时，我们只保证了端到端的 at-least-once 语义。</p>
<p>为了实现端到端 exactly-once，我们还需要对外部存储系统、以及 Sink 连接器有额外的要求。能够保证 exactly-once 一致性的写入方式有两种：<strong>幂等写入</strong><br>和<strong>事务写入</strong>，我们需要外部存储系统对这两种写入方式的支持，而 Flink 也为提供了一些 Sink 连接器接口。</p>
<h5 id="幂等-idempotent-写入"><a href="#幂等-idempotent-写入" class="headerlink" title="幂等(idempotent)写入"></a><strong>幂等(idempotent)写入</strong></h5><p>所谓幂等操作，就是说一个操作可以重复执行很多次，但只导致一次结果更改。也就是说，后面再重复执行就不会对结果起作用了。</p>
<p>数学中一个典型的例子是，ex 的求导下操作，无论做多少次，得到的都是自身。而在数据处理领域，最典型的就是对 HashMap 的插入操作：如果是相同的键值对，后面的重复插入就都没什么作用了。</p>
<p>这相当于说，我们并没有真正解决数据重复计算、写入的问题；而是说，重复写入也没关系，结果不会改变。所以这种方式主要的限制在于外部存储系统必须支持这样的幂等写入：比如 Redis 中键值存储，或者关系型数据库(如 MySQL)中满足查询条件的更新操作。</p>
<p>需要注意，对于幂等写入，遇到故障进行恢复时，有可能会出现短暂的不一致。因为保存点完成之后到发生故障之间的数据，其实已经写入了一遍，回滚的时候并不能消除它们。如果有一个外部应用读取写入的数据，可能会看到奇怪的现象：短时间内，结果会突然跳回到之前的某个值，然后重播一段之前的数据。不过当数据的重放逐渐超过发生故障的点的时候，最终的结果还是一致的。</p>
<h5 id="事务-transactional-写入"><a href="#事务-transactional-写入" class="headerlink" title="事务(transactional)写入"></a>事务(transactional)写入</h5><p>如果说幂等写入对应用场景限制太多，那么事务写入可以说是更一般化的保证一致性的方式。我们都知道，事务(transaction)是应用程序中一系列严密的操作，所有操作必须成功完成，否则在每个操作中所做的所有更改都会被撤消。事务有四个基本特性：原子性(Atomicity)、 一致性(Correspondence)、隔离性(Isolation)和持久性(Durability)，这就是著名的 ACID。</p>
<p>在 Flink 流处理的结果写入外部系统时，如果能够构建一个事务，<strong>让写入操作可以随着检查点来提交和回滚</strong>，那么自然就可以解决重复写入的问题了。所以事务写入的基本思想就是：用一个事务来进行数据向外部系统的写入，这个事务是与检查点绑定在一起的。当 Sink 任务遇到 barrier 时，开始保存状态的同时就开启一个事务，接下来所有数据的写入都在这个事务中；待到当前检查点保存完毕时，将事务提交，所有写入的数据就真正可用了。如果中间过程出现故障，状态会回退到上一个检查点，而当前事务没有正常关闭(因为当前检查点没有保存完)，所以也会回滚，写入到外部的数据就被撤销了。</p>
<p>具体来说，又有两种实现方式：<strong>预写日志(WAL)<strong>和</strong>两阶段提交(2PC)</strong> </p>
<p><strong>预写日志(write-ahead-log，WAL)</strong></p>
<p>我们发现，事务提交是需要外部存储系统支持事务的，否则没有办法真正实现写入的回撤。 那对于一般不支持事务的存储系统，能够实现事务写入呢？预写日志(WAL)就是一种非常简单的方式。具体步骤是：1、先把结果数据作为日志(log)状态保存起来 2、进行检查点保存时，也会将这些结果数据一并做持久化存储 3、在收到检查点完成的通知时，将所有结果一次性写入外部系统。 我们会发现，这种方式类似于检查点完成时做一个批处理，一次性的写入会带来一些性能上的问题；而优点就是比较简单，由于数据提前在状态后端中做了缓存，所以无论什么外部存储系统，理论上都能用这种方式一批搞定。在 Flink 中 DataStream API 提供了一个模板类 GenericWriteAheadSink，用来实现这种事务型的写入方式。</p>
<p>需要注意的是，预写日志这种一批写入的方式，有可能会写入失败；所以在执行写入动作之后，必须等待发送成功的返回确认消息。在成功写入所有数据后，在内部再次确认相应的检查点，这才代表着检查点的真正完成。这里需要将确认信息也进行持久化保存，在故障恢复时，只有存在对应的确认信息，才能保证这批数据已经写入，可以恢复到对应的检查点位置。 但这种”再次确认”的方式，也会有一些缺陷。如果我们的检查点已经成功保存、数据也成功地一批写入到了外部系统，但是最终保存确认信息时出现了故障，Flink 最终还是会认为没有成功写入。于是发生故障时，不会使用这个检查点，而是需要回退到上一个；这样就会导致这批数据的重复写入。</p>
<p><strong>两阶段提交(two-phase-commit，2PC)</strong></p>
<p>前面提到的各种实现 exactly-once 的方式，多少都有点缺陷，有没有更好的方法呢？自然是有的，这就是传说中的两阶段提交(2PC)。</p>
<p>顾名思义，它的想法是分成两个阶段：先做”预提交”，等检查点完成之后再正式提交。 这种提交方式是真正基于事务的，它需要外部系统提供事务支持。</p>
<p>具体的实现步骤为：1、当第一条数据到来时，或者收到检查点的分界线时，Sink 任务都会启动一个事务。 2、接下来接收到的所有数据，都通过这个事务写入外部系统；这时由于事务没有提交，所以数据尽管写入了外部系统，但是不可用，是”预提交”的状态。3、当 Sink 任务收到 JobManager 发来检查点完成的通知时，正式提交事务，写入的结果就真正可用了。 当中间发生故障时，当前未提交的事务就会回滚，于是所有写入外部系统的数据也就实现了撤回。这种两阶段提交(2PC)的方式充分利用了 Flink 现有的检查点机制：分界线的到来， 就标志着开始一个新事务；而收到来自 JobManager 的 checkpoint 成功的消息，就是提交事务的指令。每个结果数据的写入，依然是流式的，不再有预写日志时批处理的性能问题；最终提交时，也只需要额外发送一个确认信息。所以 2PC 协议不仅真正意义上实现了 exactly-once，而且通过搭载 Flink 的检查点机制来实现事务，只给系统增加了很少的开销。</p>
<p>Flink 提供了 TwoPhaseCommitSinkFunction 接口，方便我们自定义实现两阶段提交的 SinkFunction 的实现，提供了真正端到端的 exactly-once 保证。</p>
<p>不过两阶段提交虽然精巧，却对外部系统有很高的要求。这里将 2PC 对外部系统的要求列举如下:</p>
<p>1、外部系统必须提供事务支持，或者Sink任务必须能够模拟外部系统上的事务。</p>
<p>2、在检查点的间隔期间里，必须能够开启一个事务并接受数据写入。</p>
<p>3、在收到检查点完成的通知之前，事务必须是“等待提交”的状态。在故障恢复的情况下，这可能需要一些时间。如果这个时候外部系统关闭事务(例如超时了)，那么未提交的数据就会丢失。</p>
<p>4、Sink任务必须能够在进程失败后恢复事务。</p>
<p>5、提交事务必须是幂等操作。也就是说，事务的重复提交应该是无效的。</p>
<p>可见，2PC 在实际应用同样会受到比较大的限制。具体在项目中的选型，最终还应该是一致性级别和处理性能的权衡考量。</p>
<h4 id="Flink-和-Kafka-连接时的精确一次保证"><a href="#Flink-和-Kafka-连接时的精确一次保证" class="headerlink" title="Flink 和 Kafka 连接时的精确一次保证"></a>Flink 和 Kafka 连接时的精确一次保证</h4><p>在流处理的应用中，最佳的数据源当然就是可重置偏移量的消息队列了；它不仅可以提供<strong>数据重放</strong>的功能，而且天生就是以流的方式存储和处理数据的。所以作为大数据工具中消息队列的代表，Kafka 可以说与 Flink 是天作之合，实际项目中也经常会看到以 Kafka 作为<strong>数据源</strong>和<strong>写入的外部系统</strong>的应用。</p>
<h5 id="整体介绍"><a href="#整体介绍" class="headerlink" title="整体介绍"></a>整体介绍</h5><p>既然是端到端的 exactly-once，我们依然可以从三个组件的角度来进行分析：</p>
<p>(1)Flink 内部</p>
<p>Flink 内部可以通过<strong>检查点机制</strong>保证状态和处理结果的 exactly-once 语义。</p>
<p>(2)输入端</p>
<p>输入数据源端的 Kafka 可以对数据进行持久化保存，并可以**重置偏移量(offset)**。所以我们可以在 Source 任务(FlinkKafkaConsumer)中将当前读取的偏移量保存为算子状态，写入到检查点中；当发生故障时，从检查点中读取恢复状态，并由连接器 FlinkKafkaConsumer 向 Kafka 重新提交偏移量，就可以重新消费数据、保证结果的一致性了。</p>
<p>(3)输出端<br>输出端保证 exactly-once 的最佳实现，当然就是**两阶段提交(2PC)**。Flink 官方实现的 Kafka 连接器中，提供了写入到 Kafka 的 FlinkKafkaProducer，它就实现了 TwoPhaseCommitSinkFunction 接口：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlinkKafkaProducer</span>&lt;<span class="title">IN</span>&gt; <span class="keyword">extends</span> <span class="title">TwoPhaseCommitSinkFunction</span>&lt;<span class="title">IN</span>, <span class="title">FlinkKafkaProducer</span>.<span class="title">KafkaTransactionState</span>, <span class="title">FlinkKafkaProducer</span>.<span class="title">KafkaTransactionContext</span>&gt; </span>&#123;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>也就是说，我们写入 Kafka 的过程实际上是一个两段式的提交：处理完毕得到结果，写入 Kafka 时是基于事务的”预提交”；等到检查点保存完毕，才会提交事务进行”正式提交”。如果中间出现故障，事务进行回滚，预提交就会被放弃；恢复状态之后，也只能恢复所有已经确认提交的操作。</p>
<h5 id="具体步骤"><a href="#具体步骤" class="headerlink" title="具体步骤"></a>具体步骤</h5><p><img src="/Flink/%E6%88%AA%E5%B1%8F2022-07-11%20%E4%B8%8B%E5%8D%884.33.23.png" alt="截屏2022-07-11 下午4.33.23"></p>
<p>这是一个 Flink 与 Kafka 构建的完整数据管道，Source 任务从 Kafka 读取数据，经过一系列处理(比如窗口计算)，然后由 Sink 任务将结果再写入 Kafka。</p>
<p>Flink 与 Kafka 连接的两阶段提交，离不开检查点的配合，这个过程需要 JobManager 协调各个 TaskManager 进行状态快照，而检查点具体存储位置则是由状态后端(State Backend)来配置管理的。一般情况，我们会将检查点存储到分布式文件系统上。</p>
<p>实现端到端 exactly-once 的具体过程可以分解如下：</p>
<p><strong>(1)启动检查点保存</strong></p>
<p>检查点保存的启动，标志着我们进入了两阶段提交协议的”预提交”阶段。当然，现在还没有具体提交的数据。</p>
<p><img src="/Flink/%E6%88%AA%E5%B1%8F2022-07-11%20%E4%B8%8B%E5%8D%884.35.15.png" alt="截屏2022-07-11 下午4.35.15"></p>
<p>如图 10-15 所示，JobManager 通知各个 TaskManager 启动检查点保存，Source 任务会将检查点分界线(barrier)注入数据流。这个 barrier 可以将数据流中的数据，分为进入当前检查点的集合和进入下一个检查点的集合。</p>
<p><strong>(2)算子任务对状态做快照</strong><br>分界线(barrier)会在算子间传递下去。每个算子收到 barrier 时，会将当前的状态做个快照，保存到状态后端。</p>
<p><img src="/Flink/%E6%88%AA%E5%B1%8F2022-07-11%20%E4%B8%8B%E5%8D%884.36.20.png" alt="截屏2022-07-11 下午4.36.20"></p>
<p>如图 10-16 所示，Source 任务将 barrier 插入数据流后，也会将当前读取数据的偏移量作为状态写入检查点，存入状态后端；然后把 barrier 向下游传递，自己就可以继续读取数据了。接下来 barrier 传递到了内部的 Window 算子，它同样会对自己的状态进行快照保存，写入远程的持久化存储。</p>
<p><strong>(3)Sink 任务开启事务，进行预提交</strong></p>
<p><img src="/Flink/%E6%88%AA%E5%B1%8F2022-07-11%20%E4%B8%8B%E5%8D%884.37.24.png" alt="截屏2022-07-11 下午4.37.24"></p>
<p>如图 10-17 所示，分界线(barrier)终于传到了 Sink 任务，这时 Sink 任务会开启一个事务。接下来到来的所有数据，Sink 任务都会通过这个事务来写入 Kafka。这里 barrier 是检查点的分界线，也是事务的分界线。由于之前的检查点可能尚未完成，因此上一个事务也可能尚未提交；此时 barrier 的到来开启了新的事务，上一个事务尽管可能没有被提交，但也不再接收新的数据了。</p>
<p>对于 Kafka 而言，提交的数据会被标记为未确认(uncommitted)。这个过程就是所谓的预提交(pre-commit)。</p>
<p><strong>(4)检查点保存完成，提交事务</strong></p>
<p>当所有算子的快照都完成，也就是这次的检查点保存最终完成时，JobManager 会向所有任务发确认通知，告诉大家当前检查点已成功保存，如图 10-18 所示。</p>
<p><img src="/Flink/%E6%88%AA%E5%B1%8F2022-07-11%20%E4%B8%8B%E5%8D%884.39.01.png" alt="截屏2022-07-11 下午4.39.01"></p>
<p>当 Sink 任务收到确认通知后，就会正式提交之前的事务，把之前未确认的数据标为已确认，接下来就可以正常消费了。</p>
<p>在任务运行中的任何阶段失败，都会从上一次的状态恢复，所有没有正式提交的数据也会回滚。这样，Flink 和 Kafka 连接构成的流处理系统，就实现了端到端的 exactly-once 状态一致性。</p>
<h5 id="需要的配置"><a href="#需要的配置" class="headerlink" title="需要的配置"></a>需要的配置</h5><p>在具体应用中，实现真正的端到端 exactly-once，还需要有一些额外的配置：</p>
<p>(1)必须启用检查点</p>
<p>(2)在 FlinkKafkaProducer 的构造函数中传入参数 Semantic.EXACTLY_ONCE</p>
<p>(3)配置 Kafka 读取数据的消费者的隔离级别</p>
<p>这里所说的 Kafka，是写入的外部系统。预提交阶段数据已经写入，只是被标记为未提交(uncommitted)，而 Kafka 中默认的隔离级别 isolation.level 是 read_uncommitted，也就是可以读取未提交的数据。这样一来，外部应用就可以直接消费未提交的数据，对于事务性的保证就失效了。所以应该将隔离级别配置为 read_committed，表示消费者遇到未提交的消息时，会停止从分区中消费数据，直到消息被标记为已提交才会再次恢复消费。当然，这样做的话，外部应用消费数据就会有显著的延迟。</p>
<p>(4)事务超时配置</p>
<p>Flink 的 Kafka 连接器中配置的事务超时时间 transaction.timeout.ms 默认是 1 小时，而 Kafka集群配置的事务最大超时时间 transaction.max.timeout.ms 默认是 15 分钟。所以在检查点保存时间很长时，有可能出现 Kafka 已经认为事务超时了，丢弃了预提交的数据；而 Sink 任务认为还可以继续等待。如果接下来检查点保存成功，发生故障后回滚到这个检查点的状态，这部分数据就被真正丢掉了。所以这两个超时时间，前者应该小于等于后者。</p>
<h2 id="TableAPI和SQL"><a href="#TableAPI和SQL" class="headerlink" title="TableAPI和SQL"></a>TableAPI和SQL</h2><p>在 Flink 中这两种 API 被集成在一起，SQL 执行的对象也是 Flink 中的表(Table)，所以我们一般会认为它们是一体的。Flink 是批流统一的处理框架，无论是批处理(DataSet API)还是流处理(DataStream API)，在上层应用中都可以直接使用 Table API 或者 SQL 来实现；这两种 API 对于一张表执行相同的查询操作，得到的结果是完全一样的。</p>
<p>如果我们对关系型数据库和 SQL 非常熟悉，那么 Table API 和 SQL 的使用其实非常简单：只要得到一个表(Table)，然后对它调用 Table API，或者直接写 SQL 就可以了。接下来我们就以一个非常简单的例子上手，初步了解一下这种高层级 API 的使用方法。</p>
<p>我们想要在代码中使用 Table API，必须引入相关的依赖：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-api-java-bridge_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>这里的依赖是一个 Java 的桥接器(bridge)，主要就是负责 Table API 和下层 DataStream API 的连接支持，按照不同的语言分为 Java 版和 Scala 版。</p>
<p>如果我们希望在本地的集成开发环境(IDE)里运行Table API和SQL，还需要引入以下依赖：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-planner-blink_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-scala_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>这里主要添加的依赖是一个”计划器”(planner)，它是 Table API 的核心组件，负责提供运行时环境，并生成程序的执行计划。这里我们用到的是新版的blink planner。由于Flink安装包的 lib 目录下会自带 planner，所以在生产集群环境中提交的作业不需要打包这个依赖。而在 Table API 的内部实现上，部分相关的代码是用 Scala 实现的，所以还需要额外添加一个 Scala 版流处理的相关依赖。</p>
<p>另外，如果想实现自定义的数据格式来做序列化，可以引入下面的依赖:</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="简单实例"><a href="#简单实例" class="headerlink" title="简单实例"></a>简单实例</h3><p>有了基本的依赖，接下来我们就可以尝试在 Flink 代码中使用 Table API 和 SQL 了。比如， 我们可以自定义一些 Event 类型(包含了 user、url 和 timestamp 三个字段，作为输入的数据源；而后从中提取 url 地址和用户名 user 两个字段作为输出。</p>
<p>如果使用 DataStream API，我们可以直接读取数据源后，用一个简单转换算子 map 来做字段的提取。而这个需求直接写 SQL 的话，实现会更加简单：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> url, <span class="keyword">user</span> <span class="keyword">from</span> EventTable;</span><br></pre></td></tr></table></figure>

<p>这里我们把流中所有数据组成的表叫作 EventTable。在 Flink 代码中直接对这个表执行上面的 SQL，就可以得到想要提取的数据了。</p>
<p>在代码中具体实现如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TableExample</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">		<span class="comment">//获取流执行环境</span></span><br><span class="line">		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">		env.setParallelism(<span class="number">1</span>);</span><br><span class="line">		<span class="comment">//读取数据源</span></span><br><span class="line">		SingleOutputStreamOperator&lt;Event&gt; eventStream = env.fromElements(</span><br><span class="line">                    <span class="keyword">new</span> Event(<span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;./home&quot;</span>, <span class="number">1000L</span>),</span><br><span class="line">										<span class="keyword">new</span> Event(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;./cart&quot;</span>, <span class="number">1000L</span>),</span><br><span class="line">										<span class="keyword">new</span> Event(<span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;./prod?id=1&quot;</span>, <span class="number">5</span> * <span class="number">1000L</span>),</span><br><span class="line">										<span class="keyword">new</span> Event(<span class="string">&quot;Cary&quot;</span>, <span class="string">&quot;./home&quot;</span>, <span class="number">60</span> * <span class="number">1000L</span>),</span><br><span class="line">										<span class="keyword">new</span> Event(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;./prod?id=3&quot;</span>, <span class="number">90</span> * <span class="number">1000L</span>),</span><br><span class="line">										<span class="keyword">new</span> Event(<span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;./prod?id=7&quot;</span>, <span class="number">105</span> * <span class="number">1000L</span>)</span><br><span class="line">		);</span><br><span class="line">		<span class="comment">//获取表环境</span></span><br><span class="line">		StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);</span><br><span class="line">		<span class="comment">//将数据流转换成表</span></span><br><span class="line">		Table eventTable = tableEnv.fromDataStream(eventStream);</span><br><span class="line">		<span class="comment">//用执行SQL的方式提取数据</span></span><br><span class="line">		Table visitTable = tableEnv.sqlQuery(<span class="string">&quot;select url, user from &quot;</span> + eventTable);</span><br><span class="line">		<span class="comment">//将表转换成数据流，打印输出</span></span><br><span class="line">    tableEnv.toDataStream(visitTable).print();</span><br><span class="line">		<span class="comment">//执行程序</span></span><br><span class="line">		env.execute();</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这里我们需要创建一个表环境(TableEnvironment)，然后将数据流(DataStream)转换成一个表(Table)；之后就可以执行 SQL 在这个表中查询数据了。查询得到的结果依然是一个表，把它重新转换成流就可以打印输出了。</p>
<p>代码执行的结果如下：</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line">+<span class="selector-tag">I</span><span class="selector-attr">[./home, Alice]</span></span><br><span class="line">+<span class="selector-tag">I</span><span class="selector-attr">[./cart, Bob]</span></span><br><span class="line">+<span class="selector-tag">I</span><span class="selector-attr">[./prod?id=1, Alice]</span></span><br><span class="line">+<span class="selector-tag">I</span><span class="selector-attr">[./home, Cary]</span></span><br><span class="line">+<span class="selector-tag">I</span><span class="selector-attr">[./prod?id=3, Bob]</span></span><br><span class="line">+<span class="selector-tag">I</span><span class="selector-attr">[./prod?id=7, Alice]</span></span><br></pre></td></tr></table></figure>

<p>基于 Table 我们也可以调用一系列查询方法直接进行转换，这就是所谓 Table API 的处理方式：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//用Table API方式提取数据</span></span><br><span class="line">Table clickTable2 = eventTable.select($(<span class="string">&quot;url&quot;</span>), $(<span class="string">&quot;user&quot;</span>));</span><br></pre></td></tr></table></figure>

<h3 id="基本API"><a href="#基本API" class="headerlink" title="基本API"></a>基本API</h3><h4 id="程序架构"><a href="#程序架构" class="headerlink" title="程序架构"></a>程序架构</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//创建表环境</span></span><br><span class="line">TableEnvironment tableEnv = ...;</span><br><span class="line"><span class="comment">//创建输入表，连接外部系统读取数据</span></span><br><span class="line">tableEnv.executeSql(<span class="string">&quot;CREATE TEMPORARY TABLE inputTable ... WITH ( &#x27;connector&#x27; = ... )&quot;</span>);</span><br><span class="line"><span class="comment">//注册一个表，连接到外部系统，用于输出</span></span><br><span class="line">tableEnv.executeSql(<span class="string">&quot;CREATE TEMPORARY TABLE outputTable ... WITH ( &#x27;connector&#x27; = ... )&quot;</span>);</span><br><span class="line"><span class="comment">//执行 SQL 对表进行查询转换，得到一个新的表</span></span><br><span class="line">Table table1 = tableEnv.sqlQuery(<span class="string">&quot;SELECT ... FROM inputTable... &quot;</span>);</span><br><span class="line"><span class="comment">//使用 Table API 对表进行查询转换，得到一个新的表</span></span><br><span class="line">Table table2 = tableEnv.from(<span class="string">&quot;inputTable&quot;</span>).select(...);</span><br><span class="line"><span class="comment">//将得到的结果写入输出表</span></span><br><span class="line">TableResult tableResult = table1.executeInsert(<span class="string">&quot;outputTable&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>这里不是从一个 DataStream 转换成 Table，而是通过执行 DDL 来直接创建一个表。这里执行的 CREATE 语句中<strong>用 WITH 指定了外部系统的连接器</strong>，于是就可以连接外部系统读取数据了。这其实是更加一般化的程序架构，因为这样我们就可以完全抛开 DataStream API，直接用 SQL 语句实现全部的流处理过程。而后面对于输出表的定义是完全一样的。可以发现，<strong>在创建表的过程中，其实并不区分输入还是输出</strong>，只需要将这个表<strong>注册</strong>进来、连接到外部系统就可以了；这里的 inputTable、 outputTable 只是注册的表名，并不代表处理逻辑，可以随意更换。至于表的具体作用，则要等到执行后面的查询转换操作时才能明确。我们直接从 inputTable 中查询数据，那么 inputTable 就是输入表；而 outputTable 会接收另外表的结果进行写入，那么就是输出表。</p>
<h4 id="创建表环境"><a href="#创建表环境" class="headerlink" title="创建表环境"></a>创建表环境</h4><p>对于 Flink 这样的流处理框架来说，数据流和表在结构上还是有所区别的。所以使用 Table API 和 SQL 需要一个特别的运行时环境，这就是所谓的表环境(TableEnvironment)。它主要负责：</p>
<p>(1)注册 Catalog 和表</p>
<p>(2)执行 SQL 查询</p>
<p>(3)注册用户自定义函数(UDF)</p>
<p>(4)DataStream 和表之间的转换</p>
<p>这里的 Catalog 就是目录，与标准 SQL 中的概念是一致的，主要用来管理所有数据库(database)和表(table)的元数据(metadata)。通过 Catalog 可以方便地对数据库和表进行查询的管理，所以可以认为我们所定义的表都会挂靠在某个目录下，这样就可以快速检索。 在表环境中可以由用户自定义 Catalog，并在其中注册表和自定义函数(UDF)。默认的 Catalog 就叫作 default_catalog。</p>
<p>每个表和 SQL 的执行，都必须绑定在一个表环境(TableEnvironment)中。TableEnvironment 是Table API中提供的基本接口类，可以通过调用静态的create()方法来创建一个表环境实例。 方法需要传入一个环境的配置参数 EnvironmentSettings，它可以指定当前表环境的执行模式和计划器(planner)。执行模式有批处理和流处理两种选择，默认是流处理模式；计划器默认使用 blink planner。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.TableEnvironment;</span><br><span class="line"></span><br><span class="line">EnvironmentSettings settings = EnvironmentSettings</span><br><span class="line">  																		.newInstance()</span><br><span class="line">  																		.inStreamingMode() <span class="comment">// 使用流处理模式</span></span><br><span class="line">																			.build();</span><br><span class="line">TableEnvironment tableEnv = TableEnvironment.create(settings);</span><br></pre></td></tr></table></figure>

<p>对于流处理场景，其实默认配置就完全够用了。所以我们也可以用另一种更加简单的方式来创建表环境：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.EnvironmentSettings;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.java.StreamTableEnvironment;</span><br><span class="line"></span><br><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);</span><br></pre></td></tr></table></figure>

<p>这里我们引入了一个**流式表环境(StreamTableEnvironment)**，它是继承自 TableEnvironment 的子接口。调用它的 create()方法，只需要直接将当前的流执行环境 (StreamExecutionEnvironment)传入，就可以创建出对应的流式表环境了。</p>
<h4 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h4><p>表(Table)是我们非常熟悉的一个概念，它是关系型数据库中数据存储的基本形式，也是 SQL 执行的基本对象。Flink 中的表概念也并不特殊，是由多个行数据构成的，每个行(Row) 又可以有定义好的多个列(Column)字段；整体来看，表就是固定类型的数据组成的二维矩阵。</p>
<p>为了方便地查询表，表环境中会维护一个目录(Catalog)和表的对应关系。所以表都是通过 Catalog 来进行注册创建的。表在环境中有一个唯一的 ID，由三部分组成：目录(catalog)名，数据库(database)名，以及表名。在默认情况下，目录名为 default_catalog，数据库名为 default_database。所以如果我们直接创建一个叫作 MyTable 的表，它的 ID 就是：</p>
<figure class="highlight ceylon"><table><tr><td class="code"><pre><span class="line"><span class="keyword">default</span><span class="number">_</span>catalog.<span class="keyword">default</span><span class="number">_</span>database.MyTable</span><br></pre></td></tr></table></figure>

<p>具体创建表的方式，有通过**连接器(connector)<strong>和</strong>虚拟表(virtual tables)**两种。</p>
<h5 id="连接器表-Connector-Tables"><a href="#连接器表-Connector-Tables" class="headerlink" title="连接器表(Connector Tables)"></a>连接器表(Connector Tables)</h5><p>最直观的创建表的方式，就是通过连接器(connector)连接到一个外部系统，然后定义出对应的表结构。例如我们可以连接到 Kafka 或者文件系统，将存储在这些外部系统的数据以表的形式定义出来，这样对表的读写就可以通过连接器转换成对外部系统的读写了。当我们在表环境中读取这张表，连接器就会从外部系统读取数据并进行转换；而当我们向这张表写入数据，连接器就会将数据输出(Sink)到外部系统中。</p>
<p>在代码中，我们可以调用表环境的 executeSql()方法，可以传入一个 DDL 作为参数执行 SQL 操作。这里我们传入一个 CREATE 语句进行表的创建，并通过 WITH 关键字指定连接到外部系统的连接器：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">tableEnv.executeSql(<span class="string">&quot;CREATE [TEMPORARY] TABLE MyTable ... WITH ( &#x27;connector&#x27;= ... )&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>这里的 TEMPORARY 关键字可以省略。这里没有定义 Catalog 和 Database，所以都是默认的，表的完整 ID 就是default_catalog.default_database.MyTable。如果希望使用自定义的目录名和库名，可以在环境中进行设置：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">tEnv.useCatalog(<span class="string">&quot;custom_catalog&quot;</span>);</span><br><span class="line">tEnv.useDatabase(<span class="string">&quot;custom_database&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>这样我们创建的表完整 ID 就变成了 custom_catalog.custom_database.MyTable。之后在表环境中创建的所有表，ID 也会都以 custom_catalog.custom_database 作为前缀。</p>
<h5 id="虚拟表-Virtual-Tables"><a href="#虚拟表-Virtual-Tables" class="headerlink" title="虚拟表(Virtual Tables)"></a>虚拟表(Virtual Tables)</h5><p>在环境中注册之后，我们就可以在 SQL 中直接使用这张表进行查询转换了。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Table newTable = tableEnv.sqlQuery(<span class="string">&quot;SELECT ... FROM MyTable... &quot;</span>);</span><br></pre></td></tr></table></figure>

<p>这里调用了表环境的 sqlQuery()方法，直接传入一条 SQL 语句作为参数执行查询，得到的结果是一个 Table 对象。Table 是 Table API 中提供的核心接口类，就代表了一个 Java 中定义的表实例。得到的 newTable 是一个中间转换结果，如果之后又希望直接使用这个表执行 SQL，又该怎么做呢？</p>
<p>由于 newTable 是一个 Table 对象，并没有在表环境中注册；所以我们还需要将这个中间结果表注册到环境中，才能在 SQL 中使用：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">tableEnv.createTemporaryView(<span class="string">&quot;NewTable&quot;</span>, newTable);</span><br></pre></td></tr></table></figure>

<p>我们发现，这里的注册其实是创建了一个虚拟表(Virtual Table)。这个概念与 SQL 语法中的视图(View)非常类似，所以调用的方法也叫作创建虚拟视图(createTemporaryView)。 视图之所以是虚拟的，是因为我们并不会直接保存这个表的内容，并没有实体；只是在用到这张表的时候，会将它对应的查询语句嵌入到 SQL 中。</p>
<p>注册为虚拟表之后，我们就又可以在 SQL 中直接使用 NewTable 进行查询转换了。不难看到，通过虚拟表可以非常方便地让 SQL 分步骤执行得到中间结果，这为代码编写提供了很大的便利。另外，虚拟表也可以让我们在 Table API 和 SQL 之间进行自由切换。一个 Java 中的 Table 对象可以直接调用 Table API 中定义好的查询转换方法，得到一个中间结果表；这跟对注册好的表直接执行 SQL 结果是一样的。</p>
<h4 id="表的查询"><a href="#表的查询" class="headerlink" title="表的查询"></a>表的查询</h4><p>创建好了表，接下来自然就是对表进行查询转换了。对一个表的查询(Query)操作，就对应着流数据的转换(Transform)处理。</p>
<p>Flink 为我们提供了两种查询方式：SQL 和 Table API。</p>
<h5 id="执行-SQL-进行查询"><a href="#执行-SQL-进行查询" class="headerlink" title="执行 SQL 进行查询"></a>执行 SQL 进行查询</h5><p>调用表环境的 sqlQuery()方法，传入一个字符串形式的 SQL 查询语句就可以了。执行得到的结果，是一个 Table 对象。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//创建表环境</span></span><br><span class="line">TableEnvironment tableEnv = ...;</span><br><span class="line"><span class="comment">//创建表</span></span><br><span class="line">tableEnv.executeSql(<span class="string">&quot;CREATE TABLE EventTable ... WITH ( &#x27;connector&#x27; = ... )&quot;</span>);</span><br><span class="line"><span class="comment">//查询用户Alice的点击事件，并提取表中前两个字段 </span></span><br><span class="line">Table aliceVisitTable = tableEnv.sqlQuery(</span><br><span class="line">   <span class="string">&quot;SELECT user, url &quot;</span> +</span><br><span class="line">   <span class="string">&quot;FROM EventTable &quot;</span> +</span><br><span class="line">   <span class="string">&quot;WHERE user = &#x27;Alice&#x27; &quot;</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<p>前 Flink 支持标准 SQL 中的绝大部分用法，并提供了丰富的计算函数。这样我们就可以把已有的技术迁移过来，像在 MySQL、Hive 中那样直接通过编写 SQL 实现自己的处理需求，从而大大降低了 Flink 上手的难度。</p>
<p>例如，我们也可以通过 GROUP BY 关键字定义分组聚合，调用 COUNT()、SUM()这样的函数来进行统计计算：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Table urlCountTable = tableEnv.sqlQuery(</span><br><span class="line">   <span class="string">&quot;SELECT user, COUNT(url) &quot;</span> +</span><br><span class="line">   <span class="string">&quot;FROM EventTable &quot;</span> +</span><br><span class="line">   <span class="string">&quot;GROUP BY user &quot;</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<p>上面的例子得到的是一个新的 Table 对象，我们可以再次将它注册为虚拟表继续在 SQL 中调用。另外，我们也可以直接将查询的结果写入到已经注册的表中，这需要调用表环境的 executeSql() 方法来执行 DDL，传入的是一个 INSERT 语句：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//注册表</span></span><br><span class="line">tableEnv.executeSql(<span class="string">&quot;CREATE TABLE EventTable ... WITH ( &#x27;connector&#x27; = ... )&quot;</span>);</span><br><span class="line">tableEnv.executeSql(<span class="string">&quot;CREATE TABLE OutputTable ... WITH ( &#x27;connector&#x27; = ... )&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//将查询结果输出到OutputTable中 </span></span><br><span class="line">tableEnv.executeSql (</span><br><span class="line"><span class="string">&quot;INSERT INTO OutputTable &quot;</span> +</span><br><span class="line">   <span class="string">&quot;SELECT user, url &quot;</span> +</span><br><span class="line">   <span class="string">&quot;FROM EventTable &quot;</span> +</span><br><span class="line">   <span class="string">&quot;WHERE user = &#x27;Alice&#x27; &quot;</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<h5 id="调用-Table-API-进行查询"><a href="#调用-Table-API-进行查询" class="headerlink" title="调用 Table API 进行查询"></a>调用 Table API 进行查询</h5><p>另外一种查询方式就是调用Table API。这是嵌入在Java和Scala语言内的查询API，核心就是 Table 接口类，通过一步步链式调用 Table 的方法，就可以定义出所有的查询转换操作。 每一步方法调用的返回结果，都是一个 Table。</p>
<p>由于 Table API 是基于 Table 的 Java 实例进行调用的，因此我们首先要得到表的 Java 对象。 基于环境中已注册的表，可以通过表环境的 from() 方法非常容易地得到一个 Table 对象：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Table eventTable = tableEnv.from(<span class="string">&quot;EventTable&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>传入的参数就是注册好的表名。注意这里 <strong>eventTable 是一个 Table 对象</strong>，而 <strong>EventTable 是在环境中注册的表名</strong>。得到 Table 对象之后，就可以调用 API 进行各种转换操作了，得到的是一个新的 Table 对象：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Table maryClickTable = eventTable</span><br><span class="line">       .where($(<span class="string">&quot;user&quot;</span>).isEqual(<span class="string">&quot;Alice&quot;</span>))</span><br><span class="line">       .select($(<span class="string">&quot;url&quot;</span>), $(<span class="string">&quot;user&quot;</span>));</span><br></pre></td></tr></table></figure>

<p>这里每个方法的参数都是一个表达式(Expression)，用方法调用的形式直观地说明了想要表达的内容；$符号用来指定表中的一个字段。上面的代码和直接执行 SQL 是等效的。 Table API 是嵌入编程语言中的 DSL，SQL 中的很多特性和功能必须要有对应的实现才可以使用，因此跟直接写 SQL 比起来肯定就要麻烦一些。目前 Table API 支持的功能相对更少， 可以预见未来 Flink 社区也会以扩展 SQL 为主，为大家提供更加通用的接口方式。</p>
<h5 id="两种-API-的结合使用"><a href="#两种-API-的结合使用" class="headerlink" title="两种 API 的结合使用"></a>两种 API 的结合使用</h5><p>可以发现，无论是调用 Table API 还是执行 SQL，得到的结果都是一个 Table 对象；所以这两种 API 的查询可以很方便地结合在一起。</p>
<p>(1)无论是那种方式得到的 Table 对象，都可以继续调用 Table API 进行查询转换</p>
<p>(2)如果想要对一个表执行 SQL 操作(用 FROM 关键字引用)，必须先在环境中对它进行注册。所以我们可以通过创建虚拟表的方式实现两者的转换</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">tableEnv.createTemporaryView(<span class="string">&quot;MyTable&quot;</span>, myTable);</span><br></pre></td></tr></table></figure>

<p>注意：这里的<strong>第一个参数MyTable是注册的表名</strong>，而<strong>第二个参数myTable是Java中的Table对象</strong>。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Table clickTable = tableEnvironment.sqlQuery(<span class="string">&quot;select url, user from &quot;</span> + eventTable);</span><br></pre></td></tr></table></figure>

<p>这其实是一种简略的写法，我们将 Table 对象名 eventTable 直接以字符串拼接的形式添加到 SQL 语句中，在解析时会自动注册一个同名的虚拟表到环境中，这样就省略了创建虚拟视图的步骤。</p>
<h4 id="输出表"><a href="#输出表" class="headerlink" title="输出表"></a>输出表</h4><p>表的创建和查询，就对应着流处理中的读取数据源(Source)和转换(Transform)；而最后一个步骤 Sink，也就是将结果数据输出到外部系统，就对应着表的输出操作。</p>
<p>在代码上，输出一张表最直接的方法，就是调用 Table 的方法 executeInsert() 方法将一个 Table 写入到注册过的表中，方法传入的参数就是注册的表名。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//注册表，用于输出数据到外部系统</span></span><br><span class="line">tableEnv.executeSql(<span class="string">&quot;CREATE TABLE OutputTable ... WITH ( &#x27;connector&#x27; = ... )&quot;</span>);</span><br><span class="line"><span class="comment">//经过查询转换，得到结果表</span></span><br><span class="line">Table result = ...</span><br><span class="line"><span class="comment">//将结果表写入已注册的输出表中 </span></span><br><span class="line">result.executeInsert(<span class="string">&quot;OutputTable&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>在底层，表的输出是通过将数据写入到 TableSink 来实现的。TableSink 是 Table API 中提供的一个向外部系统写入数据的通用接口，可以支持不同的文件格式(比如 CSV、Parquet)、 存储数据库(比如 JDBC、HBase、Elasticsearch)和消息队列(比如 Kafka)。它有些类似于 DataStream API 中调用 addSink()方法时传入的 SinkFunction，有不同的连接器对它进行了实现。 </p>
<p>这里可以发现，我们在环境中注册的表，其实在写入数据的时候就对应着一个 TableSink。</p>
<h4 id="表和流的转换"><a href="#表和流的转换" class="headerlink" title="表和流的转换"></a>表和流的转换</h4><p>创建表环境开始，历经表的创建、查询转换和输出，我们已经可以使用 Table API 和 SQL 进行完整的流处理了。不过在应用的开发过程中，我们测试业务逻辑一般不会将结果直接写入到外部系统，而是在本地控制台打印输出。对于 DataStream 这非常容易，直接调用 print() 方法就可以看到结果数据流的内容了；但对于 Table 就比较悲剧——它没有提供 print() 方法。 这该怎么办呢？</p>
<p>在 Flink 中我们可以将 Table 再转换成 DataStream，然后进行打印输出。这就涉及了表和流的转换。</p>
<h5 id="将表-Table-转换成流-DataStream"><a href="#将表-Table-转换成流-DataStream" class="headerlink" title="将表(Table)转换成流(DataStream)"></a>将表(Table)转换成流(DataStream)</h5><p><strong>(1)调用 toDataStream() 方法</strong></p>
<p>将一个 Table 对象转换成 DataStream 非常简单，只要直接调用表环境的方法 toDataStream() 就可以了。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Table aliceVisitTable = tableEnv.sqlQuery(</span><br><span class="line">	<span class="string">&quot;SELECT user, url &quot;</span> +</span><br><span class="line">	<span class="string">&quot;FROM EventTable &quot;</span> +</span><br><span class="line">	<span class="string">&quot;WHERE user = &#x27;Alice&#x27; &quot;</span></span><br><span class="line">  );</span><br><span class="line"><span class="comment">//将表转换成数据流</span></span><br><span class="line">tableEnv.toDataStream(aliceVisitTable).print();</span><br></pre></td></tr></table></figure>

<p><strong>(2)调用 toChangelogStream() 方法</strong></p>
<p>将 maryClickTable 转换成流打印输出是很简单的；然而，如果我们同样希望将用户点击次数统计表 urlCountTable 进行打印输出，就会抛出一个 TableException 异常：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Exception in thread <span class="string">&quot;main&quot;</span> org.apache.flink.table.api.TableException: Table sink <span class="string">&#x27;default_catalog.default_database.Unregistered_DataStream_Sink_1&#x27;</span> doesn<span class="string">&#x27;t support consuming update changes ...</span></span><br></pre></td></tr></table></figure>

<p>这表示当前的 TableSink 并不支持表的更新(update)操作。这是什么意思呢？因为 print 本身也可以看作一个 Sink 操作，所以这个异常就是说打印输出的 Sink 操作不支持对数据进行更新。具体来说，urlCountTable 这个表中进行了<strong>分组聚合统计</strong>，所以表中的每一行是会<strong>更新</strong>的。也就是说，Alice的第一个点击事件到来，表中会有一行(Alice, 1)；第二个点击事件到来，这一行就要更新为(Alice, 2)。但之前的(Alice, 1)已经打印输出了，我们怎么能对它进行更改呢？所以就会抛出异常。</p>
<p>解决的思路是，对于这样有更新操作的表，我们不要试图直接把它转换成 DataStream 打印输出，而是记录一下它的更新日志(change log)。这样一来，对于表的所有更新操作， 就变成了一条更新日志的流，我们就可以转换成流打印输出了。</p>
<p>代码中需要调用的是表环境的 toChangelogStream()方法：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Table urlCountTable = tableEnv.sqlQuery(</span><br><span class="line">   <span class="string">&quot;SELECT user, COUNT(url) &quot;</span> +</span><br><span class="line">   <span class="string">&quot;FROM EventTable &quot;</span> +</span><br><span class="line">   <span class="string">&quot;GROUP BY user &quot;</span></span><br><span class="line">);</span><br><span class="line"><span class="comment">//将表转换成更新日志流</span></span><br><span class="line">tableEnv.toChangelogStream(urlCountTable).print();</span><br></pre></td></tr></table></figure>

<p>与更新日志流(Changelog Streams)对应的，是那些只做了简单转换、没有进行聚合统计的表，例如前面提到的maryClickTable。它们的特点是数据只会插入、不会更新，所以也被叫作仅插入流(Insert-Only Streams)。</p>
<h5 id="将流-DataStream-转换成表-Table"><a href="#将流-DataStream-转换成表-Table" class="headerlink" title="将流(DataStream)转换成表(Table)"></a>将流(DataStream)转换成表(Table)</h5><p><strong>(1)调用 fromDataStream()方法</strong></p>
<p>想要将一个DataStream转换成表也很简单，可以通过调用表环境的fromDataStream()方法来实现，返回的就是一个Table对象。例如，我们可以直接将事件流 eventStream 转换成一个表：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="comment">// 获取表环境</span></span><br><span class="line">StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);</span><br><span class="line"><span class="comment">// 读取数据源</span></span><br><span class="line">SingleOutputStreamOperator&lt;Event&gt; eventStream = env.addSource(...)</span><br><span class="line"><span class="comment">// 将数据流转换成表</span></span><br><span class="line">Table eventTable = tableEnv.fromDataStream(eventStream);</span><br></pre></td></tr></table></figure>

<p>由于流中的数据本身就是定义好的 POJO 类型 Event，所以我们将流转换成表之后，每一行数据就对应着一个 Event，而表中的列名就对应着 Event 中的属性。另外，我们还可以在 fromDataStream() 方法中增加参数，用来指定提取哪些属性作为表中的字段名，并可以任意指定位置：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 提取 Event 中的 timestamp 和 url 作为表中的列</span></span><br><span class="line">Table eventTable2 = tableEnv.fromDataStream(eventStream, $(<span class="string">&quot;timestamp&quot;</span>), $(<span class="string">&quot;url&quot;</span>));</span><br></pre></td></tr></table></figure>

<p>需要注意的是，timestamp 本身是 SQL 中的关键字，所以我们在定义表名、列名时要尽量避免。这时可以通过表达式的 <strong>as()</strong> 方法对字段进行重命名：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 将 timestamp 字段重命名为 ts</span></span><br><span class="line">Table eventTable2 = tableEnv.fromDataStream(eventStream, $(<span class="string">&quot;timestamp&quot;</span>).as(<span class="string">&quot;ts&quot;</span>), $(<span class="string">&quot;url&quot;</span>));</span><br></pre></td></tr></table></figure>

<p><strong>(2)调用 createTemporaryView()方法</strong></p>
<p>调用 fromDataStream() 方法简单直观，可以直接实现 DataStream 到 Table 的转换；不过如果我们希望直接在 SQL 中引用这张表，就还需要调用表环境的 createTemporaryView() 方法来创建虚拟视图了。</p>
<p>对于这种场景，也有一种更简洁的调用方式。我们可以直接调用 createTemporaryView() 方法创建虚拟表，传入的两个参数，第一个依然是注册的表名，而第二个可以直接就是 DataStream。之后仍旧可以传入多个参数，用来指定表中的字段</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">tableEnv.createTemporaryView(<span class="string">&quot;EventTable&quot;</span>, eventStream,$(<span class="string">&quot;timestamp&quot;</span>).as(<span class="string">&quot;ts&quot;</span>),$(<span class="string">&quot;url&quot;</span>));</span><br></pre></td></tr></table></figure>

<p><strong>(3)调用 fromChangelogStream ()方法</strong></p>
<p>表环境还提供了一个方法 fromChangelogStream()，可以将一个更新日志流转换成表。这个方法要求流中的数据类型只能是 Row，而且每一个数据都需要指定当前行的更新类型 (RowKind)；所以一般是由连接器帮我们实现的，直接应用比较少见。</p>
<h5 id="支持的数据类型"><a href="#支持的数据类型" class="headerlink" title="支持的数据类型"></a>支持的数据类型</h5><p>整体来看，DataStream 中支持的数据类型，Table 中也是都支持的，只不过在进行转换时需要注意一些细节。</p>
<p><strong>(1)原子类型</strong></p>
<p>在 Flink 中，基础数据类型(Integer、Double、String)和通用数据类型(也就是不可再拆分的数据类型)统一称作原子类型。原子类型的 DataStream，转换之后就成了<strong>只有一列</strong>的 Table，列字段(field)的数据类型可以由原子类型推断出。另外，还可以在 fromDataStream() 方法里增加参数，用来重新命名列字段。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">StreamTableEnvironment tableEnv = ...;</span><br><span class="line">DataStream&lt;Long&gt; stream = ...;</span><br><span class="line"><span class="comment">// 将数据流转换成动态表，动态表只有一个字段，重命名为 myLong</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, $(<span class="string">&quot;myLong&quot;</span>));</span><br></pre></td></tr></table></figure>

<p><strong>(2)Tuple 类型</strong></p>
<p>当原子类型不做重命名时，默认的字段名就是f0，容易想到，这其实就是将原子类型看作了一元组 Tuple1 的处理结果。Table 支持 Flink 中定义的元组类型 Tuple，对应在表中字段名默认就是元组中元素的属性名 f0、f1、f2…。所有字段都可以被重新排序，也可以提取其中的一部分字段。字段还可以通过调用表达式的 as() 方法来进行重命名。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">StreamTableEnvironment tableEnv = ...;</span><br><span class="line">DataStream&lt;Tuple2&lt;Long, Integer&gt;&gt; stream = ...;</span><br><span class="line"><span class="comment">// 将数据流转换成只包含 f1 字段的表</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, $(<span class="string">&quot;f1&quot;</span>));</span><br><span class="line"><span class="comment">// 将数据流转换成包含 f0 和 f1 字段的表，在表中 f0 和 f1 位置交换</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, $(<span class="string">&quot;f1&quot;</span>), $(<span class="string">&quot;f0&quot;</span>));</span><br><span class="line"><span class="comment">// 将 f1 字段命名为 myInt，f0 命名为 myLong</span></span><br><span class="line">Table table = tableEnv.fromDataStream(stream, $(<span class="string">&quot;f1&quot;</span>).as(<span class="string">&quot;myInt&quot;</span>), $(<span class="string">&quot;f0&quot;</span>).as(<span class="string">&quot;myLong&quot;</span>));</span><br></pre></td></tr></table></figure>

<p><strong>(3)POJO 类型</strong></p>
<p>Flink 也支持多种数据类型组合成的复合类型，最典型的就是简单 Java 对象(POJO 类型)。由于 POJO 中已经定义好了可读性强的字段名，这种类型的数据流转换成 Table 就显得无比顺畅了。将 POJO 类型的 DataStream 转换成 Table，如果不指定字段名称，就会直接使用原始 POJO 类型中的字段名称。POJO 中的字段同样可以被重新排序、提却和重命名。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">StreamTableEnvironment tableEnv = ...;</span><br><span class="line">DataStream&lt;Event&gt; stream = ...;</span><br><span class="line">Table table = tableEnv.fromDataStream(stream);</span><br><span class="line">Table table = tableEnv.fromDataStream(stream,$(<span class="string">&quot;user&quot;</span>));</span><br><span class="line">Table table = tableEnv.fromDataStream(stream,$(<span class="string">&quot;user&quot;</span>).as(<span class="string">&quot;myUser&quot;</span>),$(<span class="string">&quot;url&quot;</span>).as(<span class="string">&quot;myUrl&quot;</span>));</span><br></pre></td></tr></table></figure>

<p><strong>(4)Row 类型</strong></p>
<p>Flink 中还定义了一个在关系型表中更加通用的数据类型——行(Row)，它是 Table 中数据的基本组织形式。Row 类型也是一种复合类型，它的长度固定，而且无法直接推断出每个字段的类型，所以在使用时必须指明具体的类型信息；我们在创建 Table 时调用的 CREATE 语句就会将所有的字段名称和类型指定，这在 Flink 中被称为表的模式结构(Schema)。除此之外，Row 类型还附加了一个属性 RowKind，用来表示当前行在更新操作中的类型。这样， Row 就可以用来表示更新日志流(changelog stream)中的数据，从而架起了 Flink 中流和表的转换桥梁。所以在更新日志流中，元素的类型必须是 Row，而且需要调用 ofKind() 方法来指定更新类型。下面是一个具体的例子：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataStream&lt;Row&gt; dataStream =</span><br><span class="line">   env.fromElements(</span><br><span class="line">       Row.ofKind(RowKind.INSERT, <span class="string">&quot;Alice&quot;</span>, <span class="number">12</span>),</span><br><span class="line">       Row.ofKind(RowKind.INSERT, <span class="string">&quot;Bob&quot;</span>, <span class="number">5</span>),</span><br><span class="line">Row.ofKind(RowKind.UPDATE_BEFORE, <span class="string">&quot;Alice&quot;</span>, <span class="number">12</span>),</span><br><span class="line">       Row.ofKind(RowKind.UPDATE_AFTER, <span class="string">&quot;Alice&quot;</span>, <span class="number">100</span>));</span><br><span class="line"><span class="comment">// 将更新日志流转换为表</span></span><br><span class="line">Table table = tableEnv.fromChangelogStream(dataStream);</span><br></pre></td></tr></table></figure>

<h5 id="综合应用示例"><a href="#综合应用示例" class="headerlink" title="综合应用示例"></a>综合应用示例</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">// 获取流环境</span></span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 读取数据源</span></span><br><span class="line">    SingleOutputStreamOperator&lt;Event&gt; eventStream = env</span><br><span class="line">            .fromElements(</span><br><span class="line">                    <span class="keyword">new</span> Event(<span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;./home&quot;</span>, <span class="number">1000L</span>),</span><br><span class="line">                    <span class="keyword">new</span> Event(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;./cart&quot;</span>, <span class="number">1000L</span>),</span><br><span class="line">                    <span class="keyword">new</span> Event(<span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;./prod?id=1&quot;</span>, <span class="number">5</span> * <span class="number">1000L</span>),</span><br><span class="line">                    <span class="keyword">new</span> Event(<span class="string">&quot;Cary&quot;</span>, <span class="string">&quot;./home&quot;</span>, <span class="number">60</span> * <span class="number">1000L</span>),</span><br><span class="line">                    <span class="keyword">new</span> Event(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;./prod?id=3&quot;</span>, <span class="number">90</span> * <span class="number">1000L</span>),</span><br><span class="line">                    <span class="keyword">new</span> Event(<span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;./prod?id=7&quot;</span>, <span class="number">105</span> * <span class="number">1000L</span>)</span><br><span class="line">            );</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取表环境</span></span><br><span class="line">    StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将数据流转换成表</span></span><br><span class="line">    tableEnv.createTemporaryView(<span class="string">&quot;EventTable&quot;</span>, eventStream);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 查询Alice的访问url列表</span></span><br><span class="line">    Table aliceVisitTable = tableEnv.sqlQuery(<span class="string">&quot;SELECT url, user FROM EventTable WHERE user = &#x27;Alice&#x27;&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 统计每个用户的点击次数</span></span><br><span class="line">    Table urlCountTable = tableEnv.sqlQuery(<span class="string">&quot;SELECT user, COUNT(url) FROM EventTable GROUP BY user&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将表转换成数据流，在控制台打印输出</span></span><br><span class="line">    tableEnv.toDataStream(aliceVisitTable).print(<span class="string">&quot;alice visit&quot;</span>);</span><br><span class="line">    tableEnv.toChangelogStream(urlCountTable).print(<span class="string">&quot;count&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 执行程序</span></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="流处理中的表"><a href="#流处理中的表" class="headerlink" title="流处理中的表"></a>流处理中的表</h3><p>当我们将一个 Table 转换成 DataStream 时，有仅插入流(Insert-Only Streams)和更新日志流(Changelog Streams) 两种不同的方式，具体使用哪种方式取决于表中是否存在更新(update)操作。</p>
<p>这种麻烦其实是不可避免的。我们知道，Table API 和 SQL 本质上都是基于关系型表的操作方式；而关系型表(Table)本身是有界的，更适合批处理的场景。所以在 MySQL、Hive 这样的固定数据集中进行查询，使用 SQL 就会显得得心应手。而对于 Flink 这样的流处理框架来说，要处理的是源源不断到来的无界数据流，我们无法等到数据都到齐再做查询，每来一条数据就应该更新一次结果；这时如果一定要使用表和 SQL 进行处理，就会显得有些别扭了，需要引入一些特殊的概念。</p>
<h4 id="动态表和持续查询"><a href="#动态表和持续查询" class="headerlink" title="动态表和持续查询"></a>动态表和持续查询</h4><p>流处理面对的数据是连续不断的，这导致了流处理中的表跟我们熟悉的关系型数据库中的表完全不同；而基于表执行的查询操作，也就有了新的含义。</p>
<p>如果我们希望把流数据转换成表的形式，那么这<strong>表中的数据就会不断增长</strong>；如果进一步基于表执行 SQL 查询，那么得到的结果就不是一成不变的，而是会随着新数据的到来持续更新。</p>
<p><strong>动态表(Dynamic Tables)</strong></p>
<p>当流中有新数据到来，初始的表中会插入一行；而基于这个表定义的 SQL 查询，就应该在之前的基础上更新结果。这样得到的表就会不断地动态变化，被称为动态表(Dynamic Tables)。动态表是 Flink 在 Table API 和 SQL 中的核心概念，它为流数据处理提供了表和 SQL 支持。 我们所熟悉的表一般用来做批处理，面向的是固定的数据集，可以认为是静态表；而动态表则完全不同，它里面的数据会随时间变化。</p>
<p>其实动态表的概念，我们在传统的关系型数据库中已经有所接触。数据库中的表，其实是一系列 INSERT、UPDATE 和 DELETE 语句执行的结果；在关系型数据库中，我们一般把它称为更新日志流(changelog stream)。如果我们保存了表在某一时刻的快照(snapshot)，那么接下来只要读取更新日志流，就可以得到表之后的变化过程和最终结果了。在很多高级关系型数据库(比如 Oracle、DB2)中都有物化视图(Materialized Views)的概念，可以用来缓存 SQL 查询的结果；它的更新其实就是不停地处理更新日志流的过程。</p>
<p><strong>持续查询(Continuous Query)</strong></p>
<p>动态表可以像静态的批处理表一样进行查询操作。由于数据在不断变化，因此基于它定义的 SQL 查询也不可能执行一次就得到最终结果。这样一来，我们对动态表的查询也就永远不会停止，一直在随着新数据的到来而继续执行。这样的查询就被称作持续查询(Continuous Query)。对动态表定义的查询操作，都是持续查询；而持续查询的结果也会是一个动态表。</p>
<p>由于每次数据到来都会触发查询操作，因此可以认为一次查询面对的数据集，就是当前输入动态表中收到的所有数据。这相当于是对输入动态表做了一个快照(snapshot)，当作有限数据集进行批处理；流式数据的到来会触发连续不断的快照查询，像动画一样连贯起来，就构成了持续查询。</p>
<p><img src="/Flink/%E6%88%AA%E5%B1%8F2022-07-18%20%E4%B8%8B%E5%8D%885.10.28.png" alt="截屏2022-07-18 下午5.10.28"></p>
<p>持续查询的步骤如下：</p>
<p>(1)流(stream)被转换为动态表(dynamic table)；</p>
<p>(2)对动态表进行持续查询(continuous query)，生成新的动态表；</p>
<p>(3)生成的动态表被转换成流。</p>
<p>这样，只要 API 将流和动态表的转换封装起来，我们就可以直接在数据流上执行 SQL 查询，用处理表的方式来做流处理了。</p>
<h4 id="用-SQL-持续查询"><a href="#用-SQL-持续查询" class="headerlink" title="用 SQL 持续查询"></a>用 SQL 持续查询</h4><h5 id="更新-Update-查询"><a href="#更新-Update-查询" class="headerlink" title="更新(Update)查询"></a>更新(Update)查询</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//分组聚合统计每个用户的点击次数</span></span><br><span class="line"><span class="comment">//结果表的更新日志(changelog) 流中，包含了 INSERT 和 UPDATE 两种操作</span></span><br><span class="line">Table urlCountTable = tableEnv.sqlQuery(<span class="string">&quot;SELECT user, COUNT(url) as cnt FROM EventTable GROUP BY user&quot;</span>);</span><br></pre></td></tr></table></figure>

<h5 id="追加-Append-查询"><a href="#追加-Append-查询" class="headerlink" title="追加(Append)查询"></a>追加(Append)查询</h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//结果表的更新日志 (changelog)流中只有 INSERT 操作</span></span><br><span class="line">Table aliceVisitTable = tableEnv.sqlQuery(<span class="string">&quot;SELECT url, user FROM EventTable WHERE user = &#x27;Cary&#x27;&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>我们似乎可以总结一个规律：只要用到了聚合，在之前的结果上有叠加，就会产生更新操作，就是一个更新查询。但事实上，更新查询的判断标准是结果表中的数据是否会有 UPDATE 操作，如果聚合的结果不再改变，那么同样也不是更新查询。</p>
<p>什么时候聚合的结果会保持不变呢？一个典型的例子就是窗口聚合。</p>
<p>我们考虑开一个滚动窗口，统计每一小时内所有用户的点击次数，并在结果表中增加一个 endT 字段，表示当前统计窗口的结束时间。这时结果表的字段定义如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">[</span><br><span class="line">user: VARCHAR, <span class="comment">// 用户名</span></span><br><span class="line">endT: TIMESTAMP, <span class="comment">// 窗口结束时间 </span></span><br><span class="line">cnt: BIGINT <span class="comment">// 用户访问 url 的次数</span></span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p><img src="/Flink/%E6%88%AA%E5%B1%8F2022-07-18%20%E4%B8%8B%E5%8D%888.23.04.png" alt="截屏2022-07-18 下午8.23.04"></p>
<p>由于窗口的统计结果是一次性写入结果表的，所以结果表的更新日志流中只会包含插入 INSERT 操作，而没有更新 UPDATE 操作。所以这里的持续查询，依然是一个追加(Append)查询。结果表 result 如果转换成 DataStream，可以直接调用 toDataStream()方 法。</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line">+<span class="selector-tag">I</span><span class="selector-attr">[Alice, 1970-01-01T01:00, 3]</span></span><br><span class="line">+<span class="selector-tag">I</span><span class="selector-attr">[Bob, 1970-01-01T01:00, 1]</span></span><br><span class="line">+<span class="selector-tag">I</span><span class="selector-attr">[Cary, 1970-01-01T02:00, 2]</span></span><br><span class="line">+<span class="selector-tag">I</span><span class="selector-attr">[Bob, 1970-01-01T02:00, 1]</span></span><br></pre></td></tr></table></figure>

<h5 id="查询限制"><a href="#查询限制" class="headerlink" title="查询限制"></a>查询限制</h5><p>在实际应用中，有些持续查询会因为计算代价太高而受到限制。所谓的代价太高，可能是由于需要维护的状态持续增长，也可能是由于更新数据的计算太复杂。</p>
<p><strong>状态大小</strong></p>
<p>用持续查询做流处理，往往会运行至少几周到几个月；所以持续查询处理的数据总量可能非常大。例如我们之前举的更新查询的例子，需要记录每个用户访问 url 的次数。如果随着时间的推移用户数越来越大，那么要维护的状态也将逐渐增长，最终可能会耗尽存储空间导致查询失败。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">user</span>, <span class="built_in">COUNT</span>(url)</span><br><span class="line"><span class="keyword">FROM</span> clicks</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">user</span>;</span><br></pre></td></tr></table></figure>

<p><strong>更新计算</strong></p>
<p>于有些查询来说，更新计算的复杂度可能很高。每来一条新的数据，更新结果的时候可能需要全部重新计算，并且对很多已经输出的行进行更新。一个典型的例子就是 RANK() 函数， 它会基于一组数据计算当前值的排名。例如下面的 SQL 查询，会根据用户最后一次点击的时间为每个用户计算一个排名。当我们收到一个新的数据，用户的最后一次点击时间(lastAction)就会更新，进而所有用户必须重新排序计算一个新的排名。当一个用户的排名发生改变时，被他超过的那些用户的排名也会改变；这样的更新操作无疑代价巨大，而且还会随着用户的增多越来越严重。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">user</span>, <span class="built_in">RANK</span>() <span class="keyword">OVER</span> (<span class="keyword">ORDER</span> <span class="keyword">BY</span> lastAction)</span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line">  <span class="keyword">SELECT</span> <span class="keyword">user</span>, <span class="built_in">MAX</span>(ts) <span class="keyword">AS</span> lastAction <span class="keyword">FROM</span> EventTable <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">user</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<h3 id="时间属性和窗口"><a href="#时间属性和窗口" class="headerlink" title="时间属性和窗口"></a>时间属性和窗口</h3><p>基于时间的操作(比如时间窗口)，需要定义相关的时间语义和时间数据来源的信息。在 Table API 和 SQL 中，会给表单独提供一个逻辑上的时间字段，专门用来在表处理程序中指示时间。</p>
<p>所以所谓的时间属性(time attributes)，其实就是每个表模式结构(schema)的一部分。 它可以在创建表的 DDL 里直接定义为一个字段，也可以在 DataStream 转换成表时定义。一旦定义了时间属性，它就可以作为一个普通字段引用，并且可以在基于时间的操作中使用。</p>
<p>时间属性的数据类型为 TIMESTAMP，它的行为类似于常规时间戳，可以直接访问并且进行计算。</p>
<p>按照时间语义的不同，我们可以把时间属性的定义分成事件时间(event time)和处理时间(processing time)两种情况。</p>
<h4 id="事件时间"><a href="#事件时间" class="headerlink" title="事件时间"></a>事件时间</h4><p><strong>在创建表的 DDL 中定义</strong></p>
<p>在创建表的DDL(CREATE TABLE语句)中，可以增加一个字段，通过WATERMARK 语句来定义事件时间属性。WATERMARK 语句主要用来定义水位线(watermark)的生成表达式，这个表达式会将带有事件时间戳的字段标记为事件时间属性，并在它基础上给出水位线的延迟时间。具体定义方式如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> EventTable(</span><br><span class="line">  <span class="keyword">user</span> STRING,</span><br><span class="line">  url STRING,</span><br><span class="line">  ts <span class="type">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">  WATERMARK <span class="keyword">FOR</span> ts <span class="keyword">AS</span> ts <span class="operator">-</span> <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">SECOND</span></span><br><span class="line">) <span class="keyword">WITH</span> ( ...</span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<p>这里我们把 ts 字段定义为事件时间属性，而且基于 ts 设置了 5 秒的水位线延迟。这里的 5秒 是以时间间隔的形式定义的，格式是 INTERVAL &lt;数值&gt; &lt;时间单位&gt;：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">SECOND</span></span><br></pre></td></tr></table></figure>

<p>这里的数值必须用<strong>单引号</strong>引起来，而单位用 SECOND 和 SECONDS 是等效的。</p>
<p>Flink 中支持的事件时间属性数据类型必须为 <strong>TIMESTAMP</strong> 或者 <strong>TIMESTAMP_LTZ</strong>。这里 TIMESTAMP_LTZ 是指带有本地时区信息的时间戳(TIMESTAMP WITH LOCAL TIME ZONE)；一般情况下如果数据中的时间戳是”年-月-日-时-分-秒”的形式，那就是不带时区信息的，可以将事件时间属性定义为 TIMESTAMP 类型。</p>
<p>而如果原始的时间戳就是一个长整型的毫秒数，这时就需要另外定义一个字段来表示事件时间属性，类型定义为 TIMESTAMP_LTZ 会更方便：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> events (</span><br><span class="line">  <span class="keyword">user</span> STRING,</span><br><span class="line">  url STRING,</span><br><span class="line">  ts <span class="type">BIGINT</span>,</span><br><span class="line">  ts_ltz <span class="keyword">AS</span> TO_TIMESTAMP_LTZ(ts, <span class="number">3</span>),</span><br><span class="line">  WATERMARK <span class="keyword">FOR</span> ts_ltz <span class="keyword">AS</span> time_ltz <span class="operator">-</span> <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">SECOND</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">... );</span><br></pre></td></tr></table></figure>

<p>这里我们另外定义了一个字段 ts_ltz，是把长整型的 ts 转换为 TIMESTAMP_LTZ 得到的；进而使用 WATERMARK 语句将它设为事件时间属性，并设置 5 秒的水位线延迟。</p>
<p><strong>在数据流转换为表时定义</strong></p>
<p>事件时间属性也可以在将 DataStream 转换为表的时候来定义。我们调用 fromDataStream() 方法创建表时，可以追加参数来定义表中的字段结构；这时可以给某个字段加上 .rowtime() 后缀，就表示将当前字段指定为事件时间属性。这个字段可以是数据中本不存在、额外追加上去的逻辑字段，就像之前 DDL 中定义的第二种情况；也可以是本身固有的字段，那么这个字段就会被事件时间属性所覆盖，类型也会被转换为 TIMESTAMP。不论那种方式，时间属性字段中保存的都是事件的时间戳(TIMESTAMP类型)。</p>
<p>需要注意的是，这种方式只负责指定时间属性，而时间戳的提取和水位线的生成应该之前就在 DataStream 上定义好了。由于 DataStream 中没有时区概念，因此 Flink 会将事件时间属性解析成不带时区的 TIMESTAMP 类型，所有的时间值都被当作 UTC 标准时间。</p>
<p>在代码中的定义方式如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 方法一:</span></span><br><span class="line"><span class="comment">// 流中数据类型为二元组 Tuple2，包含两个字段；需要自定义提取时间戳并生成水位线 </span></span><br><span class="line">DataStream&lt;Tuple2&lt;String, String&gt;&gt; stream = inputStream.assignTimestampsAndWatermarks(...);</span><br><span class="line"><span class="comment">// 声明一个额外的逻辑字段作为事件时间属性</span></span><br><span class="line">Table table = tEnv.fromDataStream(stream, $(<span class="string">&quot;user&quot;</span>), $(<span class="string">&quot;url&quot;</span>), $(<span class="string">&quot;ts&quot;</span>).rowtime());</span><br><span class="line"></span><br><span class="line"><span class="comment">// 方法二:</span></span><br><span class="line"><span class="comment">// 流中数据类型为三元组 Tuple3，最后一个字段就是事件时间戳 </span></span><br><span class="line">DataStream&lt;Tuple3&lt;String, String, Long&gt;&gt; stream = inputStream.assignTimestampsAndWatermarks(...);</span><br><span class="line"><span class="comment">// 不再声明额外字段，直接用最后一个字段作为事件时间属性</span></span><br><span class="line">Table table = tEnv.fromDataStream(stream, $(<span class="string">&quot;user&quot;</span>), $(<span class="string">&quot;url&quot;</span>), $(<span class="string">&quot;ts&quot;</span>).rowtime());</span><br></pre></td></tr></table></figure>

<h4 id="处理时间"><a href="#处理时间" class="headerlink" title="处理时间"></a>处理时间</h4><p>相比之下处理时间就比较简单了，它就是我们的系统时间，使用时不需要提取时间戳 (timestamp)和生成水位线(watermark)。因此在定义处理时间属性时，必须要额外声明一个字段，专门用来保存当前的处理时间。</p>
<p>类似地，处理时间属性的定义也有两种方式：创建表 DDL 中定义，或者在数据流转换成表时定义。</p>
<p><strong>在创建表的 DDL 中定义</strong></p>
<p>在创建表的 DDL(CREATE TABLE 语句)中，可以增加一个额外的字段，通过调用系统内置的 PROCTIME() 函数来指定当前的处理时间属性，返回的类型是 TIMESTAMP_LTZ。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> EventTable(</span><br><span class="line">  <span class="keyword">user</span> STRING,</span><br><span class="line">  url STRING,</span><br><span class="line">  ts <span class="keyword">AS</span> PROCTIME()</span><br><span class="line">) <span class="keyword">WITH</span> ( ...</span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<p>这里的时间属性，其实是以计算列(computed column)的形式定义出来的。所谓的计算列是Flink SQL中引入的特殊概念，可以用一个AS语句来在表中产生数据中不存在的列， 并且可以利用原有的列、各种运算符及内置函数。在前面事件时间属性的定义中，将 ts 字段转换成 TIMESTAMP_LTZ 类型的 ts_ltz，也是计算列的定义方式。</p>
<p><strong>在数据流转换为表时定义</strong></p>
<p>处理时间属性同样可以在将 DataStream 转换为表的时候来定义。我们调用 fromDataStream() 方法创建表时，可以用.proctime()后缀来指定处理时间属性字段。由于处理时间是系统时间，原始数据中并没有这个字段，所以处理时间属性一定不能定义在一个已有字段上，只能定义在表结构<strong>所有字段的最后</strong>，作为额外的逻辑字段出现。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataStream&lt;Tuple2&lt;String, String&gt;&gt; stream = ...; </span><br><span class="line"><span class="comment">// 声明一个额外的字段作为处理时间属性字段</span></span><br><span class="line">Table table = tEnv.fromDataStream(stream, $(<span class="string">&quot;user&quot;</span>), $(<span class="string">&quot;url&quot;</span>), $(<span class="string">&quot;ts&quot;</span>).proctime());</span><br></pre></td></tr></table></figure>

<h4 id="窗口"><a href="#窗口" class="headerlink" title="窗口"></a>窗口</h4><p>窗口可以将无界流切割成大小有限的桶(bucket)来做计算，通过截取有限数据集来处理无限的流数据。在 DataStream API 中提供了对不同类型的窗口进行定义和处理的接口，而在 Table API 和 SQL 中，类似的功能也都可以实现。</p>
<h5 id="分组窗口-Group-Window，老版本"><a href="#分组窗口-Group-Window，老版本" class="headerlink" title="分组窗口(Group Window，老版本)"></a>分组窗口(Group Window，老版本)</h5><p>在 Flink 1.12 之前的版本中，Table API 和 SQL 提供了一组分组窗口(Group Window) 函数，常用的时间窗口如滚动窗口、滑动窗口、会话窗口都有对应的实现；具体在 SQL 中就是调用 TUMBLE()、HOP()、SESSION()，传入时间属性字段、窗口大小等参数就可以了。以滚动窗口为例：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">TUMBLE(ts, <span class="type">INTERVAL</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">HOUR</span>)</span><br></pre></td></tr></table></figure>

<p>这里的 ts 是定义好的时间属性字段，窗口大小用时间间隔 INTERVAL 来定义。</p>
<p>在进行窗口计算时，分组窗口是将窗口本身当作一个字段对数据进行分组的，可以对组内的数据进行聚合。基本使用方式如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Table result = tableEnv.sqlQuery(</span><br><span class="line">                  <span class="string">&quot;SELECT &quot;</span> +</span><br><span class="line">                      <span class="string">&quot;user, &quot;</span> +</span><br><span class="line">                      <span class="string">&quot;TUMBLE_END(ts, INTERVAL &#x27;1&#x27; HOUR) as endT, &quot;</span> +</span><br><span class="line">                      <span class="string">&quot;COUNT(url) AS cnt &quot;</span> +</span><br><span class="line">									<span class="string">&quot;FROM EventTable &quot;</span> +</span><br><span class="line">									<span class="string">&quot;GROUP BY &quot;</span> + <span class="comment">// 使用窗口和用户名进行分组</span></span><br><span class="line">  										<span class="string">&quot;user, &quot;</span> +</span><br><span class="line">											<span class="string">&quot;TUMBLE(ts, INTERVAL &#x27;1&#x27; HOUR)&quot;</span> <span class="comment">// 定义 1 小时滚动窗口</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<p>这里定义了 1 小时的滚动窗口，将窗口和用户 user 一起作为分组的字段。用聚合函数 COUNT() 对分组数据的个数进行了聚合统计，并将结果字段重命名为 cnt；用 TUPMBLE_END() 函数获取滚动窗口的结束时间，重命名为 endT 提取出来。</p>
<p>分组窗口的功能比较有限，只支持窗口聚合，所以目前已经处于弃用(deprecated)的状态。</p>
<h5 id="窗口表值函数-Windowing-TVFs，新版本"><a href="#窗口表值函数-Windowing-TVFs，新版本" class="headerlink" title="窗口表值函数(Windowing TVFs，新版本)"></a>窗口表值函数(Windowing TVFs，新版本)</h5><p>从 1.13 版本开始，Flink 开始使用窗口表值函数(Windowing table-valued functions， Windowing TVFs)来定义窗口。窗口表值函数是 Flink 定义的多态表函数(PTF)，可以将表进行扩展后返回。表函数(table function)可以看作是返回一个表的函数。</p>
<p>目前 Flink 提供了以下几个窗口 TVF：</p>
<p><strong>滚动窗口(Tumbling Windows)</strong></p>
<p><strong>滑动窗口(Hop Windows，跳跃窗口)</strong></p>
<p><strong>累积窗口(Cumulate Windows)</strong></p>
<p><strong>会话窗口(Session Windows，目前尚未完全支持)</strong></p>
<p>窗口表值函数可以完全替代传统的分组窗口函数。窗口 TVF 更符合 SQL 标准，性能得到了优化，拥有更强大的功能；可以支持基于窗口的复杂计算，例如窗口 Top-N、窗口联结(window join)等等。当然，目前窗口 TVF 的功能还不完善，会话窗口和很多高级功能还不支持，不过正在快速地更新完善。可以预见在未来的版本中，窗口 TVF 将越来越强大，将会是窗口处理的唯一入口。</p>
<p>在窗口 TVF 的返回值中，除去原始表中的所有列，还增加了用来描述窗口的额外 3 个列：窗口起始点(window_start)、窗口结束点(window_end)、窗口时间(window_time)。 起始点和结束点比较好理解，这里的窗口时间指的是窗口中的时间属性，它的值等于 window_end - 1ms，所以相当于是窗口中能够包含数据的最大时间戳。</p>
<p>在 SQL 中的声明方式，与以前的分组窗口是类似的，直接调用 TUMBLE()、HOP()、 CUMULATE()就可以实现滚动、滑动和累积窗口，不过传入的参数会有所不同。</p>
<p><strong>(1)滚动窗口(TUMBLE)</strong></p>
<p>在 SQL 中通过调用 TUMBLE()函数就可以声明一个滚动窗口，只有一个核心参数就是窗口大小(size)。在 SQL 中不考虑计数窗口，所以滚动窗口就是滚动时间窗口，参数中还需要将当前的时间属性字段传入；另外，窗口 TVF 本质上是表函数，可以对表进行扩展，所以还应该把当前查询的表作为参数整体传入。具体声明如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">TUMBLE(TABLE EventTable, DESCRIPTOR(ts), INTERVAL <span class="string">&#x27;1&#x27;</span> HOUR)</span><br></pre></td></tr></table></figure>

<p><strong>(2)滑动窗口(HOP)</strong></p>
<p>滑动窗口的使用与滚动窗口类似，可以通过设置滑动步长来控制统计输出的频率。在 SQL 中通过调用 HOP() 来声明滑动窗口；除了也要传入表名、时间属性外，还需要传入窗口大小(size)和滑动步长(slide)两个参数。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">HOP(TABLE EventTable, DESCRIPTOR(ts), INTERVAL <span class="string">&#x27;5&#x27;</span> MINUTES, INTERVAL <span class="string">&#x27;1&#x27;</span> HOURS));</span><br></pre></td></tr></table></figure>

<p>这里我们基于时间属性 ts，在表 EventTable 上创建了大小为 1 小时的滑动窗口，每 5 分钟滑动一次。需要注意的是，紧跟在时间属性字段后面的第三个参数是步长(slide)，第四个参数才是窗口大小(size)。</p>
<p><strong>(3)累积窗口(CUMULATE)</strong></p>
<p>滚动窗口和滑动窗口，可以用来计算大多数周期性的统计指标。不过在实际应用中还会遇到这样一类需求：我们的统计周期可能较长，因此希望中间每隔一段时间就输出一次当前的统计值；与滑动窗口不同的是，在一个统计周期内，我们会多次输出统计值，它们应该是不断叠加累积的。</p>
<p>例如，我们按天来统计网站的PV(Page View，页面浏览量)，如果用1天的滚动窗口， 那需要到每天 24 点才会计算一次，输出频率太低；如果用滑动窗口，计算频率可以更高，但统计的就变成了”过去 24 小时的 PV”。所以我们真正希望的是，还是按照自然日统计每天的 PV，不过需要每隔 1 小时就输出一次当天到目前为止的 PV 值。这种特殊的窗口就叫作累积窗口(Cumulate Window)。</p>
<p>累积窗口是窗口 TVF 中新增的窗口功能，它会在一定的统计周期内进行累积计算。累积窗口中有两个核心的参数：最大窗口长度(max window size)和累积步长(step)。所谓的<strong>最大窗口长度其实就是我们所说的统计周期</strong>，最终目的就是统计这段时间内的数据。在 SQL 中可以用 CUMULATE() 函数来定义，具体如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">CUMULATE(TABLE EventTable, DESCRIPTOR(ts), INTERVAL <span class="string">&#x27;1&#x27;</span> HOURS, INTERVAL <span class="string">&#x27;1&#x27;</span> DAYS))</span><br></pre></td></tr></table></figure>

<p>这里我们基于时间属性 ts，在表 EventTable 上定义了一个统计周期为 1 天、累积步长为 1 小时的累积窗口。注意第三个参数为步长 step，第四个参数则是最大窗口长度。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//累计窗口案例</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 读取数据源，并分配时间戳、生成水位线</span></span><br><span class="line">    SingleOutputStreamOperator&lt;Event&gt; eventStream = env</span><br><span class="line">            .fromElements(</span><br><span class="line">                    <span class="keyword">new</span> Event(<span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;./home&quot;</span>, <span class="number">1000L</span>),</span><br><span class="line">                    <span class="keyword">new</span> Event(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;./cart&quot;</span>, <span class="number">1000L</span>),</span><br><span class="line">                    <span class="keyword">new</span> Event(<span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;./prod?id=1&quot;</span>, <span class="number">25</span> * <span class="number">60</span> * <span class="number">1000L</span>),</span><br><span class="line">                    <span class="keyword">new</span> Event(<span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;./prod?id=4&quot;</span>, <span class="number">55</span> * <span class="number">60</span> * <span class="number">1000L</span>),</span><br><span class="line">                    <span class="keyword">new</span> Event(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;./prod?id=5&quot;</span>, <span class="number">3600</span> * <span class="number">1000L</span> + <span class="number">60</span> * <span class="number">1000L</span>),</span><br><span class="line">                    <span class="keyword">new</span> Event(<span class="string">&quot;Cary&quot;</span>, <span class="string">&quot;./home&quot;</span>, <span class="number">3600</span> * <span class="number">1000L</span> + <span class="number">30</span> * <span class="number">60</span> * <span class="number">1000L</span>),</span><br><span class="line">                    <span class="keyword">new</span> Event(<span class="string">&quot;Cary&quot;</span>, <span class="string">&quot;./prod?id=7&quot;</span>, <span class="number">3600</span> * <span class="number">1000L</span> + <span class="number">59</span> * <span class="number">60</span> * <span class="number">1000L</span>)</span><br><span class="line">            )</span><br><span class="line">            .assignTimestampsAndWatermarks(</span><br><span class="line">                    WatermarkStrategy.&lt;Event&gt;forMonotonousTimestamps()</span><br><span class="line">                            .withTimestampAssigner(<span class="keyword">new</span> SerializableTimestampAssigner&lt;Event&gt;() &#123;</span><br><span class="line">                                <span class="meta">@Override</span></span><br><span class="line">                                <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Event element, <span class="keyword">long</span> recordTimestamp)</span> </span>&#123;</span><br><span class="line">                                    <span class="keyword">return</span> element.timestamp;</span><br><span class="line">                                &#125;</span><br><span class="line">                            &#125;)</span><br><span class="line">            );</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建表环境</span></span><br><span class="line">    StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将数据流转换成表，并指定时间属性</span></span><br><span class="line">    Table eventTable = tableEnv.fromDataStream(</span><br><span class="line">            eventStream,</span><br><span class="line">            $(<span class="string">&quot;user&quot;</span>),</span><br><span class="line">            $(<span class="string">&quot;url&quot;</span>),</span><br><span class="line">            $(<span class="string">&quot;timestamp&quot;</span>).rowtime().as(<span class="string">&quot;ts&quot;</span>)</span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 为方便在SQL中引用，在环境中注册表EventTable</span></span><br><span class="line">    tableEnv.createTemporaryView(<span class="string">&quot;EventTable&quot;</span>, eventTable);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置累积窗口，执行SQL统计查询</span></span><br><span class="line">    Table result = tableEnv</span><br><span class="line">            .sqlQuery(</span><br><span class="line">                    <span class="string">&quot;SELECT &quot;</span> +</span><br><span class="line">                            <span class="string">&quot;user, &quot;</span> +</span><br><span class="line">                            <span class="string">&quot;window_end AS endT, &quot;</span> +</span><br><span class="line">                            <span class="string">&quot;COUNT(url) AS cnt &quot;</span> +</span><br><span class="line">                            <span class="string">&quot;FROM TABLE( &quot;</span> +</span><br><span class="line">                            <span class="string">&quot;CUMULATE( TABLE EventTable, &quot;</span> +    <span class="comment">// 定义累积窗口</span></span><br><span class="line">                            <span class="string">&quot;DESCRIPTOR(ts), &quot;</span> +</span><br><span class="line">                            <span class="string">&quot;INTERVAL &#x27;30&#x27; MINUTE, &quot;</span> +</span><br><span class="line">                            <span class="string">&quot;INTERVAL &#x27;1&#x27; HOUR)) &quot;</span> +</span><br><span class="line">                            <span class="string">&quot;GROUP BY user, window_start, window_end &quot;</span></span><br><span class="line">            );</span><br><span class="line"></span><br><span class="line">    tableEnv.toDataStream(result).print();</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//窗口案例</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1. 在创建表的DDL中直接定义时间属性</span></span><br><span class="line">    String createDDL = <span class="string">&quot;CREATE TABLE clickTable (&quot;</span> +</span><br><span class="line">            <span class="string">&quot; user_name STRING, &quot;</span> +</span><br><span class="line">            <span class="string">&quot; url STRING, &quot;</span> +</span><br><span class="line">            <span class="string">&quot; ts BIGINT, &quot;</span> +</span><br><span class="line">            <span class="string">&quot; et AS TO_TIMESTAMP( FROM_UNIXTIME(ts / 1000) ), &quot;</span> +</span><br><span class="line">            <span class="string">&quot; WATERMARK FOR et AS et - INTERVAL &#x27;1&#x27; SECOND &quot;</span> +</span><br><span class="line">            <span class="string">&quot;) WITH (&quot;</span> +</span><br><span class="line">            <span class="string">&quot; &#x27;connector&#x27; = &#x27;filesystem&#x27;, &quot;</span> +</span><br><span class="line">            <span class="string">&quot; &#x27;path&#x27; = &#x27;input/clicks.csv&#x27;, &quot;</span> +</span><br><span class="line">            <span class="string">&quot; &#x27;format&#x27; =  &#x27;csv&#x27; &quot;</span> +</span><br><span class="line">            <span class="string">&quot;)&quot;</span>;</span><br><span class="line"></span><br><span class="line">    tableEnv.executeSql(createDDL);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 在流转换成Table时定义时间属性</span></span><br><span class="line">    SingleOutputStreamOperator&lt;Event&gt; clickStream = env.addSource(<span class="keyword">new</span> ClickSource())</span><br><span class="line">            .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Event&gt;forBoundedOutOfOrderness(Duration.ZERO)</span><br><span class="line">                    .withTimestampAssigner(<span class="keyword">new</span> SerializableTimestampAssigner&lt;Event&gt;() &#123;</span><br><span class="line">                        <span class="meta">@Override</span></span><br><span class="line">                        <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Event event, <span class="keyword">long</span> l)</span> </span>&#123;</span><br><span class="line">                            <span class="keyword">return</span> event.timestamp;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;));</span><br><span class="line"></span><br><span class="line">    Table clickTable = tableEnv.fromDataStream(clickStream, $(<span class="string">&quot;user&quot;</span>), $(<span class="string">&quot;url&quot;</span>), $(<span class="string">&quot;timestamp&quot;</span>).as(<span class="string">&quot;ts&quot;</span>),</span><br><span class="line">            $(<span class="string">&quot;et&quot;</span>).rowtime());</span><br><span class="line"></span><br><span class="line">      clickTable.printSchema();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 聚合查询转换</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1. 分组聚合</span></span><br><span class="line">    Table aggTable = tableEnv.sqlQuery(<span class="string">&quot;SELECT user_name, COUNT(1) FROM clickTable GROUP BY user_name&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 分组窗口聚合</span></span><br><span class="line">    Table groupWindowResultTable = tableEnv.sqlQuery(<span class="string">&quot;SELECT &quot;</span> +</span><br><span class="line">            <span class="string">&quot;user_name, &quot;</span> +</span><br><span class="line">            <span class="string">&quot;COUNT(1) AS cnt, &quot;</span> +</span><br><span class="line">            <span class="string">&quot;TUMBLE_END(et, INTERVAL &#x27;10&#x27; SECOND) as endT &quot;</span> +</span><br><span class="line">            <span class="string">&quot;FROM clickTable &quot;</span> +</span><br><span class="line">            <span class="string">&quot;GROUP BY &quot;</span> +                     <span class="comment">// 使用窗口和用户名进行分组</span></span><br><span class="line">            <span class="string">&quot;  user_name, &quot;</span> +</span><br><span class="line">            <span class="string">&quot;  TUMBLE(et, INTERVAL &#x27;10&#x27; SECOND)&quot;</span> <span class="comment">// 定义1小时滚动窗口</span></span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3. 窗口聚合</span></span><br><span class="line">    <span class="comment">// 3.1 滚动窗口</span></span><br><span class="line">    Table tumbleWindowResultTable = tableEnv.sqlQuery(<span class="string">&quot;SELECT user_name, COUNT(url) AS cnt, &quot;</span> +</span><br><span class="line">            <span class="string">&quot; window_end AS endT &quot;</span> +</span><br><span class="line">            <span class="string">&quot;FROM TABLE( &quot;</span> +</span><br><span class="line">            <span class="string">&quot;  TUMBLE( TABLE clickTable, DESCRIPTOR(et), INTERVAL &#x27;10&#x27; SECOND)&quot;</span> +</span><br><span class="line">            <span class="string">&quot;) &quot;</span> +</span><br><span class="line">            <span class="string">&quot;GROUP BY user_name, window_start, window_end &quot;</span></span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3.2 滑动窗口</span></span><br><span class="line">    Table hopWindowResultTable = tableEnv.sqlQuery(<span class="string">&quot;SELECT user_name, COUNT(url) AS cnt, &quot;</span> +</span><br><span class="line">            <span class="string">&quot; window_end AS endT &quot;</span> +</span><br><span class="line">            <span class="string">&quot;FROM TABLE( &quot;</span> +</span><br><span class="line">            <span class="string">&quot;  HOP( TABLE clickTable, DESCRIPTOR(et), INTERVAL &#x27;5&#x27; SECOND, INTERVAL &#x27;10&#x27; SECOND)&quot;</span> +</span><br><span class="line">            <span class="string">&quot;) &quot;</span> +</span><br><span class="line">            <span class="string">&quot;GROUP BY user_name, window_start, window_end &quot;</span></span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3.3 累积窗口</span></span><br><span class="line">    Table cumulateWindowResultTable = tableEnv.sqlQuery(<span class="string">&quot;SELECT user_name, COUNT(url) AS cnt, &quot;</span> +</span><br><span class="line">            <span class="string">&quot; window_end AS endT &quot;</span> +</span><br><span class="line">            <span class="string">&quot;FROM TABLE( &quot;</span> +</span><br><span class="line">            <span class="string">&quot;  CUMULATE( TABLE clickTable, DESCRIPTOR(et), INTERVAL &#x27;5&#x27; SECOND, INTERVAL &#x27;10&#x27; SECOND)&quot;</span> +</span><br><span class="line">            <span class="string">&quot;) &quot;</span> +</span><br><span class="line">            <span class="string">&quot;GROUP BY user_name, window_start, window_end &quot;</span></span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4. 开窗聚合</span></span><br><span class="line">    Table overWindowResultTable = tableEnv.sqlQuery(<span class="string">&quot;SELECT user_name, &quot;</span> +</span><br><span class="line">            <span class="string">&quot; avg(ts) OVER (&quot;</span> +</span><br><span class="line">            <span class="string">&quot;   PARTITION BY user_name &quot;</span> +</span><br><span class="line">            <span class="string">&quot;   ORDER BY et &quot;</span> +</span><br><span class="line">            <span class="string">&quot;   ROWS BETWEEN 3 PRECEDING AND CURRENT ROW&quot;</span> +</span><br><span class="line">            <span class="string">&quot;) AS avg_ts &quot;</span> +</span><br><span class="line">            <span class="string">&quot;FROM clickTable&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 结果表转换成流打印输出</span></span><br><span class="line">    tableEnv.toChangelogStream(aggTable).print(<span class="string">&quot;agg: &quot;</span>);</span><br><span class="line">    tableEnv.toDataStream(groupWindowResultTable).print(<span class="string">&quot;group window: &quot;</span>);</span><br><span class="line">    tableEnv.toDataStream(tumbleWindowResultTable).print(<span class="string">&quot;tumble window: &quot;</span>);</span><br><span class="line">    tableEnv.toDataStream(hopWindowResultTable).print(<span class="string">&quot;hop window: &quot;</span>);</span><br><span class="line">    tableEnv.toDataStream(cumulateWindowResultTable).print(<span class="string">&quot;cumulate window: &quot;</span>);</span><br><span class="line">    tableEnv.toDataStream(overWindowResultTable).print(<span class="string">&quot;over window: &quot;</span>);</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="聚合-Aggregation-查询"><a href="#聚合-Aggregation-查询" class="headerlink" title="聚合(Aggregation)查询"></a>聚合(Aggregation)查询</h3><p>Flink 中的 SQL 是流处理与标准 SQL 结合的产物，所以聚合查询也可以分成两种：流处理中特有的聚合(主要指窗口聚合)，以及 SQL 原生的聚合查询方式。</p>
<h4 id="分组聚合"><a href="#分组聚合" class="headerlink" title="分组聚合"></a>分组聚合</h4><p>SQL 中一般所说的聚合我们都很熟悉，主要是通过内置的一些聚合函数来实现的，比如SUM()、MAX()、MIN()、AVG()以及 COUNT()。它们的特点是对多条输入数据进行计算，得到一个唯一的值，属于多对一的转换。</p>
<p>在流处理中，分组聚合同样是一个持续查询，而且是一个更新查询，得到的是一个动态表；每当流中有一个新的数据到来时，都会导致结果表的更新操作。因此，想要将结果表转换成流或输出到外部系统，必须采用**撤回流(retract stream)<strong>或更</strong>新插入流(upsert stream)**的编码方式；如果在代码中直接转换成DataStream打印输出，需要调用toChangelogStream()。</p>
<p>另外，在持续查询的过程中，由于用于分组的 key 可能会不断增加，因此计算结果所需要维护的状态也会持续增长。为了防止状态无限增长耗尽资源，Flink Table API 和 SQL 可以在表环境中配置状态的生存时间(TTL)：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">TableEnvironment tableEnv = ...</span><br><span class="line"><span class="comment">// 获取表环境的配置</span></span><br><span class="line">TableConfig tableConfig = tableEnv.getConfig();</span><br><span class="line"><span class="comment">// 配置状态保持时间 </span></span><br><span class="line">tableConfig.setIdleStateRetention(Duration.ofMinutes(<span class="number">60</span>));</span><br></pre></td></tr></table></figure>

<p>或者也可以直接设置配置项 table.exec.state.ttl：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">TableEnvironment tableEnv = ...</span><br><span class="line">Configuration configuration = tableEnv.getConfig().getConfiguration();</span><br><span class="line">configuration.setString(<span class="string">&quot;table.exec.state.ttl&quot;</span>, <span class="string">&quot;60 min&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>这两种方式是等效的。需要注意，配置 TTL 有可能会导致统计结果不准确，这其实是以牺牲正确性为代价换取了资源的释放。</p>
<h4 id="窗口聚合"><a href="#窗口聚合" class="headerlink" title="窗口聚合"></a>窗口聚合</h4><p>在流处理中，往往需要将无限数据流划分成有界数据集，这就是所谓的窗口。</p>
<p>与分组聚合类似，窗口聚合也需要调用SUM()、MAX()、MIN()、COUNT()一类的聚合函数，通过 GROUP BY 子句来指定分组的字段。只不过窗口聚合时，需要将窗口信息作为分组 key 的一部分定义出来。在 Flink 1.12 版本之前，是直接把窗口自身作为分组 key 放在 GROUP BY 之后的，所以也叫分组窗口聚合；而 1.13 版本开始使用了窗口表值函数(Windowing TVF)，<strong>窗口本身返回的就是一个表</strong>，所以窗口会出现在 FROM 后面，GROUP BY 后面的则是窗口新增的字段 window_start 和 window_end。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Table result = tableEnv.sqlQuery(</span><br><span class="line">                    <span class="string">&quot;SELECT &quot;</span> +</span><br><span class="line">                       <span class="string">&quot;user, &quot;</span> +</span><br><span class="line">                       <span class="string">&quot;window_end AS endT, &quot;</span> +</span><br><span class="line">                       <span class="string">&quot;COUNT(url) AS cnt &quot;</span> +</span><br><span class="line">                    <span class="string">&quot;FROM TABLE( &quot;</span> +</span><br><span class="line">                          <span class="string">&quot;TUMBLE( TABLE EventTable, &quot;</span> +</span><br><span class="line">                          <span class="string">&quot;DESCRIPTOR(ts), &quot;</span> +</span><br><span class="line">                          <span class="string">&quot;INTERVAL &#x27;1&#x27; HOUR)) &quot;</span> +</span><br><span class="line">                    <span class="string">&quot;GROUP BY user, window_start, window_end &quot;</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<p>这里我们以 ts 作为时间属性字段、基于 EventTable 定义了 1 小时的滚动窗口，希望统计出每小时每个用户点击 url 的次数。用来分组的字段是用户名 user，以及表示窗口的 window_start 和 window_end；而 TUMBLE() 是表值函数，所以得到的是一个表(Table)，我们的聚合查询就是在这个 Table 中进行的。</p>
<h4 id="开窗-Over-聚合"><a href="#开窗-Over-聚合" class="headerlink" title="开窗(Over)聚合"></a>开窗(Over)聚合</h4><p>在标准 SQL 中还有另外一类比较特殊的聚合方式，可以针对每一行计算一个聚合值。比如说，我们可以以每一行数据为基准，计算它之前 1 小时内所有数据的平均值；也可以计算它之前 10 个数的平均值。就好像是在每一行上打开了一扇窗户、收集数据进行统计一样，这就是所谓的开窗函数。开窗函数的聚合与之前两种聚合有本质的不同：分组聚合、窗口 TVF 聚合都是多对一的关系，将数据分组之后每组只会得到一个聚合结果；而开窗函数是对每行都要做一次开窗聚合，因此聚合之后表中的行数不会有任何减少，是一个多对多的关系。</p>
<p>与标准SQL中一致，Flink SQL中的开窗函数也是通过OVER子句来实现的，所以有时开窗聚合也叫作 OVER 聚合(Over Aggregation)。基本语法如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">SELECT</span><br><span class="line">  &lt;聚合函数&gt; OVER (</span><br><span class="line">    [PARTITION BY &lt;字段 <span class="number">1</span>&gt;[, &lt;字段 <span class="number">2</span>&gt;, ...]] </span><br><span class="line">    ORDER BY &lt;时间属性字段&gt;</span><br><span class="line">    &lt;开窗范围&gt;),</span><br><span class="line">  ...</span><br><span class="line">FROM ...</span><br></pre></td></tr></table></figure>

<p>开窗范围是由 BETWEEN &lt;下界&gt; AND &lt;上界&gt; 来定义的，也就是从下界到上界的范围。目前支持的上界只能是 CURRENT ROW，也就是定义一个<strong>从之前某一行到当前行</strong>的范围，所以一般的形式为：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">BETWEEN ... PRECEDING AND CURRENT ROW</span><br></pre></td></tr></table></figure>

<p>开窗选择的范围可以基于<strong>时间</strong>，也可以基于<strong>数据的数量</strong>。所以开窗范围还应该在两种模式之间做出选择：**范围间隔(RANGE intervals)<strong>和</strong>行间隔(ROW intervals)**。</p>
<p><strong>范围间隔</strong>以 RANGE 为前缀，就是基于 ORDER BY 指定的时间字段去选取一个范围，一般就是当前行时间戳之前的一段时间。例如开窗范围选择当前行之前 1 小时的数据：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">RANGE BETWEEN INTERVAL <span class="string">&#x27;1&#x27;</span> HOUR PRECEDING AND CURRENT ROW</span><br></pre></td></tr></table></figure>

<p>行间隔以 ROWS 为前缀，就是直接确定要选多少行，由当前行出发向前选取就可以了。例如开窗范围选择当前行之前的 5 行数据(最终聚合会包括当前行，所以一共 6 条数据)：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">ROWS BETWEEN <span class="number">5</span> PRECEDING AND CURRENT ROW</span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">user</span>, ts,</span><br><span class="line">       <span class="built_in">COUNT</span>(url) <span class="keyword">OVER</span> (</span><br><span class="line">          <span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">user</span></span><br><span class="line">          <span class="keyword">ORDER</span> <span class="keyword">BY</span> ts</span><br><span class="line">          <span class="keyword">RANGE</span> <span class="keyword">BETWEEN</span> <span class="type">INTERVAL</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">HOUR</span> PRECEDING <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="type">ROW</span></span><br><span class="line">       ) <span class="keyword">AS</span> cnt</span><br><span class="line"><span class="keyword">FROM</span> EventTable</span><br></pre></td></tr></table></figure>

<p>这里我们以 ts 作为时间属性字段，对 EventTable 中的每行数据都选取它之前 1 小时的所有数据进行聚合，统计每个用户访问 url 的总次数，并重命名为 cnt。最终将表中每行的 user， ts 以及扩展出 cnt 提取出来。</p>
<p>整个开窗聚合的结果，是对每一行数据都有一个对应的聚合值，因此就像将表中扩展出了一个新的列一样。由于聚合范围上界只能到当前行，新到的数据一般不会影响之前数据的聚合结果，所以结果表只需要不断插入(INSERT)就可以了。执行上面 SQL 得到的结果表，可以用 toDataStream() 直接转换成流打印输出。</p>
<p>开窗聚合与窗口聚合(窗口 TVF 聚合)本质上不同，不过也还是有一些相似之处的：它们都是在无界的数据流上划定了一个范围，截取出有限数据集进行聚合统计；这其实都是窗口的思路。事实上，在 Table API 中确实就定义了两类窗口：分组窗口(GroupWindow)和开窗窗口(OverWindow)；而在 SQL 中，也可以用 WINDOW 子句来在 SELECT 外部单独定义 一个 OVER 窗口：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">user</span>, ts,</span><br><span class="line">  <span class="built_in">COUNT</span>(url) <span class="keyword">OVER</span> w <span class="keyword">AS</span> cnt,</span><br><span class="line">  <span class="built_in">MAX</span>(<span class="keyword">CHAR_LENGTH</span>(url)) <span class="keyword">OVER</span> w <span class="keyword">AS</span> max_url</span><br><span class="line"><span class="keyword">FROM</span> EventTable</span><br><span class="line"><span class="keyword">WINDOW</span> w <span class="keyword">AS</span> (</span><br><span class="line">  <span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">user</span></span><br><span class="line">  <span class="keyword">ORDER</span> <span class="keyword">BY</span> ts</span><br><span class="line">  <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span> <span class="number">2</span> PRECEDING <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="type">ROW</span>)</span><br></pre></td></tr></table></figure>

<p>上面的 SQL 中定义了一个选取之前 2 行数据的 OVER 窗口，并重命名为 w；接下来就可以基于它调用多个聚合函数，扩展出更多的列提取出来。比如这里除统计 url 的个数外，还统计了 url 的最大长度：首先用 CHAR_LENGTH() 函数计算出 url 的长度，再调用聚合函数 MAX() 进行聚合统计。这样，我们就可以方便重复引用定义好的 OVER 窗口了，大大增强了代码的可读性。</p>
<h4 id="TopN"><a href="#TopN" class="headerlink" title="TopN"></a>TopN</h4><h5 id="普通TopN"><a href="#普通TopN" class="headerlink" title="普通TopN"></a><strong>普通TopN</strong></h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1. 在创建表的DDL中直接定义时间属性</span></span><br><span class="line">    String createDDL = <span class="string">&quot;CREATE TABLE clickTable (&quot;</span> +</span><br><span class="line">            <span class="string">&quot; `user` STRING, &quot;</span> +</span><br><span class="line">            <span class="string">&quot; url STRING, &quot;</span> +</span><br><span class="line">            <span class="string">&quot; ts BIGINT, &quot;</span> +</span><br><span class="line">            <span class="string">&quot; et AS TO_TIMESTAMP( FROM_UNIXTIME(ts / 1000) ), &quot;</span> +</span><br><span class="line">            <span class="string">&quot; WATERMARK FOR et AS et - INTERVAL &#x27;1&#x27; SECOND &quot;</span> +</span><br><span class="line">            <span class="string">&quot;) WITH (&quot;</span> +</span><br><span class="line">            <span class="string">&quot; &#x27;connector&#x27; = &#x27;filesystem&#x27;, &quot;</span> +</span><br><span class="line">            <span class="string">&quot; &#x27;path&#x27; = &#x27;input/clicks.csv&#x27;, &quot;</span> +</span><br><span class="line">            <span class="string">&quot; &#x27;format&#x27; =  &#x27;csv&#x27; &quot;</span> +</span><br><span class="line">            <span class="string">&quot;)&quot;</span>;</span><br><span class="line"></span><br><span class="line">    tableEnv.executeSql(createDDL);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 普通Top N，选取当前所有用户中浏览量最大的2个</span></span><br><span class="line"></span><br><span class="line">    Table topNResultTable = tableEnv.sqlQuery(<span class="string">&quot;SELECT user, cnt, row_num &quot;</span> +</span><br><span class="line">            <span class="string">&quot;FROM (&quot;</span> +</span><br><span class="line">            <span class="string">&quot;   SELECT *, ROW_NUMBER() OVER (&quot;</span> +</span><br><span class="line">            <span class="string">&quot;      ORDER BY cnt DESC&quot;</span> +</span><br><span class="line">            <span class="string">&quot;   ) AS row_num &quot;</span> +</span><br><span class="line">            <span class="string">&quot;   FROM (SELECT user, COUNT(url) AS cnt FROM clickTable GROUP BY user)&quot;</span> +</span><br><span class="line">            <span class="string">&quot;) WHERE row_num &lt;= 2&quot;</span>);</span><br><span class="line"></span><br><span class="line">    tableEnv.toChangelogStream(topNResultTable).print(<span class="string">&quot;top 2: &quot;</span>);</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="窗口TopN"><a href="#窗口TopN" class="headerlink" title="窗口TopN"></a><strong>窗口TopN</strong></h5><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 读取数据源，并分配时间戳、生成水位线</span></span><br><span class="line">    SingleOutputStreamOperator&lt;Event&gt; eventStream = env</span><br><span class="line">            .fromElements(</span><br><span class="line">                    <span class="keyword">new</span> Event(<span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;./home&quot;</span>, <span class="number">1000L</span>),</span><br><span class="line">                    <span class="keyword">new</span> Event(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;./cart&quot;</span>, <span class="number">1000L</span>),</span><br><span class="line">                    <span class="keyword">new</span> Event(<span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;./prod?id=1&quot;</span>,  <span class="number">25</span> * <span class="number">60</span> * <span class="number">1000L</span>),</span><br><span class="line">                    <span class="keyword">new</span> Event(<span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;./prod?id=4&quot;</span>, <span class="number">55</span> * <span class="number">60</span> * <span class="number">1000L</span>),</span><br><span class="line">                    <span class="keyword">new</span> Event(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;./prod?id=5&quot;</span>, <span class="number">3600</span> * <span class="number">1000L</span> + <span class="number">60</span> * <span class="number">1000L</span>),</span><br><span class="line">                    <span class="keyword">new</span> Event(<span class="string">&quot;Cary&quot;</span>, <span class="string">&quot;./home&quot;</span>, <span class="number">3600</span> * <span class="number">1000L</span> + <span class="number">30</span> * <span class="number">60</span> * <span class="number">1000L</span>),</span><br><span class="line">                    <span class="keyword">new</span> Event(<span class="string">&quot;Cary&quot;</span>, <span class="string">&quot;./prod?id=7&quot;</span>, <span class="number">3600</span> * <span class="number">1000L</span> + <span class="number">59</span> * <span class="number">60</span> * <span class="number">1000L</span>)</span><br><span class="line">            )</span><br><span class="line">            .assignTimestampsAndWatermarks(</span><br><span class="line">                    WatermarkStrategy.&lt;Event&gt;forMonotonousTimestamps()</span><br><span class="line">                            .withTimestampAssigner(<span class="keyword">new</span> SerializableTimestampAssigner&lt;Event&gt;() &#123;</span><br><span class="line">                                <span class="meta">@Override</span></span><br><span class="line">                                <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Event element, <span class="keyword">long</span> recordTimestamp)</span> </span>&#123;</span><br><span class="line">                                    <span class="keyword">return</span> element.timestamp;</span><br><span class="line">                                &#125;</span><br><span class="line">                            &#125;)</span><br><span class="line">            );</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建表环境</span></span><br><span class="line">    StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将数据流转换成表，并指定时间属性</span></span><br><span class="line">    Table eventTable = tableEnv.fromDataStream(</span><br><span class="line">            eventStream,</span><br><span class="line">            $(<span class="string">&quot;user&quot;</span>),</span><br><span class="line">            $(<span class="string">&quot;url&quot;</span>),</span><br><span class="line">            $(<span class="string">&quot;timestamp&quot;</span>).rowtime().as(<span class="string">&quot;ts&quot;</span>)</span><br><span class="line">            <span class="comment">// 将timestamp指定为事件时间，并命名为ts</span></span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 为方便在SQL中引用，在环境中注册表EventTable</span></span><br><span class="line">    tableEnv.createTemporaryView(<span class="string">&quot;EventTable&quot;</span>, eventTable);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 定义子查询，进行窗口聚合，得到包含窗口信息、用户以及访问次数的结果表</span></span><br><span class="line">    String subQuery =</span><br><span class="line">            <span class="string">&quot;SELECT window_start, window_end, user, COUNT(url) as cnt &quot;</span> +</span><br><span class="line">                    <span class="string">&quot;FROM TABLE ( &quot;</span> +</span><br><span class="line">                    <span class="string">&quot;TUMBLE( TABLE EventTable, DESCRIPTOR(ts), INTERVAL &#x27;1&#x27; HOUR )) &quot;</span> +</span><br><span class="line">                    <span class="string">&quot;GROUP BY window_start, window_end, user &quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 定义Top N的外层查询</span></span><br><span class="line">    String topNQuery =</span><br><span class="line">            <span class="string">&quot;SELECT * &quot;</span> +</span><br><span class="line">                    <span class="string">&quot;FROM (&quot;</span> +</span><br><span class="line">                    <span class="string">&quot;SELECT *, &quot;</span> +</span><br><span class="line">                    <span class="string">&quot;ROW_NUMBER() OVER ( &quot;</span> +</span><br><span class="line">                    <span class="string">&quot;PARTITION BY window_start, window_end &quot;</span> +</span><br><span class="line">                    <span class="string">&quot;ORDER BY cnt desc &quot;</span> +</span><br><span class="line">                    <span class="string">&quot;) AS row_num &quot;</span> +</span><br><span class="line">                    <span class="string">&quot;FROM (&quot;</span> + subQuery + <span class="string">&quot;)) &quot;</span> +</span><br><span class="line">                    <span class="string">&quot;WHERE row_num &lt;= 2&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 执行SQL得到结果表</span></span><br><span class="line">    Table result = tableEnv.sqlQuery(topNQuery);</span><br><span class="line"></span><br><span class="line">    tableEnv.toDataStream(result).print();</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="联结-Join-查询"><a href="#联结-Join-查询" class="headerlink" title="联结(Join)查询"></a>联结(Join)查询</h3><p>在流处理中，动态表的 Join 对应着两条数据流的 Join 操作。Flink SQL 中的联结查询大体上也可以分为两类：SQL 原生的联结查询方式，和流处理中特有的联结查询。</p>
<h4 id="常规联结查询"><a href="#常规联结查询" class="headerlink" title="常规联结查询"></a>常规联结查询</h4><p>常规联结(Regular Join)是 SQL 中原生定义的 Join 方式，是最通用的一类联结操作。它的具体语法与标准 SQL 的联结完全相同，通过关键字 JOIN 来联结两个表，后面用关键字 ON 来指明联结条件。按照习惯，我们一般以左侧和右侧来区分联结操作的两个表。</p>
<p>在两个动态表的联结中，任何一侧表的插入(INSERT)或更改(UPDATE)操作都会让联结的结果表发生改变。例如，如果左侧有新数据到来，那么它会与右侧表中所有之前的数据进行联结合并，右侧表之后到来的新数据也会与这条数据连接合并。所以，<strong>常规联结查询一般是更新(Update)查询</strong>。</p>
<p>与标准 SQL 一致，Flink SQL 的常规联结也可以分为<strong>内联结(INNER JOIN)<strong>和</strong>外联结 (OUTER JOIN)<strong>，区别在于结果中是否包含不符合联结条件的行。目前</strong>仅支持等值条件</strong>作为联结条件，也就是关键字 ON 后面必须是判断两表中字段相等的逻辑表达式。</p>
<h4 id="间隔联结查询"><a href="#间隔联结查询" class="headerlink" title="间隔联结查询"></a>间隔联结查询</h4><p>DataStream API 中的双流 Join，包括<strong>窗口联结(window join)<strong>和</strong>间隔联结(interval join)<strong>。两条流的 Join 就对应着 SQL 中两个表的 Join，这是流处理中特有的联结方式。</strong>目前 Flink SQL 还不支持窗口联结，而间隔联结则已经实现</strong>。</p>
<p>间隔联结(Interval Join)返回的，同样是符合约束条件的两条数据的笛卡尔积。只不过这里的约束条件除了常规的<strong>联结条件</strong>外，还多了一个<strong>时间间隔的限制</strong>。具体语法有以下要点：</p>
<p><strong>两表的联结</strong></p>
<p>间隔联结不需要用 JOIN 关键字，直接在 FROM 后将要联结的两表列出来就可以，用逗号分隔。这与标准 SQL 中的语法一致，表示一个交叉联结(Cross Join)，会返回两表中所有行的笛卡尔积。</p>
<p><strong>联结条件</strong></p>
<p>联结条件用 WHERE 子句来定义，用一个等值表达式描述。交叉联结之后再用 WHERE 进行条件筛选，效果跟内联结 INNER JOIN … ON …非常类似。</p>
<p><strong>时间间隔限制</strong></p>
<p>我们可以在 WHERE 子句中，联结条件后用 AND 追加一个时间间隔的限制条件；做法是提取左右两侧表中的时间字段，然后用一个表达式来指明两者需要满足的间隔限制。具体定义方式有下面三种，这里分别用 ltime 和 rtime 表示左右表中的时间字段：</p>
<p>(1)ltime = rtime</p>
<p>(2)ltime &gt;= rtime AND ltime &lt; rtime + INTERVAL ‘10’ MINUTE</p>
<p>(3)ltime BETWEEN rtime - INTERVAL ‘10’ SECOND AND rtime + INTERVAL ‘5’ SECOND</p>
<p>判断两者相等，这是最强的时间约束，要求两表中数据的时间必须完全一致才能匹配；一般情况下，我们还是会放宽一些，给出一个间隔。间隔的定义可以用&lt;，&lt;=，&gt;=，&gt;这一类的关系不等式，也可以用 BETWEEN … AND …这样的表达式。</p>
<p>例如，我们现在除了订单表 Order 外，还有一个发货表Shipment，要求在收到订单后 4 个小时内发货。那么我们就可以用一个间隔联结查询，把所有订单与它对应的发货信息连接合并在一起返回。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span> <span class="keyword">Order</span> o, Shipment s</span><br><span class="line"><span class="keyword">WHERE</span> o.id <span class="operator">=</span> s.order_id</span><br><span class="line"><span class="keyword">AND</span> o.order_time <span class="keyword">BETWEEN</span> s.ship_time <span class="operator">-</span> <span class="type">INTERVAL</span> <span class="string">&#x27;4&#x27;</span> <span class="keyword">HOUR</span> <span class="keyword">AND</span> s.ship_time</span><br></pre></td></tr></table></figure>

<p>在流处理中，间隔联结查询只支持具有时间属性的仅追加(Append-only)表。</p>
<p>那对于有更新操作的表，又怎么办呢？除了间隔联结之外，Flink SQL 还支持时间联结 (Temporal Join)，这主要是针对**版本表(versioned table)<strong>而言的。所谓版本表，就是记录了数据随着时间推移版本变化的表，可以理解成一个更新日志(change log)，它就是具有时间属性、还会进行更新操作的表。当我们联结某个版本表时，并不是把当前的数据连接合并起来就行了，而是希望能够根据数据发生的时间，找到当时的版本；这种根据更新时间提取当时的值进行联结的操作，就叫作</strong>时间联结(Temporal Join)**。这部分内容可以查阅官网资料。</p>
<h3 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h3><p>Flink 的 Table API 和 SQL 同样提供了函数的功能。两者在调用时略有不同：Table API 中的函数是通过数据对象的方法调用来实现的；而 SQL 则是直接引用函数名称，传入数据作为参数。例如，要把一个字符串 str 转换成全大写的形式，Table API 的写法是调用 str 这个 String 对象的 upperCase() 方法：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">str.upperCase();</span><br></pre></td></tr></table></figure>

<p>而 SQL 中的写法就是直接引用 UPPER() 函数，将 str 作为参数传入：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">UPPER(str)</span><br></pre></td></tr></table></figure>

<p>由于 Table API 是内嵌在 Java 语言中的，很多方法需要在类中额外添加，因此扩展功能比较麻烦，目前支持的函数比较少；而且 Table API 也不如 SQL 的通用性强，所以一般情况下较少使用。</p>
<p>Flink SQL 中的函数可以分为两类：一类是 SQL 中内置的系统函数，直接通过函数名调用就可以，能够实现一些常用的转换操作，比如之前我们用到的 COUNT()、CHAR_LENGTH()、UPPER() 等等；而另一类函数则是用户自定义的函数(UDF)，需要在表环境中注册才能使用。 </p>
<h4 id="UDF"><a href="#UDF" class="headerlink" title="UDF"></a>UDF</h4><p>系统函数尽管庞大，也不可能涵盖所有的功能；如果有系统函数不支持的需求，我们就需要用自定义函数(User Defined Functions，UDF)来实现了。事实上，系统内置函数仍然在不断扩充，如果我们认为自己实现的自定义函数足够通用、应用非常广泛，也可以在项目跟踪工具 JIRA 上向 Flink 开发团队提出议题(issue)，请求将新的函数添加到系统函数中。</p>
<p>Flink 的 Table API 和 SQL 提供了多种自定义函数的接口，以抽象类的形式定义。当前 UDF 主要有以下几类：</p>
<p>**标量函数(Scalar Functions)**：将输入的标量值转换成一个新的标量值。</p>
<p>**表函数(Table Functions)**：将标量值转换成一个或多个新的行数据，也就是扩展成一个表。</p>
<p>**聚合函数(Aggregate Functions)**：将多行数据里的标量值转换成一个新的标量值。</p>
<p>**表聚合函数(Table Aggregate Functions)**：将多行数据里的标量值转换成一个或多个新的行数据。</p>
<h4 id="UDF调用流程"><a href="#UDF调用流程" class="headerlink" title="UDF调用流程"></a>UDF调用流程</h4><p>要想在代码中使用自定义的函数，我们需要首先自定义对应 UDF 抽象类的实现，并在表环境中注册这个函数，然后就可以在 Table API 和 SQL 中调用了。</p>
<p><strong>(1)注册函数</strong></p>
<p>注册函数时需要调用表环境的 createTemporarySystemFunction() 方法，传入注册的函数名以及 UDF 类的 Class 对象：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 注册函数</span></span><br><span class="line">tableEnv.createTemporarySystemFunction(<span class="string">&quot;MyFunction&quot;</span>, MyFunction.class);</span><br></pre></td></tr></table></figure>

<p>我们自定义的 UDF 类叫作 MyFunction，它应该是上面四种 UDF 抽象类中某一个的具体实现；在环境中将它注册为名叫 MyFunction 的函数。这里 <strong>createTemporarySystemFunction()</strong> 方法的意思是创建了一个临时系统函数，所以 MyFunction 函数名是全局的，可以当作系统函数来使用；我们也可以用 <strong>createTemporaryFunction()</strong> 方法，注册的函数就依赖于当前的数据库(database)和目录(catalog) 了，所以这就不是系统函数，而是目录函数(catalog function)，它的完整名称应该包括所属的 database 和 catalog。一般情况下，我们直接用 createTemporarySystemFunction() 方法将 UDF 注册为系统函数就可以了。</p>
<p><strong>(2)使用 Table API 调用函数</strong></p>
<p>在 Table API 中，需要使用 call() 方法来调用自定义函数：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">tableEnv.from(<span class="string">&quot;MyTable&quot;</span>).select(call(<span class="string">&quot;MyFunction&quot;</span>, $(<span class="string">&quot;myField&quot;</span>)));</span><br></pre></td></tr></table></figure>

<p>此外，在 Table API 中也可以不注册函数，直接用内联(inline)的方式调用 UDF：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">tableEnv.from(<span class="string">&quot;MyTable&quot;</span>).select(call(SubstringFunction.class, $(<span class="string">&quot;myField&quot;</span>)));</span><br></pre></td></tr></table></figure>

<p>区别只是在于 call() 方法第一个参数不再是注册好的函数名，而直接就是函数类的 Class 对象了。</p>
<p><strong>(3)在 SQL 中调用函数</strong></p>
<p>当我们将函数注册为系统函数之后，在 SQL 中的调用就与内置系统函数完全一样了：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">tableEnv.sqlQuery(<span class="string">&quot;SELECT MyFunction(myField) FROM MyTable&quot;</span>);</span><br></pre></td></tr></table></figure>

<h4 id="标量函数-Scalar-Functions"><a href="#标量函数-Scalar-Functions" class="headerlink" title="标量函数(Scalar Functions)"></a>标量函数(Scalar Functions)</h4><p>自定义标量函数可以把 0 个、 1 个或多个标量值转换成一个标量值，它对应的<strong>输入是一行数据中的字段</strong>，<strong>输出则是唯一的值</strong>。所以从输入和输出表中行数据的对应关系看，标量函数是<strong>一对一</strong>的转换。</p>
<p>想要实现自定义的标量函数，我们需要自定义一个类来继承抽象类 ScalarFunction，并实现叫作 eval() 的求值方法。标量函数的行为就取决于求值方法的定义，它必须是<strong>公有的(public)<strong>， 而且</strong>名字必须是 eval</strong>。求值方法 eval 可以重载多次，任何数据类型都可作为求值方法的参数和返回值类型。</p>
<p>这里需要特别说明的是，ScalarFunction 抽象类中并没有定义 eval() 方法，所以我们不能直接在代码中重写(override)；但 Table API 的框架底层又要求了求值方法必须名字为 eval()。这是 Table API 和 SQL 目前还显得不够完善的地方，未来的版本应该会有所改进。</p>
<p>ScalarFunction 以及其它所有的 UDF 接口，都在 org.apache.flink.table.functions 中。</p>
<p>下面我们来看一个具体的例子。我们实现一个自定义的哈希(hash)函数 HashFunction，用来求传入对象的哈希值。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1. 自定义数据源，从流转换</span></span><br><span class="line">    SingleOutputStreamOperator&lt;Event&gt; eventStream = env.addSource(<span class="keyword">new</span> ClickSource())</span><br><span class="line">            .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Event&gt;forBoundedOutOfOrderness(Duration.ZERO)</span><br><span class="line">                    .withTimestampAssigner(<span class="keyword">new</span> SerializableTimestampAssigner&lt;Event&gt;() &#123;</span><br><span class="line">                        <span class="meta">@Override</span></span><br><span class="line">                        <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Event element, <span class="keyword">long</span> recordTimestamp)</span> </span>&#123;</span><br><span class="line">                            <span class="keyword">return</span> element.timestamp;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;)</span><br><span class="line">            );</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 将流转换成表</span></span><br><span class="line">    Table eventTable = tableEnv.fromDataStream(eventStream,</span><br><span class="line">            $(<span class="string">&quot;user&quot;</span>),</span><br><span class="line">            $(<span class="string">&quot;url&quot;</span>),</span><br><span class="line">            $(<span class="string">&quot;timestamp&quot;</span>).rowtime().as(<span class="string">&quot;ts&quot;</span>));</span><br><span class="line"></span><br><span class="line">    tableEnv.createTemporaryView(<span class="string">&quot;EventTable&quot;</span>, eventTable);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3. 注册自定义标量函数</span></span><br><span class="line">    tableEnv.createTemporarySystemFunction(<span class="string">&quot;MyHash&quot;</span>, MyHash.class);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4. 调用UDF查询转换</span></span><br><span class="line">    Table resultTable = tableEnv.sqlQuery(<span class="string">&quot;select user, MyHash(user) from EventTable&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 5. 输出到控制台</span></span><br><span class="line">    tableEnv.executeSql(<span class="string">&quot;create table output (&quot;</span> +</span><br><span class="line">            <span class="string">&quot;uname STRING, &quot;</span> +</span><br><span class="line">            <span class="string">&quot;myhash INT ) &quot;</span> +</span><br><span class="line">            <span class="string">&quot;WITH (&quot;</span> +</span><br><span class="line">            <span class="string">&quot;&#x27;connector&#x27; = &#x27;print&#x27;)&quot;</span>);</span><br><span class="line">    </span><br><span class="line">    resultTable.executeInsert(<span class="string">&quot;output&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"> <span class="comment">// 自定义一个ScalarFunction</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyHash</span> <span class="keyword">extends</span> <span class="title">ScalarFunction</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">eval</span><span class="params">(String str)</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> str.hashCode();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="表函数-Table-Functions"><a href="#表函数-Table-Functions" class="headerlink" title="表函数(Table Functions)"></a>表函数(Table Functions)</h4><p>跟标量函数一样，表函数的输入参数也可以是 0 个、1 个或多个标量值；不同的是，它可以<strong>返回任意多行数据</strong>。多行数据事实上就构成了一个表，所以表函数可以认为就是返回一个表的函数，这是一个<strong>一对多</strong>的转换关系。<strong>窗口 TVF 本质上就是表函数</strong>。</p>
<p>类似地，要实现自定义的表函数，需要自定义类来继承抽象类 TableFunction，内部必须要实现的也是一个名为 eval 的求值方法。与标量函数不同的是，TableFunction 类本身是有一 个泛型参数 T 的，这就是表函数返回数据的类型；而 eval()方法没有返回类型，内部也没有 return 语句，是通过调用 collect() 方法来发送想要输出的行数据的。回忆一下 DataStream API 中的 FlatMapFunction 和 ProcessFunction，它们的 flatMap 和 processElement 方法也没有返回值，也是通过 out.collect() 来向下游发送数据的。</p>
<p>我们使用表函数，可以对一行数据得到一个表，这和 Hive 中的 UDTF 非常相似。那对于原先输入的整张表来说，又该得到什么呢?一个简单的想法是，就让输入表中的每一行，与它转换得到的表进行联结(join)，然后再拼成一个完整的大表，这就相当于对原来的表进行了扩展。在 Hive 的 SQL 语法中，提供了侧向视图(lateral view，也叫横向视图)的功能，可以将表中的一行数据拆分成多行；Flink SQL 也有类似的功能，是用 <strong>LATERAL TABLE</strong> 语法来实现的。</p>
<p>在 SQL 中调用表函数，需要使用 LATERAL TABLE(<TableFunction>)来生成扩展的侧向表，然后与原始表进行联结(Join)。这里的Join操作可以是直接做交叉联结(cross join)， 在 FROM 后用逗号分隔两个表就可以；也可以是以 ON TRUE 为条件的左联结(LEFT JOIN)。</TableFunction></p>
<p>下面是表函数的一个具体示例。我们实现了一个分隔字符串的函数 SplitFunction，可以将一个字符串转换成(字符串，长度)的二元组。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1. 自定义数据源，从流转换</span></span><br><span class="line">    SingleOutputStreamOperator&lt;Event&gt; eventStream = env.addSource(<span class="keyword">new</span> ClickSource())</span><br><span class="line">            .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Event&gt;forBoundedOutOfOrderness(Duration.ZERO)</span><br><span class="line">                    .withTimestampAssigner(<span class="keyword">new</span> SerializableTimestampAssigner&lt;Event&gt;() &#123;</span><br><span class="line">                        <span class="meta">@Override</span></span><br><span class="line">                        <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Event element, <span class="keyword">long</span> recordTimestamp)</span> </span>&#123;</span><br><span class="line">                            <span class="keyword">return</span> element.timestamp;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;)</span><br><span class="line">            );</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 将流转换成表</span></span><br><span class="line">    Table eventTable = tableEnv.fromDataStream(eventStream,</span><br><span class="line">            $(<span class="string">&quot;user&quot;</span>),</span><br><span class="line">            $(<span class="string">&quot;url&quot;</span>),</span><br><span class="line">            $(<span class="string">&quot;timestamp&quot;</span>).rowtime().as(<span class="string">&quot;ts&quot;</span>));</span><br><span class="line"></span><br><span class="line">    tableEnv.createTemporaryView(<span class="string">&quot;EventTable&quot;</span>, eventTable);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3. 注册自定义表函数</span></span><br><span class="line">    tableEnv.createTemporarySystemFunction(<span class="string">&quot;MySplit&quot;</span>, MySplit.class);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4. 调用UDF查询转换</span></span><br><span class="line">    Table resultTable = tableEnv.sqlQuery(<span class="string">&quot;select user, url, word, length &quot;</span> +</span><br><span class="line">            <span class="string">&quot;from EventTable, LATERAL TABLE( MySplit(url) ) AS T(word, length)&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 5. 输出到控制台</span></span><br><span class="line">    tableEnv.executeSql(<span class="string">&quot;create table output (&quot;</span> +</span><br><span class="line">            <span class="string">&quot;uname STRING, &quot;</span> +</span><br><span class="line">            <span class="string">&quot;url STRING, &quot;</span> +</span><br><span class="line">            <span class="string">&quot;word STRING, &quot;</span> +</span><br><span class="line">            <span class="string">&quot;length INT) &quot;</span> +</span><br><span class="line">            <span class="string">&quot;WITH (&quot;</span> +</span><br><span class="line">            <span class="string">&quot;&#x27;connector&#x27; = &#x27;print&#x27;)&quot;</span>);</span><br><span class="line">    </span><br><span class="line">    resultTable.executeInsert(<span class="string">&quot;output&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 自定义一个TableFunction，注意有泛型，这里输出的是两个字段，二元组</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MySplit</span> <span class="keyword">extends</span> <span class="title">TableFunction</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Integer</span>&gt;&gt;</span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">eval</span><span class="params">(String str)</span></span>&#123;</span><br><span class="line">        String[] fields = str.split(<span class="string">&quot;\\?&quot;</span>);    <span class="comment">// 转义问号，以及反斜杠本身</span></span><br><span class="line">        <span class="keyword">for</span> (String field : fields)&#123;</span><br><span class="line">            collect(Tuple2.of(field, field.length()));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 在 SQL 里调用注册好的函数 </span></span><br><span class="line"><span class="comment">// 1. 交叉联结 </span></span><br><span class="line">tableEnv.sqlQuery(</span><br><span class="line">  <span class="string">&quot;SELECT myField, word, length &quot;</span> +</span><br><span class="line"><span class="string">&quot;FROM MyTable, LATERAL TABLE(SplitFunction(myField))&quot;</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 2. 带 ON TRUE 条件的左联结</span></span><br><span class="line">tableEnv.sqlQuery(</span><br><span class="line">  <span class="string">&quot;SELECT myField, word, length &quot;</span> +</span><br><span class="line">  <span class="string">&quot;FROM MyTable &quot;</span> +</span><br><span class="line">  <span class="string">&quot;LEFT JOIN LATERAL TABLE(SplitFunction(myField)) ON TRUE&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 重命名侧向表中的字段 </span></span><br><span class="line">tableEnv.sqlQuery(</span><br><span class="line">  <span class="string">&quot;SELECT myField, newWord, newLength &quot;</span> +</span><br><span class="line">  <span class="string">&quot;FROM MyTable &quot;</span> +</span><br><span class="line">  <span class="string">&quot;LEFT JOIN LATERAL TABLE(SplitFunction(myField)) AS T(newWord, newLength) ON TRUE&quot;</span>);</span><br></pre></td></tr></table></figure>

<h4 id="聚合函数-Aggregate-Functions"><a href="#聚合函数-Aggregate-Functions" class="headerlink" title="聚合函数(Aggregate Functions)"></a>聚合函数(Aggregate Functions)</h4><p>用户自定义聚合函数(User Defined AGGregate function，UDAGG)会把一行或多行数据 (也就是一个表)聚合成一个标量值。这是一个标准的<strong>多对一</strong>的转换。</p>
<p>聚合函数的概念我们之前已经接触过多次，如 SUM()、MAX()、MIN()、AVG()、COUNT() 都是常见的系统内置聚合函数。而如果有些需求无法直接调用系统函数解决，我们就必须自定义聚合函数来实现功能了。</p>
<p>自定义聚合函数需要继承抽象类 AggregateFunction。AggregateFunction 有两个泛型参数 &lt;T, ACC&gt;，T 表示聚合输出的结果类型，ACC 则表示聚合的中间状态类型。</p>
<p>Flink SQL 中的聚合函数的工作原理如下：</p>
<p>(1)首先，它需要创建一个累加器(accumulator)，用来存储聚合的中间结果。这与 DataStream API 中的AggregateFunction非常类似，累加器就可以看作是一个聚合状态。调用 createAccumulator() 方法可以创建一个空的累加器。</p>
<p>(2)对于输入的每一行数据，都会调用 accumulate() 方法来更新累加器，这是聚合的核心过程。</p>
<p>(3)当所有的数据都处理完之后，通过调用 getValue() 方法来计算并返回最终的结果。</p>
<p>下面举一个具体的示例。在常用的系统内置聚合函数里，可以用 AVG()来计算平均值;如 果我们现在希望计算的是某个字段的“加权平均值”，又该怎么做呢?系统函数里没有现成的 实现，所以只能自定义一个聚合函数 WeightedAvg 来计算了。</p>
<p>比如我们要从学生的分数表 ScoreTable 中计算每个学生的加权平均分。为了计算加权平均值，应该从输入的每行数据中提取两个值作为参数：要计算的分数值 score，以及它的权重 weight。而在聚合过程中，累加器(accumulator)需要存储当前的加权总和sum，以及目前数据的个数 count。这可以用一个二元组来表示，也可以单独定义一个类 WeightedAvgAccum， 里面包含 sum 和 count 两个属性，用它的对象实例来作为聚合的累加器。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1. 自定义数据源，从流转换</span></span><br><span class="line">    SingleOutputStreamOperator&lt;Event&gt; eventStream = env.addSource(<span class="keyword">new</span> ClickSource())</span><br><span class="line">            .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Event&gt;forBoundedOutOfOrderness(Duration.ZERO)</span><br><span class="line">                    .withTimestampAssigner(<span class="keyword">new</span> SerializableTimestampAssigner&lt;Event&gt;() &#123;</span><br><span class="line">                        <span class="meta">@Override</span></span><br><span class="line">                        <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Event element, <span class="keyword">long</span> recordTimestamp)</span> </span>&#123;</span><br><span class="line">                            <span class="keyword">return</span> element.timestamp;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;)</span><br><span class="line">            );</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 将流转换成表</span></span><br><span class="line">    Table eventTable = tableEnv.fromDataStream(eventStream,</span><br><span class="line">            $(<span class="string">&quot;user&quot;</span>),</span><br><span class="line">            $(<span class="string">&quot;url&quot;</span>),</span><br><span class="line">            $(<span class="string">&quot;timestamp&quot;</span>).as(<span class="string">&quot;ts&quot;</span>),</span><br><span class="line">            $(<span class="string">&quot;rt&quot;</span>).rowtime());</span><br><span class="line">    tableEnv.createTemporaryView(<span class="string">&quot;EventTable&quot;</span>, eventTable);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3. 注册自定义表函数</span></span><br><span class="line">    tableEnv.createTemporarySystemFunction(<span class="string">&quot;WeightedAverage&quot;</span>, WeightedAverage.class);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4. 调用UDF查询转换，这里权重直接给1</span></span><br><span class="line">    Table resultTable = tableEnv.sqlQuery(<span class="string">&quot;select user, &quot;</span> +</span><br><span class="line">            <span class="string">&quot;  WeightedAverage(ts, 1) as weighted_avg &quot;</span> +</span><br><span class="line">            <span class="string">&quot;from EventTable &quot;</span> +</span><br><span class="line">            <span class="string">&quot;group by user&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 5. 输出到控制台</span></span><br><span class="line">    tableEnv.executeSql(<span class="string">&quot;create table output (&quot;</span> +</span><br><span class="line">            <span class="string">&quot;uname STRING, &quot;</span> +</span><br><span class="line">            <span class="string">&quot;weighted_avg BIGINT) &quot;</span> +</span><br><span class="line">            <span class="string">&quot;WITH (&quot;</span> +</span><br><span class="line">            <span class="string">&quot;&#x27;connector&#x27; = &#x27;print&#x27;)&quot;</span>);</span><br><span class="line">    resultTable.executeInsert(<span class="string">&quot;output&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 单独定义一个累加器类型</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">WeightedAvgAccumulator</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">long</span> sum = <span class="number">0</span>;    <span class="comment">// 加权和</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span> count = <span class="number">0</span>;    <span class="comment">// 数据个数</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 自定义一个AggregateFunction，求加权平均值</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">WeightedAverage</span> <span class="keyword">extends</span> <span class="title">AggregateFunction</span>&lt;<span class="title">Long</span>, <span class="title">WeightedAvgAccumulator</span>&gt;</span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">getValue</span><span class="params">(WeightedAvgAccumulator accumulator)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (accumulator.count == <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;    <span class="comment">// 防止除数为0</span></span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="keyword">return</span> accumulator.sum / accumulator.count;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> WeightedAvgAccumulator <span class="title">createAccumulator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> WeightedAvgAccumulator();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 累加计算方法，类似于add</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accumulate</span><span class="params">(WeightedAvgAccumulator accumulator, Long iValue, Integer iWeight)</span></span>&#123;</span><br><span class="line">        accumulator.sum += iValue * iWeight;    <span class="comment">// 这个值要算iWeight次</span></span><br><span class="line">        accumulator.count += iWeight;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="表聚合函数-Table-Aggregate-Functions"><a href="#表聚合函数-Table-Aggregate-Functions" class="headerlink" title="表聚合函数(Table Aggregate Functions)"></a>表聚合函数(Table Aggregate Functions)</h4><p>用户自定义表聚合函数(UDTAGG)可以把一行或多行数据(也就是一个表)聚合成另一张表，结果表中可以有多行多列。很明显，这就像表函数和聚合函数的结合体，是一个<strong>多对多</strong>的转换。</p>
<p>自定义表聚合函数需要继承抽象类 TableAggregateFunction。TableAggregateFunction 的结构和原理与 AggregateFunction 非常类似，同样有两个泛型参数&lt;T, ACC&gt;，用一个 ACC 类型的累加器(accumulator)来存储聚合的中间结果。聚合函数中必须实现的三个方法，在 TableAggregateFunction 中也必须对应实现：</p>
<p>createAccumulator()：创建累加器的方法，与 AggregateFunction 中用法相同。</p>
<p>accumulate()：聚合计算的核心方法，与 AggregateFunction 中用法相同。</p>
<p>emitValue()：所有输入行处理完成后，输出最终计算结果的方法。这个方法对应着 AggregateFunction 中的 getValue() 方法；区别在于 emitValue 没有输出类型，而输入参数有两个：第一个是 ACC 类型的累加器，第二个则是用于输出数据的收集器 out，它的类型为 Collect<T>。所以很明显，表聚合函数输出数据不是直接 return，而是调用 out.collect() 方法，调用多次就可以输出多行数据了；这一点与表函数非常相似。另外，emitValue() 在抽象类中也没有定义，无法override，必须手动实现。</T></p>
<p>表聚合函数得到的是一张表；在流处理中做持续查询，应该每次都会把这个表重新计算输出。如果输入一条数据后，只是对结果表里一行或几行进行了更新(Update)，这时我们重新计算整个表、全部输出显然就不够高效了。为了提高处理效率，TableAggregateFunction 还提供了一个 emitUpdateWithRetract() 方法，它可以在结果表发生变化时，以撤回(retract)老数据、发送新数据的方式增量地进行更新。如果同时定义了 emitValue() 和 emitUpdateWithRetract() 两个方法，在进行更新操作时会优先调用 emitUpdateWithRetract()。</p>
<p>表聚合函数相对比较复杂，它的一个典型应用场景就是 TopN 查询。比如我们希望选出 一组数据排序后的前两名，这就是最简单的 TOP-2 查询。没有现成的系统函数，那么我们就可以自定义一个表聚合函数来实现这个功能。在累加器中应该能够保存当前最大的两个值，每当来一条新数据就在 accumulate() 方法中进行比较更新，最终在 emitValue() 中调用两次 out.collect() 将前两名数据输出。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1. 自定义数据源，从流转换</span></span><br><span class="line">    SingleOutputStreamOperator&lt;Event&gt; eventStream = env.addSource(<span class="keyword">new</span> ClickSource())</span><br><span class="line">            .assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Event&gt;forBoundedOutOfOrderness(Duration.ZERO)</span><br><span class="line">                    .withTimestampAssigner(<span class="keyword">new</span> SerializableTimestampAssigner&lt;Event&gt;() &#123;</span><br><span class="line">                        <span class="meta">@Override</span></span><br><span class="line">                        <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Event element, <span class="keyword">long</span> recordTimestamp)</span> </span>&#123;</span><br><span class="line">                            <span class="keyword">return</span> element.timestamp;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;)</span><br><span class="line">            );</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 将流转换成表</span></span><br><span class="line">    Table eventTable = tableEnv.fromDataStream(eventStream,</span><br><span class="line">            $(<span class="string">&quot;user&quot;</span>),</span><br><span class="line">            $(<span class="string">&quot;url&quot;</span>),</span><br><span class="line">            $(<span class="string">&quot;timestamp&quot;</span>).as(<span class="string">&quot;ts&quot;</span>),</span><br><span class="line">            $(<span class="string">&quot;rt&quot;</span>).rowtime());</span><br><span class="line">    tableEnv.createTemporaryView(<span class="string">&quot;EventTable&quot;</span>, eventTable);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3. 开滚动窗口聚合，得到每个用户在每个窗口中的浏览量</span></span><br><span class="line">    Table windowAggTable = tableEnv.sqlQuery(<span class="string">&quot;select user, count(url) as cnt, &quot;</span> +</span><br><span class="line">            <span class="string">&quot;window_end &quot;</span> +</span><br><span class="line">            <span class="string">&quot;from TABLE(&quot;</span> +</span><br><span class="line">            <span class="string">&quot;  TUMBLE( TABLE EventTable, DESCRIPTOR(rt), INTERVAL &#x27;10&#x27; SECOND )&quot;</span> +</span><br><span class="line">            <span class="string">&quot;)&quot;</span> +</span><br><span class="line">            <span class="string">&quot;group by user,&quot;</span> +</span><br><span class="line">            <span class="string">&quot;  window_start,&quot;</span> +</span><br><span class="line">            <span class="string">&quot;  window_end&quot;</span>);</span><br><span class="line">    tableEnv.createTemporaryView(<span class="string">&quot;AggTable&quot;</span>, windowAggTable);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4. 注册表聚合函数函数</span></span><br><span class="line">    tableEnv.createTemporarySystemFunction(<span class="string">&quot;Top2&quot;</span>, Top2.class);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 5. 在Table API中调用函数</span></span><br><span class="line">  	<span class="comment">// 目前 SQL 中没有直接使用表聚合函数的方式，所以需要使用 Table API 的方式来调用</span></span><br><span class="line">    Table resultTable = tableEnv.from(<span class="string">&quot;AggTable&quot;</span>)</span><br><span class="line">            .groupBy($(<span class="string">&quot;window_end&quot;</span>))</span><br><span class="line">            .flatAggregate(call(<span class="string">&quot;Top2&quot;</span>, $(<span class="string">&quot;cnt&quot;</span>)).as(<span class="string">&quot;value&quot;</span>, <span class="string">&quot;rank&quot;</span>))</span><br><span class="line">            .select($(<span class="string">&quot;window_end&quot;</span>), $(<span class="string">&quot;value&quot;</span>), $(<span class="string">&quot;rank&quot;</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 6. 输出到控制台</span></span><br><span class="line">    tableEnv.toChangelogStream(resultTable).print();</span><br><span class="line"></span><br><span class="line">    env.execute();</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 聚合累加器的类型定义，包含最大的第一和第二两个数据</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Top2Accumulator</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> Long first;</span><br><span class="line">    <span class="keyword">public</span> Long second;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 自定义表聚合函数，查询一组数中最大的两个，返回值为(数值，排名)的二元组</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Top2</span> <span class="keyword">extends</span> <span class="title">TableAggregateFunction</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">Long</span>, <span class="title">Integer</span>&gt;, <span class="title">Top2Accumulator</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Top2Accumulator <span class="title">createAccumulator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Top2Accumulator acc = <span class="keyword">new</span> Top2Accumulator();</span><br><span class="line">        acc.first = Long.MIN_VALUE;    <span class="comment">// 为方便比较，初始值给最小值</span></span><br><span class="line">        acc.second = Long.MIN_VALUE;</span><br><span class="line">        <span class="keyword">return</span> acc;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 每来一个数据调用一次，判断是否更新累加器</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accumulate</span><span class="params">(Top2Accumulator acc, Long value)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (value &gt; acc.first) &#123;</span><br><span class="line">            acc.second = acc.first;</span><br><span class="line">            acc.first = value;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (value &gt; acc.second) &#123;</span><br><span class="line">            acc.second = value;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 输出(数值，排名)的二元组，输出两行数据</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">emitValue</span><span class="params">(Top2Accumulator acc, Collector&lt;Tuple2&lt;Long, Integer&gt;&gt; out)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (acc.first != Long.MIN_VALUE) &#123;</span><br><span class="line">            out.collect(Tuple2.of(acc.first, <span class="number">1</span>));</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (acc.second != Long.MIN_VALUE) &#123;</span><br><span class="line">            out.collect(Tuple2.of(acc.second, <span class="number">2</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="SQL客户端"><a href="#SQL客户端" class="headerlink" title="SQL客户端"></a>SQL客户端</h3><p>有了Table API和SQL，我们就可以使用熟悉的SQL来编写查询语句进行流处理了。不过，这种方式还是将 SQL 语句嵌入到 Java/Scala 代码中进行的；另外，写完的代码后想要提交作业还需要使用工具进行打包。这都给 Flink 的使用设置了门槛，如果不是 Java/Scala 程序员，即使是非常熟悉 SQL 的工程师恐怕也会望而生畏了。</p>
<p>基于这样的考虑，Flink 为我们提供了一个工具来进行 Flink 程序的编写、测试和提交，这工具叫作 <strong>SQL 客户端</strong>。SQL 客户端提供了一个命令行交互界面(CLI)，我们可以在里面非常容易地编写 SQL 进行查询，就像使用 MySQL 一样；整个 Flink 应用编写、提交的过程全变成了写 SQL，不需要写一行 Java/Scala 代码。</p>
<p>具体使用流程如下：</p>
<p>(1)首先启动本地集群</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./bin/start-cluster.sh</span><br></pre></td></tr></table></figure>

<p>(2)启动 Flink SQL 客户端</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./bin/sql-client.sh</span><br></pre></td></tr></table></figure>

<p>SQL 客户端的启动脚本同样位于 Flink 的 bin 目录下。默认的启动模式是 embedded，也就是说客户端是一个嵌入在本地的进程，这是目前唯一支持的模式。未来会支持连接到远程 SQL 客户端的模式。</p>
<p>(3)设置运行模式</p>
<p>启动客户端后，就进入了命令行界面，这时就可以开始写 SQL 了。一般我们会在开始之前对环境做一些设置，比较重要的就是运行模式。首先是表环境的运行时模式，有<strong>流处理</strong>和<strong>批处理</strong>两个选项。默认为流处理：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Flink SQL&gt; SET &#x27;execution.runtime-mode&#x27; = &#x27;streaming&#x27;;</span><br></pre></td></tr></table></figure>

<p>其次是 SQL 客户端的执行结果模式，主要有 table、changelog、tableau 三种，默认为 table 模式：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Flink SQL&gt; SET &#x27;sql-client.execution.result-mode&#x27; = &#x27;table&#x27;;</span><br></pre></td></tr></table></figure>

<p>table 模式就是最普通的表处理模式，结果会以逗号分隔每个字段；changelog 则是更新日志模式，会在数据前加上 <strong>+(表示插入)</strong> 或 <strong>-(表示撤回)</strong> 的前缀；而 tableau 则是经典的可视化表模式，结果会是一个虚线框的表格。</p>
<p>此外我们还可以做一些其它可选的设置，比如之前提到的空闲状态生存时间(TTL)：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Flink SQL&gt; SET &#x27;table.exec.state.ttl&#x27; = &#x27;1000&#x27;;</span><br></pre></td></tr></table></figure>

<p>除了在命令行进行设置，我们也可以直接在 SQL 客户端的配置文件 sql-cli-defaults.yaml 中进行各种配置，甚至还可以在这个 yaml 文件里预定义表、函数和 catalog。关于配置文件的更多用法，可以查阅官网的详细说明。</p>
<p>(4)执行 SQL 查询</p>
<p>接下来就可以愉快的编写 SQL 语句了，这跟操作 MySQL、Oracle 等关系型数据库没什么区别。</p>
<p>在 SQL 客户端中，每定义一个 SQL 查询，就会把它作为一个 Flink 作业提交到集群上执行。所以通过这种方式，我们可以快速地对流处理程序进行开发测试。</p>
<h3 id="连接到外部系统"><a href="#连接到外部系统" class="headerlink" title="连接到外部系统"></a>连接到外部系统</h3><p>在 Table API 和 SQL 编写的 Flink 程序中，可以在创建表的时候用 WITH 子句指定连接器 (connector)，这样就可以连接到外部系统进行数据交互了。</p>
<p>架构中的 TableSource 负责从外部系统中读取数据并转换成表，TableSink 则负责将结果表写入外部系统。在 Flink 1.13 的 API 调用中，已经不去区分 TableSource 和 TableSink，我们只要建立到外部系统的连接并创建表就可以，Flink 自动会从程序的处理逻辑中解析出它们的用途。</p>
<p>Flink的Table API和SQL支持了各种不同的连接器。当然，最简单的其实就是上一节中提到的连接到控制台打印输出：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> ResultTable (</span><br><span class="line">   <span class="keyword">user</span> STRING,</span><br><span class="line">   cnt <span class="type">BIGINT</span></span><br><span class="line"><span class="keyword">WITH</span> (</span><br><span class="line"><span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<p>这里只需要在 WITH 中定义 connector 为 print 就可以了。而对于其它的外部系统，则需要增加一些配置项。</p>
<h4 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h4><p><strong>引入依赖</strong></p>
<p>Kafka 的 SQL 连接器可以从 Kafka 的主题(topic)读取数据转换成表，也可以将表数据写入 Kafka 的主题。换句话说，创建表的时候指定连接器为 Kafka，则这个表既可以作为输入表，也可以作为输出表。</p>
<p>想要在 Flink 程序中使用 Kafka 连接器，需要引入如下依赖：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>这里引入的 Flink 和 Kafka 的连接器，与之前 DataStream API 中引入的连接器是一样的。如果想在 SQL 客户端里使用 Kafka 连接器，还需要下载对应的 jar 包放到 lib 目录下。</p>
<p>另外，Flink 为各种连接器提供了一系列的**表格式(table formats)**，比如 CSV、JSON、 Avro、Parquet 等等。这些表格式定义了底层存储的二进制数据和表的列之间的转换方式，相当于表的序列化工具。对于 Kafka 而言，CSV、JSON、Avro 等主要格式都是支持的，根据 Kafka 连接器中配置的格式，我们可能需要引入对应的依赖支持。以 CSV 为例：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-csv<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>由于 SQL 客户端中已经内置了 CSV、JSON 的支持，因此使用时无需专门引入；而对于没有内置支持的格式(比如 Avro)，则仍然要下载相应的 jar 包。关于连接器的格式细节详见官网说明。</p>
<p><strong>创建连接到 Kafka 的表</strong></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> KafkaTable (</span><br><span class="line">  `<span class="keyword">user</span>` STRING,</span><br><span class="line">  `url` STRING,</span><br><span class="line">  `ts` <span class="type">TIMESTAMP</span>(<span class="number">3</span>) METADATA <span class="keyword">FROM</span> <span class="string">&#x27;timestamp&#x27;</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;topic&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;events&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.bootstrap.servers&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;localhost:9092&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.group.id&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;testGroup&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;scan.startup.mode&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;earliest-offset&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;csv&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>这里定义了 Kafka 连接器对应的主题(topic)，Kafka 服务器，消费者组 ID，消费者起始模式以及表格式。需要特别说明的是，在 KafkaTable 的字段中有一个 ts，它的声明中用到了 METADATA FROM，这是表示一个元数据列(metadata column)，它是由Kafka连接器的元数据 timestamp 生成的。<strong>这里的 timestamp 其实就是 Kafka 中数据自带的时间戳</strong>，我们把它直接作为元数据提取出来，转换成一个新的字段 ts。</p>
<p><strong>Upsert Kafka</strong></p>
<p>常情况下，Kafka 作为保持数据顺序的消息队列，读取和写入都应该是流式的数据，对应在表中就是仅追加(append-only)模式。如果我们想要将有更新操作(比如分组聚合)的结果表写入 Kafka，就会因为 Kafka 无法识别撤回(retract)或更新插入(upsert)消息而导致异常。</p>
<p>为了解决这个问题，Flink专门增加了一个<strong>更新插入Kafka(Upsert Kafka)连接器</strong>。这个连接器支持以更新插入(UPSERT)的方式向 Kafka 的 topic 中读写数据。</p>
<p>具体来说，Upsert Kafka 连接器处理的是更新日志(changlog)流。如果作为 TableSource， 连接器会将读取到的topic中的数据(key, value)，解释为对当前key的数据值的更新(UPDATE)， 也就是查找动态表中 key 对应的一行数据，将 value 更新为最新的值；因为是 Upsert 操作，所以如果没有 key 对应的行，那么也会执行插入(INSERT)操作。另外，如果遇到 value 为空 (null)，连接器就把这条数据理解为对相应 key 那一行的删除(DELETE)操作。</p>
<p>如果作为 TableSink，Upsert Kafka 连接器会将有更新操作的结果表，转换成更新日志 (changelog)流。如果遇到插入(INSERT)或者更新后(UPDATE_AFTER)的数据，对应的是一个添加(add)消息，那么就直接正常写入 Kafka 主题；如果是删除(DELETE)或者更新前的数据，对应是一个撤回(retract)消息，那么就把 value 为空(null)的数据写入 Kafka。 由于 Flink 是根据键(key)的值对数据进行分区的，这样就可以保证同一个 key 上的更新和删除消息都会落到同一个分区中。</p>
<p>下面是一个创建和使用 Upsert Kafka 表的例子：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> pageviews_per_region (</span><br><span class="line">  user_region STRING,</span><br><span class="line">  pv <span class="type">BIGINT</span>,</span><br><span class="line">  uv <span class="type">BIGINT</span>,</span><br><span class="line">  <span class="keyword">PRIMARY</span> KEY (user_region) <span class="keyword">NOT</span> ENFORCED</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;upsert-kafka&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;topic&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;pageviews_per_region&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.bootstrap.servers&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;...&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;key.format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;avro&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;value.format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;avro&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> pageviews (</span><br><span class="line">  user_id <span class="type">BIGINT</span>,</span><br><span class="line">  page_id <span class="type">BIGINT</span>,</span><br><span class="line">  viewtime <span class="type">TIMESTAMP</span>,</span><br><span class="line">  user_region STRING,</span><br><span class="line">  WATERMARK <span class="keyword">FOR</span> viewtime <span class="keyword">AS</span> viewtime <span class="operator">-</span> <span class="type">INTERVAL</span> <span class="string">&#x27;2&#x27;</span> <span class="keyword">SECOND</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;topic&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;pageviews&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.bootstrap.servers&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;...&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;json&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 计算 pv、uv 并插入到 upsert-kafka 表中 </span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> pageviews_per_region <span class="keyword">SELECT</span></span><br><span class="line">  user_region,</span><br><span class="line">  <span class="built_in">COUNT</span>(<span class="operator">*</span>),</span><br><span class="line">  <span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> user_id)</span><br><span class="line"><span class="keyword">FROM</span> pageviews</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> user_region;</span><br></pre></td></tr></table></figure>

<p>这里我们从 Kafka 表 pageviews 中读取数据，统计每个区域的 PV(全部浏览量)和 UV (对用户去重)，这是一个分组聚合的更新查询，得到的结果表会不停地更新数据。为了将结果表写入Kafka的pageviews_per_region主题，我们定义了一个Upsert Kafka表，它的字段中需要用PRIMARY KEY来指定主键，并且在WITH子句中分别指定key和value的序列化格式。</p>
<h4 id="文件系统"><a href="#文件系统" class="headerlink" title="文件系统"></a>文件系统</h4><p>另一类非常常见的外部系统就是文件系统(File System)了。Flink 提供了文件系统的连接器，支持从<strong>本地</strong>或者<strong>分布式的文件系统</strong>中读写数据。这个连接器是内置在 Flink 中的，所以使用它并不需要额外引入依赖。</p>
<p>下面是一个连接到文件系统的示例：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> MyTable (</span><br><span class="line">  column_name1 <span class="type">INT</span>,</span><br><span class="line">  column_name2 STRING,</span><br><span class="line">  ...</span><br><span class="line">  part_name1 <span class="type">INT</span>,</span><br><span class="line">  part_name2 STRING</span><br><span class="line">) </span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (part_name1, part_name2) </span><br><span class="line"><span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;filesystem&#x27;</span>, <span class="comment">-- 连接器类型</span></span><br><span class="line">  <span class="string">&#x27;path&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;...&#x27;</span>, <span class="comment">-- 文件路径</span></span><br><span class="line">  <span class="string">&#x27;format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;...&#x27;</span> <span class="comment">-- 文件格式 </span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>这里在 WITH 前使用了 PARTITIONED BY 对数据进行了分区操作。文件系统连接器支持对分区文件的访问。</p>
<h4 id="JDBC"><a href="#JDBC" class="headerlink" title="JDBC"></a>JDBC</h4><p>关系型数据表本身就是 SQL 最初应用的地方，所以我们也会希望能直接向关系型数据库中读写表数据。Flink 提供的 JDBC 连接器可以通过 JDBC 驱动程序(driver)向任意的关系型数据库<strong>读写</strong>数据，比如 MySQL、PostgreSQL、Derby 等。</p>
<p>作为 TableSink 向数据库写入数据时，运行的模式取决于创建表的 DDL 是否定义了主键 (primary key)。如果有主键，那么 JDBC 连接器就将以更新插入(Upsert)模式运行，可以向外部数据库发送按照指定键(key)的更新(UPDATE)和删除(DELETE)操作；如果没有定义主键，那么就将在追加(Append)模式下运行，不支持更新和删除操作。</p>
<p><strong>引入依赖</strong></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-jdbc_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>此外，为了连接到特定的数据库，我们还用引入相关的驱动器依赖，比如 MySQL：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.38<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><strong>创建 JDBC 表</strong></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 创建一张连接到MySQL的表 </span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> MyTable (</span><br><span class="line">  id <span class="type">BIGINT</span>,</span><br><span class="line">  name STRING,</span><br><span class="line">  age <span class="type">INT</span>,</span><br><span class="line">  status <span class="type">BOOLEAN</span>,</span><br><span class="line">  <span class="keyword">PRIMARY</span> KEY (id) <span class="keyword">NOT</span> ENFORCED</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">	<span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;jdbc&#x27;</span>,</span><br><span class="line">	<span class="string">&#x27;url&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;jdbc:mysql://localhost:3306/mydatabase&#x27;</span>,</span><br><span class="line">	<span class="string">&#x27;table-name&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;users&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 将另一张表 T 的数据写入到 MyTable 表中 </span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> MyTable</span><br><span class="line"><span class="keyword">SELECT</span> id, name, age, status <span class="keyword">FROM</span> T;</span><br></pre></td></tr></table></figure>

<p>这里创建表的 DDL 中定义了主键，所以数据会以 Upsert 模式写入到 MySQL 表中；而到 MySQL 的连接，是通过 WITH 子句中的 url 定义的。要注意写入 MySQL 中真正的表名称是 users，而 MyTable 是注册在 Flink 表环境中的表。</p>
<h4 id="Elasticsearch"><a href="#Elasticsearch" class="headerlink" title="Elasticsearch"></a>Elasticsearch</h4><p>Elasticsearch 作为分布式搜索分析引擎，在大数据应用中有非常多的场景。Flink 提供的 Elasticsearch 的 SQL 连接器<strong>只能作为 TableSink</strong>，可以将表数据写入 Elasticsearch 的索引(index)。 Elasticsearch连接器的使用与JDBC连接器非常相似，写入数据的模式同样是由创建表的DDL 中是否有主键定义决定的。</p>
<p><strong>引入依赖</strong></p>
<p>具体的依赖与 Elasticsearch 服务器的版本有关，对于 6.x 版本引入依赖如下：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-elasticsearch6_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>对于 Elasticsearch 7 以上的版本，引入的依赖则是：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-elasticsearch7_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><strong>创建连接到 Elasticsearch 的表</strong></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 创建一张连接到Elasticsearch的表 </span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> MyTable (</span><br><span class="line">  user_id STRING,</span><br><span class="line">  user_name STRING</span><br><span class="line">  uv <span class="type">BIGINT</span>,</span><br><span class="line">  pv <span class="type">BIGINT</span>,</span><br><span class="line"><span class="keyword">PRIMARY</span> KEY (user_id) <span class="keyword">NOT</span> ENFORCED</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;elasticsearch-7&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;hosts&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;http://localhost:9200&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;index&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;users&#x27;</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<p>这里定义了主键，所以会以更新插入(Upsert)模式向 Elasticsearch 写入数据。</p>
<h4 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h4><p>作为高性能、可伸缩的分布式列存储数据库，HBase 在大数据分析中是一个非常重要的工 具。Flink 提供的 HBase 连接器支持面向 HBase 集群的<strong>读写</strong>操作。</p>
<p>在流处理场景下，连接器作为 TableSink 向 HBase 写入数据时，采用的始终是更新插入 (Upsert)模式。也就是说，HBase 要求连接器必须通过定义的主键(primary key)来发送更新日志(changelog)。所以在创建表的 DDL 中，我们必须要定义行键(rowkey)字段，并将它声明为主键；如果没有用 PRIMARY KEY 子句声明主键，连接器会默认把 rowkey 作为主键。</p>
<p><strong>引入依赖</strong></p>
<p>想要在 Flink 程序中使用 HBase 连接器，需要引入对应的依赖。目前 Flink 只对 HBase 的 1.4.x 和 2.2.x 版本提供了连接器支持，而引入的依赖也应该与具体的 HBase 版本有关。对于 1.4 版本引入依赖如下：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-hbase-1.4_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>对于 HBase 2.2 版本，引入的依赖则是：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-hbase-2.2_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><strong>创建连接到 HBase 的表</strong></p>
<p>由于 HBase 并不是关系型数据库，因此转换为 Flink SQL 中的表会稍有一些麻烦。在 DDL 创建出的 HBase 表中，所有的列族(column family)都必须声明为 ROW 类型，在表中占据一个字段；而每个 family 中的列(column qualifier)则对应着 ROW 里的嵌套字段。我们不需要将 HBase 中所有的 family 和 qualifier 都在 Flink SQL 的表中声明出来，只要把那些在查询中用到的声明出来就可以了。</p>
<p>除了所有 ROW 类型的字段(对应着 HBase 中的 family)，表中还应有一个原子类型的字段，它就会被识别为 HBase 的 rowkey。在表中这个字段可以任意取名，不一定非要叫 rowkey。</p>
<p>下面是一个具体示例：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 创建一张连接到HBase的表 </span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> MyTable (</span><br><span class="line"> rowkey <span class="type">INT</span>,</span><br><span class="line"> family1 <span class="type">ROW</span><span class="operator">&lt;</span>q1 <span class="type">INT</span><span class="operator">&gt;</span>,</span><br><span class="line"> family2 <span class="type">ROW</span><span class="operator">&lt;</span>q2 STRING, q3 <span class="type">BIGINT</span><span class="operator">&gt;</span>,</span><br><span class="line"> family3 <span class="type">ROW</span><span class="operator">&lt;</span>q4 <span class="keyword">DOUBLE</span>, q5 <span class="type">BOOLEAN</span>, q6 STRING<span class="operator">&gt;</span>,</span><br><span class="line"> <span class="keyword">PRIMARY</span> KEY (rowkey) <span class="keyword">NOT</span> ENFORCED</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line"> <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;hbase-1.4&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;table-name&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;mytable&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;zookeeper.quorum&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;localhost:2181&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 假设表 T 的字段结构是 [rowkey, f1q1, f2q2, f2q3, f3q4, f3q5, f3q6]</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> MyTable</span><br><span class="line"><span class="keyword">SELECT</span> rowkey, <span class="type">ROW</span>(f1q1), <span class="type">ROW</span>(f2q2, f2q3), <span class="type">ROW</span>(f3q4, f3q5, f3q6) <span class="keyword">FROM</span> T;</span><br></pre></td></tr></table></figure>

<h4 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h4><p>Apache Hive作为一个基于Hadoop的数据仓库基础框架，可以说已经成为了进行海量数据分析的核心组件。Hive 支持类 SQL 的查询语言，可以用来方便对数据进行处理和统计分析， 而且基于 HDFS 的数据存储有非常好的可扩展性，是存储分析超大量数据集的唯一选择。Hive 的主要缺点在于查询的延迟很高，几乎成了离线分析的代言人。而 Flink 的特点就是实时性强， 所以 Flink SQL 与 Hive 的结合势在必行。</p>
<p>Flink 与 Hive 的集成比较特别。Flink 提供了 Hive 目录(HiveCatalog)功能，允许使用 Hive 的元存储(Metastore)来管理 Flink 的元数据。这带来的好处体现在两个方面：</p>
<p>(1)Metastore 可以作为一个持久化的目录，因此使用 HiveCatalog 可以跨会话存储 Flink 特定的元数据。这样一来，我们在 HiveCatalog 中执行执行创建 Kafka 表或者 ElasticSearch 表， 就可以把它们的元数据持久化存储在 Hive 的 Metastore 中；对于不同的作业会话就不需要重复创建了，直接在 SQL 查询中重用就可以。</p>
<p>(2)使用 HiveCatalog，Flink 可以作为读写 Hive 表的替代分析引擎。这样一来，在 Hive 中进行批处理会更加高效；与此同时，也有了连续在 Hive 中读写数据、进行流处理的能力，这也使得实时数仓(real-time data warehouse)成为了可能。</p>
<p>HiveCatalog 被设计为开箱即用，与现有的 Hive 配置完全兼容，我们不需要做任何的修改与调整就可以直接使用。注意只有 Blink 的计划器(planner)提供了 Hive 集成的支持，所以需要在使用 Flink SQL 时选择 Blink planner。下面我们就来看以下与 Hive 集成的具体步骤。</p>
<p><strong>引入依赖</strong></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Flink 的 Hive 连接器--&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-hive_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Hive 依赖 --&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hive.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>建议不要把这些依赖打包到结果 jar 文件中，而是在运行时的集群环境中为不同的 Hive 版本添加不同的依赖支持。具体版本对应的依赖关系，可以查询官网说明。</p>
<p><strong>连接到 Hive</strong></p>
<p>在 Flink 中连接 Hive，是通过在表环境中配置 HiveCatalog 来实现的。需要说明的是，配置 HiveCatalog 本身并不需要限定使用哪个 planner，不过对 Hive 表的读写操作只有 Blink 的 planner 才支持。所以一般我们需要将表环境的 planner 设置为 Blink。</p>
<p>下面是代码中配置 Catalog 的示例：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">EnvironmentSettings settings = EnvironmentSettings.newInstance().useBlinkPlanner().build();</span><br><span class="line">TableEnvironment tableEnv = TableEnvironment.create(settings);</span><br><span class="line"></span><br><span class="line">String name = <span class="string">&quot;myhive&quot;</span>;</span><br><span class="line">String defaultDatabase = <span class="string">&quot;mydatabase&quot;</span>;</span><br><span class="line">String hiveConfDir = <span class="string">&quot;/opt/hive-conf&quot;</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建一个 HiveCatalog，并在表环境中注册</span></span><br><span class="line">HiveCatalog hive = <span class="keyword">new</span> HiveCatalog(name, defaultDatabase, hiveConfDir);</span><br><span class="line">tableEnv.registerCatalog(<span class="string">&quot;myhive&quot;</span>, hive);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用 HiveCatalog 作为当前会话的 catalog </span></span><br><span class="line">tableEnv.useCatalog(<span class="string">&quot;myhive&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>当然，我们也可以直接启动 SQL 客户端，用 CREATE CATALOG 语句直接创建 HiveCatalog：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Flink SQL&gt; create catalog myhive with (&#x27;type&#x27; = &#x27;hive&#x27;, &#x27;hive-conf-dir&#x27; = &#x27;/opt/hive-conf&#x27;);</span><br><span class="line">[INFO] Execute statement succeed.</span><br><span class="line"></span><br><span class="line">Flink SQL&gt; use catalog myhive;</span><br><span class="line">[INFO] Execute statement succeed.</span><br></pre></td></tr></table></figure>

<p><strong>设置 SQL 方言</strong></p>
<p>我们知道，Hive 内部提供了类 SQL 的查询语言，不过语法细节与标准 SQL 会有一些出入， 相当于是 SQL 的一种方言(dialect)。为了提高与 Hive 集成时的兼容性，Flink SQL 提供了 一个非常有趣而强大的功能：可以使用方言来编写 SQL 语句。换句话说，我们可以直接在 Flink 中写 Hive SQL 来操作 Hive 表，这无疑给我们的读写处理带来了极大的方便。</p>
<p>Flink 目前支持两种 SQL 方言的配置：default 和 hive。所谓的 default 就是 Flink SQL 默认 的SQL语法了。我们需要先切换到hive方言，然后才能使用Hive SQL的语法。具体设置可以分为 SQL 和 Table API 两种方式。</p>
<p>(1)SQL 中设置</p>
<p>我们可以通过配置 table.sql-dialect 属性来设置 SQL 方言：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set table.sql-dialect=hive;</span><br></pre></td></tr></table></figure>

<p>当然，我们可以在代码中执行上面的 SET 语句，也可以直接启动 SQL 客户端来运行。如果使用 SQL 客户端，我们还可以在配置文件 sql-cli-defaults.yaml 中通过configuration模块来设置：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">execution:</span></span><br><span class="line">  <span class="attr">planner:</span> <span class="string">blink</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">batch</span></span><br><span class="line">  <span class="attr">result-mode:</span> <span class="string">table</span></span><br><span class="line"><span class="attr">configuration:</span></span><br><span class="line">  <span class="attr">table.sql-dialect:</span> <span class="string">hive</span></span><br></pre></td></tr></table></figure>

<p>(2)Table API 中设置</p>
<p>另外一种方式就是在代码中，直接使用 Table API 获取表环境的配置项来进行设置：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 配置 hive 方言 </span></span><br><span class="line">tableEnv.getConfig().setSqlDialect(SqlDialect.HIVE); </span><br><span class="line"><span class="comment">// 配置 default 方言 </span></span><br><span class="line">tableEnv.getConfig().setSqlDialect(SqlDialect.DEFAULT);</span><br></pre></td></tr></table></figure>

<p><strong>读写 Hive 表</strong></p>
<p>有了 SQL 方言的设置，我们就可以很方便的在 Flink 中创建 Hive 表并进行读写操作了。 Flink 支持以<strong>批处理</strong>和<strong>流处理</strong>模式向 Hive 中读写数据。在批处理模式下，Flink 会在执行查询语句时对 Hive 表进行一次性读取，在作业完成时将结果数据向 Hive 表进行一次性写入；而在流处理模式下，Flink 会持续监控 Hive 表，在新数据可用时增量读取，也可以持续写入新数据并增量式地让它们见。</p>
<p>更灵活的是，我们可以随时切换 SQL 方言，从其它数据源(例如 Kafka)读取数据、经转换后再写入 Hive。下面是以纯 SQL 形式编写的一个示例，我们可以启动 SQL 客户端来运行：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 设置 SQL 方言为 hive，创建 Hive 表 </span></span><br><span class="line"><span class="keyword">SET</span> table.sql<span class="operator">-</span>dialect<span class="operator">=</span>hive;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> hive_table (</span><br><span class="line">  user_id STRING,</span><br><span class="line">  order_amount <span class="keyword">DOUBLE</span></span><br><span class="line">) </span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (dt STRING, hr STRING) </span><br><span class="line">STORED <span class="keyword">AS</span> parquet </span><br><span class="line">TBLPROPERTIES (</span><br><span class="line">  <span class="string">&#x27;partition.time-extractor.timestamp-pattern&#x27;</span><span class="operator">=</span><span class="string">&#x27;$dt $hr:00:00&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;sink.partition-commit.trigger&#x27;</span><span class="operator">=</span><span class="string">&#x27;partition-time&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;sink.partition-commit.delay&#x27;</span><span class="operator">=</span><span class="string">&#x27;1 h&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;sink.partition-commit.policy.kind&#x27;</span><span class="operator">=</span><span class="string">&#x27;metastore,success-file&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 设置 SQL 方言为 default，创建 Kafka 表 </span></span><br><span class="line"><span class="keyword">SET</span> table.sql<span class="operator">-</span>dialect<span class="operator">=</span><span class="keyword">default</span>;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> kafka_table (</span><br><span class="line">  user_id STRING,</span><br><span class="line">  order_amount <span class="keyword">DOUBLE</span>,</span><br><span class="line">  log_ts <span class="type">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">  WATERMARK <span class="keyword">FOR</span> log_ts <span class="keyword">AS</span> log_ts <span class="operator">-</span> <span class="type">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">SECOND</span> <span class="comment">-- 定义水位线 </span></span><br><span class="line">) <span class="keyword">WITH</span> (...);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 将 Kafka 中读取的数据经转换后写入 Hive</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> hive_table</span><br><span class="line"><span class="keyword">SELECT</span> user_id, order_amount, DATE_FORMAT(log_ts, <span class="string">&#x27;yyyy-MM-dd&#x27;</span>), DATE_FORMAT(log_ts, <span class="string">&#x27;HH&#x27;</span>)</span><br><span class="line"><span class="keyword">FROM</span> kafka_table;</span><br></pre></td></tr></table></figure>

<p>这里我们创建 Hive 表时设置了通过分区时间来触发提交的策略。将 Kafka 中读取的数据经转换后写入 Hive，这是一个流处理的 Flink SQL 程序。</p>
<h2 id="Flink-CEP"><a href="#Flink-CEP" class="headerlink" title="Flink CEP"></a>Flink CEP</h2><p>在 Flink 的学习过程中，从基本原理和核心层 DataStream API 到底层的处理函数、再到应用层的Table API和SQL，我们已经掌握了Flink编程的各种手段，可以应对实际应用开发的各种需求了。</p>
<p>在大数据分析领域，一大类需求就是诸如 PV、UV 这样的统计指标，我们往往可以直接写 SQL 搞定；对于比较复杂的业务逻辑，SQL 中可能没有对应功能的内置函数，那么我们也可以使用 DataStream API，利用状态编程来进行实现。不过在实际应用中，还有一类需求是要检测以特定顺序先后发生的一组事件，进行统计或做报警提示，这就比较麻烦了。例如，网站做用户管理，可能需要检测连续登录失败事件的发生，这是个组合事件，其实就是登录失败和登录失败的组合；电商网站可能需要检测用户下单支付行为，这也是组合事件，下单事件之后一段时间内又会有支付事件到来，还包括了时间上的限制。</p>
<p>类似的多个事件的组合，我们把它叫作复杂事件。对于复杂时间的处理，由于涉及到事件的严格顺序，有时还有时间约束，我们很难直接用 SQL 或者 DataStream API 来完成。于是只好放大招——派底层的处理函数(process function)上阵了。处理函数确实可以搞定这些需求，不过对于非常复杂的组合事件，我们可能需要设置很多状态、定时器，并在代码中定义各种条件分支(if-else)逻辑来处理，复杂度会非常高，很可能会使代码失去可读性。怎样处理这类复杂事件呢？Flink 为我们提供了专门用于处理复杂事件的库——CEP，可以让我们更加轻松地解决这类棘手的问题。这在企业的实时风险控制中有非常重要的作用。</p>
<h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>所谓 CEP，其实就是复杂事件处理(Complex Event Processing)的缩写；而 Flink CEP，就是 Flink 实现的一个用于复杂事件处理的库(library)。</p>
<p>那到底什么是复杂事件处理呢？就是可以在事件流里，检测到特定的事件组合并进行处理，比如说连续登录失败，或者订单支付超时等等。</p>
<p>具体的处理过程是，把事件流中的一个个简单事件，通过一定的规则匹配组合起来，这就是复杂事件；然后基于这些满足规则的一组组复杂事件进行转换处理，得到想要的结果进行输出。</p>
<p>总结起来，复杂事件处理(CEP)的流程可以分成三个步骤：</p>
<p>(1)定义一个匹配规则</p>
<p>(2)将匹配规则应用到事件流上，检测满足规则的复杂事件</p>
<p>(3)对检测到的复杂事件进行处理，得到结果进行输出</p>
<p><img src="/Flink/%E6%88%AA%E5%B1%8F2022-07-22%20%E4%B8%8B%E5%8D%885.44.19.png" alt="截屏2022-07-22 下午5.44.19"></p>
<p>如图 12-1 所示，输入是不同形状的事件流，我们可以定义一个匹配规则：在圆形后面紧跟着三角形。那么将这个规则应用到输入流上，就可以检测到三组匹配的复杂事件。它们构成了一个新的复杂事件流，流中的数据就变成了一组一组的复杂事件，每个数据都包含了一 个圆形和一个三角形。接下来，我们就可以针对检测到的复杂事件，处理之后输出一个提示或报警信息了。</p>
<p>所以，CEP 是针对流处理而言的，分析的是低延迟、频繁产生的事件流。它的主要目的，就是在无界流中检测出特定的数据组合，让我们有机会掌握数据中重要的高阶特征。</p>
<h4 id="模式-Pattern"><a href="#模式-Pattern" class="headerlink" title="模式(Pattern)"></a>模式(Pattern)</h4><p>CEP 的第一步所定义的匹配规则，我们可以把它叫作模式(Pattern)。模式的定义主要就是两部分内容：</p>
<p><strong>每个简单事件的特征</strong></p>
<p><strong>简单事件之间的组合关系</strong> </p>
<p>当然，我们也可以进一步扩展模式的功能。比如，匹配检测的时间限制；每个简单事件是否可以重复出现；对于事件可重复出现的模式，遇到一个匹配后是否跳过后面的匹配；等等。 所谓事件之间的组合关系，一般就是定义谁后面接着是谁，也就是事件发生的顺序。 我们把它叫作近邻关系。可以定义严格的近邻关系，也就是两个事件之前不能有任何其他事件；也可以定义宽松的近邻关系，即只要前后顺序正确即可，中间可以有其他事件。另外，还可以反向定义，也就是谁后面不能跟着谁。</p>
<p>CEP 做的事其实就是在流上进行模式匹配。根据模式的近邻关系条件不同，可以检测连续的事件或不连续但先后发生的事件；模式还可能有时间的限制，如果在设定时间范围内没有满足匹配条件，就会导致模式匹配超时(timeout)。</p>
<p>Flink CEP 为我们提供了丰富的 API，可以实现上面关于模式的所有功能，这套 API 就叫作模式API(Pattern API)。</p>
<h3 id="快速上手"><a href="#快速上手" class="headerlink" title="快速上手"></a>快速上手</h3><p><strong>引入依赖</strong></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-cep_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>为了精简和避免依赖冲突，Flink 会保持尽量少的核心依赖。所以核心依赖中并不包括任何的连接器(conncetor)和库，这里的库就包括了 SQL、CEP 以及 ML 等等。所以如果想要在 Flink 集群中提交运行 CEP 作业，应该向 Flink SQL 那样将依赖的 jar 包放在/lib 目录下。从这个角度来看，Flink CEP 和 Flink SQL 一样，都是最顶层的应用级 API。</p>
<p><strong>简单案例</strong></p>
<p>接下来我们考虑一个具体的需求：检测用户行为，如果连续三次登录失败，就输出报警信息。很显然，这是一个复杂事件的检测处理，我们可以使用 Flink CEP 来实现。</p>
<p>我们首先定义数据的类型。这里的用户行为不再是之前的访问事件 Event 了，所以应该单独定义一个登录事件 POJO 类。具体实现如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LoginEvent</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> String userId;</span><br><span class="line">    <span class="keyword">public</span> String ipAddress;</span><br><span class="line">    <span class="keyword">public</span> String eventType;</span><br><span class="line">    <span class="keyword">public</span> Long timestamp;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">LoginEvent</span><span class="params">(String userId, String ipAddress, String eventType, Long timestamp)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.userId = userId;</span><br><span class="line">        <span class="keyword">this</span>.ipAddress = ipAddress;</span><br><span class="line">        <span class="keyword">this</span>.eventType = eventType;</span><br><span class="line">        <span class="keyword">this</span>.timestamp = timestamp;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">LoginEvent</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;LoginEvent&#123;&quot;</span> +</span><br><span class="line">                <span class="string">&quot;userId=&#x27;&quot;</span> + userId + <span class="string">&#x27;\&#x27;&#x27;</span> +</span><br><span class="line">                <span class="string">&quot;, ipAddress=&#x27;&quot;</span> + ipAddress + <span class="string">&#x27;\&#x27;&#x27;</span> +</span><br><span class="line">                <span class="string">&quot;, eventType=&#x27;&quot;</span> + eventType + <span class="string">&#x27;\&#x27;&#x27;</span> +</span><br><span class="line">                <span class="string">&quot;, timestamp=&quot;</span> + timestamp +</span><br><span class="line">                <span class="string">&#x27;&#125;&#x27;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LoginFailDetectExample</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 获取登录事件流，并提取时间戳、生成水位线</span></span><br><span class="line">        KeyedStream&lt;LoginEvent, String&gt; stream = env</span><br><span class="line">                .fromElements(</span><br><span class="line">                        <span class="keyword">new</span> LoginEvent(<span class="string">&quot;user_1&quot;</span>, <span class="string">&quot;192.168.0.1&quot;</span>, <span class="string">&quot;fail&quot;</span>, <span class="number">2000L</span>),</span><br><span class="line">                        <span class="keyword">new</span> LoginEvent(<span class="string">&quot;user_1&quot;</span>, <span class="string">&quot;192.168.0.2&quot;</span>, <span class="string">&quot;fail&quot;</span>, <span class="number">3000L</span>),</span><br><span class="line">                        <span class="keyword">new</span> LoginEvent(<span class="string">&quot;user_2&quot;</span>, <span class="string">&quot;192.168.1.29&quot;</span>, <span class="string">&quot;fail&quot;</span>, <span class="number">4000L</span>),</span><br><span class="line">                        <span class="keyword">new</span> LoginEvent(<span class="string">&quot;user_1&quot;</span>, <span class="string">&quot;171.56.23.10&quot;</span>, <span class="string">&quot;fail&quot;</span>, <span class="number">5000L</span>),</span><br><span class="line">                        <span class="keyword">new</span> LoginEvent(<span class="string">&quot;user_2&quot;</span>, <span class="string">&quot;192.168.1.29&quot;</span>, <span class="string">&quot;fail&quot;</span>, <span class="number">7000L</span>),</span><br><span class="line">                        <span class="keyword">new</span> LoginEvent(<span class="string">&quot;user_2&quot;</span>, <span class="string">&quot;192.168.1.29&quot;</span>, <span class="string">&quot;fail&quot;</span>, <span class="number">8000L</span>),</span><br><span class="line">                        <span class="keyword">new</span> LoginEvent(<span class="string">&quot;user_2&quot;</span>, <span class="string">&quot;192.168.1.29&quot;</span>, <span class="string">&quot;success&quot;</span>, <span class="number">6000L</span>)</span><br><span class="line">                )</span><br><span class="line">                .assignTimestampsAndWatermarks(</span><br><span class="line">                        WatermarkStrategy.&lt;LoginEvent&gt;forBoundedOutOfOrderness(Duration.ofSeconds(<span class="number">3</span>))</span><br><span class="line">                                .withTimestampAssigner(</span><br><span class="line">                                        <span class="keyword">new</span> SerializableTimestampAssigner&lt;LoginEvent&gt;() &#123;</span><br><span class="line">                                            <span class="meta">@Override</span></span><br><span class="line">                                            <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(LoginEvent loginEvent, <span class="keyword">long</span> l)</span> </span>&#123;</span><br><span class="line">                                                <span class="keyword">return</span> loginEvent.timestamp;</span><br><span class="line">                                            &#125;</span><br><span class="line">                                        &#125;</span><br><span class="line">                                )</span><br><span class="line">                )</span><br><span class="line">                .keyBy(r -&gt; r.userId);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 定义Pattern，连续的三个登录失败事件</span></span><br><span class="line">        Pattern&lt;LoginEvent, LoginEvent&gt; pattern = Pattern.&lt;LoginEvent&gt;begin(<span class="string">&quot;first&quot;</span>)    <span class="comment">// 以第一个登录失败事件开始</span></span><br><span class="line">                .where(<span class="keyword">new</span> SimpleCondition&lt;LoginEvent&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(LoginEvent loginEvent)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> loginEvent.eventType.equals(<span class="string">&quot;fail&quot;</span>);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .next(<span class="string">&quot;second&quot;</span>)    <span class="comment">// 接着是第二个登录失败事件</span></span><br><span class="line">                .where(<span class="keyword">new</span> SimpleCondition&lt;LoginEvent&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(LoginEvent loginEvent)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> loginEvent.eventType.equals(<span class="string">&quot;fail&quot;</span>);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .next(<span class="string">&quot;third&quot;</span>)     <span class="comment">// 接着是第三个登录失败事件</span></span><br><span class="line">                .where(<span class="keyword">new</span> SimpleCondition&lt;LoginEvent&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(LoginEvent loginEvent)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> loginEvent.eventType.equals(<span class="string">&quot;fail&quot;</span>);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 将Pattern应用到流上，检测匹配的复杂事件，得到一个PatternStream</span></span><br><span class="line">        PatternStream&lt;LoginEvent&gt; patternStream = CEP.pattern(stream, pattern);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4. 将匹配到的复杂事件选择出来，然后包装成字符串报警信息输出</span></span><br><span class="line">        patternStream</span><br><span class="line">                .select(<span class="keyword">new</span> PatternSelectFunction&lt;LoginEvent, String&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> String <span class="title">select</span><span class="params">(Map&lt;String, List&lt;LoginEvent&gt;&gt; map)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        LoginEvent first = map.get(<span class="string">&quot;first&quot;</span>).get(<span class="number">0</span>);</span><br><span class="line">                        LoginEvent second = map.get(<span class="string">&quot;second&quot;</span>).get(<span class="number">0</span>);</span><br><span class="line">                        LoginEvent third = map.get(<span class="string">&quot;third&quot;</span>).get(<span class="number">0</span>);</span><br><span class="line">                        <span class="keyword">return</span> first.userId + <span class="string">&quot; 连续三次登录失败！登录时间：&quot;</span> + first.timestamp + <span class="string">&quot;, &quot;</span> + second.timestamp + <span class="string">&quot;, &quot;</span> + third.timestamp;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .print(<span class="string">&quot;warning&quot;</span>);</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在上面的程序中，模式中的每个简单事件，会用一个.where()方法来指定一个约束条件，指明每个事件的特征，这里就是eventType 为fail。</p>
<p>而模式里表示事件之间的关系时，使用了 .next() 方法。next 是下一个的意思，表示紧挨着、中间不能有其他事件(比如登录成功)，这是一个严格近邻关系。第一个事件用 .begin() 方法表示开始。所有这些连接词都可以有一个字符串作为参数，这个字符串就可以认为是当前简单事件的名称。所以我们如果检测到一组匹配的复杂事件，里面就会有连续的三个登录失败事件，它们的名称分别叫作 first second 和 third。</p>
<p>在第三步处理复杂事件时，调用了 PatternStream 的 .select() 方法，传入一个 PatternSelectFunction 对检测到的复杂事件进行处理。而检测到的复杂事件，会放在一个 Map 中；PatternSelectFunction 内 .select() 方法有一个类型为 Map&lt;String, List<LoginEvent>&gt;的参数 map，里面就保存了检测到的匹配事件。这里的 key 是一个字符串，对应着事件的名称，而 value 是 LoginEvent 的一个列表，匹配到的登录失败事件就保存在这个列表里。最终我们提取 userId 和三次登录的时间戳，包装成字符串输出一个报警信息。</LoginEvent></p>
<p>上述代码可以进一步优化：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LoginFailDetectProExample</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 获取登录事件流，并提取时间戳、生成水位线</span></span><br><span class="line">        KeyedStream&lt;LoginEvent, String&gt; stream = env</span><br><span class="line">                .fromElements(</span><br><span class="line">                        <span class="keyword">new</span> LoginEvent(<span class="string">&quot;user_1&quot;</span>, <span class="string">&quot;192.168.0.1&quot;</span>, <span class="string">&quot;fail&quot;</span>, <span class="number">2000L</span>),</span><br><span class="line">                        <span class="keyword">new</span> LoginEvent(<span class="string">&quot;user_1&quot;</span>, <span class="string">&quot;192.168.0.2&quot;</span>, <span class="string">&quot;fail&quot;</span>, <span class="number">3000L</span>),</span><br><span class="line">                        <span class="keyword">new</span> LoginEvent(<span class="string">&quot;user_2&quot;</span>, <span class="string">&quot;192.168.1.29&quot;</span>, <span class="string">&quot;fail&quot;</span>, <span class="number">4000L</span>),</span><br><span class="line">                        <span class="keyword">new</span> LoginEvent(<span class="string">&quot;user_1&quot;</span>, <span class="string">&quot;171.56.23.10&quot;</span>, <span class="string">&quot;fail&quot;</span>, <span class="number">5000L</span>),</span><br><span class="line">                        <span class="keyword">new</span> LoginEvent(<span class="string">&quot;user_2&quot;</span>, <span class="string">&quot;192.168.1.29&quot;</span>, <span class="string">&quot;fail&quot;</span>, <span class="number">7000L</span>),</span><br><span class="line">                        <span class="keyword">new</span> LoginEvent(<span class="string">&quot;user_2&quot;</span>, <span class="string">&quot;192.168.1.29&quot;</span>, <span class="string">&quot;fail&quot;</span>, <span class="number">8000L</span>),</span><br><span class="line">                        <span class="keyword">new</span> LoginEvent(<span class="string">&quot;user_2&quot;</span>, <span class="string">&quot;192.168.1.29&quot;</span>, <span class="string">&quot;success&quot;</span>, <span class="number">6000L</span>)</span><br><span class="line">                )</span><br><span class="line">                .assignTimestampsAndWatermarks(</span><br><span class="line">                        WatermarkStrategy.&lt;LoginEvent&gt;forBoundedOutOfOrderness(Duration.ofSeconds(<span class="number">3</span>))</span><br><span class="line">                                .withTimestampAssigner(</span><br><span class="line">                                        <span class="keyword">new</span> SerializableTimestampAssigner&lt;LoginEvent&gt;() &#123;</span><br><span class="line">                                            <span class="meta">@Override</span></span><br><span class="line">                                            <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(LoginEvent loginEvent, <span class="keyword">long</span> l)</span> </span>&#123;</span><br><span class="line">                                                <span class="keyword">return</span> loginEvent.timestamp;</span><br><span class="line">                                            &#125;</span><br><span class="line">                                        &#125;</span><br><span class="line">                                )</span><br><span class="line">                )</span><br><span class="line">                .keyBy(r -&gt; r.userId);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 定义Pattern，连续的三个登录失败事件</span></span><br><span class="line">        Pattern&lt;LoginEvent, LoginEvent&gt; pattern = Pattern.&lt;LoginEvent&gt;begin(<span class="string">&quot;fail&quot;</span>)    <span class="comment">// 第一个登录失败事件</span></span><br><span class="line">                .where(<span class="keyword">new</span> SimpleCondition&lt;LoginEvent&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(LoginEvent loginEvent)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> loginEvent.eventType.equals(<span class="string">&quot;fail&quot;</span>);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;).times(<span class="number">3</span>).consecutive();    <span class="comment">// 指定是严格紧邻的三次登录失败</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 将Pattern应用到流上，检测匹配的复杂事件，得到一个PatternStream</span></span><br><span class="line">        PatternStream&lt;LoginEvent&gt; patternStream = CEP.pattern(stream, pattern);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4. 将匹配到的复杂事件选择出来，然后包装成字符串报警信息输出</span></span><br><span class="line">        SingleOutputStreamOperator&lt;String&gt; warningStream = patternStream</span><br><span class="line">                .process(<span class="keyword">new</span> PatternProcessFunction&lt;LoginEvent, String&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processMatch</span><span class="params">(Map&lt;String, List&lt;LoginEvent&gt;&gt; match, Context ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="comment">// 提取三次登录失败事件</span></span><br><span class="line">                        LoginEvent firstFailEvent = match.get(<span class="string">&quot;fail&quot;</span>).get(<span class="number">0</span>);</span><br><span class="line">                        LoginEvent secondFailEvent = match.get(<span class="string">&quot;fail&quot;</span>).get(<span class="number">1</span>);</span><br><span class="line">                        LoginEvent thirdFailEvent = match.get(<span class="string">&quot;fail&quot;</span>).get(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">                        out.collect(firstFailEvent.userId + <span class="string">&quot; 连续三次登录失败！登录时间：&quot;</span> +</span><br><span class="line">                                firstFailEvent.timestamp + <span class="string">&quot;, &quot;</span> +</span><br><span class="line">                                secondFailEvent.timestamp + <span class="string">&quot;, &quot;</span> +</span><br><span class="line">                                thirdFailEvent.timestamp);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 打印输出</span></span><br><span class="line">        warningStream.print(<span class="string">&quot;warning&quot;</span>);</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="模式API-Pattern-API"><a href="#模式API-Pattern-API" class="headerlink" title="模式API(Pattern API)"></a>模式API(Pattern API)</h3><h4 id="个体模式"><a href="#个体模式" class="headerlink" title="个体模式"></a>个体模式</h4><p>我们就把每个简单事件的匹配规则，叫作个体模式(Individual Pattern)。</p>
<p>每一个登录失败事件的选取规则，就都是一个个体模式。比如：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">.&lt;LoginEvent&gt;begin(<span class="string">&quot;first&quot;</span>) <span class="comment">// 以第一个登录失败事件开始 </span></span><br><span class="line">.where(<span class="keyword">new</span> SimpleCondition&lt;LoginEvent&gt;() &#123;</span><br><span class="line">   <span class="meta">@Override</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(LoginEvent loginEvent)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">       <span class="keyword">return</span> loginEvent.eventType.equals(<span class="string">&quot;fail&quot;</span>);</span><br><span class="line">   &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>

<p>个体模式一般都会匹配接收一个事件。</p>
<p>每个个体模式都以一个连接词开始定义的，比如 begin、next 等等，这是 Pattern 对象的一个方法(begin 是 Pattern 类的静态方法)，返回的还是一个 Pattern。这些连接词方法有一个 String 类型参数，这就是当前个体模式唯一的名字，比如这里的 first、second。在之后检测到匹配事件时，就会以这个名字来指代匹配事件。</p>
<p>个体模式需要一个过滤条件，用来指定具体的匹配规则。这个条件一般是通过调用.where()方法来实现的，具体的过滤逻辑则通过传入的 SimpleCondition 内的 .filter() 方法来定义。</p>
<p>另外，个体模式可以匹配接收一个事件，也可以接收多个事件。这听起来有点奇怪，一个单独的匹配规则可能匹配到多个事件吗？这是可能的，我们可以给个体模式增加一个量词 (quantifier)，就能够让它进行循环匹配，接收多个事件。</p>
<h5 id="量词-Quantifiers"><a href="#量词-Quantifiers" class="headerlink" title="量词(Quantifiers )"></a>量词(Quantifiers )</h5><p>个体模式后面可以跟一个量词，用来指定循环的次数。从这个角度分类，个体模式可以包括<strong>单例(singleton)模式</strong>和<strong>循环(looping)模式</strong>。默认情况下，个体模式是单例模式，匹配接收一个事件；当定义了量词之后，就变成了循环模式，可以匹配接收多个事件。</p>
<p>在循环模式中，对同样特征的事件可以匹配多次。比如我们定义个体模式为匹配形状为三角形的事件，再让它循环多次，就变成了匹配连续多个三角形的事件。注意这里的连续，只要保证前后顺序即可，中间可以有其他事件，所以是宽松近邻关系。</p>
<p>在 Flink CEP 中，可以使用不同的方法指定循环模式，主要有：</p>
<p><strong>.oneOrMore()</strong></p>
<p>匹配事件出现一次或多次，假设 a 是一个个体模式，a.oneOrMore() 表示可以匹配 1 个或多个 a 的事件组合。我们有时会用 a+ 来简单表示。</p>
<p><strong>.times(times)</strong></p>
<p>匹配事件发生特定次数(times)，例如 a.times(3) 表示 aaa。</p>
<p><strong>.times(fromTimes，toTimes)</strong></p>
<p>指定匹配事件出现的次数范围，最小次数为 fromTimes，最大次数为 toTimes。例如 a.times(2,4) 可以匹配 aa，aaa 和 aaaa。</p>
<p><strong>.greedy()</strong></p>
<p>只能用在循环模式后，使当前循环模式变得贪心(greedy)，也就是总是尽可能多地去匹配。例如 a.times(2, 4).greedy()，如果出现了连续 4 个 a，那么会直接把 aaaa 检测出来进行处理，其他任意 2 个 a 是不算匹配事件的。</p>
<p><strong>.optional()</strong></p>
<p>使当前模式成为可选的，也就是说可以满足这个匹配条件，也可以不满足。 </p>
<p>对于一个个体模式 pattern 来说，后面所有可以添加的量词如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 匹配事件出现 4 次 </span></span><br><span class="line">pattern.times(<span class="number">4</span>);</span><br><span class="line"><span class="comment">// 匹配事件出现 4 次，或者不出现</span></span><br><span class="line">pattern.times(<span class="number">4</span>).optional();</span><br><span class="line"><span class="comment">// 匹配事件出现2, 3 或者4次 </span></span><br><span class="line">pattern.times(<span class="number">2</span>, <span class="number">4</span>);</span><br><span class="line"><span class="comment">// 匹配事件出现 2, 3 或者 4 次，并且尽可能多地匹配 </span></span><br><span class="line">pattern.times(<span class="number">2</span>, <span class="number">4</span>).greedy();</span><br><span class="line"><span class="comment">// 匹配事件出现 2, 3, 4 次，或者不出现 </span></span><br><span class="line">pattern.times(<span class="number">2</span>, <span class="number">4</span>).optional();</span><br><span class="line"><span class="comment">// 匹配事件出现 2, 3, 4 次，或者不出现；并且尽可能多地匹配 </span></span><br><span class="line">pattern.times(<span class="number">2</span>, <span class="number">4</span>).optional().greedy();</span><br><span class="line"><span class="comment">// 匹配事件出现 1 次或多次 </span></span><br><span class="line">pattern.oneOrMore();</span><br><span class="line"><span class="comment">// 匹配事件出现 1 次或多次，并且尽可能多地匹配 </span></span><br><span class="line">pattern.oneOrMore().greedy();</span><br><span class="line"><span class="comment">// 匹配事件出现 1 次或多次，或者不出现 </span></span><br><span class="line">pattern.oneOrMore().optional();</span><br><span class="line"><span class="comment">// 匹配事件出现 1 次或多次，或者不出现；并且尽可能多地匹配</span></span><br><span class="line">pattern.oneOrMore().optional().greedy();</span><br><span class="line"><span class="comment">// 匹配事件出现 2 次或多次 </span></span><br><span class="line">pattern.timesOrMore(<span class="number">2</span>);</span><br><span class="line"><span class="comment">// 匹配事件出现 2 次或多次，并且尽可能多地匹配 </span></span><br><span class="line">pattern.timesOrMore(<span class="number">2</span>).greedy();</span><br><span class="line"><span class="comment">// 匹配事件出现 2 次或多次，或者不出现 </span></span><br><span class="line">pattern.timesOrMore(<span class="number">2</span>).optional()</span><br><span class="line"><span class="comment">// 匹配事件出现 2 次或多次，或者不出现；并且尽可能多地匹配 </span></span><br><span class="line">pattern.timesOrMore(<span class="number">2</span>).optional().greedy();</span><br></pre></td></tr></table></figure>

<p>正是因为个体模式可以通过量词定义为循环模式，一个模式能够匹配到多个事件，所以之前代码中事件的检测接收才会用 Map 中的一个列表(List)来保存。而之前代码中没有定义量词，都是单例模式，所以只会匹配一个事件，每个 List 中也只有一个元素：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">LoginEvent first = map.get(<span class="string">&quot;first&quot;</span>).get(<span class="number">0</span>);</span><br></pre></td></tr></table></figure>

<h5 id="条件-Conditions"><a href="#条件-Conditions" class="headerlink" title="条件(Conditions)"></a>条件(Conditions)</h5><p>对于每个个体模式，匹配事件的核心在于定义匹配条件，也就是选取事件的规则。Flink CEP 会按照这个规则对流中的事件进行筛选，判断是否接受当前的事件。</p>
<p>对于条件的定义，主要是通过调用 Pattern 对象的 .where() 方法来实现的，主要可以分为简单条件、迭代条件、复合条件、终止条件几种类型。此外，也可以调用 Pattern 对象的 .subtype() 方法来限定匹配事件的子类型。</p>
<p><strong>限定子类型</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">pattern.subtype(SubEvent.class);</span><br></pre></td></tr></table></figure>

<p>这里 SubEvent 是流中数据类型 Event 的子类型。这时，只有当事件是 SubEvent 类型时， 才可以满足当前模式 pattern 的匹配条件。</p>
<p><strong>简单条件(Simple Conditions)</strong></p>
<p>简单条件是最简单的匹配规则，只根据当前事件的特征来决定是否接受它。这在本质上其实就是一个 filter 操作。</p>
<p>代码中我们为 .where() 方法传入一个 SimpleCondition 的实例作为参数。SimpleCondition 是表示简单条件的抽象类，内部有一个 .filter() 方法，唯一的参数就是当前事件。所以它可以当作 FilterFunction 来使用。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">pattern.where(<span class="keyword">new</span> SimpleCondition&lt;Event&gt;() &#123; </span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(Event value)</span> </span>&#123; </span><br><span class="line">    <span class="keyword">return</span> value.user.startsWith(<span class="string">&quot;A&quot;</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>

<p><strong>迭代条件(Iterative Conditions)</strong></p>
<p>简单条件只能基于当前事件做判断，能够处理的逻辑比较有限。在实际应用中，我们可能需要将当前事件跟之前的事件做对比，才能判断出要不要接受当前事件。这种需要依靠之前事件来做判断的条件，就叫作迭代条件(Iterative Condition)。</p>
<p>在 Flink CEP 中，提供了 IterativeCondition 抽象类。这其实是更加通用的条件表达，查看源码可以发现，.where() 方法本身要求的参数类型就是 IterativeCondition；而之前的 SimpleCondition 是它的一个子类。</p>
<p>在 IterativeCondition 中同样需要实现一个 filter() 方法，不过与 SimpleCondition 中不同的是，这个方法有两个参数：除了当前事件之外，还有一个上下文 Context。调用这个上下文的 .getEventsForPattern() 方法，传入一个模式名称，就可以拿到这个模式中已匹配到的所有数据了。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">middle.oneOrMore()</span><br><span class="line">      .where(<span class="keyword">new</span> IterativeCondition&lt;Event&gt;() &#123;</span><br><span class="line">         <span class="meta">@Override</span></span><br><span class="line">         <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(Event value, Context&lt;Event&gt; ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="comment">// 事件中的 user 必须以 A 开头</span></span><br><span class="line">            <span class="keyword">if</span> (!value.user.startsWith(<span class="string">&quot;A&quot;</span>)) &#123;</span><br><span class="line">               <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">int</span> sum = value.amount;</span><br><span class="line">            </span><br><span class="line">            <span class="comment">// 获取当前模式之前已经匹配的事件，求所有事件 amount 之和</span></span><br><span class="line">            <span class="keyword">for</span> (Event event : ctx.getEventsForPattern(<span class="string">&quot;middle&quot;</span>)) &#123;</span><br><span class="line">              sum += event.amount;</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">            <span class="comment">// 在总数量小于 100 时，当前事件满足匹配规则，可以匹配成功</span></span><br><span class="line">            <span class="keyword">return</span> sum &lt; <span class="number">100</span>;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;);</span><br></pre></td></tr></table></figure>

<p>上面代码中当前模式名称就叫作middle，这是一个循环模式，可以接受事件发生一次或多次。于是下面的迭代条件中，我们通过 ctx.getEventsForPattern(“middle”) 获取当前模式已经接受的事件，计算它们的数量(amount)之和；再加上当前事件中的数量，如果总和小于 100，就接受当前事件，否则就不匹配。当然，在迭代条件中我们也可以基于当前事件做出判断，比如代码中要求 user 必须以 A 开头。最终我们的匹配规则就是：事件的 user 必须以 A 开头；并且循环匹配的所有事件 amount 之和必须小于 100。这里的 Event 与之前定义的 POJO 不同，增加了 amount 属性。</p>
<p>可以看到，迭代条件能够获取已经匹配的事件，如果自身又是循环模式(比如量词 oneOrMore)，那么两者结合就可以捕获自身之前接收的数据，据此来判断是否接受当前事件。 这个功能非常强大，我们可以由此实现更加复杂的需求，比如可以要求只有大于之前数据的平均值，才接受当前事件。</p>
<p>另外迭代条件中的上下文 Context 也可以获取到时间相关的信息，比如事件的时间戳和当前的处理时间(processing time)。</p>
<p><strong>组合条件(Combining Conditions)</strong></p>
<p>最直接的想法是，可以在简单条件或者迭代条件的 .filter() 方法中，增加多个判断逻辑。可以通过 if-else 的条件分支分别定义多个条件，也可以直接在 return 返回时给一个多条件的逻辑组合(与、或、非)。不过这样会让代码变得臃肿，可读性降低。更好的方式是独立定义多个条件，然后在外部把它们连接起来，构成一个组合条件(Combining Condition)。</p>
<p>最简单的组合条件，就是 .where() 后面再接一个 .where() 。因为前面提到过，一个条件就像是一个 filter 操作，所以每次调用 .where() 方法都相当于做了一次过滤，连续多次调用就表示多重过滤，最终匹配的事件自然就会同时满足所有条件。这相当于就是多个条件的逻辑与(AND)。</p>
<p>而多个条件的逻辑或(OR)，则可以通过 .where() 后加一个 .or() 来实现。这里的 .or() 方法与 .where() 一样，传入一个 IterativeCondition 作为参数，定义一个独立的条件；它和之前 .where() 定义的条件只要满足一个，当前事件就可以成功匹配。</p>
<p>当然，子类型限定条件(subtype)也可以和其他条件结合起来，成为组合条件，如下所示：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">pattern.subtype(SubEvent.class)</span><br><span class="line">   .where(<span class="keyword">new</span> SimpleCondition&lt;SubEvent&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(SubEvent value)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> ... <span class="comment">// some condition</span></span><br><span class="line">        &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>

<p>这里可以看到，SimpleCondition 的泛型参数也变成了 SubEvent，所以匹配出的事件就既满足子类型限制，又符合过滤筛选的简单条件；这也是一个逻辑与的关系。</p>
<p><strong>终止条件(Stop Conditions)</strong></p>
<p>对于循环模式而言，还可以指定一个终止条件(Stop Condition)，表示遇到某个特定事件时当前模式就不再继续循环匹配了。 </p>
<p>终止条件的定义是通过调用模式对象的 .until() 方法来实现的，同样传入一个 IterativeCondition 作为参数。需要注意的是，终止条件只与 oneOrMore() 或者 oneOrMore().optional() 结合使用。因为在这种循环模式下，我们不知道后面还有没有事件可以匹配，只好把之前匹配的事件作为状态缓存起来继续等待，这等待无穷无尽；如果一直等下去，缓存的状态越来越多，最终会耗尽内存。所以这种循环模式必须有个终点，当 .until() 指定的条件满足时，循环终止，这样就可以清空状态释放内存了。</p>
<h4 id="组合模式"><a href="#组合模式" class="headerlink" title="组合模式"></a>组合模式</h4><p>有了定义好的个体模式，就可以尝试按一定的顺序把它们连接起来，定义一个完整的复杂事件匹配规则了。这种将多个个体模式组合起来的完整模式，就叫作组合模式(Combining Pattern)，为了跟个体模式区分有时也叫作模式序列(Pattern Sequence)。</p>
<p>一个组合模式有以下形式：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Pattern&lt;Event, ?&gt; pattern = Pattern</span><br><span class="line">       .&lt;Event&gt;begin(<span class="string">&quot;start&quot;</span>).where(...)</span><br><span class="line">       .next(<span class="string">&quot;next&quot;</span>).where(...)</span><br><span class="line">       .followedBy(<span class="string">&quot;follow&quot;</span>).where(...)</span><br><span class="line">       ...</span><br></pre></td></tr></table></figure>

<p>可以看到，组合模式确实就是一个模式序列，是用诸如 begin、next、followedBy 等表 示先后顺序的连接词将个体模式串连起来得到的。在这样的语法调用中，每个事件匹配的条件是什么、各个事件之间谁先谁后、近邻关系如何都定义得一目了然。每一个连接词方法调用之后，得到的都仍然是一个 Pattern 的对象；所以从 Java 对象的角度看，组合模式与个体模式是一样的，都是Pattern。</p>
<p><strong>初始模式(Initial Pattern)</strong></p>
<p>所有的组合模式，都必须以一个初始模式开头；而初始模式必须通过调用 Pattern 的静态方法 .begin() 来创建。如下所示：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Pattern&lt;Event, ?&gt; start = Pattern.&lt;Event&gt;begin(<span class="string">&quot;start&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>这里我们调用 Pattern 的 .begin() 方法创建了一个初始模式。传入的 String 类型的参数就是模式的名称；而 begin 方法需要传入一个类型参数，这就是模式要检测流中事件的基本类型，这里我们定义为 Event。调用的结果返回一个 Pattern 的对象实例。Pattern 有两个泛型参数，第一个就是检测事件的基本类型 Event，跟 begin 指定的类型一致；第二个则是当前模式里事件的子类型，由子类型限制条件指定。我们这里用类型通配符(?)代替，就可以从上下文直接推断了。</p>
<p><strong>近邻条件(Contiguity Conditions)</strong></p>
<p>在初始模式之后，我们就可以按照复杂事件的顺序追加模式，组合成模式序列了。模式之间的组合是通过一些连接词方法实现的，这些连接词指明了先后事件之间有着怎样的近邻关系，这就是所谓的近邻条件(Contiguity Conditions，也叫连续性条件)。</p>
<p>Flink CEP 中提供了三种近邻关系：</p>
<p>严格近邻(Strict Contiguity)</p>
<p>匹配的事件严格地按顺序一个接一个出现，中间不会有任何其他事件。代码中对应的就是 Pattern 的 .next() 方法，名称上就能看出来，下一个自然就是紧挨着的。</p>
<p>宽松近邻(Relaxed Contiguity)</p>
<p>松近邻只关心事件发生的顺序，而放宽了对匹配事件的距离要求，也就是说两个匹配的事件之间可以有其他不匹配的事件出现。代码中对应 .followedBy() 方法，很明显这表示跟在后面就可以，不需要紧紧相邻。</p>
<p>非确定性宽松近邻(Non-Deterministic Relaxed Contiguity)</p>
<p>这种近邻关系更加宽松。所谓非确定性是指可以重复使用之前已经匹配过的事件；这种近邻条件下匹配到的不同复杂事件，可以以同一个事件作为开始，所以匹配结果一般会比宽松近邻更多。</p>
<p><strong>其他限制条件</strong></p>
<p>除了上面提到的 next()、followedBy()、followedByAny() 可以分别表示三种近邻条件，我们还可以用否定的连接词来组合个体模式。主要包括：</p>
<p>.notNext()</p>
<p>表示前一个模式匹配到的事件后面，不能紧跟着某种事件。</p>
<p>.notFollowedBy()</p>
<p>表示前一个模式匹配到的事件后面，不会出现某种事件。这里需要注意，由于 notFollowedBy() 是没有严格限定的；流数据不停地到来，我们永远不能保证之后不会出现某种事件。所以一个模式序列不能以 notFollowedBy() 结尾，这个限定条件主要用来表示两个事件中间不会出现某种事件。</p>
<p>另外，Flink CEP 中还可以为模式指定一个时间限制，这是通过调用 .within() 方法实现的。 方法传入一个时间参数，这是模式序列中第一个事件到最后一个事件之间的最大时间间隔，只有在这期间成功匹配的复杂事件才是有效的。一个模式序列中只能有一个时间限制，调用 .within() 的位置不限；如果多次调用则会以最小的那个时间间隔为准。</p>
<p>下面是模式序列中所有限制条件在代码中的定义：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 严格近邻条件</span></span><br><span class="line">Pattern&lt;Event, ?&gt; strict = start.next(<span class="string">&quot;middle&quot;</span>).where(...);</span><br><span class="line"><span class="comment">// 宽松近邻条件</span></span><br><span class="line">Pattern&lt;Event, ?&gt; relaxed = start.followedBy(<span class="string">&quot;middle&quot;</span>).where(...);</span><br><span class="line"><span class="comment">// 非确定性宽松近邻条件</span></span><br><span class="line">Pattern&lt;Event, ?&gt; nonDetermin = start.followedByAny(<span class="string">&quot;middle&quot;</span>).where(...);</span><br><span class="line"><span class="comment">// 不能严格近邻条件</span></span><br><span class="line">Pattern&lt;Event, ?&gt; strictNot = start.notNext(<span class="string">&quot;not&quot;</span>).where(...);</span><br><span class="line"><span class="comment">// 不能宽松近邻条件</span></span><br><span class="line">Pattern&lt;Event, ?&gt; relaxedNot = start.notFollowedBy(<span class="string">&quot;not&quot;</span>).where(...);</span><br><span class="line"><span class="comment">// 时间限制条件 </span></span><br><span class="line">middle.within(Time.seconds(<span class="number">10</span>));</span><br></pre></td></tr></table></figure>

<p><strong>循环模式中的近邻条件</strong></p>
<p>.consecutive()</p>
<p>为循环模式中的匹配事件增加严格的近邻条件，保证所有匹配事件是严格连续的。也就是说，一旦中间出现了不匹配的事件，当前循环检测就会终止。这起到的效果跟模式序列中的 next() 一样，需要与循环量词 times()、oneOrMore()配合使用。</p>
<p>.allowCombinations()</p>
<p>除严格近邻外，也可以为循环模式中的事件指定非确定性宽松近邻条件，表示可以重复使用已经匹配的事件。这需要调用 .allowCombinations() 方法来实现，实现的效果与 .followedByAny() 相同。</p>
<h4 id="模式组"><a href="#模式组" class="headerlink" title="模式组"></a>模式组</h4><p>一般来说，代码中定义的模式序列，就是我们在业务逻辑中匹配复杂事件的规则。不过在有些非常复杂的场景中，可能需要划分多个阶段，每个阶段又有一连串的匹配规则。为了应对这样的需求，Flink CEP 允许我们以嵌套的方式来定义模式。</p>
<p>之前在模式序列中，我们用 begin()、next()、followedBy()、followedByAny()这样的连接词来组合个体模式，这些方法的参数就是一个个体模式的名称；而现在它们可以直接以一个模式序列作为参数，就将模式序列又一次连接组合起来了。这样得到的就是一个模式组(Groups of Patterns)。</p>
<p>在模式组中，每一个模式序列就被当作了某一阶段的匹配条件，返回的类型是一个 GroupPattern。而 GroupPattern 本身是 Pattern 的子类；所以个体模式和组合模式能调用的方法，比如 times()、oneOrMore()、optional()之类的量词，模式组一般也是可以用的。</p>
<p>具体在代码中的应用如下所示：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 以模式序列作为初始模式</span></span><br><span class="line">Pattern&lt;Event, ?&gt; start = Pattern.begin(</span><br><span class="line">       Pattern.&lt;Event&gt;begin(<span class="string">&quot;start_start&quot;</span>).where(...)</span><br><span class="line">             .followedBy(<span class="string">&quot;start_middle&quot;</span>).where(...)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在 start 后定义严格近邻的模式序列，并重复匹配两次 </span></span><br><span class="line">Pattern&lt;Event, ?&gt; strict = start.next(</span><br><span class="line">       Pattern.&lt;Event&gt;begin(<span class="string">&quot;next_start&quot;</span>).where(...)</span><br><span class="line">             .followedBy(<span class="string">&quot;next_middle&quot;</span>).where(...)</span><br><span class="line">).times(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在 start 后定义宽松近邻的模式序列，并重复匹配一次或多次 </span></span><br><span class="line">Pattern&lt;Event, ?&gt; relaxed = start.followedBy(</span><br><span class="line">       Pattern.&lt;Event&gt;begin(<span class="string">&quot;followedby_start&quot;</span>).where(...)</span><br><span class="line">             .followedBy(<span class="string">&quot;followedby_middle&quot;</span>).where(...)</span><br><span class="line">).oneOrMore();</span><br><span class="line"></span><br><span class="line"><span class="comment">//在 start 后定义非确定性宽松近邻的模式序列，可以匹配一次，也可以不匹配 </span></span><br><span class="line">Pattern&lt;Event, ?&gt; nonDeterminRelaxed = start.followedByAny(</span><br><span class="line">       Pattern.&lt;Event&gt;begin(<span class="string">&quot;followedbyany_start&quot;</span>).where(...)</span><br><span class="line">             .followedBy(<span class="string">&quot;followedbyany_middle&quot;</span>).where(...)</span><br><span class="line">).optional();</span><br></pre></td></tr></table></figure>

<h4 id="匹配后跳过策略"><a href="#匹配后跳过策略" class="headerlink" title="匹配后跳过策略"></a>匹配后跳过策略</h4><p>在 Flink CEP 中，由于有循环模式和非确定性宽松近邻的存在，同一个事件有可能会重复利用，被分配到不同的匹配结果中。这样会导致匹配结果规模增大，有时会显得非常冗余。当然，非确定性宽松近邻条件，本来就是为了放宽限制、扩充匹配结果而设计的；我们主要是针对循环模式来考虑匹配结果的精简。</p>
<p>如果对循环模式增加了.greedy()的限制，那么就会尽可能多地匹配事件，这样就可以砍掉那些子集上的匹配了。不过这种方式还是略显简单粗暴，如果我们想要精确控制事件的匹配应该跳过哪些情况，那就需要制定另外的策略了。</p>
<p>在 Flink CEP 中，提供了模式的匹配后跳过策略(After Match Skip Strategy)，专门用来精准控制循环模式的匹配结果。这个策略可以在 Pattern 的初始模式定义中，作为 begin() 的第二个参数传入：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Pattern.begin(<span class="string">&quot;start&quot;</span>, AfterMatchSkipStrategy.noSkip())</span><br><span class="line">       .where(...)</span><br><span class="line">			 ...</span><br></pre></td></tr></table></figure>

<p>匹配后跳过策略 AfterMatchSkipStrategy 是一个抽象类，它有多个具体的实现，可以通过调用对应的静态方法来返回对应的策略实例。这里我们配置的是不做跳过处理，这也是默认策略。</p>
<p>下面我们举例来说明不同的跳过策略。例如我们要检测的复杂事件模式为：开始是用户名为 a 的事件(简写为事件 a，下同)，可以重复一次或多次；然后跟着一个用户名为 b 的事件，a 事件和 b 事件之间可以有其他事件(宽松近邻)。用简写形式可以直接写作：a+ followedBy b。在代码中定义 Pattern 如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Pattern.&lt;Event&gt;begin(<span class="string">&quot;a&quot;</span>).where(<span class="keyword">new</span> SimpleCondition&lt;Event&gt;() &#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(Event value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">   <span class="keyword">return</span> value.user.equals(<span class="string">&quot;a&quot;</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;).oneOrMore()</span><br><span class="line">  .followedBy(<span class="string">&quot;b&quot;</span>).where(<span class="keyword">new</span> SimpleCondition&lt;Event&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(Event value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">     <span class="keyword">return</span> value.user.equals(<span class="string">&quot;b&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">   &#125;);</span><br></pre></td></tr></table></figure>

<p>我们如果输入事件序列 “a a a b” ——这里为了区分前后不同的 a 事件，可以记作 “a1 a2 a3 b” ——那么应该检测到 6 个匹配结果：(a1 a2 a3 b)，(a1 a2 b)，(a1 b)，(a2 a3 b)，(a2 b)， (a3 b)。如果在初始模式的量词 .oneOrMore() 后加上 .greedy() 定义为贪心匹配，那么结果就是: (a1 a2 a3 b)，(a2 a3 b)，(a3 b)，每个事件作为开头只会出现一次。</p>
<p>接下来我们讨论不同跳过策略对匹配结果的影响：</p>
<p><strong>不跳过(NO_SKIP)</strong></p>
<p>代码调用 AfterMatchSkipStrategy.noSkip()。这是默认策略，所有可能的匹配都会输出。所以这里会输出完整的 6 个匹配。</p>
<p><strong>跳至下一个(SKIP_TO_NEXT)</strong></p>
<p>代码调用 AfterMatchSkipStrategy.skipToNext()。找到一个 a1 开始的最大匹配之后，跳过 a1 开始的所有其他匹配，直接从下一个 a2 开始匹配起。当然 a2 也是如此跳过其他匹配。最终得到(a1 a2 a3 b)，(a2 a3 b)，(a3 b)。可以看到，这种跳过策略跟使用 .greedy() 效果是相同的。</p>
<p><strong>跳过所有子匹配(SKIP_PAST_LAST_EVENT)</strong></p>
<p>代码调用 AfterMatchSkipStrategy.skipPastLastEvent()。找到 a1 开始的匹配(a1 a2 a3 b)之后，直接跳过所有 a1 直到 a3 开头的匹配，相当于把这些子匹配都跳过了。最终得到(a1 a2 a3 b)，这是最为精简的跳过策略。</p>
<p><strong>跳至第一个(SKIP_TO_FIRST[a])</strong></p>
<p>代码调用 AfterMatchSkipStrategy.skipToFirst(“a”)，这里传入一个参数，指明跳至哪个模式的第一个匹配事件。找到 a1 开始的匹配(a1 a2 a3 b)后，跳到以最开始一个 a(也就是 a1) 为开始的匹配，相当于只留下 a1 开始的匹配。最终得到(a1 a2 a3 b)，(a1 a2 b)，(a1 b)。</p>
<p><strong>跳至最后一个(SKIP_TO_LAST[a])</strong></p>
<p>代码调用 AfterMatchSkipStrategy.skipToLast(“a”)，同样传入一个参数，指明跳至哪个模式的最后一个匹配事件。找到 a1 开始的匹配(a1 a2 a3 b)后，跳过所有 a1、a2 开始的匹配，跳到以最后一个 a(也就是 a3)为开始的匹配。最终得到(a1 a2 a3 b)，(a3 b)。</p>
<h3 id="模式的检测处理"><a href="#模式的检测处理" class="headerlink" title="模式的检测处理"></a>模式的检测处理</h3><p>Pattern API 是 Flink CEP 的核心，也是最复杂的一部分。不过利用 Pattern API 定义好模式还只是整个复杂事件处理的第一步，接下来还需要将模式应用到事件流上、检测提取匹配的复杂事件并定义处理转换的方法，最终得到想要的输出信息。</p>
<h4 id="将模式应用到流上"><a href="#将模式应用到流上" class="headerlink" title="将模式应用到流上"></a>将模式应用到流上</h4><p>将模式应用到事件流上的代码非常简单，只要调用 CEP 类的静态方法 .pattern()，将数据流(DataStream)和模式(Pattern)作为两个参数传入就可以了。最终得到的是一个 PatternStream：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">DataStream&lt;Event&gt; inputStream = ...</span><br><span class="line">Pattern&lt;Event, ?&gt; pattern = ...</span><br><span class="line">PatternStream&lt;Event&gt; patternStream = CEP.pattern(inputStream, pattern);</span><br></pre></td></tr></table></figure>

<p>这里的 DataStream，也可以通过 keyBy 进行按键分区得到 KeyedStream，接下来对复杂事件的检测就会针对不同的 key 单独进行了。</p>
<p>模式中定义的复杂事件，发生是有先后顺序的，这里先后的判断标准取决于具体的时间语义。默认情况下采用事件时间语义，那么事件会以各自的时间戳进行排序；如果是处理时间语义，那么所谓先后就是数据到达的顺序。对于时间戳相同或是同时到达的事件，我们还可以在 CEP.pattern() 中传入一个比较器作为第三个参数，用来进行更精确的排序：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 可选的事件比较器</span></span><br><span class="line">EventComparator&lt;Event&gt; comparator = ...</span><br><span class="line">PatternStream&lt;Event&gt; patternStream = CEP.pattern(input, pattern, comparator);</span><br></pre></td></tr></table></figure>

<p>得到 PatternStream 后，接下来要做的就是对匹配事件的检测处理了。</p>
<h4 id="处理匹配事件"><a href="#处理匹配事件" class="headerlink" title="处理匹配事件"></a>处理匹配事件</h4><p>基于 PatternStream 可以调用一些转换方法，对匹配的复杂事件进行检测和处理，并最终得到一个正常的 DataStream。这个转换的过程与窗口的处理类似：将模式应用到流上得到 PatternStream，就像在流上添加窗口分配器得到 WindowedStream；而之后的转换操作，就像定义具体处理操作的窗口函数，对收集到的数据进行分析计算，得到结果进行输出，最后回到 DataStream 的类型来。</p>
<p>PatternStream 的转换操作主要可以分成两种：简单便捷的选择提取(select)操作，和更加通用、更加强大的处理(process)操作。与 DataStream 的转换类似，具体实现也是在调用 API 时传入一个函数类：选择操作传入的是一个 PatternSelectFunction，处理操作传入的则是一个 PatternProcessFunction。</p>
<h5 id="匹配事件的选择提取-select"><a href="#匹配事件的选择提取-select" class="headerlink" title="匹配事件的选择提取(select)"></a>匹配事件的选择提取(select)</h5><p>处理匹配事件最简单的方式，就是从 PatternStream 中直接把匹配的复杂事件提取出来，包装成想要的信息输出，这个操作就是选择(select)。</p>
<p><strong>PatternSelectFunction</strong></p>
<p>代码中基于 PatternStream 直接调用 .select() 方法，传入一个 PatternSelectFunction 作为参数。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">PatternStream&lt;Event&gt; patternStream = CEP.pattern(inputStream, pattern); </span><br><span class="line">DataStream&lt;String&gt; result = patternStream.select(<span class="keyword">new</span> MyPatternSelectFunction());</span><br></pre></td></tr></table></figure>

<p>这里的 MyPatternSelectFunction 是 PatternSelectFunction 的一个具体实现。 PatternSelectFunction 是 Flink CEP 提供的一个函数类接口，它会将检测到的匹配事件保存在一个 Map 里，对应的 key 就是这些事件的名称。这里的事件名称就对应着在模式中定义的每个个体模式的名称；而个体模式可以是循环模式，一个名称会对应多个事件，所以最终保存在 Map 里的 value 就是一个事件的列表(List)。</p>
<p>下面是 MyPatternSelectFunction 的一个具体实现：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyPatternSelectFunction</span> <span class="keyword">implements</span> <span class="title">PatternSelectFunction</span>&lt;<span class="title">Event</span>, <span class="title">String</span>&gt;</span>&#123; </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">select</span><span class="params">(Map&lt;String, List&lt;Event&gt;&gt; pattern)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">      Event startEvent = pattern.get(<span class="string">&quot;start&quot;</span>).get(<span class="number">0</span>);</span><br><span class="line">      Event middleEvent = pattern.get(<span class="string">&quot;middle&quot;</span>).get(<span class="number">0</span>);</span><br><span class="line">      <span class="keyword">return</span> startEvent.toString() + <span class="string">&quot; &quot;</span> + middleEvent.toString();</span><br><span class="line">		&#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>PatternSelectFunction 里需要实现一个 select() 方法，这个方法每当检测到一组匹配的复杂事件时都会调用一次。它以保存了匹配复杂事件的 Map 作为输入，经自定义转换后得到输出信息返回。这里我们假设之前定义的模式序列中，有名为start和middle的两个个体模式，于是可以通过这个名称从 Map 中选择提取出对应的事件。注意调用 Map 的.get(key)方法后得到的是一个事件的 List；如果个体模式是单例的，那么 List 中只有一个元素，直接调用.get(0) 就可以把它取出。当然，如果个体模式是循环的，List 中就有可能有多个元素了。</p>
<p>连续登录失败检测的改进，我们可以将匹配到的事件包装成 String 类型的报警信息输出：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 1. 定义 Pattern，登录失败事件，循环检测 3 次 </span></span><br><span class="line">Pattern&lt;LoginEvent, LoginEvent&gt; pattern = Pattern</span><br><span class="line">       .&lt;LoginEvent&gt;begin(<span class="string">&quot;fails&quot;</span>)</span><br><span class="line">       .where(<span class="keyword">new</span> SimpleCondition&lt;LoginEvent&gt;() &#123;</span><br><span class="line">          <span class="meta">@Override</span></span><br><span class="line">          <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(LoginEvent loginEvent)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">             <span class="keyword">return</span> loginEvent.eventType.equals(<span class="string">&quot;fail&quot;</span>);</span><br><span class="line">          &#125;</span><br><span class="line">       &#125;).times(<span class="number">3</span>).consecutive();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2. 将 Pattern 应用到流上，检测匹配的复杂事件，得到一个 PatternStream </span></span><br><span class="line">PatternStream&lt;LoginEvent&gt; patternStream = CEP.pattern(stream, pattern);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3. 将匹配到的复杂事件选择出来，然后包装成报警信息输出 </span></span><br><span class="line">patternStream.select(<span class="keyword">new</span> PatternSelectFunction&lt;LoginEvent, String&gt;() &#123;</span><br><span class="line">          <span class="meta">@Override</span></span><br><span class="line">          <span class="function"><span class="keyword">public</span> String <span class="title">select</span><span class="params">(Map&lt;String, List&lt;LoginEvent&gt;&gt; map)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">          <span class="comment">// 只有一个模式，匹配到了 3 个事件，放在 List 中</span></span><br><span class="line">            LoginEvent first = map.get(<span class="string">&quot;fails&quot;</span>).get(<span class="number">0</span>);</span><br><span class="line">            LoginEvent second = map.get(<span class="string">&quot;fails&quot;</span>).get(<span class="number">1</span>);</span><br><span class="line">            LoginEvent third = map.get(<span class="string">&quot;fails&quot;</span>).get(<span class="number">2</span>);</span><br><span class="line">            <span class="keyword">return</span> first.userId + <span class="string">&quot; 连续三次登录失败!登录时间:&quot;</span> + first.timestamp</span><br><span class="line">            + <span class="string">&quot;, &quot;</span> + second.timestamp + <span class="string">&quot;, &quot;</span> + third.timestamp;</span><br><span class="line">          &#125;</span><br><span class="line">       &#125;)</span><br><span class="line">       .print(<span class="string">&quot;warning&quot;</span>);</span><br></pre></td></tr></table></figure>

<p><strong>PatternFlatSelectFunction</strong></p>
<p>除此之外，PatternStream 还有一个类似的方法是.flatSelect()，传入的参数是一个PatternFlatSelectFunction。从名字上就能看出，这是 PatternSelectFunction 的扁平化版本；内部需要实现一个 flatSelect() 方法，它与之前 select() 的不同就在于没有返回值，而是多了一个收集器(Collector)参数 out，通过调用 out.collet() 方法就可以实现多次发送输出数据了。</p>
<p>例如上面的代码可以写成：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 3. 将匹配到的复杂事件选择出来，然后包装成报警信息输出</span></span><br><span class="line">patternStream.flatSelect(<span class="keyword">new</span> PatternFlatSelectFunction&lt;LoginEvent, String&gt;() &#123; </span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatSelect</span><span class="params">(Map&lt;String, List&lt;LoginEvent&gt;&gt; map, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    LoginEvent first = map.get(<span class="string">&quot;fails&quot;</span>).get(<span class="number">0</span>);</span><br><span class="line">    LoginEvent second = map.get(<span class="string">&quot;fails&quot;</span>).get(<span class="number">1</span>);</span><br><span class="line">    LoginEvent third = map.get(<span class="string">&quot;fails&quot;</span>).get(<span class="number">2</span>);</span><br><span class="line">    out.collect(first.userId + <span class="string">&quot; 连续三次登录失败!登录时间:&quot;</span> + first.timestamp +</span><br><span class="line">    <span class="string">&quot;, &quot;</span> + second.timestamp + <span class="string">&quot;, &quot;</span> + third.timestamp);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;).print(<span class="string">&quot;warning&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>可见 PatternFlatSelectFunction 使用更加灵活，完全能够覆盖 PatternSelectFunction 的功能。 这跟 FlatMapFunction 与 MapFunction 的区别是一样的。</p>
<h5 id="匹配事件的通用处理-process"><a href="#匹配事件的通用处理-process" class="headerlink" title="匹配事件的通用处理(process)"></a>匹配事件的通用处理(process)</h5><p>自1.8版本之后，Flink CEP引入了对于匹配事件的通用检测处理方式，那就是直接调用 PatternStream 的 .process() 方法，传入一个 PatternProcessFunction。这看起来就像是我们熟悉的处理函数(process function)，它也可以访问一个上下文(Context)，进行更多的操作。</p>
<p>所以 PatternProcessFunction 功能更加丰富、调用更加灵活，可以完全覆盖其他接口，也就成为了目前官方推荐的处理方式。事实上，PatternSelectFunction 和 PatternFlatSelectFunction 在 CEP 内部执行时也会被转换成 PatternProcessFunction。</p>
<p>我们可以使用 PatternProcessFunction 将之前的代码重写如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 3. 将匹配到的复杂事件选择出来，然后包装成报警信息输出 </span></span><br><span class="line">patternStream.process(<span class="keyword">new</span> PatternProcessFunction&lt;LoginEvent, String&gt;() &#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processMatch</span><span class="params">(Map&lt;String, List&lt;LoginEvent&gt;&gt; map, Context ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    LoginEvent first = map.get(<span class="string">&quot;fails&quot;</span>).get(<span class="number">0</span>);</span><br><span class="line">    LoginEvent second = map.get(<span class="string">&quot;fails&quot;</span>).get(<span class="number">1</span>);</span><br><span class="line">    LoginEvent third = map.get(<span class="string">&quot;fails&quot;</span>).get(<span class="number">2</span>);</span><br><span class="line">    out.collect(first.userId + <span class="string">&quot; 连续三次登录失败!登录时间:&quot;</span> + first.timestamp +</span><br><span class="line">    <span class="string">&quot;, &quot;</span> + second.timestamp + <span class="string">&quot;, &quot;</span> + third.timestamp);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;).print(<span class="string">&quot;warning&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>可以看到，PatternProcessFunction 中必须实现一个 processMatch()方法；这个方法与之前的 flatSelect() 类似，只是多了一个上下文 Context 参数。利用这个上下文可以获取当前的时间信息，比如事件的时间戳(timestamp)或者处理时间(processing time)；还可以调用 .output() 方法将数据输出到侧输出流。侧输出流的功能是处理函数的一大特性，我们已经非常熟悉；而在 CEP 中，侧输出流一般被用来处理超时事件。</p>
<h4 id="处理超时事件"><a href="#处理超时事件" class="headerlink" title="处理超时事件"></a>处理超时事件</h4><p>复杂事件的检测结果一般只有两种：要么匹配，要么不匹配。检测处理的过程具体如下：</p>
<p>(1)如果当前事件符合模式匹配的条件，就接受该事件，保存到对应的 Map 中；</p>
<p>(2)如果在模式序列定义中，当前事件后面还应该有其他事件，就继续读取事件流进行检测；如果模式序列的定义已经全部满足，那么就成功检测到了一组匹配的复杂事件，调用 PatternProcessFunction 的 processMatch() 方法进行处理；</p>
<p>(3)如果当前事件不符合模式匹配的条件，就丢弃该事件；</p>
<p>(4)如果当前事件破坏了模式序列中定义的限制条件，比如不满足严格近邻要求，那么当前已检测的一组部分匹配事件都被丢弃，重新开始检测。</p>
<p>不过在有时间限制的情况下，需要考虑的问题会有一点特别。比如我们用 .within() 指定了模式检测的时间间隔，超出这个时间当前这组检测就应该失败了。然而这种超时失败跟真正的匹配失败不同，它其实是一种部分成功匹配；因为只有在开头能够正常匹配的前提下，没有等到后续的匹配事件才会超时。所以往往不应该直接丢弃，而是要输出一个提示或报警信息。这就要求我们有能力捕获并处理超时事件。</p>
<h5 id="使用-PatternProcessFunction"><a href="#使用-PatternProcessFunction" class="headerlink" title="使用 PatternProcessFunction"></a>使用 PatternProcessFunction</h5><p>在 Flink CEP 中，提供了一个专门捕捉超时的部分匹配事件的接口，叫作 TimedOutPartialMatchHandler。这个接口需要实现一个 processTimedOutMatch()方法，可以将超时的、已检测到的部分匹配事件放在一个 Map 中，作为方法的第一个参数；方法的第二个 参数则是 PatternProcessFunction 的上下文 Context。所以这个接口必须与 PatternProcessFunction 结合使用，对处理结果的输出则需要利用侧输出流来进行。</p>
<p>代码中的调用方式如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyPatternProcessFunction</span> <span class="keyword">extends</span> <span class="title">PatternProcessFunction</span>&lt;<span class="title">Event</span>, <span class="title">String</span>&gt; <span class="keyword">implements</span> <span class="title">TimedOutPartialMatchHandler</span>&lt;<span class="title">Event</span>&gt; </span>&#123;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 正常匹配事件的处理</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processMatch</span><span class="params">(Map&lt;String, List&lt;Event&gt;&gt; match, Context ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">         ...</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 超时部分匹配事件的处理</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processTimedOutMatch</span><span class="params">(Map&lt;String, List&lt;Event&gt;&gt; match, Context ctx)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">    Event startEvent = match.get(<span class="string">&quot;start&quot;</span>).get(<span class="number">0</span>);</span><br><span class="line">    OutputTag&lt;Event&gt; outputTag = <span class="keyword">new</span> OutputTag&lt;Event&gt;(<span class="string">&quot;time-out&quot;</span>)&#123;&#125;;</span><br><span class="line">    ctx.output(outputTag, startEvent);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>我们在 processTimedOutMatch() 方法中定义了一个输出标签(OutputTag)。调用 ctx.output() 方法，就可以将超时的部分匹配事件输出到标签所标识的侧输出流了。</p>
<h5 id="使用-PatternTimeoutFunction"><a href="#使用-PatternTimeoutFunction" class="headerlink" title="使用 PatternTimeoutFunction"></a>使用 PatternTimeoutFunction</h5><p>上文提到的 PatternProcessFunction 通过实现 TimedOutPartialMatchHandler 接口扩展出了处理超时事件的能力，这是官方推荐的做法。此外，Flink CEP 中也保留了简化版的 PatternSelectFunction，它无法直接处理超时事件，不过我们可以通过调用 PatternStream 的 .select() 方法时多传入一个 PatternTimeoutFunction 参数来实现这一点。</p>
<p>PatternTimeoutFunction 是早期版本中用于捕获超时事件的接口。它需要实现一个 timeout() 方法，同样会将部分匹配的事件放在一个 Map 中作为参数传入，此外还有一个参数是当前的时间戳。提取部分匹配事件进行处理转换后，可以将通知或报警信息输出。</p>
<p>由于调用 .select() 方法后会得到唯一的 DataStream，所以正常匹配事件和超时事件的处理结果不应该放在同一条流中。正常匹配事件的处理结果会进入转换后得到的 DataStream，而超时事件的处理结果则会进入侧输出流;这个侧输出流需要另外传入一个侧输出标签(OutputTag) 来指定。</p>
<p>所以最终我们在调用 PatternStream 的 .select() 方法时需要传入三个参数：侧输出流标签 (OutputTag)，超时事件处理函数 PatternTimeoutFunction，匹配事件提取函数 PatternSelectFunction。下面是一个代码中的调用方式：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 定义一个侧输出流标签，用于标识超时侧输出流</span></span><br><span class="line">OutputTag&lt;String&gt; timeoutTag = <span class="keyword">new</span> OutputTag&lt;String&gt;(<span class="string">&quot;timeout&quot;</span>)&#123;&#125;;</span><br><span class="line"><span class="comment">// 将匹配到的，和超时部分匹配的复杂事件提取出来，然后包装成提示信息输出</span></span><br><span class="line">SingleOutputStreamOperator&lt;String&gt; resultStream = patternStream.select(timeoutTag,</span><br><span class="line">      <span class="comment">// 超时部分匹配事件的处理</span></span><br><span class="line">      <span class="keyword">new</span> PatternTimeoutFunction&lt;Event, String&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">timeout</span><span class="params">(Map&lt;String, List&lt;Event&gt;&gt; pattern, <span class="keyword">long</span> timeoutTimestamp)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">          Event event = pattern.get(<span class="string">&quot;start&quot;</span>).get(<span class="number">0</span>);</span><br><span class="line">          <span class="keyword">return</span> <span class="string">&quot;超时:&quot;</span> + event.toString(); </span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="comment">// 正常匹配事件的处理</span></span><br><span class="line">      <span class="keyword">new</span> PatternSelectFunction&lt;Event, String&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">select</span><span class="params">(Map&lt;String, List&lt;Event&gt;&gt; pattern)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">          ... </span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将正常匹配和超时部分匹配的处理结果流打印输出 </span></span><br><span class="line">resultStream.print(<span class="string">&quot;matched&quot;</span>);</span><br><span class="line">resultStream.getSideOutput(timeoutTag).print(<span class="string">&quot;timeout&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>这里需要注意的是，在超时事件处理的过程中，从 Map 里只能取到已经检测到匹配的那些事件；如果取可能未匹配的事件并调用它的对象方法，则可能会报空指针异常 (NullPointerException)。另外，超时事件处理的结果进入侧输出流，正常匹配事件的处理结 果进入主流，两者的数据类型可以不同。</p>
<h5 id="应用实例"><a href="#应用实例" class="headerlink" title="应用实例"></a>应用实例</h5><p>在电商平台中，最终创造收入和利润的是用户下单购买的环节。用户下单的行为可以表明用户对商品的需求，但在现实中，并不是每次下单都会被用户立刻支付。当拖延一段时间后，用户支付的意愿会降低。所以为了让用户更有紧迫感从而提高支付转化率，同时也为了防范订单支付环节的安全风险，电商网站往往会对订单状态进行监控，设置一个失效时间(比如 15 分钟)，如果下单后一段时间仍未支付，订单就会被取消。</p>
<p>首先定义出要处理的数据类型。我们面对的是订单事件，主要包括用户对订单的创建(下单)和支付两种行为。因此可以定义 POJO 类 OrderEvent 如下，其中属性字段包括用户 ID、 订单 ID、事件类型(操作类型)以及时间戳。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderEvent</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> String userId;</span><br><span class="line">    <span class="keyword">public</span> String orderId;</span><br><span class="line">    <span class="keyword">public</span> String eventType;</span><br><span class="line">    <span class="keyword">public</span> Long timestamp;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">OrderEvent</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">OrderEvent</span><span class="params">(String userId, String orderId, String eventType, Long timestamp)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.userId = userId;</span><br><span class="line">        <span class="keyword">this</span>.orderId = orderId;</span><br><span class="line">        <span class="keyword">this</span>.eventType = eventType;</span><br><span class="line">        <span class="keyword">this</span>.timestamp = timestamp;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;OrderEvent&#123;&quot;</span> +</span><br><span class="line">                <span class="string">&quot;userId=&#x27;&quot;</span> + userId + <span class="string">&#x27;\&#x27;&#x27;</span> +</span><br><span class="line">                <span class="string">&quot;orderId=&#x27;&quot;</span> + orderId + <span class="string">&#x27;\&#x27;&#x27;</span> +</span><br><span class="line">                <span class="string">&quot;, eventType=&#x27;&quot;</span> + eventType + <span class="string">&#x27;\&#x27;&#x27;</span> +</span><br><span class="line">                <span class="string">&quot;, timestamp=&quot;</span> + timestamp +</span><br><span class="line">                <span class="string">&#x27;&#125;&#x27;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>当前需求的重点在于对超时未支付的用户进行监控提醒，也就是需要检测有下单行为、但 15 分钟内没有支付行为的复杂事件。在下单和支付之间，可以有其他操作(比如对订单的修改)，所以两者之间是宽松近邻关系。</p>
<p>很明显，我们重点要处理的是超时的部分匹配事件。对原始的订单事件流按照订单 ID 进行分组，然后检测每个订单的下单-支付复杂事件，如果出现超时事件需要输出报警提示信息。</p>
<p>整体代码实现如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderTimeoutDetectExample</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取订单事件流，并提取时间戳、生成水位线</span></span><br><span class="line">        KeyedStream&lt;OrderEvent, String&gt; stream = env</span><br><span class="line">                .fromElements(</span><br><span class="line">                        <span class="keyword">new</span> OrderEvent(<span class="string">&quot;user_1&quot;</span>, <span class="string">&quot;order_1&quot;</span>, <span class="string">&quot;create&quot;</span>, <span class="number">1000L</span>),</span><br><span class="line">                        <span class="keyword">new</span> OrderEvent(<span class="string">&quot;user_2&quot;</span>, <span class="string">&quot;order_2&quot;</span>, <span class="string">&quot;create&quot;</span>, <span class="number">2000L</span>),</span><br><span class="line">                        <span class="keyword">new</span> OrderEvent(<span class="string">&quot;user_1&quot;</span>, <span class="string">&quot;order_1&quot;</span>, <span class="string">&quot;modify&quot;</span>, <span class="number">10</span> * <span class="number">1000L</span>),</span><br><span class="line">                        <span class="keyword">new</span> OrderEvent(<span class="string">&quot;user_1&quot;</span>, <span class="string">&quot;order_1&quot;</span>, <span class="string">&quot;pay&quot;</span>, <span class="number">60</span> * <span class="number">1000L</span>),</span><br><span class="line">                        <span class="keyword">new</span> OrderEvent(<span class="string">&quot;user_2&quot;</span>, <span class="string">&quot;order_3&quot;</span>, <span class="string">&quot;create&quot;</span>, <span class="number">10</span> * <span class="number">60</span> * <span class="number">1000L</span>),</span><br><span class="line">                        <span class="keyword">new</span> OrderEvent(<span class="string">&quot;user_2&quot;</span>, <span class="string">&quot;order_3&quot;</span>, <span class="string">&quot;pay&quot;</span>, <span class="number">20</span> * <span class="number">60</span> * <span class="number">1000L</span>)</span><br><span class="line">                )</span><br><span class="line">                .assignTimestampsAndWatermarks(</span><br><span class="line">                        WatermarkStrategy.&lt;OrderEvent&gt;forMonotonousTimestamps()</span><br><span class="line">                                .withTimestampAssigner(</span><br><span class="line">                                        <span class="keyword">new</span> SerializableTimestampAssigner&lt;OrderEvent&gt;() &#123;</span><br><span class="line">                                            <span class="meta">@Override</span></span><br><span class="line">                                            <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(OrderEvent event, <span class="keyword">long</span> l)</span> </span>&#123;</span><br><span class="line">                                                <span class="keyword">return</span> event.timestamp;</span><br><span class="line">                                            &#125;</span><br><span class="line">                                        &#125;</span><br><span class="line">                                )</span><br><span class="line">                )</span><br><span class="line">                .keyBy(order -&gt; order.orderId);    <span class="comment">// 按照订单ID分组</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 定义Pattern</span></span><br><span class="line">        Pattern&lt;OrderEvent, ?&gt; pattern = Pattern</span><br><span class="line">                .&lt;OrderEvent&gt;begin(<span class="string">&quot;create&quot;</span>)    <span class="comment">// 首先是下单事件</span></span><br><span class="line">                .where(<span class="keyword">new</span> SimpleCondition&lt;OrderEvent&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(OrderEvent value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> value.eventType.equals(<span class="string">&quot;create&quot;</span>);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .followedBy(<span class="string">&quot;pay&quot;</span>)    <span class="comment">// 之后是支付事件；中间可以修改订单，宽松近邻</span></span><br><span class="line">                .where(<span class="keyword">new</span> SimpleCondition&lt;OrderEvent&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(OrderEvent value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="keyword">return</span> value.eventType.equals(<span class="string">&quot;pay&quot;</span>);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .within(Time.minutes(<span class="number">15</span>));    <span class="comment">// 限制在15分钟之内</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 将Pattern应用到流上，检测匹配的复杂事件，得到一个PatternStream</span></span><br><span class="line">        PatternStream&lt;OrderEvent&gt; patternStream = CEP.pattern(stream, pattern);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 将匹配到的，和超时部分匹配的复杂事件提取出来，然后包装成提示信息输出</span></span><br><span class="line">        SingleOutputStreamOperator&lt;String&gt; payedOrderStream = patternStream.process(<span class="keyword">new</span> OrderPayPatternProcessFunction());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4. 定义一个测输出流标签，用于标识超时测输出流</span></span><br><span class="line">        OutputTag&lt;String&gt; timeoutTag = <span class="keyword">new</span> OutputTag&lt;String&gt;(<span class="string">&quot;timeout&quot;</span>) &#123;&#125;;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5. 将正常匹配和超时部分匹配的处理结果流打印输出</span></span><br><span class="line">        payedOrderStream.print(<span class="string">&quot;payed&quot;</span>);</span><br><span class="line">        payedOrderStream.getSideOutput(timeoutTag).print(<span class="string">&quot;timeout&quot;</span>);</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 实现自定义的PatternProcessFunction，需实现TimedOutPartialMatchHandler接口</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderPayPatternProcessFunction</span> <span class="keyword">extends</span> <span class="title">PatternProcessFunction</span>&lt;<span class="title">OrderEvent</span>, <span class="title">String</span>&gt; <span class="keyword">implements</span> <span class="title">TimedOutPartialMatchHandler</span>&lt;<span class="title">OrderEvent</span>&gt; </span>&#123;</span><br><span class="line">        <span class="comment">// 处理正常匹配事件</span></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processMatch</span><span class="params">(Map&lt;String, List&lt;OrderEvent&gt;&gt; match, Context ctx, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            OrderEvent payEvent = match.get(<span class="string">&quot;pay&quot;</span>).get(<span class="number">0</span>);</span><br><span class="line">            out.collect(<span class="string">&quot;订单 &quot;</span> + payEvent.orderId + <span class="string">&quot; 已支付！&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 处理超时未支付事件</span></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processTimedOutMatch</span><span class="params">(Map&lt;String, List&lt;OrderEvent&gt;&gt; match, Context ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            OrderEvent createEvent = match.get(<span class="string">&quot;create&quot;</span>).get(<span class="number">0</span>);</span><br><span class="line">            ctx.output(<span class="keyword">new</span> OutputTag&lt;String&gt;(<span class="string">&quot;timeout&quot;</span>)&#123;&#125;, <span class="string">&quot;订单 &quot;</span> + createEvent.orderId + <span class="string">&quot; 超时未支付！用户为：&quot;</span> + createEvent.userId);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>用处理函数进行状态编程，结合定时器也可以实现同样的功能，但明显 CEP 的实现更加方便， 也更容易迁移和扩展。</p>
<h4 id="处理迟到数据"><a href="#处理迟到数据" class="headerlink" title="处理迟到数据"></a>处理迟到数据</h4><p>CEP 主要处理的是先后发生的一组复杂事件，所以事件的顺序非常关键。前面已经说过，事件先后顺序的具体定义与时间语义有关。如果是处理时间语义，那比较简单，只要按照数据处理的系统时间算就可以了；而如果是事件时间语义，需要按照事件自身的时间戳来排序。这就有可能出现时间戳大的事件先到、时间戳小的事件后到的现象，也就是所谓的乱序数据或迟到数据。</p>
<p>在 Flink CEP 中沿用了通过设置水位线(watermark)延迟来处理乱序数据的做法。当一个事件到来时，并不会立即做检测匹配处理，而是先放入一个缓冲区(buffer)。缓冲区内的数据，会按照时间戳由小到大排序；当一个水位线到来时，就会将缓冲区中所有时间戳小于水位线的事件依次取出，进行检测匹配。这样就保证了匹配事件的顺序和事件时间的进展一致，处理的顺序就一定是正确的。这里水位线的延迟时间，也就是事件在缓冲区等待的最大时间。</p>
<p>这样又会带来另一个问题：水位线延迟时间不可能保证将所有乱序数据完美包括进来，总会有一些事件延迟比较大，以至于等它到来的时候水位线早已超过了它的时间戳。这时之前的数据都已处理完毕，这样的迟到数据就只能被直接丢弃了——这与窗口对迟到数据的默认处理一致。</p>
<p>我们自然想到，如果不希望迟到数据丢掉，应该也可以借鉴窗口的做法。Flink CEP 同样提供了将迟到事件输出到侧输出流的方式：我们可以基于 PatternStream 直接调用 .sideOutputLateData() 方法，传入一个 OutputTag，将迟到数据放入侧输出流另行处理。代码中调用方式如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">PatternStream&lt;Event&gt; patternStream = CEP.pattern(input, pattern);</span><br><span class="line"><span class="comment">// 定义一个侧输出流的标签</span></span><br><span class="line">OutputTag&lt;String&gt; lateDataOutputTag = <span class="keyword">new</span> OutputTag&lt;String&gt;(<span class="string">&quot;late-data&quot;</span>)&#123;&#125;;</span><br><span class="line">SingleOutputStreamOperator&lt;ComplexEvent&gt; result = patternStream .sideOutputLateData(lateDataOutputTag) <span class="comment">// 将迟到数据输出到侧输出流 </span></span><br><span class="line">  .select(</span><br><span class="line">      <span class="comment">// 处理正常匹配数据</span></span><br><span class="line">      <span class="keyword">new</span> PatternSelectFunction&lt;Event, ComplexEvent&gt;() &#123;...&#125;</span><br><span class="line">    );</span><br><span class="line"><span class="comment">// 从结果中提取侧输出流</span></span><br><span class="line">DataStream&lt;String&gt; lateData = result.getSideOutput(lateDataOutputTag);</span><br></pre></td></tr></table></figure>

<p>可以看到，整个处理流程与窗口非常相似。经处理匹配数据得到结果数据流之后，可以调用 .getSideOutput() 方法来提取侧输出流，捕获迟到数据进行额外处理。</p>
<h3 id="CEP-的状态机实现"><a href="#CEP-的状态机实现" class="headerlink" title="CEP 的状态机实现"></a>CEP 的状态机实现</h3><p>Flink CEP 中对复杂事件的检测，关键在模式的定义。我们会发现 CEP 中模式的定义方式比较复杂，而且与正则表达式非常相似：正则表达式在字符串上匹配符合模板的字符序列，而 Flink CEP 则是在事件流上匹配符合模式定义的复杂事件。</p>
<p>前面我们分析过 CEP 检测处理的流程，可以认为检测匹配事件的过程中会有初始(没有任何匹配)、检测中(部分匹配成功)、匹配成功、匹配失败等不同的状态。随着每个事件的到来，都会改变当前检测的状态；而这种改变跟当前事件的特性有关、也跟当前所处的状态有关。这样的系统，其实就是一个状态机(state machine)。这也正是正则表达式底层引擎的实现原理。</p>
<p>所以 Flink CEP 的底层工作原理其实与正则表达式是一致的，是一个非确定有限状态自动机(Nondeterministic Finite Automaton，NFA)。NFA 的原理涉及到较多数学知识，我们这里不做详细展开，而是用一个具体的例子来说明一下状态机的工作方式，以更好地理解 CEP 的原理。</p>
<p>我们回顾一下应用案例，检测用户连续三次登录失败的复杂事件。用 Flink CEP 中的 Pattern API 可以很方便地把它定义出来；如果我们现在不用 CEP，而是用 DataStream API 和处理函数来实现，应该怎么做呢？</p>
<p>这需要设置状态，并根据输入的事件不断更新状态。当然因为这个需求不是很复杂，我们也可以用嵌套的 if-else 条件判断将它实现，不过这样做的代码可读性和扩展性都会很差。更好的方式，就是实现一个状态机。</p>
<p><img src="/Flink/%E6%88%AA%E5%B1%8F2022-07-26%20%E4%B8%8B%E5%8D%885.05.03.png" alt="截屏2022-07-26 下午5.05.03"></p>
<p>如图 12-4 所示，即为状态转移的过程，从初始状态(INITIAL)出发，遇到一个类型为 fail 的登录失败事件，就开始进入部分匹配的状态；目前只有一个 fail 事件，我们把当前状态记作 S1。基于 S1 状态，如果继续遇到 fail 事件，那么就有两个 fail 事件，记作 S2。基于 S2 状态如果再次遇到 fail 事件，那么就找到了一组匹配的复杂事件，把当前状态记作 Matched，就可以输出报警信息了。需要注意的是，报警完毕，需要立即重置状态回 S2；因为如果接下来再遇到 fail 事件，就又满足了新的连续三次登录失败，需要再次报警。</p>
<p>而不论是初始状态，还是 S1、S2 状态，只要遇到类型为 success 的登录成功事件，就会跳转到结束状态，记作 Terminal。此时当前检测完毕，之前的部分匹配应该全部清空，所以需要立即重置状态到 Initial，重新开始下一轮检测。所以这里我们真正参与状态转移的，其实只有 Initial、S1、S2 三个状态，Matched 和 Terminal 是为了方便我们做其他操作(比如输出报警、清空状态)的临时标记状态，不等新事件到来马上就会跳转。</p>
<p>完整代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NFAExample</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取登录事件流，这里与时间无关，就不生成水位线了</span></span><br><span class="line">        KeyedStream&lt;LoginEvent, String&gt; stream = env.fromElements(</span><br><span class="line">                        <span class="keyword">new</span> LoginEvent(<span class="string">&quot;user_1&quot;</span>, <span class="string">&quot;192.168.0.1&quot;</span>, <span class="string">&quot;fail&quot;</span>, <span class="number">2000L</span>),</span><br><span class="line">                        <span class="keyword">new</span> LoginEvent(<span class="string">&quot;user_1&quot;</span>, <span class="string">&quot;192.168.0.2&quot;</span>, <span class="string">&quot;fail&quot;</span>, <span class="number">3000L</span>),</span><br><span class="line">                        <span class="keyword">new</span> LoginEvent(<span class="string">&quot;user_2&quot;</span>, <span class="string">&quot;192.168.1.29&quot;</span>, <span class="string">&quot;fail&quot;</span>, <span class="number">4000L</span>),</span><br><span class="line">                        <span class="keyword">new</span> LoginEvent(<span class="string">&quot;user_1&quot;</span>, <span class="string">&quot;171.56.23.10&quot;</span>, <span class="string">&quot;fail&quot;</span>, <span class="number">5000L</span>),</span><br><span class="line">                        <span class="keyword">new</span> LoginEvent(<span class="string">&quot;user_2&quot;</span>, <span class="string">&quot;192.168.1.29&quot;</span>, <span class="string">&quot;success&quot;</span>, <span class="number">6000L</span>),</span><br><span class="line">                        <span class="keyword">new</span> LoginEvent(<span class="string">&quot;user_2&quot;</span>, <span class="string">&quot;192.168.1.29&quot;</span>, <span class="string">&quot;fail&quot;</span>, <span class="number">7000L</span>),</span><br><span class="line">                        <span class="keyword">new</span> LoginEvent(<span class="string">&quot;user_2&quot;</span>, <span class="string">&quot;192.168.1.29&quot;</span>, <span class="string">&quot;fail&quot;</span>, <span class="number">8000L</span>)</span><br><span class="line">                )</span><br><span class="line">                .keyBy(r -&gt; r.userId);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将数据依次输入状态机进行处理</span></span><br><span class="line">        DataStream&lt;String&gt; alertStream = stream</span><br><span class="line">                .flatMap(<span class="keyword">new</span> StateMachineMapper());</span><br><span class="line"></span><br><span class="line">        alertStream.print(<span class="string">&quot;warning&quot;</span>);</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@SuppressWarnings(&quot;serial&quot;)</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">StateMachineMapper</span> <span class="keyword">extends</span> <span class="title">RichFlatMapFunction</span>&lt;<span class="title">LoginEvent</span>, <span class="title">String</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 声明当前用户对应的状态</span></span><br><span class="line">        <span class="keyword">private</span> ValueState&lt;State&gt; currentState;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration conf)</span> </span>&#123;</span><br><span class="line">            <span class="comment">// 获取状态对象</span></span><br><span class="line">            currentState = getRuntimeContext().getState(<span class="keyword">new</span> ValueStateDescriptor&lt;&gt;(<span class="string">&quot;state&quot;</span>, State.class));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(LoginEvent event, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="comment">// 获取状态，如果状态为空，置为初始状态</span></span><br><span class="line">            State state = currentState.value();</span><br><span class="line">            <span class="keyword">if</span> (state == <span class="keyword">null</span>) &#123;</span><br><span class="line">                state = State.Initial;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 基于当前状态，输入当前事件时跳转到下一状态</span></span><br><span class="line">            State nextState = state.transition(event.eventType);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (nextState == State.Matched) &#123;</span><br><span class="line">                <span class="comment">// 如果检测到匹配的复杂事件，输出报警信息</span></span><br><span class="line">                out.collect(event.userId + <span class="string">&quot; 连续三次登录失败&quot;</span>);</span><br><span class="line">                <span class="comment">// 需要跳转回S2状态，这里直接不更新状态就可以了</span></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (nextState == State.Terminal) &#123;</span><br><span class="line">                <span class="comment">// 如果到了终止状态，就重置状态，准备重新开始</span></span><br><span class="line">                currentState.update(State.Initial);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// 如果还没结束，更新状态（状态跳转），继续读取事件</span></span><br><span class="line">                currentState.update(nextState);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 状态机实现</span></span><br><span class="line">    <span class="keyword">public</span> <span class="class"><span class="keyword">enum</span> <span class="title">State</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        Terminal,    <span class="comment">// 匹配失败，当前匹配终止</span></span><br><span class="line"></span><br><span class="line">        Matched,    <span class="comment">// 匹配成功</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// S2状态</span></span><br><span class="line">        S2(<span class="keyword">new</span> Transition(<span class="string">&quot;fail&quot;</span>, Matched), <span class="keyword">new</span> Transition(<span class="string">&quot;success&quot;</span>, Terminal)),</span><br><span class="line"></span><br><span class="line">        <span class="comment">// S1状态</span></span><br><span class="line">        S1(<span class="keyword">new</span> Transition(<span class="string">&quot;fail&quot;</span>, S2), <span class="keyword">new</span> Transition(<span class="string">&quot;success&quot;</span>, Terminal)),</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 初始状态</span></span><br><span class="line">        Initial(<span class="keyword">new</span> Transition(<span class="string">&quot;fail&quot;</span>, S1), <span class="keyword">new</span> Transition(<span class="string">&quot;success&quot;</span>, Terminal));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">final</span> Transition[] transitions;    <span class="comment">// 状态转移规则</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 状态的构造方法，可以传入一组状态转移规则来定义状态</span></span><br><span class="line">        State(Transition... transitions) &#123;</span><br><span class="line">            <span class="keyword">this</span>.transitions = transitions;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 状态的转移方法，根据当前输入事件类型，从定义好的转移规则中找到下一个状态</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> State <span class="title">transition</span><span class="params">(String eventType)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">for</span> (Transition t : transitions) &#123;</span><br><span class="line">                <span class="keyword">if</span> (t.getEventType().equals(eventType)) &#123;</span><br><span class="line">                    <span class="keyword">return</span> t.getTargetState();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 如果没有找到转移规则，说明已经结束，回到初始状态</span></span><br><span class="line">            <span class="keyword">return</span> Initial;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 定义状态转移类，包括两个属性：当前事件类型和目标状态</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Transition</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 触发状态转移的当前事件类型</span></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">final</span> String eventType;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 转移的目标状态</span></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">final</span> State targetState;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="title">Transition</span><span class="params">(String eventType, State targetState)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.eventType = checkNotNull(eventType);</span><br><span class="line">            <span class="keyword">this</span>.targetState = checkNotNull(targetState);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">getEventType</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> eventType;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> State <span class="title">getTargetState</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> targetState;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>运行代码，可以看到输出与之前 CEP 的实现是完全一样的。显然，如果所有的复杂事件处理都自己设计状态机来实现是非常繁琐的，而且中间逻辑非常容易出错；所以 Flink CEP 将底层 NFA 全部实现好并封装起来，这样我们处理复杂事件时只要调上层的 Pattern API 就可以， 无疑大大降低了代码的复杂度，提高了编程的效率。</p>
]]></content>
      <categories>
        <category>实时计算</category>
      </categories>
  </entry>
  <entry>
    <title>Hive</title>
    <url>/Hive/</url>
    <content><![CDATA[<h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>Hive：由Facebook开源用于解决海量结构化日志的数据统计工具。</p>
<p>Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类SQL查询功能。</p>
<p>本质是：将HiveSQL转化成MapReduce程序。</p>
<p>（1）Hive处理的数据存储在HDFS</p>
<p>（2）Hive分析数据底层的实现是MapReduce</p>
<p>（3）执行程序运行在Yarn上</p>
<h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><p>1）Hive的执行延迟比较高，因此Hive常用于数据分析，对实时性要求不高的场合</p>
<p>2）Hive优势在于处理大数据，对于处理小数据没有优势，因为Hive的执行延迟比较高</p>
<p>3）Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数</p>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><p>1）Hive的HQL表达能力有限</p>
<p>（1）迭代式算法无法表达</p>
<p>（2）数据挖掘方面不擅长，由于MapReduce数据处理流程的限制，效率更高的算法却无法实现。</p>
<p>2）Hive的效率比较低</p>
<p>（1）Hive自动生成的MapReduce作业，通常情况下不够智能化</p>
<p>（2）Hive调优比较困难，粒度较粗</p>
<h3 id="Hive和数据库比较"><a href="#Hive和数据库比较" class="headerlink" title="Hive和数据库比较"></a>Hive和数据库比较</h3><p>1.由于Hive是针对数据仓库应用设计的，而数据仓库的内容是读多写少的。因此，Hive中不建议对数据的改写，所有的数据都是在加载的时候确定好的。</p>
<p>2.Hive 在查询数据的时候，由于没有索引，需要扫描整个表，因此延迟较高。另外一个导致 Hive 执行延迟高的因素是 MapReduce框架。由于MapReduce 本身具有较高的延迟，因此在利用MapReduce 执行Hive查询时，也会有较高的延迟。</p>
<h2 id="Hive安装地址"><a href="#Hive安装地址" class="headerlink" title="Hive安装地址"></a>Hive安装地址</h2><p>Hive官网地址</p>
<p><a href="http://hive.apache.org/">http://hive.apache.org/</a></p>
<p>文档查看地址</p>
<p><a href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted">https://cwiki.apache.org/confluence/display/Hive/GettingStarted</a></p>
<p>下载地址</p>
<p><a href="http://archive.apache.org/dist/hive/">http://archive.apache.org/dist/hive/</a></p>
<p>github地址</p>
<p><a href="https://github.com/apache/hive">https://github.com/apache/hive</a></p>
<h2 id="MySql安装"><a href="#MySql安装" class="headerlink" title="MySql安装"></a>MySql安装</h2><p>1）检查当前系统是否安装过mysql</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">rpm -qa|grep mariadb</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mariadb-libs-5.5.56-2.el7.x86_64 //如果存在通过如下命令卸载</span><br><span class="line">sudo rpm -e --nodeps  mariadb-libs  //用此命令卸载mariadb</span><br></pre></td></tr></table></figure>

<p>2）将MySQL安装包拷贝到/opt/software目录下</p>
<p>3）解压MySQL安装包</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mkdir mysql-libs</span><br><span class="line">tar -xf mysql-5.7.28-1.el7.x86_64.rpm-bundle.tar -C ./mysql-libs/</span><br></pre></td></tr></table></figure>

<p>4）在安装目录下执行rpm安装</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd mysql-libs</span><br><span class="line">sudo rpm -ivh mysql-community-common-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">sudo rpm -ivh mysql-community-libs-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">sudo rpm -ivh mysql-community-libs-compat-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">sudo rpm -ivh mysql-community-client-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">sudo rpm -ivh mysql-community-server-5.7.28-1.el7.x86_64.rpm</span><br></pre></td></tr></table></figure>

<p>如果Linux是最小化安装的，在安装mysql-community-server-5.7.28-1.el7.x86_64.rpm时可能会出现如下错误:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">警告：mysql-community-server-5.7.28-1.el7.x86_64.rpm: 头V3 DSA/SHA1 Signature, 密钥 ID 5072e1f5: NOKEY</span><br><span class="line">错误：依赖检测失败：</span><br><span class="line">    libaio.so.1()(64bit) 被 mysql-community-server-5.7.28-1.el7.x86_64 需要</span><br><span class="line"></span><br><span class="line">    libaio.so.1(LIBAIO_0.1)(64bit) 被 mysql-community-server-5.7.28-1.el7.x86_64 需要</span><br><span class="line"></span><br><span class="line">    libaio.so.1(LIBAIO_0.4)(64bit) 被 mysql-community-server-5.7.28-1.el7.x86_64 需要</span><br></pre></td></tr></table></figure>

<p>通过yum安装缺少的依赖,然后重新安装mysql-community-server-5.7.28-1.el7.x86_64 即可</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo yum install -y libaio</span><br></pre></td></tr></table></figure>

<p>5）删除/etc/my.cnf文件中datadir指向的目录下的所有内容，如果有内容的情况下</p>
<p>查看datadir的值：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">datadir=/var/lib/mysql</span><br></pre></td></tr></table></figure>

<p>删除/var/lib/mysql目录下的所有内容:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd /var/lib/mysql</span><br><span class="line">sudo rm -rf ./*   //注意执行命令的位置</span><br></pre></td></tr></table></figure>

<p>6）初始化数据库</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo mysqld --initialize --user=mysql</span><br></pre></td></tr></table></figure>

<p>7）查看临时生成的root用户密码</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo cat /var/log/mysqld.log</span><br></pre></td></tr></table></figure>

<p>8）启动mysql服务</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo systemctl start mysqld</span><br></pre></td></tr></table></figure>

<p>9）登录mysql数据库</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mysql -uroot -p</span><br><span class="line">Enter password:  输入临时生成的密码</span><br></pre></td></tr></table></figure>

<p>10）必须先修改root用户的密码，否则执行其他操作会报错</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; set password = password(&quot;新密码&quot;)</span><br></pre></td></tr></table></figure>

<p>11）修改mysql库下user表中的root用户允许任意ip连接</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; update mysql.user set host=&#x27;%&#x27; where user=&#x27;root&#x27;;</span><br><span class="line">mysql&gt; flush privileges;</span><br></pre></td></tr></table></figure>

<h2 id="Hive安装部署"><a href="#Hive安装部署" class="headerlink" title="Hive安装部署"></a>Hive安装部署</h2><p>1）把 apache-hive-3.1.2-bin.tar.gz 上传到linux的 /opt/software 目录下</p>
<p>2）解压 apache-hive-3.1.2-bin.tar.gz 到/opt/module 目录下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -zxvf /opt/software/apache-hive-3.1.2-bin.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>

<p>3）修改 apache-hive-3.1.2-bin 的名称为 hive</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mv /opt/module/apache-hive-3.1.2-bin/ /opt/module/hive</span><br></pre></td></tr></table></figure>

<p>4）修改etc/profile.d/my_env.sh，添加环境变量</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo vim /etc/profile.d/my_env.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">HIVE_HOME</span></span><br><span class="line">HIVE_HOME=/opt/module/hive</span><br><span class="line">PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HIVE_HOME/bin</span><br><span class="line">export PATH JAVA_HOME HADOOP_HOME HIVE_HOME</span><br></pre></td></tr></table></figure>

<p>5）解决日志jar包冲突</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mv $HIVE_HOME/lib/log4j-slf4j-impl-2.10.0.jar $HIVE_HOME/lib/log4j-slf4j-impl-2.10.0.bak</span><br></pre></td></tr></table></figure>

<h2 id="Hive元数据配置到MySql"><a href="#Hive元数据配置到MySql" class="headerlink" title="Hive元数据配置到MySql"></a>Hive元数据配置到MySql</h2><h3 id="拷贝驱动"><a href="#拷贝驱动" class="headerlink" title="拷贝驱动"></a>拷贝驱动</h3><p>将MySQL的JDBC驱动拷贝到Hive的lib目录下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cp /opt/software/mysql-connector-java-5.1.37.jar $HIVE_HOME/lib</span><br></pre></td></tr></table></figure>

<h3 id="配置Metastore到MySql"><a href="#配置Metastore到MySql" class="headerlink" title="配置Metastore到MySql"></a>配置Metastore到MySql</h3><p>在$HIVE_HOME/conf目录下新建hive-site.xml文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim $HIVE_HOME/conf/hive-site.xml</span><br></pre></td></tr></table></figure>

<p>添加如下内容</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot;?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- jdbc连接的URL --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://linux1:3306/metastore?useSSL=false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- jdbc连接的Driver--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">&lt;!-- jdbc连接的username--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">&lt;!-- jdbc连接的password --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>211819<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">&lt;!-- Hive默认在HDFS的工作目录 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">&lt;!-- Hive元数据存储版本的验证 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.schema.verification<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">&lt;!-- 指定存储元数据要连接的地址 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://linux1:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">&lt;!-- 指定hiveserver2连接的端口号 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">value</span>&gt;</span>10000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">&lt;!-- 指定hiveserver2连接的host --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.bind.host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>linux1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">&lt;!-- 元数据存储授权 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.event.db.notification.api.auth<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="安装Tez引擎"><a href="#安装Tez引擎" class="headerlink" title="安装Tez引擎"></a>安装Tez引擎</h3><p>1）将tez安装包拷贝到集群，并解压tar包</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mkdir /opt/module/tez</span><br><span class="line">tar -zxvf /opt/software/tez-0.10.1-SNAPSHOT-minimal.tar.gz -C /opt/module/tez</span><br></pre></td></tr></table></figure>

<p>2）上传tez依赖到HDFS</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop fs -mkdir /tez</span><br><span class="line">hadoop fs -put /opt/software/tez-0.10.1-SNAPSHOT.tar.gz /tez</span><br></pre></td></tr></table></figure>

<p>3）新建tez-site.xml</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim $HADOOP_HOME/etc/hadoop/tez-site.xml</span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.lib.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;fs.defaultFS&#125;/tez/tez-0.10.1-SNAPSHOT.tar.gz<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.use.cluster.hadoop-libs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.am.resource.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>1024<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.am.resource.cpu.vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.container.max.java.heap.fraction<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>0.4<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.task.resource.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>1024<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.task.resource.cpu.vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>4）修改Hadoop环境变量</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim $HADOOP_HOME/etc/hadoop/shellprofile.d/tez.sh</span><br></pre></td></tr></table></figure>

<p>添加Tez的Jar包相关信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop_add_profile tez</span><br><span class="line">function _tez_hadoop_classpath</span><br><span class="line">&#123;</span><br><span class="line">  hadoop_add_classpath &quot;$HADOOP_HOME/etc/hadoop&quot; after</span><br><span class="line">  hadoop_add_classpath &quot;/opt/module/tez/*&quot; after</span><br><span class="line">  hadoop_add_classpath &quot;/opt/module/tez/lib/*&quot; after</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>5）修改Hive的计算引擎</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim $HIVE_HOME/conf/hive-site.xml</span><br></pre></td></tr></table></figure>

<p>添加</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.execution.engine<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>tez<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.tez.container.size<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1024<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>6）解决日志jar包冲突</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">rm /opt/module/tez/lib/slf4j-log4j12-1.7.10.jar</span><br></pre></td></tr></table></figure>

<h2 id="启动Hive"><a href="#启动Hive" class="headerlink" title="启动Hive"></a>启动Hive</h2><h3 id="初始化元数据库"><a href="#初始化元数据库" class="headerlink" title="初始化元数据库"></a>初始化元数据库</h3><p>登录mysql</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mysql -uroot -p</span><br></pre></td></tr></table></figure>

<p>新建Hive元数据库</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; create database metastore;</span><br><span class="line">mysql&gt; quit;</span><br></pre></td></tr></table></figure>

<p>初始化Hive元数据库</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">schematool -initSchema -dbType mysql -verbose</span><br></pre></td></tr></table></figure>

<h3 id="启动metastore和hiveserver2"><a href="#启动metastore和hiveserver2" class="headerlink" title="启动metastore和hiveserver2"></a>启动metastore和hiveserver2</h3><p><strong>启动metastore（不配置 hive.metastore.uris 的话可以直接启动hive，相当于hive直接访问mysql，不需要先启动metastore服务；配置 hive.metastore.uris 相当于hive通过metastore服务连接mysql）</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive --service metastore</span><br></pre></td></tr></table></figure>

<p>注意: 启动后窗口不能再操作，需打开一个新的shell窗口做别的操作</p>
<p><strong>启动 hiveserver2（通过JDBC连接hive需要启动hiveserver2）</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive --service hiveserver2</span><br></pre></td></tr></table></figure>

<p>注意: 启动后窗口不能再操作，需打开一个新的shell窗口做别的操作</p>
<p>编写hive服务启动脚本（可以只使用一个命令窗口）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim $HIVE_HOME/bin/hiveservices.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">HIVE_LOG_DIR=$HIVE_HOME/logs</span><br><span class="line">if [ ! -d $HIVE_LOG_DIR ]</span><br><span class="line">then</span><br><span class="line">	mkdir -p $HIVE_LOG_DIR</span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span><span class="bash">检查进程是否运行正常，参数1为进程名，参数2为进程端口</span></span><br><span class="line">function check_process()</span><br><span class="line">&#123;</span><br><span class="line">    pid=$(ps -ef 2&gt;/dev/null | grep -v grep | grep -i $1 | awk &#x27;&#123;print $2&#125;&#x27;)</span><br><span class="line">    ppid=$(netstat -nltp 2&gt;/dev/null | grep $2 | awk &#x27;&#123;print $7&#125;&#x27; | cut -d &#x27;/&#x27; -f 1)</span><br><span class="line">    echo $pid</span><br><span class="line">    [[ &quot;$pid&quot; =~ &quot;$ppid&quot; ]] &amp;&amp; [ &quot;$ppid&quot; ] &amp;&amp; return 0 || return 1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">function hive_start()</span><br><span class="line">&#123;</span><br><span class="line">    metapid=$(check_process HiveMetastore 9083)</span><br><span class="line">    cmd=&quot;nohup hive --service metastore &gt;$HIVE_LOG_DIR/metastore.log 2&gt;&amp;1 &amp;&quot;</span><br><span class="line">    cmd=$cmd&quot; sleep 4; hdfs dfsadmin -safemode wait &gt;/dev/null 2&gt;&amp;1&quot;</span><br><span class="line">    [ -z &quot;$metapid&quot; ] &amp;&amp; eval $cmd || echo &quot;Metastroe服务已启动&quot;</span><br><span class="line">    server2pid=$(check_process HiveServer2 10000)</span><br><span class="line">    cmd=&quot;nohup hive --service hiveserver2 &gt;$HIVE_LOG_DIR/hiveServer2.log 2&gt;&amp;1 &amp;&quot;</span><br><span class="line">    [ -z &quot;$server2pid&quot; ] &amp;&amp; eval $cmd || echo &quot;HiveServer2服务已启动&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">function hive_stop()</span><br><span class="line">&#123;</span><br><span class="line">    metapid=$(check_process HiveMetastore 9083)</span><br><span class="line">    [ &quot;$metapid&quot; ] &amp;&amp; kill $metapid || echo &quot;Metastore服务未启动&quot;</span><br><span class="line">    server2pid=$(check_process HiveServer2 10000)</span><br><span class="line">    [ &quot;$server2pid&quot; ] &amp;&amp; kill $server2pid || echo &quot;HiveServer2服务未启动&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)</span><br><span class="line">    hive_start</span><br><span class="line">    ;;</span><br><span class="line">&quot;stop&quot;)</span><br><span class="line">    hive_stop</span><br><span class="line">    ;;</span><br><span class="line">&quot;restart&quot;)</span><br><span class="line">    hive_stop</span><br><span class="line">    sleep 2</span><br><span class="line">    hive_start</span><br><span class="line">    ;;</span><br><span class="line">&quot;status&quot;)</span><br><span class="line">    check_process HiveMetastore 9083 &gt;/dev/null &amp;&amp; echo &quot;Metastore服务运行正常&quot; || echo &quot;Metastore服务运行异常&quot;</span><br><span class="line">    check_process HiveServer2 10000 &gt;/dev/null &amp;&amp; echo &quot;HiveServer2服务运行正常&quot; || echo &quot;HiveServer2服务运行异常&quot;</span><br><span class="line">    ;;</span><br><span class="line">*)</span><br><span class="line">    echo Invalid Args!</span><br><span class="line">    echo &#x27;Usage: &#x27;$(basename $0)&#x27; start|stop|restart|status&#x27;</span><br><span class="line">    ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">chmod +x $HIVE_HOME/bin/hiveservices.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hiveservices.sh start</span><br></pre></td></tr></table></figure>

<h3 id="HiveJDBC访问"><a href="#HiveJDBC访问" class="headerlink" title="HiveJDBC访问"></a>HiveJDBC访问</h3><p>1）启动beeline客户端</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hive]$ bin/beeline -u jdbc:hive2://linux1:10000 -n vincent</span><br></pre></td></tr></table></figure>

<p>2）看到如下界面</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">Connecting</span> to jdbc:hive<span class="number">2</span>://linux<span class="number">1</span>:<span class="number">10000</span></span><br><span class="line"><span class="attribute">Connected</span> to: Apache Hive (version <span class="number">3</span>.<span class="number">1</span>.<span class="number">2</span>)</span><br><span class="line"><span class="attribute">Driver</span>: Hive JDBC (version <span class="number">3</span>.<span class="number">1</span>.<span class="number">2</span>)</span><br><span class="line"><span class="attribute">Transaction</span> isolation: TRANSACTION_REPEATABLE_READ</span><br><span class="line"><span class="attribute">Beeline</span> version <span class="number">3</span>.<span class="number">1</span>.<span class="number">2</span> by Apache Hive</span><br></pre></td></tr></table></figure>

<h3 id="Hive访问"><a href="#Hive访问" class="headerlink" title="Hive访问"></a>Hive访问</h3><p>1）启动hive客户端</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hive]$ bin/hive</span><br></pre></td></tr></table></figure>

<p>2）看到如下界面</p>
<figure class="highlight gradle"><table><tr><td class="code"><pre><span class="line">which: no hbase in (<span class="regexp">/usr/</span>local<span class="regexp">/bin:/u</span>sr<span class="regexp">/bin:/u</span>sr<span class="regexp">/local/</span>sbin:<span class="regexp">/usr/</span>sbin:<span class="regexp">/opt/m</span>odule<span class="regexp">/jdk1.8.0_212/</span>bin:<span class="regexp">/opt/m</span>odule<span class="regexp">/hadoop-3.1.3/</span>bin:<span class="regexp">/opt/m</span>odule<span class="regexp">/hadoop-3.1.3/</span>sbin:<span class="regexp">/opt/m</span>odule<span class="regexp">/hive/</span>bin:<span class="regexp">/home/</span>vincent<span class="regexp">/.local/</span>bin:<span class="regexp">/home/</span>vincent/bin)</span><br><span class="line">Hive Session ID = <span class="number">486</span>a2ca7-<span class="number">44</span>d4-<span class="number">41</span>a1-a5fb-<span class="number">4564</span>f3f3d54f</span><br><span class="line"></span><br><span class="line">Logging initialized using configuration in jar:<span class="keyword">file</span>:<span class="regexp">/opt/m</span>odule<span class="regexp">/hive/</span>lib<span class="regexp">/hive-common-3.1.2.jar!/</span>hive-log4j2.properties Async: <span class="keyword">true</span></span><br><span class="line">Hive Session ID = <span class="number">187</span>d1d0a-b70d-<span class="number">4</span>fe5-<span class="number">92</span>d9-<span class="number">514</span>e256a4dd0</span><br></pre></td></tr></table></figure>

<p>3）打印当前库和表头</p>
<p>在hive-site.xml中加入如下两个配置:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim $HIVE_HOME/conf/hive-site.xml</span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.header<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether to print the names of the columns in query output.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.current.db<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether to include the current database in the Hive prompt.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="Hive常用交互命令"><a href="#Hive常用交互命令" class="headerlink" title="Hive常用交互命令"></a>Hive常用交互命令</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/hive -help</span><br></pre></td></tr></table></figure>

<p>-e 不进入hive的交互窗口执行sql语句</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/hive -e &quot;select id from student;&quot;</span><br></pre></td></tr></table></figure>

<p>-f 执行脚本中的sql语句</p>
<p>1）在/opt/module/hive/下创建datas目录并在datas目录下创建hivef.sql文件</p>
<p>2）文件中写入正确的sql语句</p>
<p>3）执行文件中的sql语句</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hive]$ bin/hive -f /opt/module/hive/datas/hivef.sql</span><br></pre></td></tr></table></figure>

<p>4）执行文件中的sql语句并将结果写入文件中</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hive]$ bin/hive -f /opt/module/hive/datas/hivef.sql  &gt; /opt/module/datas/hive_result.txt</span><br></pre></td></tr></table></figure>

<p>在hive cli命令窗口中如何查看hdfs文件系统</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">hive(default)&gt;</span><span class="bash">dfs -ls /;</span></span><br></pre></td></tr></table></figure>

<p>查看在hive中输入的所有历史命令</p>
<p>进入当前用户的根目录/root或者/home/vincent</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 ~]$ cat .hivehistory</span><br></pre></td></tr></table></figure>

<h2 id="Hive常见属性配置"><a href="#Hive常见属性配置" class="headerlink" title="Hive常见属性配置"></a>Hive常见属性配置</h2><p>1）Hive的log默认存放在/tmp/vincent/hive.log目录下（当前用户名下）</p>
<p>2）修改hive的log存放到/opt/module/hive/logs</p>
<p>修改 /opt/module/hive/conf/hive-log4j2.properties.template 文件名称为 hive-log4j2.properties</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 conf]$ mv hive-log4j2.properties.template hive-log4j2.properties</span><br></pre></td></tr></table></figure>

<p>在hive-log4j2.properties文件中修改log存放位置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive.log.dir=/opt/module/hive/logs</span><br></pre></td></tr></table></figure>

<h2 id="参数配置方式"><a href="#参数配置方式" class="headerlink" title="参数配置方式"></a>参数配置方式</h2><p>查看当前所有的配置信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"><span class="built_in">set</span></span></span><br></pre></td></tr></table></figure>

<p>参数配置的三种方式</p>
<p>1）配置文件方式</p>
<p>默认配置文件：hive-default.xml</p>
<p>用户自定义配置文件：hive-site.xml</p>
<p>注意：用户自定义配置会覆盖默认配置。另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。配置文件的设定对本机启动的所有Hive进程都有效。</p>
<p>2）命令行参数方式</p>
<p>启动Hive时，可以在命令行添加 -hiveconf param=value 来设定参数。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hive]$ bin/hive -hiveconf mapred.reduce.tasks=10;</span><br></pre></td></tr></table></figure>

<p>注意：仅对本次hive启动有效</p>
<p>3）参数声明方式</p>
<p>查看参数设置：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks;</span><br></pre></td></tr></table></figure>

<p>可以在HQL中使用SET关键字设定参数</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; set mapred.reduce.tasks=100;</span><br></pre></td></tr></table></figure>

<p>注意：仅对本次hive启动有效。</p>
<p>上述三种设定方式的优先级依次递增。即 配置文件&lt;命令行参数&lt;参数声明。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在会话建立以前已经完成了。</p>
<h2 id="Hive数据类型"><a href="#Hive数据类型" class="headerlink" title="Hive数据类型"></a>Hive数据类型</h2><h3 id="基本数据类型"><a href="#基本数据类型" class="headerlink" title="基本数据类型"></a>基本数据类型</h3><table>
<thead>
<tr>
<th>Hive数据类型</th>
<th>Java数据类型</th>
<th>长度</th>
<th>例子</th>
</tr>
</thead>
<tbody><tr>
<td>TINYINT</td>
<td>byte</td>
<td>1byte有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>SMALINT</td>
<td>short</td>
<td>2byte有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>INT</td>
<td>int</td>
<td>4byte有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>BIGINT</td>
<td>long</td>
<td>8byte有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>BOOLEAN</td>
<td>boolean</td>
<td>布尔类型，true或者false</td>
<td>TRUE  FALSE</td>
</tr>
<tr>
<td>FLOAT</td>
<td>float</td>
<td>单精度浮点数</td>
<td>3.14159</td>
</tr>
<tr>
<td>DOUBLE</td>
<td>double</td>
<td>双精度浮点数</td>
<td>3.14159</td>
</tr>
<tr>
<td>STRING</td>
<td>string</td>
<td>字符系列。可以指定字符集。可以使用单引号或者双引号。</td>
<td>‘now is the time’ “for all good men”</td>
</tr>
<tr>
<td>TIMESTAMP</td>
<td></td>
<td>时间类型</td>
<td></td>
</tr>
<tr>
<td>BINARY</td>
<td></td>
<td>字节数组</td>
<td></td>
</tr>
</tbody></table>
<p><strong>对于Hive的String类型相当于数据库的varchar类型，该类型是一个可变的字符串，不过它不能声明其中最多能存储多少个字符，理论上它可以存储2GB的字符数。</strong></p>
<h3 id="集合数据类型"><a href="#集合数据类型" class="headerlink" title="集合数据类型"></a>集合数据类型</h3><table>
<thead>
<tr>
<th>数据类型</th>
<th>描述</th>
<th>语法示例</th>
</tr>
</thead>
<tbody><tr>
<td>STRUCT</td>
<td>和c语言中的struct类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是STRUCT{first STRING, last STRING},那么第1个元素可以通过字段.first来引用。</td>
<td>struct()例如struct&lt;street:string, city:string&gt;</td>
</tr>
<tr>
<td>MAP</td>
<td>MAP是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是MAP，其中键-&gt;值对是’first’-&gt;’John’和’last’-&gt;’Doe’，那么可以通过字段名[‘last’]获取最后一个元素</td>
<td>map()例如map&lt;string, int&gt;</td>
</tr>
<tr>
<td>ARRAY</td>
<td>数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’, ‘Doe’]，那么第2个元素可以通过数组名[1]进行引用。</td>
<td>Array()例如array<string></string></td>
</tr>
</tbody></table>
<p>Hive有三种复杂数据类型ARRAY、MAP 和 STRUCT。ARRAY和MAP与Java中的Array和Map类似，而STRUCT与C语言中的Struct类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套。</p>
<p>案例实操</p>
<p>1）假设某表有如下一行，我们用JSON格式来表示其数据结构。在Hive下访问的格式为</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;name&quot;</span>: <span class="string">&quot;songsong&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;friends&quot;</span>: [<span class="string">&quot;bingbing&quot;</span> , <span class="string">&quot;lili&quot;</span>] ,       <span class="comment">//列表Array, </span></span><br><span class="line">    <span class="attr">&quot;children&quot;</span>: &#123;                      			 <span class="comment">//键值Map,</span></span><br><span class="line">        <span class="attr">&quot;xiao song&quot;</span>: <span class="number">18</span> ,</span><br><span class="line">        <span class="attr">&quot;xiaoxiao song&quot;</span>: <span class="number">19</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="string">&quot;address&quot;</span>: &#123;                      			 <span class="comment">//结构Struct,</span></span><br><span class="line">        <span class="attr">&quot;street&quot;</span>: <span class="string">&quot;hui long guan&quot;</span> ,</span><br><span class="line">        <span class="attr">&quot;city&quot;</span>: <span class="string">&quot;beijing&quot;</span> </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>2）基于上述数据结构，我们在Hive里创建对应的表，并导入数据</p>
<p>创建本地测试文件test.txt</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">songsong</span>,bingbing_lili,xiao song:<span class="number">18</span>_xiaoxiao song:<span class="number">19</span>,hui long guan_beijing</span><br><span class="line"><span class="attribute">yangyang</span>,caicai_susu,xiao yang:<span class="number">18</span>_xiaoxiao yang:<span class="number">19</span>,chao yang_beijing</span><br></pre></td></tr></table></figure>

<p>注意：MAP，STRUCT和ARRAY里的元素间关系都可以用同一个字符表示，这里用“_”。</p>
<p>3）Hive上创建测试表test</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test(</span><br><span class="line">name string,</span><br><span class="line">friends <span class="keyword">array</span><span class="operator">&lt;</span>string<span class="operator">&gt;</span>,</span><br><span class="line">children map<span class="operator">&lt;</span>string, <span class="type">int</span><span class="operator">&gt;</span>,</span><br><span class="line">address struct<span class="operator">&lt;</span>street:string, city:string<span class="operator">&gt;</span></span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line">collection items terminated <span class="keyword">by</span> <span class="string">&#x27;_&#x27;</span></span><br><span class="line">map keys terminated <span class="keyword">by</span> <span class="string">&#x27;:&#x27;</span></span><br><span class="line">lines terminated <span class="keyword">by</span> <span class="string">&#x27;\n&#x27;</span>;</span><br></pre></td></tr></table></figure>

<p>字段解释：</p>
<p>row format delimited fields terminated by ‘,’  – 列分隔符</p>
<p>collection items terminated by ‘_’  –MAP STRUCT 和 ARRAY 中元素之间的分隔符(数据分割符号)</p>
<p>map keys terminated by ‘:’            – MAP中的key与value的分隔符</p>
<p>lines terminated by ‘\n’;                – 行分隔符</p>
<p>4）导入文本数据到测试表</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ hadoop fs -put test.txt /user/hive/warehouse/test/</span><br></pre></td></tr></table></figure>

<p>5）访问三种集合列里的数据，以下分别是ARRAY，MAP，STRUCT的访问方式</p>
<p>访问STRUCT结构中元素    address.city    访问MAP中元素    children[‘xiao song’]    访问ARRAY元素    friends[1]</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select friends[1],children[&#x27;xiao song&#x27;],address.city from test</span><br><span class="line">where name=&quot;songsong&quot;;</span><br><span class="line">OK</span><br><span class="line">_c0     _c1     city</span><br><span class="line">lili    18      beijing</span><br><span class="line">Time taken: 0.076 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure>

<h3 id="类型转化"><a href="#类型转化" class="headerlink" title="类型转化"></a>类型转化</h3><p><strong>Hive的基本数据类型是可以进行隐式转换的，类似于Java的类型转换</strong>，例如某表达式使用INT类型，TINYINT会自动转换为INT类型，<strong>但是Hive不会进行反向转化</strong>，例如，某表达式使用TINYINT类型，INT不会自动转换为TINYINT类型，它会返回错误，除非使用CAST操作。</p>
<p>1）隐式类型转换规则如下</p>
<p>（1）任何整数类型都可以隐式地转换为一个范围更广的类型，如TINYINT可以转换成INT，INT可以转换成    BIGINT。</p>
<p>（2）所有整数类型、FLOAT和STRING类型都可以隐式地转换成DOUBLE。</p>
<p>（3）TINYINT、SMALLINT、INT都可以转换为FLOAT。</p>
<p>（4）BOOLEAN类型不可以转换为任何其它的类型。</p>
<p>2）可以使用CAST操作显示进行数据类型转换</p>
<p>例如CAST(‘1’ AS INT)将把字符串’1’ 转换成整数1；如果强制类型转换失败，如执行CAST(‘X’ AS INT)，表达式返回空值 NULL。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select &#x27;1&#x27; + 2, cast(&#x27;1&#x27; as int) + 2;</span><br></pre></td></tr></table></figure>

<figure class="highlight asciidoc"><table><tr><td class="code"><pre><span class="line"><span class="code">+------+</span>------+</span><br><span class="line"><span class="section">| _c0  | _c1  |</span></span><br><span class="line"><span class="section">+------+------+</span></span><br><span class="line"><span class="section">| 3.0  | 3    |</span></span><br><span class="line"><span class="section">+------+------+</span></span><br></pre></td></tr></table></figure>

<h2 id="DDL数据定义"><a href="#DDL数据定义" class="headerlink" title="DDL数据定义"></a>DDL数据定义</h2><h3 id="创建数据库"><a href="#创建数据库" class="headerlink" title="创建数据库"></a>创建数据库</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CREATE DATABASE [IF NOT EXISTS] database_name</span><br><span class="line">[COMMENT database_comment]</span><br><span class="line">[LOCATION hdfs_path]</span><br><span class="line">[WITH DBPROPERTIES (property_name=property_value, ...)];</span><br></pre></td></tr></table></figure>

<p>1）创建一个数据库，数据库在HDFS上的默认存储路径是/user/hive/warehouse/*.db</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create database db_hive;</span><br></pre></td></tr></table></figure>

<p>2）避免要创建的数据库已经存在错误，增加if not exists判断。（标准写法）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create database db_hive;</span><br><span class="line">FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Database db_hive already exists</span><br><span class="line">hive (default)&gt; create database if not exists db_hive;</span><br></pre></td></tr></table></figure>

<p>3）创建一个数据库，指定数据库在HDFS上存放的位置</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create database db_hive2 location &#x27;/db_hive2.db&#x27;;</span><br></pre></td></tr></table></figure>

<h3 id="查询数据库"><a href="#查询数据库" class="headerlink" title="查询数据库"></a>查询数据库</h3><h4 id="显示数据库"><a href="#显示数据库" class="headerlink" title="显示数据库"></a>显示数据库</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive&gt; show databases;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive&gt; show databases like &#x27;db_hive*&#x27;;</span><br><span class="line">OK</span><br><span class="line">db_hive</span><br><span class="line">db_hive_1</span><br></pre></td></tr></table></figure>

<h4 id="查看数据库详情"><a href="#查看数据库详情" class="headerlink" title="查看数据库详情"></a>查看数据库详情</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive&gt; desc database db_hive;</span><br></pre></td></tr></table></figure>

<p>更详细信息 extended</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive&gt; desc database extended db_hive;</span><br></pre></td></tr></table></figure>

<h4 id="切换当前数据库"><a href="#切换当前数据库" class="headerlink" title="切换当前数据库"></a>切换当前数据库</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; use db_hive;</span><br></pre></td></tr></table></figure>

<h3 id="修改数据库"><a href="#修改数据库" class="headerlink" title="修改数据库"></a>修改数据库</h3><p>用户可以使用ALTER DATABASE命令为某个数据库的DBPROPERTIES设置键-值对属性值，来描述这个数据库的属性信息。数据库的其他元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter database db_hive set dbproperties(&#x27;createtime&#x27;=&#x27;20170830&#x27;);</span><br></pre></td></tr></table></figure>

<p>在hive中查看修改结果</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive&gt; desc database extended db_hive;</span><br></pre></td></tr></table></figure>

<figure class="highlight asciidoc"><table><tr><td class="code"><pre><span class="line"><span class="code">+----------+</span>----------<span class="code">+----------------------------------------------------+</span>-------------<span class="code">+-------------+</span>------------------------+</span><br><span class="line"><span class="section">| db_name  | comment  |                      location                      | owner_name  | owner_type  |       parameters       |</span></span><br><span class="line"><span class="section">+----------+----------+----------------------------------------------------+-------------+-------------+------------------------+</span></span><br><span class="line"><span class="section">| db_hive  |          | hdfs://linux1:9820/user/hive/warehouse/db_hive.db  | vincent     | USER        | &#123;createtime=20170830&#125;  |</span></span><br><span class="line"><span class="section">+----------+----------+----------------------------------------------------+-------------+-------------+------------------------+</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="删除数据库"><a href="#删除数据库" class="headerlink" title="删除数据库"></a>删除数据库</h3><p>1）删除空数据库</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive&gt;drop database db_hive2;</span><br></pre></td></tr></table></figure>

<p>2）如果删除的数据库不存在，最好采用if exists判断数据库是否存在</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive&gt; drop database db_hive;</span><br><span class="line">FAILED: SemanticException [Error 10072]: Database does not exist: db_hive</span><br><span class="line">hive&gt; drop database if exists db_hive2;</span><br></pre></td></tr></table></figure>

<p>3）如果数据库不为空，可以采用cascade命令，强制删除</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive&gt; drop database db_hive;</span><br><span class="line">FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. InvalidOperationException(message:Database db_hive is not empty. One or more tables exist.)</span><br><span class="line">hive&gt; drop database db_hive cascade;</span><br></pre></td></tr></table></figure>

<h3 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h3><p>1）建表语法</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name </span><br><span class="line">[(col_name data_type [COMMENT col_comment], ...)] </span><br><span class="line">[COMMENT table_comment] </span><br><span class="line">[PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] </span><br><span class="line">[CLUSTERED BY (col_name, col_name, ...) </span><br><span class="line">[SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] </span><br><span class="line">[ROW FORMAT row_format] </span><br><span class="line">[STORED AS file_format] </span><br><span class="line">[LOCATION hdfs_path]</span><br><span class="line">[TBLPROPERTIES (property_name=property_value, ...)]</span><br><span class="line">[AS select_statement]</span><br></pre></td></tr></table></figure>

<p>2）字段解释说明</p>
<p>（1）CREATE TABLE 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。</p>
<p>（2）EXTERNAL关键字可以让用户创建一个外部表，在建表的同时可以指定一个指向实际数据的路径（LOCATION），在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。</p>
<p>（3）COMMENT：为表和列添加注释。</p>
<p><strong>（4）PARTITIONED BY创建分区表</strong></p>
<p><strong>（5）CLUSTERED BY创建分桶表</strong></p>
<p><strong>（6）SORTED BY不常用，对桶中的一个或多个列另外排序</strong></p>
<p>（7）ROW FORMAT </p>
<p>DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char]</p>
<p>​    [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] </p>
<p>  | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, …)]</p>
<p>用户在建表的时候可以自定义SerDe或者使用自带的SerDe。如果没有指定ROW FORMAT 或者ROW FORMAT DELIMITED，将会使用自带的SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的SerDe，Hive通过SerDe确定表的具体的列的数据。</p>
<p>SerDe是Serialize/Deserilize的简称， hive使用Serde进行行对象的序列与反序列化。</p>
<p>（8）STORED AS指定存储文件类型</p>
<p>常用的存储文件类型：SEQUENCEFILE（二进制序列文件）、TEXTFILE（文本）、RCFILE（列式存储格式文件）</p>
<p>如果文件数据是纯文本，可以使用STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCEFILE。</p>
<p>（9）LOCATION ：指定表在HDFS上的存储位置。</p>
<p>（10）AS：后跟查询语句，根据查询结果创建表。</p>
<p>（11）LIKE允许用户复制现有的表结构，但是不复制数据。</p>
<h4 id="管理表"><a href="#管理表" class="headerlink" title="管理表"></a>管理表</h4><p><strong>默认创建的表都是所谓的管理表，有时也被称为内部表。</strong>因为这种表，Hive会（或多或少地）控制着数据的生命周期。Hive默认情况下会将这些表的数据存储在由配置项hive.metastore.warehouse.dir(例如，/user/hive/warehouse)所定义的目录的子目录下。    <strong>当我们删除一个管理表时，Hive也会删除这个表中数据。管理表不适合和其他工具共享数据。</strong></p>
<p>案例实操</p>
<p>0）实际数据</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">1001</span>	ss<span class="number">1</span></span><br><span class="line"><span class="attribute">1002</span>	ss<span class="number">2</span></span><br><span class="line"><span class="attribute">1003</span>	ss<span class="number">3</span></span><br><span class="line"><span class="attribute">1004</span>	ss<span class="number">4</span></span><br><span class="line"><span class="attribute">1005</span>	ss<span class="number">5</span></span><br><span class="line"><span class="attribute">1006</span>	ss<span class="number">6</span></span><br><span class="line"><span class="attribute">1007</span>	ss<span class="number">7</span></span><br><span class="line"><span class="attribute">1008</span>	ss<span class="number">8</span></span><br><span class="line"><span class="attribute">1009</span>	ss<span class="number">9</span></span><br><span class="line"><span class="attribute">1010</span>	ss<span class="number">10</span></span><br><span class="line"><span class="attribute">1011</span>	ss<span class="number">11</span></span><br><span class="line"><span class="attribute">1012</span>	ss<span class="number">12</span></span><br><span class="line"><span class="attribute">1013</span>	ss<span class="number">13</span></span><br><span class="line"><span class="attribute">1014</span>	ss<span class="number">14</span></span><br><span class="line"><span class="attribute">1015</span>	ss<span class="number">15</span></span><br><span class="line"><span class="attribute">1016</span>	ss<span class="number">16</span></span><br></pre></td></tr></table></figure>

<p>1）创建内部表</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table if not exists student(</span><br><span class="line">id int, </span><br><span class="line">name string</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;</span><br><span class="line">stored as textfile</span><br><span class="line">location &#x27;/user/hive/warehouse/student&#x27;;</span><br></pre></td></tr></table></figure>

<p>2）根据查询结果创建表（查询的结果会添加到新创建的表中）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table if not exists student2 as select id, name from student;</span><br></pre></td></tr></table></figure>

<p>3）根据已经存在的表结构创建表（<strong>只复制表结构</strong>）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table if not exists student3 like student;</span><br></pre></td></tr></table></figure>

<p>4）查询表的类型</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; desc formatted student2;</span><br><span class="line">Table Type:             MANAGED_TABLE</span><br></pre></td></tr></table></figure>

<h4 id="外部表"><a href="#外部表" class="headerlink" title="外部表"></a>外部表</h4><p>因为表是外部表，所以Hive并非认为其完全拥有这份数据。<strong>删除该表并不会删除掉这份数据，不过描述表的元数据信息会被删除掉。</strong></p>
<p>管理表和外部表的使用场景</p>
<p>每天将收集到的网站日志定期流入HDFS文本文件。在外部表（原始日志表）的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过SELECT+INSERT进入内部表。</p>
<p>外部表删除后，hdfs中的数据还在，但是metadata中dept的元数据已被删除</p>
<h4 id="管理表与外部表的互相转换"><a href="#管理表与外部表的互相转换" class="headerlink" title="管理表与外部表的互相转换"></a>管理表与外部表的互相转换</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">alter table student set tblproperties(&#x27;EXTERNAL&#x27;=&#x27;TRUE&#x27;);</span><br><span class="line">alter table student set tblproperties(&#x27;EXTERNAL&#x27;=&#x27;FALSE&#x27;);</span><br></pre></td></tr></table></figure>

<p>注意：(‘EXTERNAL’=’TRUE’)和(‘EXTERNAL’=’FALSE’)为固定写法，区分大小写。</p>
<h3 id="修改表"><a href="#修改表" class="headerlink" title="修改表"></a>修改表</h3><h4 id="重命名表"><a href="#重命名表" class="headerlink" title="重命名表"></a>重命名表</h4><p>ALTER TABLE table_name RENAME TO new_table_name</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition2 rename to dept_partition3;</span><br></pre></td></tr></table></figure>

<h4 id="增加、修改和删除表分区"><a href="#增加、修改和删除表分区" class="headerlink" title="增加、修改和删除表分区"></a>增加、修改和删除表分区</h4><h4 id="增加-修改-替换列信息"><a href="#增加-修改-替换列信息" class="headerlink" title="增加/修改/替换列信息"></a>增加/修改/替换列信息</h4><p>更新列</p>
<p>ALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name]</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept change column deptdesc description string;</span><br></pre></td></tr></table></figure>

<p>增加和替换列</p>
<p>ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], …)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept add columns(deptdesc string);</span><br><span class="line">hive (default)&gt; alter table dept replace columns(deptno string, dname string, loc string);</span><br></pre></td></tr></table></figure>

<h3 id="删除表"><a href="#删除表" class="headerlink" title="删除表"></a>删除表</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; drop table dept;</span><br></pre></td></tr></table></figure>

<h2 id="分区表与分桶表"><a href="#分区表与分桶表" class="headerlink" title="分区表与分桶表"></a>分区表与分桶表</h2><h3 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h3><p>分区表实际上就是对应一个HDFS文件系统上的独立的文件夹，该文件夹下是该分区所有的数据文件。Hive中的分区就是分目录，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过WHERE子句中的表达式选择查询所需要的指定的分区，这样的查询效率会提高很多。</p>
<p>1）引入分区表</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><span class="line">dept_20200401.log		<span class="regexp">//</span><span class="number">10</span>	ACCOUNTING	<span class="number">1700</span></span><br><span class="line">										<span class="regexp">//</span><span class="number">20</span>	RESEARCH	<span class="number">1800</span></span><br><span class="line">dept_20200402.log		<span class="regexp">//</span><span class="number">30</span>	SALES	<span class="number">1900</span></span><br><span class="line">										<span class="regexp">//</span><span class="number">40</span>	OPERATIONS	<span class="number">1700</span></span><br><span class="line">dept_20200403.log		<span class="regexp">//</span><span class="number">50</span>	TEST	<span class="number">2000</span></span><br><span class="line">										<span class="regexp">//</span><span class="number">60</span>	DEV	<span class="number">1900</span></span><br></pre></td></tr></table></figure>

<p>2）创建分区表语法</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create table dept_partition(</span><br><span class="line">deptno int,</span><br><span class="line">dname string,</span><br><span class="line">loc string</span><br><span class="line">)</span><br><span class="line">partitioned by (day string)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure>

<p><strong>注意：分区字段不能是表中已经存在的数据，可以将分区字段看作表的伪列。</strong></p>
<p>3）加载数据到分区表中</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/hive/datas/dept_20200401.log&#x27; into table dept_partition partition(day=&#x27;20200401&#x27;);</span><br><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/hive/datas/dept_20200402.log&#x27; into table dept_partition partition(day=&#x27;20200402&#x27;);</span><br><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/hive/datas/dept_20200403.log&#x27; into table dept_partition partition(day=&#x27;20200403&#x27;);</span><br></pre></td></tr></table></figure>

<p>注意：分区表加载数据时，必须指定分区</p>
<p>4）查询分区表中数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition where day=&#x27;20200401&#x27;;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from dept_partition where day=&#x27;20200401&#x27;</span><br><span class="line">              union</span><br><span class="line">              select * from dept_partition where day=&#x27;20200402&#x27;</span><br><span class="line">              union</span><br><span class="line">              select * from dept_partition where day=&#x27;20200403&#x27;;</span><br><span class="line">hive (default)&gt; select * from dept_partition where day=&#x27;20200401&#x27; or</span><br><span class="line">                day=&#x27;20200402&#x27; or day=&#x27;20200403&#x27; ;</span><br></pre></td></tr></table></figure>

<p>5）增加分区</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition add partition(day=&#x27;20200404&#x27;);</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition add partition(day=&#x27;20200405&#x27;) partition(day=&#x27;20200406&#x27;);</span><br></pre></td></tr></table></figure>

<p>6）删除分区</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition drop partition (day=&#x27;20200406&#x27;);</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; alter table dept_partition drop partition (day=&#x27;20200404&#x27;), partition(day=&#x27;20200405&#x27;);</span><br></pre></td></tr></table></figure>

<p>7）查看分区表有多少分区</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive&gt; show partitions dept_partition;</span><br></pre></td></tr></table></figure>

<p>8）查看分区表结构</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive&gt; desc formatted dept_partition; </span><br></pre></td></tr></table></figure>

<p>如果一天的的日志数据也很大，如何将一天的日志分区？</p>
<p>1）创建二级分区表</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table dept_partition2(</span><br><span class="line">deptno int,</span><br><span class="line">dname string,</span><br><span class="line">loc string</span><br><span class="line">)</span><br><span class="line">partitioned by (day string, hour string)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure>

<p>2）加载数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">load data local inpath &#x27;/opt/module/hive/datas/dept_20200401.log&#x27; into table</span><br><span class="line">dept_partition2 partition(day=&#x27;20200401&#x27;, hour=&#x27;12&#x27;);</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select * from dept_partition2 where day=&#x27;20200401&#x27; and hour=&#x27;12&#x27;;</span><br></pre></td></tr></table></figure>

<p>3）将数据直接上传到分区目录上，让分区表和数据产生关联的三种方式</p>
<p>1.上传数据后修复</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir -p /user/hive/warehouse/dept_partition2/day=20200401/hour=13;</span><br><span class="line"></span><br><span class="line">hive (default)&gt; dfs -put /opt/module/hive/datas/dept_20200401.log /user/hive/warehouse/dept_partition2/day=20200401/hour=13;</span><br><span class="line"></span><br><span class="line">hive (default)&gt; msck repair table dept_partition2;</span><br><span class="line"></span><br><span class="line">hive (default)&gt; select * from dept_partition2 where day=&#x27;20200401&#x27; and hour=&#x27;13&#x27;;</span><br></pre></td></tr></table></figure>

<p>2.创建文件夹后load数据到分区</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir -p /user/hive/warehouse/dept_partition2/day=20200401/hour=14;</span><br><span class="line"></span><br><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/hive/datas/dept_20200401.log&#x27; into table dept_partition2 partition(day=&#x27;20200401&#x27;,hour=&#x27;14&#x27;);</span><br><span class="line"></span><br><span class="line">hive (default)&gt; select * from dept_partition2 where day=&#x27;20200401&#x27; and hour=&#x27;14&#x27;;</span><br></pre></td></tr></table></figure>

<h3 id="动态分区调整"><a href="#动态分区调整" class="headerlink" title="动态分区调整"></a>动态分区调整</h3><p>关系型数据库中，对分区表Insert数据时候，数据库自动会根据分区字段的值，将数据插入到相应的分区中，Hive中也提供了类似的机制，即动态分区(Dynamic Partition)，只不过，使用Hive的动态分区，需要进行相应的配置。</p>
<p>（1）开启动态分区功能（默认true，开启）</p>
<figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="attr">hive.exec.dynamic.partition</span>=<span class="literal">true</span></span><br></pre></td></tr></table></figure>

<p>（2）设置为<strong>非严格模式</strong>（动态分区的模式，默认strict，表示必须指定至少一个分区为静态分区，nonstrict模式表示允许所有的分区字段都可以使用动态分区。）</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">hive<span class="selector-class">.exec</span><span class="selector-class">.dynamic</span><span class="selector-class">.partition</span>.mode=nonstrict</span><br></pre></td></tr></table></figure>

<p>（3）在<strong>所有执行MR的节点</strong>上，最大一共可以创建多少个动态分区。默认1000</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">hive<span class="selector-class">.exec</span><span class="selector-class">.max</span><span class="selector-class">.dynamic</span>.partitions=<span class="number">1000</span></span><br></pre></td></tr></table></figure>

<p>（4）在<strong>每个执行MR的节点上</strong>，最大可以创建多少个动态分区。该参数需要根据实际的数据来设定。比如：源数据中包含了一年的数据，即day字段有365个值，那么该参数就需要设置成大于365，如果使用默认值100，则会报错。</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">hive<span class="selector-class">.exec</span><span class="selector-class">.max</span><span class="selector-class">.dynamic</span><span class="selector-class">.partitions</span>.pernode=<span class="number">100</span></span><br></pre></td></tr></table></figure>

<p>（5）整个MR Job中，最大可以创建多少个HDFS文件。默认100000</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">hive<span class="selector-class">.exec</span><span class="selector-class">.max</span><span class="selector-class">.created</span>.files=<span class="number">100000</span></span><br></pre></td></tr></table></figure>

<p>（6）当有空分区生成时，是否抛出异常。一般不需要设置。默认false</p>
<figure class="highlight vbscript"><table><tr><td class="code"><pre><span class="line">hive.<span class="keyword">error</span>.<span class="keyword">on</span>.<span class="literal">empty</span>.partition=<span class="literal">false</span></span><br></pre></td></tr></table></figure>

<p>需求：将dept表中的数据按照地区（loc字段），插入到目标表dept_partition的相应分区中。</p>
<p>（1）创建目标分区表</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create table dept_partition_dy(id int, name string) partitioned by (loc int) row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure>

<p>（2）设置动态分区</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">set hive.exec.dynamic.partition.mode = nonstrict;</span><br><span class="line"></span><br><span class="line">hive (default)&gt; insert into table dept_partition_dy partition(loc) select deptno, dname, loc from dept_partition;</span><br></pre></td></tr></table></figure>

<p>（3）查看目标分区表的分区情况</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; show partitions dept_partition;</span><br><span class="line">hive (default)&gt; show partitions dept_partition_dy;</span><br></pre></td></tr></table></figure>

<h3 id="分桶表"><a href="#分桶表" class="headerlink" title="分桶表"></a>分桶表</h3><p>分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理的分区。<strong>对于一张表或者分区，Hive 可以进一步组织成桶，也就是更为细粒度的数据范围划分。</strong></p>
<p>分桶是将数据集分解成更容易管理的若干部分的另一个技术。</p>
<p><strong>分区针对的是数据的存储路径；分桶针对的是数据文件。</strong></p>
<p><strong>分桶表字段是表内字段。</strong></p>
<p>1）先创建分桶表，通过直接导入数据文件的方式</p>
<p>（1）数据准备</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">1001</span>	ss<span class="number">1</span></span><br><span class="line"><span class="attribute">1002</span>	ss<span class="number">2</span></span><br><span class="line"><span class="attribute">1003</span>	ss<span class="number">3</span></span><br><span class="line"><span class="attribute">1004</span>	ss<span class="number">4</span></span><br><span class="line"><span class="attribute">1005</span>	ss<span class="number">5</span></span><br><span class="line"><span class="attribute">1006</span>	ss<span class="number">6</span></span><br><span class="line"><span class="attribute">1007</span>	ss<span class="number">7</span></span><br><span class="line"><span class="attribute">1008</span>	ss<span class="number">8</span></span><br><span class="line"><span class="attribute">1009</span>	ss<span class="number">9</span></span><br><span class="line"><span class="attribute">1010</span>	ss<span class="number">10</span></span><br><span class="line"><span class="attribute">1011</span>	ss<span class="number">11</span></span><br><span class="line"><span class="attribute">1012</span>	ss<span class="number">12</span></span><br><span class="line"><span class="attribute">1013</span>	ss<span class="number">13</span></span><br><span class="line"><span class="attribute">1014</span>	ss<span class="number">14</span></span><br><span class="line"><span class="attribute">1015</span>	ss<span class="number">15</span></span><br><span class="line"><span class="attribute">1016</span>	ss<span class="number">16</span></span><br></pre></td></tr></table></figure>

<p>（2）创建分桶表</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table stu_buck(id int, name string)</span><br><span class="line">clustered by(id) </span><br><span class="line">into 4 buckets</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure>

<p>（3）查看表结构</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; desc formatted stu_buck;</span><br><span class="line">Num Buckets:            4   </span><br></pre></td></tr></table></figure>

<p>（4）导入数据到分桶表</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt;dfs -mkdir /user/hive/warehouse/stu_buck;</span><br><span class="line"></span><br><span class="line">hive (default)&gt;set hive.execution.engine = mr;			//只有mr引擎支持往分桶表load数据</span><br><span class="line"></span><br><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/hive/datas/student.txt&#x27; overwrite into table stu_buck;</span><br><span class="line"></span><br><span class="line">hive (default)&gt;msck repair table stu_buck;</span><br></pre></td></tr></table></figure>

<p>Hive的分桶采用对分桶字段的值进行哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中</p>
<h3 id="抽样查询"><a href="#抽样查询" class="headerlink" title="抽样查询"></a>抽样查询</h3><p>对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive可以通过对表进行抽样来满足这个需求。</p>
<p>查询表stu_buck中的数据：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from stu_buck tablesample(bucket 1 out of 4 on id);</span><br></pre></td></tr></table></figure>

<h2 id="DML数据操作"><a href="#DML数据操作" class="headerlink" title="DML数据操作"></a>DML数据操作</h2><h3 id="数据导入"><a href="#数据导入" class="headerlink" title="数据导入"></a>数据导入</h3><h4 id="向表中装载数据（Load）"><a href="#向表中装载数据（Load）" class="headerlink" title="向表中装载数据（Load）"></a>向表中装载数据（Load）</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive&gt; load data [local] inpath &#x27;数据的path&#x27; [overwrite] into table student [partition (partcol1=val1,…)];</span><br></pre></td></tr></table></figure>

<p>（1）load data:表示加载数据</p>
<p>（2）local:表示从本地加载数据到hive表；否则从HDFS加载数据到hive表</p>
<p>（3）inpath:表示加载数据的路径</p>
<p>（4）overwrite:表示覆盖表中已有数据，否则表示追加</p>
<p>（5）into table:表示加载到哪张表</p>
<p>（6）student:表示具体的表</p>
<p>（7）partition:表示上传到指定分区</p>
<p>实操案例</p>
<p>0）创建一张表</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create table student(id string, name string) row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure>

<p>1）加载本地文件到hive</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/hive/datas/student.txt&#x27; into table default.student;</span><br></pre></td></tr></table></figure>

<p>2）加载HDFS文件到hive中</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -put /opt/module/hive/datas/student.txt /user/vincent/hive;</span><br><span class="line">hive (default)&gt; load data inpath &#x27;/user/vincent/hive/student.txt&#x27; into table default.student;</span><br></pre></td></tr></table></figure>

<p>3）加载数据覆盖表中已有的数据（overwrite）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -put /opt/module/datas/student.txt /user/vincent/hive;</span><br><span class="line">hive (default)&gt; load data inpath &#x27;/user/vincent/hive/student.txt&#x27; overwrite into table default.student;</span><br></pre></td></tr></table></figure>

<h4 id="通过查询语句向表中插入数据（Insert）"><a href="#通过查询语句向表中插入数据（Insert）" class="headerlink" title="通过查询语句向表中插入数据（Insert）"></a>通过查询语句向表中插入数据（Insert）</h4><p>0）创建一张表</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create table student_par(id int, name string) row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure>

<p>1）基本插入数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; insert into table student_par values(1,&#x27;wangwu&#x27;),(2,&#x27;zhaoliu&#x27;);</span><br></pre></td></tr></table></figure>

<p>2）基本模式插入（根据单张表查询结果）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite table student select id, name from student where month=&#x27;201709&#x27;;</span><br></pre></td></tr></table></figure>

<p>insert into：以追加数据的方式插入到表或分区，原有数据不会删除</p>
<p>insert overwrite：会覆盖表中已存在的数据</p>
<p>注意：insert不支持插入部分字段</p>
<p>3）多表（多分区）插入模式（根据多张表查询结果）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; from student</span><br><span class="line">              insert overwrite table student partition(month=&#x27;201707&#x27;)</span><br><span class="line">              select id, name where month=&#x27;201709&#x27;</span><br><span class="line">              insert overwrite table student partition(month=&#x27;201706&#x27;)</span><br><span class="line">              select id, name where month=&#x27;201709&#x27;;</span><br></pre></td></tr></table></figure>

<h4 id="查询语句中创建表并加载数据（As-Select）"><a href="#查询语句中创建表并加载数据（As-Select）" class="headerlink" title="查询语句中创建表并加载数据（As Select）"></a>查询语句中创建表并加载数据（As Select）</h4><p>根据查询结果创建表（查询的结果会添加到新创建的表中）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create table if not exists student3 as select id, name from student;</span><br></pre></td></tr></table></figure>

<h4 id="创建表时通过Location指定加载数据路径"><a href="#创建表时通过Location指定加载数据路径" class="headerlink" title="创建表时通过Location指定加载数据路径"></a>创建表时通过Location指定加载数据路径</h4><p>1）上传数据到hdfs上</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -mkdir /student;</span><br><span class="line">hive (default)&gt; dfs -put /opt/module/datas/student.txt /student;</span><br></pre></td></tr></table></figure>

<p>2）创建表，并指定在hdfs上的位置</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create external table if not exists student5(</span><br><span class="line">              id int, name string</span><br><span class="line">              )</span><br><span class="line">              row format delimited fields terminated by &#x27;\t&#x27;</span><br><span class="line">              location &#x27;/student;</span><br></pre></td></tr></table></figure>

<p>3）查询数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from student5;</span><br></pre></td></tr></table></figure>

<h4 id="Import数据到指定Hive表中"><a href="#Import数据到指定Hive表中" class="headerlink" title="Import数据到指定Hive表中"></a>Import数据到指定Hive表中</h4><p>注意：先用export导出后，再将数据导入。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; import table student2 from &#x27;/user/hive/warehouse/export/student&#x27;;</span><br></pre></td></tr></table></figure>

<h3 id="数据导出"><a href="#数据导出" class="headerlink" title="数据导出"></a>数据导出</h3><h4 id="Insert导出"><a href="#Insert导出" class="headerlink" title="Insert导出"></a>Insert导出</h4><p>1）将查询的结果导出到本地</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite local directory &#x27;/opt/module/hive/datas/export/student&#x27; select * from student;</span><br></pre></td></tr></table></figure>

<p>2）将查询的结果格式化导出到本地</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive(default)&gt;insert overwrite local directory &#x27;/opt/module/hive/datas/export/student1&#x27; ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27; select * from student;</span><br></pre></td></tr></table></figure>

<p>3）将查询的结果导出到HDFS上(没有local)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite directory &#x27;/user/vincent/student2&#x27;</span><br><span class="line">             ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27; </span><br><span class="line">             select * from student;</span><br></pre></td></tr></table></figure>

<h4 id="Hadoop命令导出到本地"><a href="#Hadoop命令导出到本地" class="headerlink" title="Hadoop命令导出到本地"></a>Hadoop命令导出到本地</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -get /user/hive/warehouse/student/student.txt</span><br><span class="line">/opt/module/datas/export/student3.txt;</span><br></pre></td></tr></table></figure>

<h4 id="Hive-Shell-命令导出"><a href="#Hive-Shell-命令导出" class="headerlink" title="Hive Shell 命令导出"></a>Hive Shell 命令导出</h4><p>基本语法：（hive -f/-e 执行语句或者脚本 &gt; file）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hive]$ bin/hive -e &#x27;select * from default.student;&#x27; &gt;</span><br><span class="line"> /opt/module/hive/datas/export/student4.txt;</span><br></pre></td></tr></table></figure>

<h4 id="Export导出到HDFS上"><a href="#Export导出到HDFS上" class="headerlink" title="Export导出到HDFS上"></a>Export导出到HDFS上</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; export table default.student to</span><br><span class="line"> &#x27;/user/hive/warehouse/export/student&#x27;;</span><br></pre></td></tr></table></figure>

<p>export和import主要用于两个Hadoop平台集群之间Hive表迁移。</p>
<h4 id="Sqoop导出"><a href="#Sqoop导出" class="headerlink" title="Sqoop导出"></a>Sqoop导出</h4><h4 id="清除表中数据（Truncate）"><a href="#清除表中数据（Truncate）" class="headerlink" title="清除表中数据（Truncate）"></a>清除表中数据（Truncate）</h4><p>注意：Truncate只能删除管理表，不能删除外部表中数据。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; truncate table student;</span><br></pre></td></tr></table></figure>

<h2 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h2><figure class="highlight n1ql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> [<span class="keyword">ALL</span> | <span class="keyword">DISTINCT</span>] select_expr, select_expr, ...</span><br><span class="line">  <span class="keyword">FROM</span> table_reference</span><br><span class="line">  [<span class="keyword">WHERE</span> where_condition]</span><br><span class="line">  [<span class="keyword">GROUP</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">  [<span class="keyword">ORDER</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">  [<span class="keyword">CLUSTER</span> <span class="keyword">BY</span> col_list</span><br><span class="line">    | [DISTRIBUTE <span class="keyword">BY</span> col_list] [SORT <span class="keyword">BY</span> col_list]</span><br><span class="line">  ]</span><br><span class="line"> [<span class="keyword">LIMIT</span> <span class="keyword">number</span>]</span><br></pre></td></tr></table></figure>

<h3 id="基本查询（Select…From）"><a href="#基本查询（Select…From）" class="headerlink" title="基本查询（Select…From）"></a>基本查询（Select…From）</h3><p>案例操作</p>
<p>0）原始数据</p>
<p>dept:</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">10</span>	ACCOUNTING	<span class="number">1700</span></span><br><span class="line"><span class="attribute">20</span>	RESEARCH	<span class="number">1800</span></span><br><span class="line"><span class="attribute">30</span>	SALES	<span class="number">1900</span></span><br><span class="line"><span class="attribute">40</span>	OPERATIONS	<span class="number">1700</span></span><br></pre></td></tr></table></figure>

<p>emp：</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">7369</span>	SMITH	CLERK	<span class="number">7902</span>	<span class="number">1980</span>-<span class="number">12</span>-<span class="number">17</span>	<span class="number">800</span>.<span class="number">00</span>		<span class="number">20</span></span><br><span class="line"><span class="attribute">7499</span>	ALLEN	SALESMAN	<span class="number">7698</span>	<span class="number">1981</span>-<span class="number">2</span>-<span class="number">20</span>	<span class="number">1600</span>.<span class="number">00</span>	<span class="number">300</span>.<span class="number">00</span>	<span class="number">30</span></span><br><span class="line"><span class="attribute">7521</span>	WARD	SALESMAN	<span class="number">7698</span>	<span class="number">1981</span>-<span class="number">2</span>-<span class="number">22</span>	<span class="number">1250</span>.<span class="number">00</span>	<span class="number">500</span>.<span class="number">00</span>	<span class="number">30</span></span><br><span class="line"><span class="attribute">7566</span>	JONES	MANAGER	<span class="number">7839</span>	<span class="number">1981</span>-<span class="number">4</span>-<span class="number">2</span>	<span class="number">2975</span>.<span class="number">00</span>		<span class="number">20</span></span><br><span class="line"><span class="attribute">7654</span>	MARTIN	SALESMAN	<span class="number">7698</span>	<span class="number">1981</span>-<span class="number">9</span>-<span class="number">28</span>	<span class="number">1250</span>.<span class="number">00</span>	<span class="number">1400</span>.<span class="number">00</span>	<span class="number">30</span></span><br><span class="line"><span class="attribute">7698</span>	BLAKE	MANAGER	<span class="number">7839</span>	<span class="number">1981</span>-<span class="number">5</span>-<span class="number">1</span>	<span class="number">2850</span>.<span class="number">00</span>		<span class="number">30</span></span><br><span class="line"><span class="attribute">7782</span>	CLARK	MANAGER	<span class="number">7839</span>	<span class="number">1981</span>-<span class="number">6</span>-<span class="number">9</span>	<span class="number">2450</span>.<span class="number">00</span>		<span class="number">10</span></span><br><span class="line"><span class="attribute">7788</span>	SCOTT	ANALYST	<span class="number">7566</span>	<span class="number">1987</span>-<span class="number">4</span>-<span class="number">19</span>	<span class="number">3000</span>.<span class="number">00</span>		<span class="number">20</span></span><br><span class="line"><span class="attribute">7839</span>	KING	PRESIDENT		<span class="number">1981</span>-<span class="number">11</span>-<span class="number">17</span>	<span class="number">5000</span>.<span class="number">00</span>		<span class="number">10</span></span><br><span class="line"><span class="attribute">7844</span>	TURNER	SALESMAN	<span class="number">7698</span>	<span class="number">1981</span>-<span class="number">9</span>-<span class="number">8</span>	<span class="number">1500</span>.<span class="number">00</span>	<span class="number">0</span>.<span class="number">00</span>	<span class="number">30</span></span><br><span class="line"><span class="attribute">7876</span>	ADAMS	CLERK	<span class="number">7788</span>	<span class="number">1987</span>-<span class="number">5</span>-<span class="number">23</span>	<span class="number">1100</span>.<span class="number">00</span>		<span class="number">20</span></span><br><span class="line"><span class="attribute">7900</span>	JAMES	CLERK	<span class="number">7698</span>	<span class="number">1981</span>-<span class="number">12</span>-<span class="number">3</span>	<span class="number">950</span>.<span class="number">00</span>		<span class="number">30</span></span><br><span class="line"><span class="attribute">7902</span>	FORD	ANALYST	<span class="number">7566</span>	<span class="number">1981</span>-<span class="number">12</span>-<span class="number">3</span>	<span class="number">3000</span>.<span class="number">00</span>		<span class="number">20</span></span><br><span class="line"><span class="attribute">7934</span>	MILLER	CLERK	<span class="number">7782</span>	<span class="number">1982</span>-<span class="number">1</span>-<span class="number">23</span>	<span class="number">1300</span>.<span class="number">00</span>		<span class="number">10</span></span><br></pre></td></tr></table></figure>

<p>1）创建部门表</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create external table if not exists default.dept(</span><br><span class="line">deptno int,</span><br><span class="line">dname string,</span><br><span class="line">loc int</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure>

<p>2）创建员工表</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create external table if not exists default.emp(</span><br><span class="line">empno int,</span><br><span class="line">ename string,</span><br><span class="line">job string,</span><br><span class="line">mgr int,</span><br><span class="line">hiredate string, </span><br><span class="line">sal double, </span><br><span class="line">comm double,</span><br><span class="line">deptno int)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure>

<p>3）导入数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into table dept;</span><br><span class="line">load data local inpath &#x27;/opt/module/datas/emp.txt&#x27; into table emp;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select empno, ename from emp;</span><br><span class="line">select * from emp;</span><br></pre></td></tr></table></figure>

<p>注意：</p>
<p>（1）SQL 语言大小写不敏感。 </p>
<p>（2）SQL 可以写在一行或者多行</p>
<p>（3）关键字不能被缩写也不能分行</p>
<p>（4）各子句一般要分行写。</p>
<p>（5）使用缩进提高语句的可读性。</p>
<h4 id="列别名"><a href="#列别名" class="headerlink" title="列别名"></a>列别名</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select ename AS name, deptno dn from emp;</span><br></pre></td></tr></table></figure>

<h4 id="算术运算符"><a href="#算术运算符" class="headerlink" title="算术运算符"></a>算术运算符</h4><table>
<thead>
<tr>
<th>运算符</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>A+B</td>
<td>A和B 相加</td>
</tr>
<tr>
<td>A-B</td>
<td>A减去B</td>
</tr>
<tr>
<td>A*B</td>
<td>A和B 相乘</td>
</tr>
<tr>
<td>A/B</td>
<td>A除以B</td>
</tr>
<tr>
<td>A%B</td>
<td>A对B取余</td>
</tr>
<tr>
<td>A&amp;B</td>
<td>A和B按位取与</td>
</tr>
<tr>
<td>A|B</td>
<td>A和B按位取或</td>
</tr>
<tr>
<td>A^B</td>
<td>A和B按位取异或</td>
</tr>
<tr>
<td>~A</td>
<td>A按位取反</td>
</tr>
</tbody></table>
<h4 id="常用函数"><a href="#常用函数" class="headerlink" title="常用函数"></a>常用函数</h4><p>求总行数（count）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select count(*) cnt from emp;</span><br></pre></td></tr></table></figure>

<p>求工资的最大值（max）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select max(sal) max_sal from emp;</span><br></pre></td></tr></table></figure>

<p>求工资的最小值（min）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select min(sal) min_sal from emp;</span><br></pre></td></tr></table></figure>

<p>求工资的总和（sum）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select sum(sal) sum_sal from emp; </span><br></pre></td></tr></table></figure>

<p>求工资的平均值（avg）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select avg(sal) avg_sal from emp;</span><br></pre></td></tr></table></figure>

<h4 id="Limit语句"><a href="#Limit语句" class="headerlink" title="Limit语句"></a>Limit语句</h4><p>LIMIT子句用于限制返回的行数。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from emp limit 5;</span><br><span class="line">hive (default)&gt; select * from emp limit 2,3;</span><br></pre></td></tr></table></figure>

<h4 id="Where语句"><a href="#Where语句" class="headerlink" title="Where语句"></a>Where语句</h4><p>WHERE子句紧随FROM子句，where子句中不能使用字段别名。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where sal &gt; 1000;</span><br></pre></td></tr></table></figure>

<h4 id="比较运算符（Between-In-Is-Null）"><a href="#比较运算符（Between-In-Is-Null）" class="headerlink" title="比较运算符（Between/In/Is Null）"></a>比较运算符（Between/In/Is Null）</h4><table>
<thead>
<tr>
<th align="left">操作符</th>
<th>支持的数据类型</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td align="left">A=B</td>
<td>基本数据类型</td>
<td>如果A等于B则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td align="left">A&lt;=&gt;B</td>
<td>基本数据类型</td>
<td>如果A和B都为NULL，则返回TRUE，如果一边为NULL，返回False</td>
</tr>
<tr>
<td align="left">A&lt;&gt;B, A!=B</td>
<td>基本数据类型</td>
<td>A或者B为NULL则返回NULL；如果A不等于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td align="left">A&lt;B</td>
<td>基本数据类型</td>
<td>A或者B为NULL，则返回NULL；如果A小于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td align="left">A&lt;=B</td>
<td>基本数据类型</td>
<td>A或者B为NULL，则返回NULL；如果A小于等于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td align="left">A&gt;B</td>
<td>基本数据类型</td>
<td>A或者B为NULL，则返回NULL；如果A大于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td align="left">A&gt;=B</td>
<td>基本数据类型</td>
<td>A或者B为NULL，则返回NULL；如果A大于等于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td align="left">A [NOT] BETWEEN B AND C</td>
<td>基本数据类型</td>
<td>如果A，B或者C任一为NULL，则结果为NULL。如果A的值大于等于B而且小于或等于C，则结果为TRUE，反之为FALSE。如果使用NOT关键字则可达到相反的效果。</td>
</tr>
<tr>
<td align="left">A IS NULL</td>
<td>所有数据类型</td>
<td>如果A等于NULL，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td align="left">A IS NOT NULL</td>
<td>所有数据类型</td>
<td>如果A不等于NULL，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td align="left">IN(数值1, 数值2)</td>
<td>所有数据类型</td>
<td>使用 IN运算显示列表中的值</td>
</tr>
<tr>
<td align="left">A [NOT] LIKE B</td>
<td>STRING 类型</td>
<td>B是一个SQL下的简单正则表达式，也叫通配符模式，如果A与其匹配的话，则返回TRUE；反之返回FALSE。B的表达式说明如下：‘x%’表示A必须以字母‘x’开头，‘%x’表示A必须以字母’x’结尾，而‘%x%’表示A包含有字母’x’,可以位于开头，结尾或者字符串中间。如果使用NOT关键字则可达到相反的效果。</td>
</tr>
<tr>
<td align="left">A RLIKE B, A REGEXP B</td>
<td>STRING 类型</td>
<td>B是基于java的正则表达式，如果A与其匹配，则返回TRUE；反之返回FALSE。匹配使用的是JDK中的正则表达式接口实现的，因为正则也依据其中的规则。例如，正则表达式必须和整个字符串A相匹配，而不是只需与其字符串匹配。</td>
</tr>
</tbody></table>
<p>案例实操</p>
<p>（1）查询出薪水等于5000的所有员工</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where sal = 5000;</span><br></pre></td></tr></table></figure>

<p>（2）查询工资在500到1000的员工信息</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where sal between 500 and 1000;</span><br></pre></td></tr></table></figure>

<p>（3）查询comm为空的所有员工信息</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where comm is null;</span><br></pre></td></tr></table></figure>

<p>（4）查询工资是1500或5000的员工信息</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where sal IN (1500, 5000);</span><br></pre></td></tr></table></figure>

<h4 id="Like和RLike"><a href="#Like和RLike" class="headerlink" title="Like和RLike"></a>Like和RLike</h4><p>1）使用LIKE运算选择类似的值</p>
<p>2）选择条件可以包含字符或数字</p>
<p>% 代表零个或多个字符(任意个字符)。</p>
<p>_ 代表一个字符。</p>
<p>3）RLIKE子句</p>
<p>RLIKE子句是Hive中这个功能的一个扩展，其可以通过Java的正则表达式这个更强大的语言来指定匹配条件。</p>
<p>4）案例实操</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where ename LIKE &#x27;A%&#x27;;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where ename LIKE &#x27;_A%&#x27;;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where ename  RLIKE &#x27;[A]&#x27;;</span><br></pre></td></tr></table></figure>

<h4 id="逻辑运算符（And-Or-Not）"><a href="#逻辑运算符（And-Or-Not）" class="headerlink" title="逻辑运算符（And/Or/Not）"></a>逻辑运算符（And/Or/Not）</h4><table>
<thead>
<tr>
<th>操作符</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>AND</td>
<td>逻辑并</td>
</tr>
<tr>
<td>OR</td>
<td>逻辑或</td>
</tr>
<tr>
<td>NOT</td>
<td>逻辑否</td>
</tr>
</tbody></table>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from emp where sal&gt;1000 and deptno=30;</span><br><span class="line">hive (default)&gt; select * from emp where sal&gt;1000 or deptno=30;</span><br><span class="line">hive (default)&gt; select * from emp where deptno not IN(30, 20);</span><br></pre></td></tr></table></figure>

<h3 id="分组"><a href="#分组" class="headerlink" title="分组"></a>分组</h3><h4 id="Group-By语句"><a href="#Group-By语句" class="headerlink" title="Group By语句"></a>Group By语句</h4><p>GROUP BY语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作。</p>
<p>计算emp表每个部门的平均工资</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select t.deptno, avg(t.sal) avg_sal from emp t group by t.deptno;</span><br></pre></td></tr></table></figure>

<p>计算emp每个部门中每个岗位的最高薪水</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select t.deptno, t.job, max(t.sal) max_sal from emp t group by t.deptno, t.job;</span><br></pre></td></tr></table></figure>

<h4 id="Having语句"><a href="#Having语句" class="headerlink" title="Having语句"></a>Having语句</h4><p>1）having与where不同点</p>
<p>（1）where后面不能写分组函数，而having后面可以使用分组函数。</p>
<p>（2）having只用于group by分组统计语句。</p>
<p>求平均薪水大于2000的部门</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select deptno, avg(sal) avg_sal from emp group by deptno having avg_sal &gt; 2000;</span><br></pre></td></tr></table></figure>

<h3 id="Join语句"><a href="#Join语句" class="headerlink" title="Join语句"></a>Join语句</h3><h4 id="等值Join"><a href="#等值Join" class="headerlink" title="等值Join"></a>等值Join</h4><p>Hive支持通常的SQL JOIN语句，但是只支持等值连接，不支持非等值连接。</p>
<p>根据员工表和部门表中的部门编号相等，查询员工编号、员工名称和部门名称：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; </span><br><span class="line">select e.empno, e.ename, d.deptno, d.dname </span><br><span class="line">from emp e </span><br><span class="line">join dept d </span><br><span class="line">on e.deptno = d.deptno;</span><br></pre></td></tr></table></figure>

<h4 id="表的别名"><a href="#表的别名" class="headerlink" title="表的别名"></a>表的别名</h4><p>1）好处</p>
<p>（1）使用别名可以简化查询。</p>
<p>（2）使用表名前缀可以提高执行效率。</p>
<p>2）案例实操</p>
<p>合并员工表和部门表</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select e.empno, e.ename, d.deptno from emp e join dept d on e.deptno = d.deptno;</span><br></pre></td></tr></table></figure>

<h4 id="内连接"><a href="#内连接" class="headerlink" title="内连接"></a>内连接</h4><p>只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select e.empno, e.ename, d.deptno from emp e join dept d on e.deptno</span><br><span class="line"> = d.deptno;</span><br></pre></td></tr></table></figure>

<h4 id="左外连接"><a href="#左外连接" class="headerlink" title="左外连接"></a>左外连接</h4><p>JOIN操作符左边表中符合WHERE子句的所有记录将会被返回。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select e.empno, e.ename, d.deptno from emp e left join dept d on e.deptno = d.deptno;</span><br></pre></td></tr></table></figure>

<h4 id="右外连接"><a href="#右外连接" class="headerlink" title="右外连接"></a>右外连接</h4><p>JOIN操作符右边表中符合WHERE子句的所有记录将会被返回。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select e.empno, e.ename, d.deptno from emp e right join dept d on e.deptno = d.deptno;</span><br></pre></td></tr></table></figure>

<h4 id="满外连接"><a href="#满外连接" class="headerlink" title="满外连接"></a>满外连接</h4><p>将会返回所有表中符合WHERE语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用NULL值替代。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select e.empno, e.ename, d.deptno from emp e full join dept d on e.deptno = d.deptno;</span><br></pre></td></tr></table></figure>

<h4 id="多表连接"><a href="#多表连接" class="headerlink" title="多表连接"></a>多表连接</h4><p>连接 n个表，至少需要n-1个连接条件。例如：连接三个表，至少需要两个连接条件。</p>
<p>location.txt</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="number">1700	</span><span class="string">Beijing</span></span><br><span class="line"><span class="number">1800	</span><span class="string">London</span></span><br><span class="line"><span class="number">1900	</span><span class="string">Tokyo</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table if not exists location(</span><br><span class="line">loc int,</span><br><span class="line">loc_name string</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/location.txt&#x27; into table location;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt;SELECT e.ename, d.dname, l.loc_name</span><br><span class="line">FROM   emp e </span><br><span class="line">JOIN   dept d</span><br><span class="line">ON     d.deptno = e.deptno </span><br><span class="line">JOIN   location l</span><br><span class="line">ON     d.loc = l.loc;</span><br></pre></td></tr></table></figure>

<p>大多数情况下，Hive会对每对JOIN连接对象启动一个MapReduce任务。本例中会首先启动一个MapReduce job对表e和表d进行连接操作，然后会再启动一个MapReduce job将第一个MapReduce job的输出和表l;进行连接操作。</p>
<h4 id="笛卡尔积"><a href="#笛卡尔积" class="headerlink" title="笛卡尔积"></a>笛卡尔积</h4><p>1）笛卡尔集会在下面条件下产生</p>
<p>（1）省略连接条件</p>
<p>（2）连接条件无效</p>
<p>（3）所有表中的所有行互相连接</p>
<p>2）案例实操</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select empno, dname from emp, dept;</span><br></pre></td></tr></table></figure>

<h3 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h3><h4 id="全局排序（Order-By）"><a href="#全局排序（Order-By）" class="headerlink" title="全局排序（Order By）"></a>全局排序（Order By）</h4><p><strong>Order By：全局排序，只有一个Reducer。</strong></p>
<p>1）使用ORDER BY子句排序</p>
<p>ASC（ascend）: 升序（默认）</p>
<p>DESC（descend）: 降序</p>
<p>2）ORDER BY子句在SELECT语句的结尾</p>
<p>3）案例实操</p>
<p>（1）查询员工信息按工资升序排列</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from emp order by sal;</span><br></pre></td></tr></table></figure>

<p>（2）查询员工信息按工资降序排列</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from emp order by sal desc;</span><br></pre></td></tr></table></figure>

<h4 id="按照别名排序"><a href="#按照别名排序" class="headerlink" title="按照别名排序"></a>按照别名排序</h4><p>按照员工薪水的2倍排序</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select ename, sal*2 twosal from emp order by twosal;</span><br></pre></td></tr></table></figure>

<h4 id="多个列排序"><a href="#多个列排序" class="headerlink" title="多个列排序"></a>多个列排序</h4><p>按照部门和工资升序排序</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select ename, deptno, sal from emp order by deptno, sal ;</span><br></pre></td></tr></table></figure>

<h4 id="每个MapReduce内部排序（Sort-By）"><a href="#每个MapReduce内部排序（Sort-By）" class="headerlink" title="每个MapReduce内部排序（Sort By）"></a>每个MapReduce内部排序（Sort By）</h4><p><strong>Sort By：对于大规模的数据集order by的效率非常低。在很多情况下，并不需要全局排序，此时可以使用sort by。</strong></p>
<p><strong>Sort by为每个reducer产生一个排序文件。每个Reducer内部进行排序，对全局结果集来说不是排序。</strong></p>
<p>1）设置reduce个数</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; set mapreduce.job.reduces=3;</span><br></pre></td></tr></table></figure>

<p>2）查看设置reduce个数</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; set mapreduce.job.reduces;</span><br></pre></td></tr></table></figure>

<p>3）根据部门编号降序查看员工信息</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from emp sort by deptno desc;</span><br></pre></td></tr></table></figure>

<p>4）将查询结果导入到文件中（按照部门编号降序排序）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite local directory &#x27;/opt/module/hive/datas/sortby-result&#x27; select * from emp sort by deptno desc;</span><br></pre></td></tr></table></figure>

<h4 id="分区排序（Distribute-By）"><a href="#分区排序（Distribute-By）" class="headerlink" title="分区排序（Distribute By）"></a>分区排序（Distribute By）</h4><p>Distribute By： 在有些情况下，我们需要<strong>控制某个特定行应该到哪个reducer</strong>，通常是为了进行后续的聚集操作。distribute by子句可以做这件事。distribute by类似MR中partition（自定义分区），进行分区，结合sort by使用。 <strong>对于distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果。</strong></p>
<p>案例实操</p>
<p>先按照部门编号分区，再按照员工编号降序排序。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; set mapreduce.job.reduces=3;</span><br><span class="line">hive (default)&gt; insert overwrite local directory &#x27;/opt/module/hive/datas/distribute-result&#x27; select * from emp distribute by deptno sort by empno desc;</span><br></pre></td></tr></table></figure>

<h4 id="Cluster-By"><a href="#Cluster-By" class="headerlink" title="Cluster By"></a>Cluster By</h4><p><strong>当distribute by和sort by字段相同时，可以使用cluster by方式。</strong></p>
<p>cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC。</p>
<p>以下两种写法等价：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from emp cluster by deptno;</span><br><span class="line">hive (default)&gt; select * from emp distribute by deptno sort by deptno;</span><br></pre></td></tr></table></figure>

<p>注意：按照部门编号分区，不一定就是固定死的数值，可以是20号和30号部门分到一个分区里面去。</p>
<h2 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h2><p>sql写顺序：select … from… where…. group by… having… order by.. limit</p>
<p>sql执行顺序：from… where…group by… having…. select … order by… limit</p>
<h3 id="系统内置函数"><a href="#系统内置函数" class="headerlink" title="系统内置函数"></a>系统内置函数</h3><p>1）查看系统自带的函数</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive&gt; show functions;</span><br></pre></td></tr></table></figure>

<p>2）显示自带的函数的用法</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive&gt; desc function upper;</span><br></pre></td></tr></table></figure>

<p>3）详细显示自带的函数的用法</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive&gt; desc function extended upper;</span><br></pre></td></tr></table></figure>

<h3 id="常用内置函数"><a href="#常用内置函数" class="headerlink" title="常用内置函数"></a>常用内置函数</h3><h4 id="空字段赋值（nvl）"><a href="#空字段赋值（nvl）" class="headerlink" title="空字段赋值（nvl）"></a>空字段赋值（nvl）</h4><p>NVL：给值为NULL的数据赋值，它的格式是NVL( value，default_value)。它的功能是如果value为NULL，则NVL函数返回default_value的值，否则返回value的值；如果两个参数都为NULL ，则返回NULL。</p>
<p>如果员工的comm为NULL，则用-1代替：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select comm, nvl(comm, -1) from emp;</span><br></pre></td></tr></table></figure>

<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">OK</span></span><br><span class="line"><span class="attribute">comm</span>    _c<span class="number">1</span></span><br><span class="line"><span class="attribute">NULL</span>    -<span class="number">1</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">300</span>.<span class="number">0</span>   <span class="number">300</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">500</span>.<span class="number">0</span>   <span class="number">500</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">NULL</span>    -<span class="number">1</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">1400</span>.<span class="number">0</span>  <span class="number">1400</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">NULL</span>    -<span class="number">1</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">NULL</span>    -<span class="number">1</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">NULL</span>    -<span class="number">1</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">NULL</span>    -<span class="number">1</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">0</span>.<span class="number">0</span>     <span class="number">0</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">NULL</span>    -<span class="number">1</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">NULL</span>    -<span class="number">1</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">NULL</span>    -<span class="number">1</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">NULL</span>    -<span class="number">1</span>.<span class="number">0</span></span><br></pre></td></tr></table></figure>

<p>如果员工的comm为NULL，则用领导id代替：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select comm, nvl(comm,mgr) from emp;</span><br></pre></td></tr></table></figure>

<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">OK</span></span><br><span class="line"><span class="attribute">comm</span>    _c<span class="number">1</span></span><br><span class="line"><span class="attribute">NULL</span>    <span class="number">7902</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">300</span>.<span class="number">0</span>   <span class="number">300</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">500</span>.<span class="number">0</span>   <span class="number">500</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">NULL</span>    <span class="number">7839</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">1400</span>.<span class="number">0</span>  <span class="number">1400</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">NULL</span>    <span class="number">7839</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">NULL</span>    <span class="number">7839</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">NULL</span>    <span class="number">7566</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">NULL</span>    NULL</span><br><span class="line"><span class="attribute">0</span>.<span class="number">0</span>     <span class="number">0</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">NULL</span>    <span class="number">7788</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">NULL</span>    <span class="number">7698</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">NULL</span>    <span class="number">7566</span>.<span class="number">0</span></span><br><span class="line"><span class="attribute">NULL</span>    <span class="number">7782</span>.<span class="number">0</span></span><br></pre></td></tr></table></figure>

<h4 id="CASE-WHEN"><a href="#CASE-WHEN" class="headerlink" title="CASE WHEN"></a>CASE WHEN</h4><p>1）数据准备</p>
<table>
<thead>
<tr>
<th>name</th>
<th>dept_id</th>
<th>sex</th>
</tr>
</thead>
<tbody><tr>
<td>悟空</td>
<td>A</td>
<td>男</td>
</tr>
<tr>
<td>大海</td>
<td>A</td>
<td>男</td>
</tr>
<tr>
<td>宋宋</td>
<td>B</td>
<td>男</td>
</tr>
<tr>
<td>凤姐</td>
<td>A</td>
<td>女</td>
</tr>
<tr>
<td>婷姐</td>
<td>B</td>
<td>女</td>
</tr>
<tr>
<td>婷婷</td>
<td>B</td>
<td>女</td>
</tr>
</tbody></table>
<p>2）需求</p>
<p>求出不同部门男女各多少人。结果如下：</p>
<p>A   2    1</p>
<p>B   1    2</p>
<p>3）创建本地emp_sex.txt，导入数据</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line">悟空	<span class="selector-tag">A</span>	男</span><br><span class="line">大海	<span class="selector-tag">A</span>	男</span><br><span class="line">宋宋	<span class="selector-tag">B</span>	男</span><br><span class="line">凤姐	<span class="selector-tag">A</span>	女</span><br><span class="line">婷姐	<span class="selector-tag">B</span>	女</span><br><span class="line">婷婷	<span class="selector-tag">B</span>	女</span><br></pre></td></tr></table></figure>

<p>4）创建hive表并导入数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table emp_sex(</span><br><span class="line">name string, </span><br><span class="line">dept_id string, </span><br><span class="line">sex string) </span><br><span class="line">row format delimited fields terminated by &quot;\t&quot;;</span><br><span class="line">load data local inpath &#x27;/opt/module/hive/datas/emp_sex.txt&#x27; into table emp_sex;</span><br></pre></td></tr></table></figure>

<p>5）按需求查询数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select </span><br><span class="line"> dept_id,</span><br><span class="line"> sum(case sex when &#x27;男&#x27; then 1 else 0 end) male_count,</span><br><span class="line"> sum(case sex when &#x27;女&#x27; then 1 else 0 end) female_count</span><br><span class="line">from emp_sex</span><br><span class="line">group by dept_id;</span><br></pre></td></tr></table></figure>

<h4 id="行转列"><a href="#行转列" class="headerlink" title="行转列"></a>行转列</h4><p>1）相关函数说明</p>
<p>CONCAT(string A/col, string B/col…)：返回输入字符串连接后的结果，支持任意个输入字符串;</p>
<p>CONCAT_WS(separator, str1, str2,…)：它是一个特殊形式的 CONCAT()。第一个参数剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是 NULL，返回值也将为 NULL。这个函数会跳过分隔符参数后的任何 NULL 和空字符串。分隔符将被加到被连接的字符串之间;</p>
<p>COLLECT_SET(col)：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生array类型字段。</p>
<p>2）数据准备</p>
<table>
<thead>
<tr>
<th>name</th>
<th>constellation</th>
<th>blood_type</th>
</tr>
</thead>
<tbody><tr>
<td>孙悟空</td>
<td>白羊座</td>
<td>A</td>
</tr>
<tr>
<td>大海</td>
<td>射手座</td>
<td>A</td>
</tr>
<tr>
<td>宋宋</td>
<td>白羊座</td>
<td>B</td>
</tr>
<tr>
<td>猪八戒</td>
<td>白羊座</td>
<td>A</td>
</tr>
<tr>
<td>凤姐</td>
<td>射手座</td>
<td>A</td>
</tr>
<tr>
<td>苍老师</td>
<td>白羊座</td>
<td>B</td>
</tr>
</tbody></table>
<p>3）需求</p>
<p>把星座和血型一样的人归类到一起。结果如下：</p>
<p>射手座,A       大海|凤姐</p>
<p>白羊座,A       孙悟空|猪八戒</p>
<p>白羊座,B       宋宋|苍老师</p>
<p>4）创建本地constellation.txt，导入数据</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line">孙悟空	白羊座	<span class="selector-tag">A</span></span><br><span class="line">大海	射手座	<span class="selector-tag">A</span></span><br><span class="line">宋宋	白羊座	<span class="selector-tag">B</span></span><br><span class="line">猪八戒	白羊座	<span class="selector-tag">A</span></span><br><span class="line">凤姐	射手座	<span class="selector-tag">A</span></span><br><span class="line">苍老师 白羊座 <span class="selector-tag">B</span></span><br></pre></td></tr></table></figure>

<p>5）创建hive表并导入数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table person_info(</span><br><span class="line">name string, </span><br><span class="line">constellation string, </span><br><span class="line">blood_type string) </span><br><span class="line">row format delimited fields terminated by &quot;\t&quot;;</span><br><span class="line">load data local inpath &quot;/opt/module/hive/datas/constellation.txt&quot; into table person_info;</span><br></pre></td></tr></table></figure>

<p>6）按需求查询数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select</span><br><span class="line">  t1.base,</span><br><span class="line">  concat_ws(&#x27;|&#x27;, collect_set(t1.name)) name</span><br><span class="line">from </span><br><span class="line">	(select</span><br><span class="line">    name,</span><br><span class="line">    concat(constellation, &quot;,&quot;, blood_type) base</span><br><span class="line">  from</span><br><span class="line">    person_info) t1</span><br><span class="line">group by t1.base;</span><br></pre></td></tr></table></figure>

<h4 id="列转行"><a href="#列转行" class="headerlink" title="列转行"></a>列转行</h4><p>1）函数说明</p>
<p><strong>EXPLODE(col)：将hive一列中复杂的array或者map结构拆分成多行。</strong></p>
<p>LATERAL VIEW</p>
<p>用法：LATERAL VIEW udtf(expression) tableAlias AS columnAlias</p>
<p>解释：用于和split, explode等UDTF一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。</p>
<p>2）数据准备</p>
<table>
<thead>
<tr>
<th>movie</th>
<th>category</th>
</tr>
</thead>
<tbody><tr>
<td>《疑犯追踪》</td>
<td>悬疑,动作,科幻,剧情</td>
</tr>
<tr>
<td>《Lie to me》</td>
<td>悬疑,警匪,动作,心理,剧情</td>
</tr>
<tr>
<td>《战狼2》</td>
<td>战争,动作,灾难</td>
</tr>
</tbody></table>
<p>3）需求</p>
<p>将电影分类中的数组数据展开。结果如下：</p>
<figure class="highlight applescript"><table><tr><td class="code"><pre><span class="line">《疑犯追踪》    悬疑</span><br><span class="line">《疑犯追踪》    动作</span><br><span class="line">《疑犯追踪》    科幻</span><br><span class="line">《疑犯追踪》    剧情</span><br><span class="line">《Lie <span class="keyword">to</span> <span class="keyword">me</span>》  悬疑</span><br><span class="line">《Lie <span class="keyword">to</span> <span class="keyword">me</span>》  警匪</span><br><span class="line">《Lie <span class="keyword">to</span> <span class="keyword">me</span>》  动作</span><br><span class="line">《Lie <span class="keyword">to</span> <span class="keyword">me</span>》  心理</span><br><span class="line">《Lie <span class="keyword">to</span> <span class="keyword">me</span>》  剧情</span><br><span class="line">《战狼<span class="number">2</span>》     战争</span><br><span class="line">《战狼<span class="number">2</span>》     动作</span><br><span class="line">《战狼<span class="number">2</span>》     灾难</span><br></pre></td></tr></table></figure>

<p>4）创建本地movie.txt，导入数据</p>
<figure class="highlight applescript"><table><tr><td class="code"><pre><span class="line">《疑犯追踪》	悬疑,动作,科幻,剧情</span><br><span class="line"></span><br><span class="line">《Lie <span class="keyword">to</span> <span class="keyword">me</span>》	悬疑,警匪,动作,心理,剧情</span><br><span class="line"></span><br><span class="line">《战狼<span class="number">2</span>》	战争,动作,灾难</span><br></pre></td></tr></table></figure>

<p>5）创建hive表并导入数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table movie_info(</span><br><span class="line">  movie string, </span><br><span class="line">  category string) </span><br><span class="line">row format delimited fields terminated by &quot;\t&quot;;</span><br><span class="line">load data local inpath &quot;/opt/module/hive/datas/movie_info.txt&quot; into table movie_info;</span><br></pre></td></tr></table></figure>

<p>6）按需求查询数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select </span><br><span class="line">	m.movie, </span><br><span class="line">	tbl.cate</span><br><span class="line">from </span><br><span class="line">	movie_info m</span><br><span class="line">lateral view </span><br><span class="line">	explode(split(category, &quot;,&quot;)) tbl as cate;</span><br></pre></td></tr></table></figure>

<h4 id="窗口函数（开窗函数）"><a href="#窗口函数（开窗函数）" class="headerlink" title="窗口函数（开窗函数）"></a>窗口函数（开窗函数）</h4><p>1）相关函数说明</p>
<p><strong>OVER()：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变化而变化。</strong></p>
<p>CURRENT ROW：当前行</p>
<p>n PRECEDING：往前n行数据</p>
<p>n FOLLOWING：往后n行数据</p>
<p>UNBOUNDED：起点，UNBOUNDED PRECEDING 表示从前面的起点开始， UNBOUNDED FOLLOWING表示到后面的终点</p>
<p>LAG(col,n,default_val)：往前第n行数据</p>
<p>LEAD(col,n, default_val)：往后第n行数据</p>
<p>first_value(col)：取分组内排序后，<strong>截止到当前行</strong>，第一个值</p>
<p>last_value(col)：取分组内排序后，<strong>截止到当前行</strong>，最后一个值</p>
<p>sum(col)：组内求和，<strong>截止到当前行</strong></p>
<p>NTILE(n)：把有序窗口的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号。注意：n必须为int类型</p>
<p>CUME_DIST()：返回小于等于当前值的行数/分组内总行数</p>
<p>percent_rank()：分组内当前行的RANK值-1/分组内总行数-1</p>
<p>2）数据准备：name，orderdate，cost</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">jack</span>,<span class="number">2017</span>-<span class="number">01</span>-<span class="number">01</span>,<span class="number">10</span></span><br><span class="line"><span class="attribute">tony</span>,<span class="number">2017</span>-<span class="number">01</span>-<span class="number">02</span>,<span class="number">15</span></span><br><span class="line"><span class="attribute">jack</span>,<span class="number">2017</span>-<span class="number">02</span>-<span class="number">03</span>,<span class="number">23</span></span><br><span class="line"><span class="attribute">tony</span>,<span class="number">2017</span>-<span class="number">01</span>-<span class="number">04</span>,<span class="number">29</span></span><br><span class="line"><span class="attribute">jack</span>,<span class="number">2017</span>-<span class="number">01</span>-<span class="number">05</span>,<span class="number">46</span></span><br><span class="line"><span class="attribute">jack</span>,<span class="number">2017</span>-<span class="number">04</span>-<span class="number">06</span>,<span class="number">42</span></span><br><span class="line"><span class="attribute">tony</span>,<span class="number">2017</span>-<span class="number">01</span>-<span class="number">07</span>,<span class="number">50</span></span><br><span class="line"><span class="attribute">jack</span>,<span class="number">2017</span>-<span class="number">01</span>-<span class="number">08</span>,<span class="number">55</span></span><br><span class="line"><span class="attribute">mart</span>,<span class="number">2017</span>-<span class="number">04</span>-<span class="number">08</span>,<span class="number">62</span></span><br><span class="line"><span class="attribute">mart</span>,<span class="number">2017</span>-<span class="number">04</span>-<span class="number">09</span>,<span class="number">68</span></span><br><span class="line"><span class="attribute">neil</span>,<span class="number">2017</span>-<span class="number">05</span>-<span class="number">10</span>,<span class="number">12</span></span><br><span class="line"><span class="attribute">mart</span>,<span class="number">2017</span>-<span class="number">04</span>-<span class="number">11</span>,<span class="number">75</span></span><br><span class="line"><span class="attribute">neil</span>,<span class="number">2017</span>-<span class="number">06</span>-<span class="number">12</span>,<span class="number">80</span></span><br><span class="line"><span class="attribute">mart</span>,<span class="number">2017</span>-<span class="number">04</span>-<span class="number">13</span>,<span class="number">94</span></span><br></pre></td></tr></table></figure>

<p>3）需求</p>
<p>（1）查询在2017年4月份购买过的顾客及总人数</p>
<p>（2）查询顾客的购买明细及月购买总额</p>
<p>（3）上述的场景, 将每个顾客的cost按照日期进行累加</p>
<p>（4）查询每个顾客上次的购买时间</p>
<p>（5）查询前20%时间的订单信息</p>
<p>4）创建本地business.txt，导入数据</p>
<p>5）创建hive表并导入数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table business(</span><br><span class="line">name string, </span><br><span class="line">orderdate string,</span><br><span class="line">cost int</span><br><span class="line">) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;,&#x27;;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">load data local inpath &quot;/opt/module/hive/datas/business.txt&quot; into table business;</span><br></pre></td></tr></table></figure>

<p>6）按需求查询数据</p>
<p>（1） 查询在2017年4月份购买过的顾客及总人数（统计窗口中有多少个组）：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select name,count(*) over () </span><br><span class="line">from business </span><br><span class="line">where substring(orderdate,1,7) = &#x27;2017-04&#x27; </span><br><span class="line">group by name;</span><br><span class="line"></span><br><span class="line">select name,collect_set(orderdate)</span><br><span class="line">from business </span><br><span class="line">where substring(orderdate,1,7) = &#x27;2017-04&#x27; </span><br><span class="line">group by name;</span><br></pre></td></tr></table></figure>

<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">name</span>    _c<span class="number">1</span></span><br><span class="line"><span class="attribute">jack</span>    <span class="number">2</span></span><br><span class="line"><span class="attribute">mart</span>    <span class="number">2</span></span><br></pre></td></tr></table></figure>

<p>此时窗口的大小为整个结果集，窗口（结果集）里边有两条数据。</p>
<p>如果不加over ()，则结果为：（统计组中数据条数）</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">name</span>    _c<span class="number">1</span></span><br><span class="line"><span class="attribute">jack</span>    <span class="number">1</span></span><br><span class="line"><span class="attribute">mart</span>    <span class="number">4</span></span><br></pre></td></tr></table></figure>

<p>（2） 查询顾客的购买明细及所有人月购买总额：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select name,orderdate,cost,sum(cost) over(partition by month(orderdate)) from business;</span><br></pre></td></tr></table></figure>

<p>查询顾客的购买明细及每个顾客的月购买总额：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select name,orderdate,cost,sum(cost) over(partition by name, month(orderdate)) from business;</span><br></pre></td></tr></table></figure>

<p>此时窗口的大小为每个分区。</p>
<p>partition by：在窗口函数中做分区</p>
<p>（3） 将所有顾客的cost按照日期进行累加：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select name, orderdate, cost, sum(cost) over(order by orderdate) sum_cost</span><br><span class="line">from business;</span><br></pre></td></tr></table></figure>

<p><strong>order by：在窗口中进行排序。窗口大小为从开始位置到当前数据位置（每次从第一行到当前行）。</strong></p>
<p>将每个顾客的cost按照日期进行累加：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select name, orderdate, cost, sum(cost) over(partition by name order by orderdate) sum_cost</span><br><span class="line">from business;</span><br></pre></td></tr></table></figure>

<p>求每个顾客的购买明细，及本次消费和上次消费的总和：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select name, orderdate, cost, </span><br><span class="line">sum(cost) over(partition by name order by orderdate rows between 1 preceding and current row) sum_cost</span><br><span class="line">from business;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select name,orderdate,cost, </span><br><span class="line">sum(cost) over() as sample1,--所有行相加 </span><br><span class="line">sum(cost) over(partition by name) as sample2,--按name分组，组内数据相加 </span><br><span class="line">sum(cost) over(partition by name order by orderdate) as sample3,--按name分组，组内数据累加 </span><br><span class="line">sum(cost) over(partition by name order by orderdate rows between UNBOUNDED PRECEDING and current row ) as sample4 ,--和sample3一样,由起点到当前行的聚合 </span><br><span class="line">sum(cost) over(partition by name order by orderdate rows between 1 PRECEDING and current row) as sample5, --当前行和前面一行做聚合 </span><br><span class="line">sum(cost) over(partition by name order by orderdate rows between 1 PRECEDING AND 1 FOLLOWING ) as sample6,--当前行和前边一行及后面一行 </span><br><span class="line">sum(cost) over(partition by name order by orderdate rows between current row and UNBOUNDED FOLLOWING ) as sample7 --当前行及后面所有行 </span><br><span class="line">from business;</span><br></pre></td></tr></table></figure>

<p>rows必须跟在Order by 子句之后，对排序的结果进行限制，使用固定的行数来限制分区中的数据行数量</p>
<p>（4） 查看顾客上次的购买时间</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select name,orderdate,cost, </span><br><span class="line">lag(orderdate,1,&#x27;1900-01-01&#x27;) over(partition by name order by orderdate ) as time1, lag(orderdate,2) over (partition by name order by orderdate) as time2 </span><br><span class="line">from business;</span><br></pre></td></tr></table></figure>

<p>（5） 查询前20%时间的订单信息</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select name,orderdate,cost from (</span><br><span class="line">  select name,orderdate,cost, ntile(5) over(order by orderdate) bucket_num</span><br><span class="line">  from business</span><br><span class="line">) t</span><br><span class="line">where bucket_num = 1;</span><br></pre></td></tr></table></figure>

<h4 id="易混淆关键字"><a href="#易混淆关键字" class="headerlink" title="易混淆关键字"></a>易混淆关键字</h4><p>建表：partitioned by（分区表）    clustered by（分桶表）    sorted by（对桶中的一个或多个列另外排序）</p>
<p>查询：order by（全局排序）    distribute by（查询做分区）    sort by（查询做排序）    cluster by（查询分区排序）</p>
<p>窗口函数：partition by（窗口函数中做分区）和 order by（窗口函数中做排序）    或者</p>
<p>​                 用 distribute by 和 sort by</p>
<h4 id="Rank"><a href="#Rank" class="headerlink" title="Rank"></a>Rank</h4><p>1）函数说明</p>
<p>RANK() 排序相同时会重复，总数不会变</p>
<p>DENSE_RANK() 排序相同时会重复，总数会减少</p>
<p>ROW_NUMBER() 会根据顺序计算</p>
<p>2）数据准备</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">孙悟空	语文	87</span><br><span class="line">孙悟空	数学	95</span><br><span class="line">孙悟空	英语	68</span><br><span class="line">大海	语文	94</span><br><span class="line">大海	数学	56</span><br><span class="line">大海	英语	84</span><br><span class="line">宋宋	语文	64</span><br><span class="line">宋宋	数学	86</span><br><span class="line">宋宋	英语	84</span><br><span class="line">婷婷	语文	65</span><br><span class="line">婷婷	数学	85</span><br><span class="line">婷婷	英语	78</span><br></pre></td></tr></table></figure>

<p>3）需求</p>
<p>计算每门学科成绩排名。</p>
<p>4）创建本地score.txt，导入数据</p>
<p>5）创建hive表并导入数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table score(</span><br><span class="line">name string,</span><br><span class="line">subject string, </span><br><span class="line">score int) </span><br><span class="line">row format delimited fields terminated by &quot;\t&quot;;</span><br><span class="line">load data local inpath &#x27;/opt/module/hive/datas/score.txt&#x27; into table score;</span><br></pre></td></tr></table></figure>

<p>6）按需求查询数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select name, subject, score,</span><br><span class="line">rank() over(partition by subject order by score desc) rk,</span><br><span class="line">dense_rank() over(partition by subject order by score desc) drk,</span><br><span class="line">row_number() over(partition by subject order by score desc) rn</span><br><span class="line">from score;</span><br></pre></td></tr></table></figure>

<figure class="highlight tap"><table><tr><td class="code"><pre><span class="line">name    subject score   rk      drk     rn</span><br><span class="line">孙悟空  数学   <span class="number"> 95 </span>    <span class="number"> 1 </span>     <span class="number"> 1 </span>      1</span><br><span class="line">宋宋    数学   <span class="number"> 86 </span>    <span class="number"> 2 </span>     <span class="number"> 2 </span>      2</span><br><span class="line">婷婷    数学   <span class="number"> 85 </span>    <span class="number"> 3 </span>     <span class="number"> 3 </span>      3</span><br><span class="line">大海    数学   <span class="number"> 56 </span>    <span class="number"> 4 </span>     <span class="number"> 4 </span>      4</span><br><span class="line">大海    英语   <span class="number"> 84 </span>    <span class="number"> 1 </span>     <span class="number"> 1 </span>      1</span><br><span class="line">宋宋    英语   <span class="number"> 84 </span>    <span class="number"> 1 </span>     <span class="number"> 1 </span>      2</span><br><span class="line">婷婷    英语   <span class="number"> 78 </span>    <span class="number"> 3 </span>     <span class="number"> 2 </span>      3</span><br><span class="line">孙悟空  英语   <span class="number"> 68 </span>    <span class="number"> 4 </span>     <span class="number"> 3 </span>      4</span><br><span class="line">大海    语文   <span class="number"> 94 </span>    <span class="number"> 1 </span>     <span class="number"> 1 </span>      1</span><br><span class="line">孙悟空  语文   <span class="number"> 87 </span>    <span class="number"> 2 </span>     <span class="number"> 2 </span>      2</span><br><span class="line">婷婷    语文   <span class="number"> 65 </span>    <span class="number"> 3 </span>     <span class="number"> 3 </span>      3</span><br><span class="line">宋宋    语文   <span class="number"> 64 </span>    <span class="number"> 4 </span>     <span class="number"> 4 </span>      4</span><br></pre></td></tr></table></figure>

<p>扩展：求出每门学科前三名的学生？</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select name, subject, score</span><br><span class="line">from (</span><br><span class="line">select name, subject, score, rank() over(partition by subject order by score desc) rk</span><br><span class="line">from score</span><br><span class="line">-- dense_rank() over(partition by subject order by score desc) drk,</span><br><span class="line">-- row_number() over(partition by subject order by score desc) rn</span><br><span class="line">)t</span><br><span class="line">where rk &lt; 4;</span><br></pre></td></tr></table></figure>

<h4 id="常用日期函数"><a href="#常用日期函数" class="headerlink" title="常用日期函数"></a>常用日期函数</h4><p>unix_timestamp:返回当前或指定时间的时间戳<br>from_unixtime：将时间戳转为日期格式<br>current_date：当前日期<br>current_timestamp：当前的日期加时间<br>to_date：抽取日期部分<br>year：获取年<br>month：获取月<br>day：获取日<br>hour：获取时<br>minute：获取分<br>second：获取秒<br>weekofyear：当前时间是一年中的第几周<br>dayofmonth：当前时间是一个月中的第几天<br>months_between： 两个日期间的月份<br>add_months：日期加减月<br>datediff：两个日期相差的天数<br>date_add：日期加天数<br>date_sub：日期减天数<br>last_day：日期的当月的最后一天</p>
<h4 id="常用取整函数"><a href="#常用取整函数" class="headerlink" title="常用取整函数"></a>常用取整函数</h4><p>round： 四舍五入<br>ceil：  向上取整<br>floor： 向下取整</p>
<h4 id="常用字符串操作函数"><a href="#常用字符串操作函数" class="headerlink" title="常用字符串操作函数"></a>常用字符串操作函数</h4><p>upper： 转大写<br>lower： 转小写<br>length： 长度<br>trim：  前后去空格<br>lpad： 向左补齐，到指定长度<br>rpad：  向右补齐，到指定长度<br>regexp_replace： SELECT regexp_replace(‘100-200’, ‘(\d+)’, ‘num’)<br>使用正则表达式匹配目标字符串，匹配成功后替换！</p>
<h4 id="集合操作"><a href="#集合操作" class="headerlink" title="集合操作"></a>集合操作</h4><p>size: 集合中元素的个数<br>map_keys: 返回map中的key<br>map_values: 返回map中的value<br>array_contains: 判断array中是否包含某个元素<br>sort_array: 将array中的元素排序</p>
<h4 id="自定义函数"><a href="#自定义函数" class="headerlink" title="自定义函数"></a>自定义函数</h4><p>官方文档：<a href="https://cwiki.apache.org/confluence/display/Hive/HivePlugins">https://cwiki.apache.org/confluence/display/Hive/HivePlugins</a></p>
<p>UDF（User-Defined-Function）</p>
<p>一进一出</p>
<p>UDAF（User-Defined Aggregation Function）</p>
<p>聚集函数，多进一出</p>
<p>如：count/max/min</p>
<p>UDTF（User-Defined Table-Generating Functions）</p>
<p>一进多出</p>
<p>如：lateral view explode()</p>
<h5 id="编程步骤"><a href="#编程步骤" class="headerlink" title="编程步骤"></a>编程步骤</h5><p>1）继承Hive提供的类</p>
<p>org.apache.hadoop.hive.ql.udf.generic.GenericUDF  </p>
<p>org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;</p>
<p>2）实现类中的抽象方法</p>
<p>3）在hive的命令行窗口创建函数</p>
<p>添加jar</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">add jar linux_jar_path</span><br></pre></td></tr></table></figure>

<p>创建function</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create [temporary] function [dbname.]function_name AS class_name;</span><br></pre></td></tr></table></figure>

<p>4）在hive的命令行窗口删除函数</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">drop [temporary] function [if exists] [dbname.]function_name;</span><br></pre></td></tr></table></figure>

<h5 id="案例实操（UDF）"><a href="#案例实操（UDF）" class="headerlink" title="案例实操（UDF）"></a>案例实操（UDF）</h5><p>0）需求</p>
<p>自定义一个UDF实现计算给定字符串的长度，例如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive(default)&gt; select my_len(&quot;abcd&quot;);</span><br></pre></td></tr></table></figure>

<figure class="highlight"><table><tr><td class="code"><pre><span class="line">4</span><br></pre></td></tr></table></figure>

<p>1）创建一个Maven工程Hive</p>
<p>2）导入依赖</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">	<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">	<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"> 		<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"> 		<span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">  </span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>3）创建一个类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyUDF</span> <span class="keyword">extends</span> <span class="title">GenericUDF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 对输入参数的判断处理和返回值类型的约定</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> objectInspectors</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> UDFArgumentException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ObjectInspector <span class="title">initialize</span><span class="params">(ObjectInspector[] objectInspectors)</span> <span class="keyword">throws</span> UDFArgumentException </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (objectInspectors.length != <span class="number">1</span>)&#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> UDFArgumentLengthException(<span class="string">&quot;Input Args Length Error&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (!objectInspectors[<span class="number">0</span>].getCategory().equals(ObjectInspector.Category.PRIMITIVE))&#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> UDFArgumentTypeException(<span class="number">0</span>, <span class="string">&quot;Input Args Type Error&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//约定函数的返回值为int</span></span><br><span class="line">        <span class="keyword">return</span> PrimitiveObjectInspectorFactory.javaIntObjectInspector;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 函数的逻辑处理</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> deferredObjects</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> HiveException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">evaluate</span><span class="params">(DeferredObject[] deferredObjects)</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line">        Object o = deferredObjects[<span class="number">0</span>].get();</span><br><span class="line">        <span class="keyword">if</span>(o == <span class="keyword">null</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> o.toString().length();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getDisplayString</span><span class="params">(String[] strings)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>4）打成jar包上传到服务器/opt/module/hive/datas/myudf.jar</p>
<p>5）将jar包添加到hive的classpath</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; add jar /opt/module/hive/datas/myudf.jar;</span><br></pre></td></tr></table></figure>

<p>6）创建临时函数与开发好的java class关联</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create temporary function my_len as &quot;com.vincent.hive.MyUDF&quot;;</span><br></pre></td></tr></table></figure>

<p>7）在hive中使用自定义的函数</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select my_len(&quot;122334&quot;);</span><br></pre></td></tr></table></figure>

<h5 id="案例实操（UDTF）"><a href="#案例实操（UDTF）" class="headerlink" title="案例实操（UDTF）"></a>案例实操（UDTF）</h5><p>0）需求</p>
<p>自定义一个UDTF实现将一个任意分割符的字符串切割成独立的单词，例如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive(default)&gt; select myudtf(&quot;hello,world,hadoop,hive&quot;, &quot;,&quot;);</span><br></pre></td></tr></table></figure>

<figure class="highlight ebnf"><table><tr><td class="code"><pre><span class="line"><span class="attribute">hello</span></span><br><span class="line"><span class="attribute">world</span></span><br><span class="line"><span class="attribute">hadoop</span></span><br><span class="line"><span class="attribute">hive</span> </span><br></pre></td></tr></table></figure>

<p>1）代码实现</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyUDTF</span> <span class="keyword">extends</span> <span class="title">GenericUDTF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> ArrayList&lt;String&gt; outList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> StructObjectInspector <span class="title">initialize</span><span class="params">(StructObjectInspector argOIs)</span> <span class="keyword">throws</span> UDFArgumentException </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">//1.定义输出数据的列名和类型</span></span><br><span class="line">        List&lt;String&gt; fieldNames = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        List&lt;ObjectInspector&gt; fieldOIs = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2.添加输出数据的列名和类型</span></span><br><span class="line">        fieldNames.add(<span class="string">&quot;lineToWord&quot;</span>);</span><br><span class="line">        fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Object[] args)</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//1.获取原始数据</span></span><br><span class="line">        String arg = args[<span class="number">0</span>].toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2.获取数据传入的第二个参数，此处为分隔符</span></span><br><span class="line">        String splitKey = args[<span class="number">1</span>].toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3.将原始数据按照传入的分隔符进行切分</span></span><br><span class="line">        String[] fields = arg.split(splitKey);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4.遍历切分后的结果，并写出</span></span><br><span class="line">        <span class="keyword">for</span> (String field : fields) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">//集合为复用的，首先清空集合</span></span><br><span class="line">            outList.clear();</span><br><span class="line"></span><br><span class="line">            <span class="comment">//将每一个单词添加至集合</span></span><br><span class="line">            outList.add(field);</span><br><span class="line"></span><br><span class="line">            <span class="comment">//将集合内容写出</span></span><br><span class="line">            forward(outList);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> HiveException </span>&#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>3）打成jar包上传到服务器/opt/module/hive/datas/myudtf.jar</p>
<p>4）将jar包添加到hive的classpath</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; add jar /opt/module/hive/datas/myudtf.jar;</span><br></pre></td></tr></table></figure>

<p>5）创建临时函数与开发好的java class关联</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; create temporary function myudtf as &quot;com.vincent.hive.MyUDTF&quot;;</span><br></pre></td></tr></table></figure>

<p>6）在hive中使用自定义的函数</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select myudtf(&quot;hello,world,hadoop,hive&quot;,&quot;,&quot;);</span><br></pre></td></tr></table></figure>

<h2 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h2><h3 id="MR支持的压缩编码"><a href="#MR支持的压缩编码" class="headerlink" title="MR支持的压缩编码"></a>MR支持的压缩编码</h3><table>
<thead>
<tr>
<th>压缩格式</th>
<th>工具</th>
<th>算法</th>
<th>文件扩展名</th>
<th>是否可切分</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>无</td>
<td>DEFLATE</td>
<td>.deflate</td>
<td>否</td>
</tr>
<tr>
<td>Gzip</td>
<td>gzip</td>
<td>DEFLATE</td>
<td>.gz</td>
<td>否</td>
</tr>
<tr>
<td>bzip2</td>
<td>bzip2</td>
<td>bzip2</td>
<td>.bz2</td>
<td>是</td>
</tr>
<tr>
<td>LZO</td>
<td>lzop</td>
<td>LZO</td>
<td>.lzo</td>
<td>是</td>
</tr>
<tr>
<td>Snappy</td>
<td>无</td>
<td>Snappy</td>
<td>.snappy</td>
<td>否</td>
</tr>
</tbody></table>
<p>为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示：</p>
<table>
<thead>
<tr>
<th>压缩格式</th>
<th>对应的编码/解码器</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
</tr>
<tr>
<td>gzip</td>
<td>org.apache.hadoop.io.compress.GzipCodec</td>
</tr>
<tr>
<td>bzip2</td>
<td>org.apache.hadoop.io.compress.BZip2Codec</td>
</tr>
<tr>
<td>LZO</td>
<td>com.hadoop.compression.lzo.LzopCodec</td>
</tr>
<tr>
<td>Snappy</td>
<td>org.apache.hadoop.io.compress.SnappyCodec</td>
</tr>
</tbody></table>
<p>压缩性能的比较：</p>
<table>
<thead>
<tr>
<th>压缩算法</th>
<th>原始文件大小</th>
<th>压缩文件大小</th>
<th>压缩速度</th>
<th>解压速度</th>
</tr>
</thead>
<tbody><tr>
<td>gzip</td>
<td>8.3GB</td>
<td>1.8GB</td>
<td>17.5MB/s</td>
<td>58MB/s</td>
</tr>
<tr>
<td>bzip2</td>
<td>8.3GB</td>
<td>1.1GB</td>
<td>2.4MB/s</td>
<td>9.5MB/s</td>
</tr>
<tr>
<td>LZO</td>
<td>8.3GB</td>
<td>2.9GB</td>
<td>49.3MB/s</td>
<td>74.6MB/s</td>
</tr>
</tbody></table>
<h3 id="压缩参数配置"><a href="#压缩参数配置" class="headerlink" title="压缩参数配置"></a>压缩参数配置</h3><p>要在Hadoop中启用压缩，可以配置如下参数（mapred-site.xml文件中）：</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>默认值</th>
<th>阶段</th>
<th>建议</th>
</tr>
</thead>
<tbody><tr>
<td>io.compression.codecs  （在core-site.xml中配置）</td>
<td>org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec, org.apache.hadoop.io.compress.Lz4Codec</td>
<td>输入压缩</td>
<td>Hadoop使用文件扩展名判断是否支持某种编解码器</td>
</tr>
<tr>
<td>mapreduce.map.output.compress</td>
<td>false</td>
<td>mapper输出</td>
<td>这个参数设为true启用压缩</td>
</tr>
<tr>
<td>mapreduce.map.output.compress.codec</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
<td>mapper输出</td>
<td>使用LZO、LZ4或snappy编解码器在此阶段压缩数据</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress</td>
<td>false</td>
<td>reducer输出</td>
<td>这个参数设为true启用压缩</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress.codec</td>
<td>org.apache.hadoop.io.compress. DefaultCodec</td>
<td>reducer输出</td>
<td>使用标准工具或者编解码器，如gzip和bzip2</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress.type</td>
<td>RECORD</td>
<td>reducer输出</td>
<td>SequenceFile输出使用的压缩类型：NONE和BLOCK</td>
</tr>
</tbody></table>
<h3 id="开启Map输出阶段压缩"><a href="#开启Map输出阶段压缩" class="headerlink" title="开启Map输出阶段压缩"></a>开启Map输出阶段压缩</h3><p>1）开启hive中间传输数据压缩功能</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt;set hive.exec.compress.intermediate=true;</span><br></pre></td></tr></table></figure>

<p>2）开启mapreduce中map输出压缩功能</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt;set mapreduce.map.output.compress=true;</span><br></pre></td></tr></table></figure>

<p>3）设置mapreduce中map输出数据的压缩方式</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt;set mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec;</span><br></pre></td></tr></table></figure>

<p>4）执行查询语句</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select count(ename) name from emp;</span><br></pre></td></tr></table></figure>

<h3 id="开启Reduce输出阶段压缩"><a href="#开启Reduce输出阶段压缩" class="headerlink" title="开启Reduce输出阶段压缩"></a>开启Reduce输出阶段压缩</h3><p>1）开启hive最终输出数据压缩功能</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt;set hive.exec.compress.output=true;</span><br></pre></td></tr></table></figure>

<p>（2）开启mapreduce最终输出数据压缩</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt;set mapreduce.output.fileoutputformat.compress=true;</span><br></pre></td></tr></table></figure>

<p>（3）设置mapreduce最终数据输出压缩方式</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; set mapreduce.output.fileoutputformat.compress.codec = org.apache.hadoop.io.compress.SnappyCodec;</span><br></pre></td></tr></table></figure>

<p>（4）设置mapreduce最终数据输出压缩为块压缩</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; set mapreduce.output.fileoutputformat.compress.type=BLOCK;</span><br></pre></td></tr></table></figure>

<p>（5）测试一下输出结果是否是压缩文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; insert overwrite local directory</span><br><span class="line"> &#x27;/opt/module/datas/distribute-result&#x27; select * from emp distribute by deptno sort by empno desc;</span><br></pre></td></tr></table></figure>

<h2 id="文件存储格式"><a href="#文件存储格式" class="headerlink" title="文件存储格式"></a>文件存储格式</h2><p>Hive支持的存储数据的格式主要有：TEXTFILE 、SEQUENCEFILE、ORC、PARQUET。</p>
<p>如图所示左边为逻辑表，右边第一个为行式存储，第二个为列式存储。</p>
<p>1）行存储的特点</p>
<p>查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。</p>
<p>2）列存储的特点</p>
<p>因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。</p>
<p>TEXTFILE和SEQUENCEFILE的存储格式都是基于行存储的；</p>
<p>ORC和PARQUET是基于列式存储的。</p>
<h3 id="TextFile格式"><a href="#TextFile格式" class="headerlink" title="TextFile格式"></a>TextFile格式</h3><p>默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2使用，但使用Gzip这种方式，hive不会对数据进行切分，从而无法对数据进行并行操作。</p>
<h3 id="Orc格式"><a href="#Orc格式" class="headerlink" title="Orc格式"></a>Orc格式</h3><p>Orc (Optimized Row Columnar)是Hive 0.11版里引入的新的存储格式。</p>
<p>如下图所示可以看到每个Orc文件由1个或多个stripe组成，每个stripe一般为HDFS的块大小，每一个stripe包含多条记录，这些记录按照列进行独立存储，对应到Parquet中的row group的概念。每个Stripe里有三部分组成，分别是Index Data，Row Data，Stripe Footer：</p>
<p>1）Index Data：一个轻量级的index，默认是每隔1W行做一个索引。这里做的索引应该只是记录某行的各字段在Row Data中的offset。</p>
<p>2）Row Data：存的是具体的数据，先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个Stream来存储。</p>
<p>3）Stripe Footer：存的是各个Stream的类型，长度等信息。</p>
<p>每个文件有一个File Footer，这里面存的是每个Stripe的行数，每个Column的数据类型信息等；每个文件的尾部是一个PostScript，这里面记录了整个文件的压缩类型以及FileFooter的长度信息等。在读取文件时，会seek到文件尾部读PostScript，从里面解析到File Footer长度，再读FileFooter，从里面解析到各个Stripe信息，再读各个Stripe，即从后往前读。</p>
<h3 id="Parquet格式"><a href="#Parquet格式" class="headerlink" title="Parquet格式"></a>Parquet格式</h3><p>Parquet文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，因此Parquet格式文件是自解析的。</p>
<p>（1）行组(Row Group)：每一个行组包含一定的行数，在一个HDFS文件中至少存储一个行组，类似于orc的stripe的概念。</p>
<p>（2）列块(Column Chunk)：在一个行组中每一列保存在一个列块中，行组中的所有列连续的存储在这个列块文件中。一个列块中的值都是相同类型的，不同的列块可能使用不同的算法进行压缩。</p>
<p>（3）页(Page)：每一个列块划分为多个页，一个页是最小的编码的单位，在同一个列块的不同页可能使用不同的编码方式。</p>
<p>通常情况下，在存储Parquet数据的时候会按照Block大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，这样可以把每一个行组由一个Mapper任务处理，增大任务执行并行度。Parquet文件的格式。</p>
<p><img src="/Hive/37.png" alt="37"></p>
<p>上图展示了一个Parquet文件的内容，一个文件中可以存储多个行组，文件的首位都是该文件的Magic Code，用于校验它是否是一个Parquet文件，Footer length记录了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量，文件的元数据中包括每一个行组的元数据信息和该文件存储数据的Schema信息。除了文件中每一个行组的元数据，每一页的开始都会存储该页的元数据，在Parquet中，有三种类型的页：数据页、字典页和索引页。数据页用于存储当前行组中该列的值，字典页存储该列值的编码字典，每一个列块中最多包含一个字典页，索引页用来存储当前行组下该列的索引，目前Parquet中还不支持索引页。</p>
<h3 id="主流文件存储格式对比实验"><a href="#主流文件存储格式对比实验" class="headerlink" title="主流文件存储格式对比实验"></a>主流文件存储格式对比实验</h3><p>TextFile</p>
<p>1）创建表，存储数据格式为TEXTFILE</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table log_text (</span><br><span class="line">track_time string,</span><br><span class="line">url string,</span><br><span class="line">session_id string,</span><br><span class="line">referer string,</span><br><span class="line">ip string,</span><br><span class="line">end_user_id string,</span><br><span class="line">city_id string</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;</span><br><span class="line">stored as textfile;</span><br></pre></td></tr></table></figure>

<p>（2）向表中加载数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; load data local inpath &#x27;/opt/module/datas/log.data&#x27; into table log_text ;</span><br></pre></td></tr></table></figure>

<p>（3）查看表中数据大小</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -du -h /user/hive/warehouse/log_text;</span><br></pre></td></tr></table></figure>

<p>Orc</p>
<p>1）创建表，存储数据格式为ORC</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table log_orc(</span><br><span class="line">track_time string,</span><br><span class="line">url string,</span><br><span class="line">session_id string,</span><br><span class="line">referer string,</span><br><span class="line">ip string,</span><br><span class="line">end_user_id string,</span><br><span class="line">city_id string</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;</span><br><span class="line">stored as orc</span><br><span class="line">tblproperties(&quot;orc.compress&quot;=&quot;NONE&quot;);</span><br></pre></td></tr></table></figure>

<p>2）向表中加载数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; insert into table log_orc select * from log_text ;</span><br></pre></td></tr></table></figure>

<p>3）查看表中数据大小</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -du -h /user/hive/warehouse/log_orc/ ;</span><br></pre></td></tr></table></figure>

<p>Parquet</p>
<p>1）创建表，存储数据格式为parquet</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table log_parquet(</span><br><span class="line">track_time string,</span><br><span class="line">url string,</span><br><span class="line">session_id string,</span><br><span class="line">referer string,</span><br><span class="line">ip string,</span><br><span class="line">end_user_id string,</span><br><span class="line">city_id string</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;</span><br><span class="line">stored as parquet ;</span><br></pre></td></tr></table></figure>

<p>2）向表中加载数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; insert into table log_parquet select * from log_text ;</span><br></pre></td></tr></table></figure>

<p>（3）查看表中数据大小</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -du -h /user/hive/warehouse/log_parquet/ ;</span><br></pre></td></tr></table></figure>

<p>存储文件的压缩比总结：</p>
<p>ORC &gt;  Parquet &gt;  textFile</p>
<p>存储文件的查询速度总结：</p>
<p>查询速度相近</p>
<h2 id="存储和压缩结合"><a href="#存储和压缩结合" class="headerlink" title="存储和压缩结合"></a>存储和压缩结合</h2><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC</a></p>
<p>ORC存储方式的压缩:</p>
<table>
<thead>
<tr>
<th>Key</th>
<th>Default</th>
<th>Notes</th>
</tr>
</thead>
<tbody><tr>
<td>orc.compress</td>
<td>ZLIB</td>
<td>high level compression (one of NONE, ZLIB, SNAPPY)</td>
</tr>
<tr>
<td>orc.compress.size</td>
<td>262,144</td>
<td>number of bytes in each compression chunk</td>
</tr>
<tr>
<td>orc.stripe.size</td>
<td>268,435,456</td>
<td>number of bytes in each stripe</td>
</tr>
<tr>
<td>orc.row.index.stride</td>
<td>10,000</td>
<td>number of rows between index entries (must be &gt;= 1000)</td>
</tr>
<tr>
<td>orc.create.index</td>
<td>true</td>
<td>whether to create row indexes</td>
</tr>
<tr>
<td>orc.bloom.filter.columns</td>
<td>“”</td>
<td>comma separated list of column names for which bloom filter should be created</td>
</tr>
<tr>
<td>orc.bloom.filter.fpp</td>
<td>0.05</td>
<td>false positive probability for bloom filter (must &gt;0.0 and &lt;1.0)</td>
</tr>
</tbody></table>
<h3 id="1）创建一个ZLIB压缩的的ORC存储方式"><a href="#1）创建一个ZLIB压缩的的ORC存储方式" class="headerlink" title="1）创建一个ZLIB压缩的的ORC存储方式"></a>1）创建一个ZLIB压缩的的ORC存储方式</h3><p>1）建表语句</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table log_orc_zlib(</span><br><span class="line">track_time string,</span><br><span class="line">url string,</span><br><span class="line">session_id string,</span><br><span class="line">referer string,</span><br><span class="line">ip string,</span><br><span class="line">end_user_id string,</span><br><span class="line">city_id string</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;</span><br><span class="line">stored as orc</span><br><span class="line">tblproperties(&quot;orc.compress&quot;=&quot;ZLIB&quot;);</span><br></pre></td></tr></table></figure>

<p>2）插入数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">insert into log_orc_zlib select * from log_text;</span><br></pre></td></tr></table></figure>

<p>3）查看插入后数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -du -h /user/hive/warehouse/log_orc_none/ ;</span><br></pre></td></tr></table></figure>

<h3 id="2）创建一个SNAPPY压缩的ORC存储方式"><a href="#2）创建一个SNAPPY压缩的ORC存储方式" class="headerlink" title="2）创建一个SNAPPY压缩的ORC存储方式"></a>2）创建一个SNAPPY压缩的ORC存储方式</h3><p>1）建表语句</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table log_orc_snappy(</span><br><span class="line">track_time string,</span><br><span class="line">url string,</span><br><span class="line">session_id string,</span><br><span class="line">referer string,</span><br><span class="line">ip string,</span><br><span class="line">end_user_id string,</span><br><span class="line">city_id string</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;</span><br><span class="line">stored as orc</span><br><span class="line">tblproperties(&quot;orc.compress&quot;=&quot;SNAPPY&quot;);</span><br></pre></td></tr></table></figure>

<p>2）插入数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">insert into log_orc_snappy select * from log_text;</span><br></pre></td></tr></table></figure>

<p>3）查看插入后数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; dfs -du -h /user/hive/warehouse/log_orc_snappy/ ;</span><br></pre></td></tr></table></figure>

<p>总结：ZLIB采用的是deflate压缩算法，比snappy压缩的小。在实际的项目开发当中，hive表的数据存储格式一般选择：orc或parquet。压缩方式一般选择snappy，lzo。</p>
<h2 id="Hive练习"><a href="#Hive练习" class="headerlink" title="Hive练习"></a>Hive练习</h2><h3 id="需求描述"><a href="#需求描述" class="headerlink" title="需求描述"></a>需求描述</h3><p>统计影音视频网站的常规指标，各种TopN指标：</p>
<figure class="highlight ada"><table><tr><td class="code"><pre><span class="line"><span class="comment">--统计视频观看数Top10</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--统计视频类别热度Top10</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--统计视频观看数Top20所属类别</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--统计视频观看数Top50所关联视频的所属类别Rank</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--统计每个类别中的视频热度Top10</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--统计每个类别视频观看数Top10</span></span><br></pre></td></tr></table></figure>



<h3 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h3><table>
<thead>
<tr>
<th>字段</th>
<th>备注</th>
<th>详细描述</th>
</tr>
</thead>
<tbody><tr>
<td>videoId</td>
<td>视频唯一id（String）</td>
<td>11位字符串</td>
</tr>
<tr>
<td>uploader</td>
<td>视频上传者（String）</td>
<td>上传视频的用户名String</td>
</tr>
<tr>
<td>age</td>
<td>视频年龄（int）</td>
<td>视频在平台上的整数天</td>
</tr>
<tr>
<td>category</td>
<td>视频类别（Array<String>）</String></td>
<td>上传视频指定的视频分类</td>
</tr>
<tr>
<td>length</td>
<td>视频长度（Int）</td>
<td>整形数字标识的视频长度</td>
</tr>
<tr>
<td>views</td>
<td>观看次数（Int）</td>
<td>视频被浏览的次数</td>
</tr>
<tr>
<td>rate</td>
<td>视频评分（Double）</td>
<td>满分5分</td>
</tr>
<tr>
<td>Ratings</td>
<td>流量（Int）</td>
<td>视频的流量，整型数字</td>
</tr>
<tr>
<td>conments</td>
<td>评论数（Int）</td>
<td>一个视频的整数评论数</td>
</tr>
<tr>
<td>relatedId</td>
<td>相关视频id（Array<String>）</String></td>
<td>相关视频的id，最多20个</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>字段</th>
<th>备注</th>
<th>字段类型</th>
</tr>
</thead>
<tbody><tr>
<td>uploader</td>
<td>上传者用户名</td>
<td>string</td>
</tr>
<tr>
<td>videos</td>
<td>上传视频数</td>
<td>int</td>
</tr>
<tr>
<td>friends</td>
<td>朋友数量</td>
<td>int</td>
</tr>
</tbody></table>
<h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><p>0）ETL</p>
<p>ETL之封装工具类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ETLUtil</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment"> 	* 数据清洗方法</span></span><br><span class="line"><span class="comment"> 	*/</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">etlData</span><span class="params">(String srcData)</span></span>&#123;</span><br><span class="line">        StringBuffer resultData = <span class="keyword">new</span> StringBuffer();</span><br><span class="line">        <span class="comment">//1. 先将数据通过\t 切割</span></span><br><span class="line">        String[] datas = srcData.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">        <span class="comment">//2. 判断长度是否小于9</span></span><br><span class="line">        <span class="keyword">if</span>(datas.length &lt;<span class="number">9</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span> ;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//3. 将数据中的视频类别的空格去掉</span></span><br><span class="line">        datas[<span class="number">3</span>]=datas[<span class="number">3</span>].replaceAll(<span class="string">&quot; &quot;</span>,<span class="string">&quot;&quot;</span>);</span><br><span class="line">        <span class="comment">//4. 将数据中的关联视频id通过&amp;拼接</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; datas.length; i++) &#123;</span><br><span class="line">            <span class="keyword">if</span>(i &lt; <span class="number">9</span>)&#123;</span><br><span class="line">                <span class="comment">//4.1 没有关联视频的情况</span></span><br><span class="line">                <span class="keyword">if</span>(i == datas.length-<span class="number">1</span>)&#123;</span><br><span class="line">                    resultData.append(datas[i]);</span><br><span class="line">                &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                    resultData.append(datas[i]).append(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                <span class="comment">//4.2 有关联视频的情况</span></span><br><span class="line">                <span class="keyword">if</span>(i == datas.length-<span class="number">1</span>)&#123;</span><br><span class="line">                    resultData.append(datas[i]);</span><br><span class="line">                &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                    resultData.append(datas[i]).append(<span class="string">&quot;&amp;&quot;</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> resultData.toString();</span><br><span class="line">    &#125;</span><br><span class="line">	&#125;  </span><br></pre></td></tr></table></figure>

<p>ETL之Mapper</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 清洗谷粒影音的原始数据</span></span><br><span class="line"><span class="comment"> * 清洗规则</span></span><br><span class="line"><span class="comment"> *  1. 将数据长度小于9的清洗掉</span></span><br><span class="line"><span class="comment"> *  2. 将数据中的视频类别中间的空格去掉   People &amp; Blogs</span></span><br><span class="line"><span class="comment"> *  3. 将数据中的关联视频id通过&amp;符号拼接</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EtlMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>,<span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Text k = <span class="keyword">new</span> Text();</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">       <span class="comment">//获取一行</span></span><br><span class="line">        String line = value.toString();</span><br><span class="line">        <span class="comment">//清洗</span></span><br><span class="line">        String resultData = ETLUtil.etlData(line);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(resultData != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="comment">//写出</span></span><br><span class="line">            k.set(resultData);</span><br><span class="line">            context.write(k,NullWritable.get());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>ETL之Driver</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EtlDriver</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        Job job  = Job.getInstance(conf);</span><br><span class="line">        job.setJarByClass(EtlDriver.class);</span><br><span class="line">        job.setMapperClass(EtlMapper.class);</span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(NullWritable.class);</span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(NullWritable.class);</span><br><span class="line">        job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line">        FileInputFormat.setInputPaths(job,<span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job,<span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>

<p>将ETL程序打包为etl.jar 并上传到Linux的 /opt/module/hive/datas 目录下。</p>
<p>上传原始数据到HDFS。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas] pwd</span><br><span class="line">/opt/module/hive/datas</span><br><span class="line">[vincent@linux1 datas] hadoop fs -mkdir -p  /gulivideo/video</span><br><span class="line">[vincent@linux1 datas] hadoop fs -mkdir -p  /gulivideo/user</span><br><span class="line">[vincent@linux1 datas] hadoop fs -put gulivideo/user/user.txt   /gulivideo/user</span><br><span class="line">[vincent@linux1 datas] hadoop fs -put gulivideo/video/*.txt   /gulivideo/video</span><br></pre></td></tr></table></figure>

<p>ETL数据</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas] hadoop jar etl.jar com.vincent.gulivideo.etl.EtlDriver /gulivideo/video /gulivideo/video/output</span><br></pre></td></tr></table></figure>



<p>创建原始数据表：gulivideo_ori，gulivideo_user_ori，</p>
<p>创建最终表：gulivideo_orc，gulivideo_user_orc</p>
<p>1）创建原始数据表：gulivideo_ori</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table gulivideo_ori(</span><br><span class="line">    videoId string, </span><br><span class="line">    uploader string, </span><br><span class="line">    age int, </span><br><span class="line">    category array&lt;string&gt;, </span><br><span class="line">    length int, </span><br><span class="line">    views int, </span><br><span class="line">    rate float, </span><br><span class="line">    ratings int, </span><br><span class="line">    comments int,</span><br><span class="line">    relatedId array&lt;string&gt;)</span><br><span class="line">row format delimited fields terminated by &quot;\t&quot;</span><br><span class="line">collection items terminated by &quot;&amp;&quot;</span><br><span class="line">stored as textfile;</span><br></pre></td></tr></table></figure>

<p>创建原始数据表: gulivideo_user_ori</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table gulivideo_user_ori(</span><br><span class="line">    uploader string,</span><br><span class="line">    videos int,</span><br><span class="line">    friends int)</span><br><span class="line">row format delimited </span><br><span class="line">fields terminated by &quot;\t&quot; </span><br><span class="line">stored as textfile;</span><br></pre></td></tr></table></figure>

<p>创建orc存储格式带snappy压缩的表：gulivideo_orc</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table gulivideo_orc(</span><br><span class="line">    videoId string, </span><br><span class="line">    uploader string, </span><br><span class="line">    age int, </span><br><span class="line">    category array&lt;string&gt;, </span><br><span class="line">    length int, </span><br><span class="line">    views int, </span><br><span class="line">    rate float, </span><br><span class="line">    ratings int, </span><br><span class="line">    comments int,</span><br><span class="line">    relatedId array&lt;string&gt;)</span><br><span class="line">stored as orc</span><br><span class="line">tblproperties(&quot;orc.compress&quot;=&quot;SNAPPY&quot;);</span><br></pre></td></tr></table></figure>

<p>创建orc存储格式带snappy压缩的表：gulivideo_user_orc</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table gulivideo_user_orc(</span><br><span class="line">    uploader string,</span><br><span class="line">    videos int,</span><br><span class="line">    friends int)</span><br><span class="line">row format delimited </span><br><span class="line">fields terminated by &quot;\t&quot; </span><br><span class="line">stored as orc</span><br><span class="line">tblproperties(&quot;orc.compress&quot;=&quot;SNAPPY&quot;);</span><br></pre></td></tr></table></figure>

<p>向ori表插入数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">load data inpath &quot;/gulivideo/video/output&quot; into table gulivideo_ori;</span><br><span class="line">load data inpath &quot;/gulivideo/user&quot; into table gulivideo_user_ori;</span><br></pre></td></tr></table></figure>

<p>向orc表插入数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">insert into table gulivideo_orc select * from gulivideo_ori;</span><br><span class="line">insert into table gulivideo_user_orc select * from gulivideo_user_ori;</span><br></pre></td></tr></table></figure>

<h3 id="业务分析"><a href="#业务分析" class="headerlink" title="业务分析"></a>业务分析</h3><h4 id="1-统计视频观看数Top10"><a href="#1-统计视频观看数Top10" class="headerlink" title="1.统计视频观看数Top10"></a>1.统计视频观看数Top10</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SELECT </span><br><span class="line">     videoId,</span><br><span class="line">     views </span><br><span class="line">FROM </span><br><span class="line">     gulivideo_orc</span><br><span class="line">ORDER BY </span><br><span class="line">     views DESC </span><br><span class="line">LIMIT 10;</span><br><span class="line">//或者</span><br><span class="line">select videoId, views from gulivideo_orc sort by views desc limit 10;</span><br></pre></td></tr></table></figure>

<h4 id="2-统计视频类别热度Top10"><a href="#2-统计视频类别热度Top10" class="headerlink" title="2.统计视频类别热度Top10"></a>2.统计视频类别热度Top10</h4><p>思路：</p>
<p>（1）即统计每个类别有多少个视频，显示出包含视频最多的前10个类别。</p>
<p>（2）我们需要按照类别group by聚合，然后count组内的videoId个数即可。</p>
<p>（3）因为当前表结构为：一个视频对应一个或多个类别。所以如果要group by类别，需要先将类别进行列转行(展开)，然后再进行count即可。</p>
<p>（4）最后按照热度排序，显示前10条。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select t1.category_name, count(t1.videoid) video_count</span><br><span class="line">from (</span><br><span class="line">    select videoid, category_name</span><br><span class="line">    from gulivideo_orc </span><br><span class="line">  	lateral view explode(category) gulivideo_orc_tmp as category_name</span><br><span class="line">) t1</span><br><span class="line">group by t1.category_name</span><br><span class="line">order by video_count desc</span><br><span class="line">limit 10</span><br></pre></td></tr></table></figure>

<h4 id="3-统计出视频观看数最高的20个视频的所属类别以及类别包含Top20视频的个数"><a href="#3-统计出视频观看数最高的20个视频的所属类别以及类别包含Top20视频的个数" class="headerlink" title="3.统计出视频观看数最高的20个视频的所属类别以及类别包含Top20视频的个数"></a>3.统计出视频观看数最高的20个视频的所属类别以及类别包含Top20视频的个数</h4><p>思路：</p>
<p>（1）先找到观看数最高的20个视频所属条目的所有信息，降序排列</p>
<p>（2）把这20条信息中的category分裂出来(列转行)</p>
<p>（3）最后查询视频分类名称和该分类下有多少个Top20的视频</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select t2.category_name, count(t2.videoid)</span><br><span class="line">from</span><br><span class="line">     (select t1.videoid, t1.`views`, category_name</span><br><span class="line">        from(</span><br><span class="line">            select videoid, `views`,category</span><br><span class="line">            from gulivideo_orc</span><br><span class="line">            order by `views` desc</span><br><span class="line">            limit 20) t1</span><br><span class="line">        lateral view explode(t1.category) tmp as category_name) t2</span><br><span class="line">group by t2.category_name</span><br></pre></td></tr></table></figure>

<h4 id="4-统计视频观看数Top50所关联视频的所属类别排序"><a href="#4-统计视频观看数Top50所关联视频的所属类别排序" class="headerlink" title="4.统计视频观看数Top50所关联视频的所属类别排序"></a>4.统计视频观看数Top50所关联视频的所属类别排序</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select t6.category_name, t6.category_count, rank() over (order by t6.category_count desc ) category_rk</span><br><span class="line">from (</span><br><span class="line">    select t5.category_name, count(t5.category_name) category_count</span><br><span class="line">    from (</span><br><span class="line">        select category_name</span><br><span class="line">        from(</span><br><span class="line">            select t2.relatedid_id, t3.category</span><br><span class="line">            from (</span><br><span class="line">                select relatedid_id</span><br><span class="line">                from(</span><br><span class="line">                    select videoid, relatedid, `views`</span><br><span class="line">                    from gulivideo_orc</span><br><span class="line">                    order by `views` desc</span><br><span class="line">                    limit 50) t1</span><br><span class="line">                lateral view explode(t1.relatedid) tmp as relatedid_id) t2 join gulivideo_orc t3</span><br><span class="line">                on t2.relatedid_id = t3.videoid) t4</span><br><span class="line">        lateral view explode(t4.category) t4_tmp as category_name) t5</span><br><span class="line">    group by (t5.category_name)) t6</span><br></pre></td></tr></table></figure>

<h4 id="5-统计每个类别中的视频热度Top10，以Music为例"><a href="#5-统计每个类别中的视频热度Top10，以Music为例" class="headerlink" title="5.统计每个类别中的视频热度Top10，以Music为例"></a>5.统计每个类别中的视频热度Top10，以Music为例</h4><p>思路：</p>
<p>（1）要想统计Music类别中的视频热度Top10，需要先找到Music类别，那么就需要将category展开，所以可以创建一张表用于存放categoryId展开的数据。</p>
<p>（2）向category展开的表中插入数据。</p>
<p>（3）统计对应类别（Music）中的视频热度。</p>
<p>统计Music类别的Top10（也可以统计其他）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select t1.videoid, t1.`views`, t1.category_name</span><br><span class="line">from (</span><br><span class="line">     select videoid, `views`, category_name</span><br><span class="line">     from gulivideo_orc</span><br><span class="line">     lateral view explode(category) tmp as category_name</span><br><span class="line">     ) t1</span><br><span class="line">where t1.category_name = &#x27;Music&#x27;</span><br><span class="line">order by t1.`views` desc</span><br><span class="line">limit 10</span><br></pre></td></tr></table></figure>

<h4 id="6-统计每个类别视频观看数Top10"><a href="#6-统计每个类别视频观看数Top10" class="headerlink" title="6.统计每个类别视频观看数Top10"></a>6.统计每个类别视频观看数Top10</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select t2.videoid, t2.`views`, t2.category_name, t2.rk</span><br><span class="line">from (</span><br><span class="line">      select t1.videoid, t1.`views`, t1.category_name, rank() over (partition by t1.category_name order by t1.`views` desc) rk</span><br><span class="line">      from (</span><br><span class="line">               select videoid, `views`, category_name</span><br><span class="line">               from gulivideo_orc</span><br><span class="line">               lateral view explode(category) tmp as category_name</span><br><span class="line">           ) t1</span><br><span class="line">     ) t2</span><br><span class="line">where t2.rk &lt;= 10</span><br></pre></td></tr></table></figure>

<h4 id="7-统计上传视频最多的用户Top10以及他们上传的视频观看次数在前20的视频"><a href="#7-统计上传视频最多的用户Top10以及他们上传的视频观看次数在前20的视频" class="headerlink" title="7.统计上传视频最多的用户Top10以及他们上传的视频观看次数在前20的视频"></a>7.统计上传视频最多的用户Top10以及他们上传的视频观看次数在前20的视频</h4><p>思路：</p>
<p>（1）求出上传视频最多的10个用户</p>
<p>（2）关联gulivideo_orc表，求出这10个用户上传的所有的视频，按照观看数取前20</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select t1.uploader, t2.videoid, t2.`views`</span><br><span class="line">from (</span><br><span class="line">         select uploader</span><br><span class="line">         from gulivideo_user_orc</span><br><span class="line">         order by videos desc</span><br><span class="line">         limit 10</span><br><span class="line">     ) t1 join gulivideo_orc t2 on t1.uploader = t2.uploader</span><br><span class="line">order by t2.`views` desc</span><br><span class="line">limit 20</span><br></pre></td></tr></table></figure>

<h2 id="Hive参数调优"><a href="#Hive参数调优" class="headerlink" title="Hive参数调优"></a>Hive参数调优</h2><p>1、启用数据压缩</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!--目的：减少存储和IO--&gt;</span></span><br><span class="line"><span class="comment">&lt;!--压缩Hive输出和中间结果--&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.exec.compress.output<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">      This controls whether the final outputs of a query (to a local/HDFS file or a Hive table) is compressed. </span><br><span class="line">      The compression codec and other options are determined from Hadoop config variables mapred.output.compress*</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.exec.compress.intermediate<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">      This controls whether intermediate files produced by Hive between multiple map-reduce jobs are compressed. </span><br><span class="line">      The compression codec and other options are determined from Hadoop config variables mapred.output.compress*</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--设置Hive中间表存储格式--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.query.result.fileformat<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>SequenceFile <span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>File format to use for a query&#x27;s intermediate results. Options are TextFile, SequenceFile, and RCfile. Default value is changed to SequenceFile since Hive 2.1.0<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>2、Job执行优化</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#并行执行多个job</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#hive的物理执行任务默认情况下是一个job执行完了之后再执行其它job，这种情况下如果想加快hive job的执行的话，可以采用并行的方式执行</span></span></span><br><span class="line">hive.exec.parallel=true (default false)</span><br><span class="line">hive.exec.parallel.thread.number=8 (default 8)</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#本地执行模式</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#在做测试的时候，如果任务量较小的时候可以使用本地执行模式，以上三个条件任意不满足的话就会提交的远程去执行</span></span></span><br><span class="line">hive.exec.mode.local.auto=true</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#Let Hive determine whether to run in local mode automatically</span></span></span><br><span class="line">hive.exec.mode.local.auto.inputbytes.max (default 128MB)</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#When hive.exec.mode.local.auto is true, input bytes should less than this for local mode</span></span></span><br><span class="line">hive.exec.mode.local.auto.input.files.max(default 4)</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#When hive.exec.mode.local.auto is true, the number of tasks should less than this for local mode</span></span></span><br></pre></td></tr></table></figure>

<p>3、择合适的引擎</p>
<figure class="highlight ebnf"><table><tr><td class="code"><pre><span class="line"><span class="attribute">mr</span></span><br><span class="line"><span class="attribute">tez</span></span><br><span class="line"><span class="attribute">spark</span></span><br></pre></td></tr></table></figure>

<p>4、map阶段优化</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!--列裁剪--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.optimize.cp<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether to enable column pruner.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--map端聚合--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.map.aggr<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether to use map-side aggregation in Hive Group By queries<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--Map端谓语下推--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.optimize.ppd<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether to enable predicate pushdown<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>5、reduce阶段优化</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!--reduce数量--&gt;</span>  </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.exec.reducers.max<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1009<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">      max number of reducers will be used. If the one specified in the configuration parameter mapred.reduce.tasks is</span><br><span class="line">      negative, Hive will use this one as the max number of reducers when automatically determine number of reducers.</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--reduce大小--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.exec.reducers.bytes.per.reducer<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1,000,000,000 <span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">    Size per reducer. The default in Hive 0.14.0 and earlier is 1 GB, that is, if the input size is 10 GB then 10 reducers will be used. In Hive 0.14.0 and later the default is 256 MB, that is, if the input size is 1 GB then 4 reducers will be used.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>6、shuffle阶段优化</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#压缩中间数据，从而减少磁盘操作以及减少网络传输数据量</span></span></span><br><span class="line">mapreduce.map.output.compress=true</span><br><span class="line">mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec</span><br></pre></td></tr></table></figure>

<p>7、join优化</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">##mapjoin</span><br><span class="line">##一个表的数据量很大而另一个表的数据量不大，这个时候可以把小表的数据拷贝到各个mapTask中，然后在map的内存中加载，这样再和大表的数据分片做<span class="keyword">join</span>，这样就可以避免将大表的数据在网络中进行shuffle，减少网络的开销，提升整个执行速度</span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join<span class="operator">=</span><span class="literal">true</span> (<span class="keyword">default</span> <span class="literal">false</span>)</span><br><span class="line"><span class="keyword">set</span> hive.mapjoin.smalltable.filesize<span class="operator">=</span> (<span class="keyword">default</span> <span class="number">25</span>M <span class="number">25000000</span>)</span><br><span class="line">##combine more map<span class="operator">-</span>side joins <span class="keyword">into</span> a single map<span class="operator">-</span>side <span class="keyword">join</span> if the size <span class="keyword">of</span> the n<span class="number">-1</span> <span class="keyword">table</span> <span class="keyword">is</span> less than <span class="number">10</span> MB <span class="keyword">using</span> hive.auto.convert.join.noconditionaltask</span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join.noconditionaltask <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join.noconditionaltask.size <span class="operator">=</span> <span class="number">20971520</span>(默认<span class="number">10000000</span>);</span><br><span class="line">##强制指定对a表做 map <span class="keyword">join</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">/</span><span class="operator">+</span>MAP <span class="keyword">JOIN</span>(a)<span class="operator">+</span><span class="operator">/</span>…a <span class="keyword">join</span> b</span><br><span class="line">##limitations</span><br><span class="line">##we can never <span class="keyword">convert</span> <span class="keyword">Full</span> <span class="keyword">outer</span> joins <span class="keyword">to</span> map<span class="operator">-</span>side joins</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">##Bucket Map <span class="keyword">Join</span></span><br><span class="line">##都是大表</span><br><span class="line">##map <span class="keyword">join</span>一起工作</span><br><span class="line">##所有要<span class="keyword">join</span>的表必须分桶，小表的分桶数是大表分桶数的倍数</span><br><span class="line">##做了bucket的列必须等于<span class="keyword">join</span>的列</span><br><span class="line">##所有表不排序</span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.enforce.bucketing<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> b1(</span><br><span class="line">   col0 string</span><br><span class="line">  ,col1 string</span><br><span class="line">  ,col2 string</span><br><span class="line">  ,col3 string</span><br><span class="line">  ,col4 string</span><br><span class="line">  ,col5 string</span><br><span class="line">  ,col6 string</span><br><span class="line">)clustered <span class="keyword">by</span> (col0) <span class="keyword">into</span> <span class="number">32</span> buckets;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> b2(</span><br><span class="line">   col0 string</span><br><span class="line">  ,col1 string</span><br><span class="line">  ,col2 string</span><br><span class="line">  ,col3 string</span><br><span class="line">  ,col4 string</span><br><span class="line">  ,col5 string</span><br><span class="line">  ,col6 string)</span><br><span class="line">clustered <span class="keyword">by</span> (col0) <span class="keyword">into</span> <span class="number">8</span> buckets;</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> hive.enforce.bucketing <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">From</span> passwords <span class="keyword">insert</span> OVERWRITE  <span class="keyword">table</span> b1 <span class="keyword">select</span> <span class="operator">*</span> limit <span class="number">10000</span>;</span><br><span class="line"><span class="keyword">From</span> passwords <span class="keyword">insert</span> OVERWRITE  <span class="keyword">table</span> b2 <span class="keyword">select</span> <span class="operator">*</span> limit <span class="number">10000</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="keyword">select</span> <span class="comment">/*+ MAPJOIN(b2) */</span> b1.<span class="operator">*</span> <span class="keyword">from</span> b1,b2 <span class="keyword">where</span> b1.col0<span class="operator">=</span>b2.col0;</span><br><span class="line"></span><br><span class="line">##Skew <span class="keyword">join</span></span><br><span class="line">##数据倾斜</span><br><span class="line">##其中一个表中数据量某一类值特别多,分配到该值的reducer,耗时较长</span><br><span class="line">##功能：</span><br><span class="line">##<span class="number">1</span>、对于skewjoin.key，在执行job时，将它们存入临时的HDFS目录。其它数据正常执行</span><br><span class="line">##<span class="number">2</span>、对倾斜数据开启map <span class="keyword">join</span>操作，对非倾斜值采取普通<span class="keyword">join</span>操作</span><br><span class="line">##<span class="number">3</span>、将倾斜数据集和非倾斜数据集进行合并操作</span><br><span class="line"><span class="keyword">set</span> hive.optimize.skewjoin<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line">##表示当记录条数超过<span class="number">100000</span>时采用skewjoin操作</span><br><span class="line"><span class="keyword">set</span> hive.skewjoin.key<span class="operator">=</span><span class="number">100000</span>;</span><br><span class="line"></span><br><span class="line">##SMB <span class="keyword">Join</span>(Sort <span class="keyword">Merge</span> Bucket <span class="keyword">Join</span>)</span><br><span class="line">##都是大表</span><br><span class="line">##表已分桶，<span class="keyword">join</span>字段是分桶字段，<span class="keyword">join</span>字段已经排序</span><br><span class="line">##小表的桶的个数是大表的整数倍</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>8、group by优化</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">##如果在不确定倾斜值的情况下，可以设置hive.groupby.skewindata参数</span><br><span class="line">##功能：会将原来的一个MaReduce阶段转化成两个MapReduce阶段，一阶段MapReduce：随机打散数据，打散后进行局部聚合（数据去重 <span class="operator">+</span> 多Task局部计数）二阶段MapReduce：对一阶段的局部聚合结果进行最终聚合（最终汇总计数）</span><br><span class="line"><span class="keyword">set</span> hive.groupby.skewindata<span class="operator">=</span><span class="literal">true</span>;</span><br></pre></td></tr></table></figure>







<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">#设置任务名</span><br><span class="line"><span class="keyword">set</span> mapreduce.job.name<span class="operator">=</span>xxxx($&#123;statis_date&#125;)  # 方便定位具体任务</span><br><span class="line"></span><br><span class="line">#输入合并参数设置</span><br><span class="line">#文件切分splitSize <span class="operator">=</span>  Math.<span class="built_in">max</span>(minSize, Math.<span class="built_in">min</span>(maxSize, blockSize));</span><br><span class="line">#maxSize，默认<span class="number">0</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.input.fileinputformat.split.maxsize<span class="operator">=</span><span class="number">2048000000</span>;</span><br><span class="line">#minSize，默认<span class="number">256</span>M</span><br><span class="line"><span class="keyword">set</span> mapreduce.input.fileinputformat.split.minsize<span class="operator">=</span><span class="number">1024000000</span>;</span><br><span class="line">#同一节点的数据块形成切片时，切片大小的最小值</span><br><span class="line"><span class="keyword">set</span> mapreduce.input.fileinputformat.split.minsize.per.node<span class="operator">=</span><span class="number">100000000</span>;</span><br><span class="line">#同一机架的数据块形成切片时，切片大小的最小值</span><br><span class="line"><span class="keyword">set</span> mapreduce.input.fileinputformat.split.minsize.per.rack<span class="operator">=</span><span class="number">100000000</span>;</span><br><span class="line">#小文件有可能是直接来自于数据源的小文件，也可能是Reduce产生的小文件</span><br><span class="line">#执行Map前进行小文件合并</span><br><span class="line"><span class="keyword">set</span> hive.input.format<span class="operator">=</span>org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</span><br><span class="line"><span class="keyword">set</span> mapred.max.split.size <span class="operator">=</span> <span class="number">256000000</span> #每一个mapper最大的输入大小</span><br><span class="line">#低于<span class="number">128</span>M就算小文件，数据在一个节点会合并，在多个不同的节点会把数据抓过来进行合并。</span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.node <span class="operator">=</span> <span class="number">128000000</span></span><br><span class="line">#一个机架下split的至少的大小(这个值决定了该机架下的文件是否需要合并)</span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.rack <span class="operator">=</span> <span class="number">100000000</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> hive.merge.mapFiles<span class="operator">=</span><span class="literal">true</span>;     #Map端小文件聚合，默认<span class="literal">true</span></span><br><span class="line"><span class="keyword">set</span> hive.merge.mapredFiles<span class="operator">=</span><span class="literal">true</span>;  #Reduce端小文件聚合，默认<span class="literal">false</span></span><br><span class="line"><span class="keyword">set</span> hive.merge.size.per.task<span class="operator">=</span><span class="number">256000000</span>; #合并文件的大小，默认<span class="number">256000000</span></span><br><span class="line">#当输出文件的平均大小小于该值时，启动一个独立的map<span class="operator">-</span>reduce任务进行文件<span class="keyword">merge</span></span><br><span class="line">hive.merge.smallfiles.avgsize<span class="operator">=</span><span class="number">16000000</span></span><br><span class="line">#不进行小文件合并</span><br><span class="line"><span class="keyword">set</span> hive.input.format<span class="operator">=</span>org.apache.hadoop.hive.ql.io.HiveInputFormat; </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.sortmerge.join<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin.sortedmerge <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.sortmerge.join.bigtable.selection.policy </span><br><span class="line">    <span class="operator">=</span> org.apache.hadoop.hive.ql.optimizer.TableSizeBasedBigTableSelectorForAutoSMJ;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>离线计算</category>
      </categories>
  </entry>
  <entry>
    <title>Kafka</title>
    <url>/Kafka/</url>
    <content><![CDATA[<p>Kafka是一个分布式的基于发布/订阅模式的消息队列，主要应用于大数据实时处理领域。</p>
<p>官网：<a href="https://kafka.apache.org/">https://kafka.apache.org</a></p>
<h2 id="Kafka基础架构"><a href="#Kafka基础架构" class="headerlink" title="Kafka基础架构"></a>Kafka基础架构</h2><p><img src="/Kafka/18.png" alt="18"></p>
<p>（1）Producer ：消息生产者，就是向kafka broker发消息的客户端；</p>
<p>（2）Consumer ：消息消费者，向kafka broker取消息的客户端；</p>
<p>（3）Consumer Group （CG）：消费者组，由多个consumer组成。<strong>消费者组内每个消费者负责消费不同分区的数据</strong>，一个分区只能由一个消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。</p>
<p>（4）Broker ：一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic。</p>
<p>（5）Topic ：可以理解为一个队列，生产者和消费者面向的都是一个topic；</p>
<p>（6）Partition：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列；</p>
<p>（7）Replica：副本，为保证集群中的某个节点发生故障时，该节点上的partition数据不丢失，且kafka仍然能够继续工作，kafka提供了副本机制，一个topic的每个分区都有若干个副本，一个leader和若干个follower。</p>
<p>（8）leader：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是leader。</p>
<p>（9）follower：每个分区多个副本中的“从”，实时从leader中同步数据，保持和leader数据的同步。leader发生故障时，某个follower会成为新的leader。</p>
<h2 id="Kafka架构深入"><a href="#Kafka架构深入" class="headerlink" title="Kafka架构深入"></a>Kafka架构深入</h2><h3 id="Kafka工作流程及文件存储机制"><a href="#Kafka工作流程及文件存储机制" class="headerlink" title="Kafka工作流程及文件存储机制"></a>Kafka工作流程及文件存储机制</h3><p><img src="/Kafka/19.png" alt="19"></p>
<p>Kafka中消息是以topic进行分类的，生产者生产消息，消费者消费消息，都是面向topic的。</p>
<p><strong>topic是逻辑上的概念，而partition是物理上的概念，每个partition对应于一个log文件，该log文件中存储的就是producer生产的数据</strong>。Producer生产的数据会被不断追加到该log文件末端，且每条数据都有自己的offset。消费者组中的每个消费者，都会实时记录自己消费到了哪个offset，以便出错恢复时，从上次的位置继续消费。</p>
<p><img src="/Kafka/20.png" alt="20"></p>
<p>由于生产者生产的消息会不断追加到log文件末尾，为防止log文件过大导致数据定位效率低下，Kafka采取了<strong>分片和索引</strong>机制，将<strong>每个partition分为多个segment</strong>。每个<strong>segment对应两个文件——.index文件和.log文件</strong>。这些文件位于一个文件夹下，该文件夹的命名规则为：<strong>topic名称+分区序号</strong>。例如，first这个topic有三个分区，则其对应的文件夹为first-0,first-1,first-2。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--first-0</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#第一个segment</span></span></span><br><span class="line">00000000000000000000.index</span><br><span class="line">00000000000000000000.log</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#第二个segment</span></span></span><br><span class="line">00000000000000170410.index</span><br><span class="line">00000000000000170410.log</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#第三个segment</span></span></span><br><span class="line">00000000000000239430.index</span><br><span class="line">00000000000000239430.log</span><br></pre></td></tr></table></figure>

<p><strong>index和log文件以当前segment的第一条消息的offset命名</strong>。下图为index文件和log文件的结构示意图。</p>
<p><img src="/Kafka/21.png" alt="21"></p>
<p>.index文件存储大量的索引信息，.log文件存储大量的数据，索引文件中的元数据指向对应数据文件中message的物理偏移地址。</p>
<h3 id="Kafka生产者"><a href="#Kafka生产者" class="headerlink" title="Kafka生产者"></a>Kafka生产者</h3><h4 id="分区策略"><a href="#分区策略" class="headerlink" title="分区策略"></a>分区策略</h4><p><strong>1）分区的原因</strong></p>
<p>（1）方便在集群中<strong>扩展</strong>，每个Partition可以通过调整以适应它所在的机器，而一个topic又可以有多个Partition组成，因此整个集群就可以适应任意大小的数据了；</p>
<p>（2）可以提高<strong>并发</strong>，因为可以<strong>以Partition为单位读写</strong>了。</p>
<p><strong>2）分区的原则</strong></p>
<p>我们需要将producer发送的数据封装成一个<strong>ProducerRecord</strong>对象。</p>
<p>（1）指明partition的情况下，直接将指明的值作为 partiton 值；</p>
<p>（2）没有指明partition值但有key的情况下，将key的hash值与topic的partition数进行取余得到partition值；</p>
<p>（3）既没有partition值又没有 key 的情况下，第一次调用时随机生成一个整数（后面每次调用在这个整数上自增），将这个值与topic可用的partition总数取余得到partition值，也就是常说的round-robin算法。</p>
<h4 id="数据可靠性保证"><a href="#数据可靠性保证" class="headerlink" title="数据可靠性保证"></a>数据可靠性保证</h4><p>为保证producer发送的数据，能可靠的发送到指定的topic，topic的每个partition收到producer发送的数据后，都需要向producer发送ack（acknowledgement确认收到），如果producer收到ack，就会进行下一轮的发送，否则重新发送数据。</p>
<h5 id="1）副本数据同步策略"><a href="#1）副本数据同步策略" class="headerlink" title="1）副本数据同步策略"></a>1）副本数据同步策略</h5><table>
<thead>
<tr>
<th>方案</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody><tr>
<td>半数以上完成同步，就发送ack</td>
<td>延迟低</td>
<td>选举新的leader时，容忍n台节点的故障，需要2n+1个副本</td>
</tr>
<tr>
<td>全部完成同步，才发送ack</td>
<td>选举新的leader时，容忍n台节点的故障，需要n+1个副本</td>
<td>延迟高</td>
</tr>
</tbody></table>
<p>Kafka选择了第二种方案，原因如下：</p>
<p>（1）同样为了容忍n台节点的故障，第一种方案需要2n+1个副本，而第二种方案只需要n+1个副本，而Kafka的每个分区都有大量的数据，第一种方案会造成大量数据的冗余。</p>
<p>（2）虽然第二种方案的网络延迟会比较高，但网络延迟对Kafka的影响较小。</p>
<h5 id="2）ISR"><a href="#2）ISR" class="headerlink" title="2）ISR"></a>2）ISR</h5><p>采用第二种方案之后，设想以下情景：leader收到数据，所有follower都开始同步数据，但有一个follower，因为某种故障，迟迟不能与leader进行同步，那leader就要一直等下去，直到它完成同步，才能发送ack。这个问题怎么解决呢？</p>
<p>Leader维护了一个动态的in-sync replica set (ISR)，意为和leader保持同步的follower集合。当ISR中的follower完成数据的同步之后，leader就会给producer发送ack。如果follower长时间未向leader同步数据，则该follower将被踢出ISR，该时间阈值由<strong>replica.lag.time.max.ms</strong>参数设定。Leader发生故障之后，就会从ISR中选举新的leader。</p>
<h5 id="3）ack应答机制"><a href="#3）ack应答机制" class="headerlink" title="3）ack应答机制"></a>3）ack应答机制</h5><p>对于某些不太重要的数据，对数据的可靠性要求不是很高，能够容忍数据的少量丢失，所以没必要等ISR中的follower全部接收成功。</p>
<p>所以Kafka为用户提供了三种可靠性级别，用户根据对可靠性和延迟的要求进行权衡，选择以下的配置。</p>
<p>acks参数配置：</p>
<p>acks：</p>
<p>0：producer不等待broker的ack，这一操作提供了一个最低的延迟，broker一接收到还没有写入磁盘就已经返回，当broker故障时有可能<strong>丢失数据</strong>；</p>
<p>1：producer等待broker的ack，partition的leader落盘成功后返回ack，如果在follower同步成功之前leader故障，那么将会<strong>丢失数据</strong>；</p>
<p><img src="/Kafka/159.png" alt="159"></p>
<p>-1（all）：producer等待broker的ack，partition的leader和follower全部落盘成功后才返回ack。但是如果在follower同步完成后，broker发送ack之前，leader发生故障，那么会造成<strong>数据重复</strong>。</p>
<p><img src="/Kafka/160.png" alt="160"></p>
<h5 id="4）故障处理细节"><a href="#4）故障处理细节" class="headerlink" title="4）故障处理细节"></a>4）故障处理细节</h5><p><img src="/Kafka/161.png" alt="161"></p>
<p>（1）follower故障</p>
<p>follower发生故障后会被临时踢出ISR，待该follower恢复后，follower会读取本地磁盘记录的上次的HW，并将log文件高于HW的部分截取掉，从HW开始向leader进行同步。等该follower的LEO大于等于该Partition的HW，即follower追上leader之后，就可以重新加入ISR了。</p>
<p>（2）leader故障</p>
<p>leader发生故障之后，会从ISR中选出一个新的leader，之后，为保证多个副本之间的数据一致性，其余的follower会先将各自的log文件高于HW的部分截掉，然后从新的leader同步数据。</p>
<p>注意：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。</p>
<h4 id="Exactly-Once语义"><a href="#Exactly-Once语义" class="headerlink" title="Exactly Once语义"></a>Exactly Once语义</h4><p><strong>将服务器的ACK级别设置为-1，可以保证Producer到Server之间不会丢失数据，即At Least Once语义。相对的，将服务器ACK级别设置为0，可以保证生产者每条消息只会被发送一次，即At Most Once语义。</strong></p>
<p>At Least Once可以保证数据不丢失，但是不能保证数据不重复；相对的，At Most Once可以保证数据不重复，但是不能保证数据不丢失。但是，对于一些非常重要的信息，比如说交易数据，下游数据消费者要求数据既不重复也不丢失，即Exactly Once语义。在0.11版本以前的Kafka，对此是无能为力的，只能保证数据不丢失，再在下游消费者对数据做全局去重。对于多个下游应用的情况，每个都需要单独做全局去重，这就对性能造成了很大影响。</p>
<p>0.11版本的Kafka，引入了一项重大特性：幂等性。<strong>所谓的幂等性就是指Producer不论向Server发送多少次重复数据，Server端都只会持久化一条</strong>。幂等性结合At Least Once语义，就构成了Kafka的Exactly Once语义。即：</p>
<p>At Least Once + 幂等性 = Exactly Once</p>
<p><strong>要启用幂等性，只需要将Producer的参数中enable.idompotence设置为true即可</strong>。Kafka的幂等性实现其实就是将原来下游需要做的去重放在了数据上游。开启幂等性的Producer在初始化的时候会被分配一个PID，发往同一Partition的消息会附带Sequence Number。而Broker端会对&lt;PID, Partition, SeqNumber&gt;做缓存，当具有相同主键的消息提交时，Broker只会持久化一条。</p>
<p>但是PID重启就会变化，同时不同的Partition也具有不同主键，所以幂等性无法保证跨分区跨会话的Exactly Once。</p>
<h3 id="Kafka消费者"><a href="#Kafka消费者" class="headerlink" title="Kafka消费者"></a>Kafka消费者</h3><h4 id="消费方式"><a href="#消费方式" class="headerlink" title="消费方式"></a>消费方式</h4><p>consumer采用pull（拉）模式从broker中读取数据。</p>
<p>push（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而pull模式则可以根据consumer的消费能力以适当的速率消费消息。</p>
<p>pull模式不足之处是，如果kafka没有数据，消费者可能会陷入循环中，一直返回空数据。针对这一点，Kafka的消费者在消费数据时会传入一个时长参数timeout，<strong>如果当前没有数据可供消费，consumer会等待一段时间之后再返回，这段时长即为timeout</strong>。</p>
<h4 id="分区分配策略"><a href="#分区分配策略" class="headerlink" title="分区分配策略"></a>分区分配策略</h4><p>一个consumer group中有多个consumer，一个 topic有多个partition，所以必然会涉及到partition的分配问题，即确定那个partition由哪个consumer来消费。</p>
<p><strong>Kafka有3种分配策略，RangeAssignor，RoundRobinAssignor，StickyAssignor。</strong></p>
<h5 id="RangeAssignor"><a href="#RangeAssignor" class="headerlink" title="RangeAssignor"></a><strong>RangeAssignor</strong></h5><p>RangeAssignor对每个Topic进行独立的分区分配。对于每一个Topic，首先对分区按照分区ID进行排序，然后订阅这个Topic的消费组的消费者再进行排序，之后尽量均衡的将分区分配给消费者。这里只能是尽量均衡，因为分区数可能无法被消费者数量整除，那么有一些消费者就会多分配到一些分区。</p>
<p><img src="/Kafka/%E6%88%AA%E5%B1%8F2021-12-28%20%E4%B8%8B%E5%8D%882.34.50.png" alt="截屏2021-12-28 下午2.34.50"></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">assign(topic, consumers) &#123;</span><br><span class="line">  <span class="comment">// 对分区和Consumer进行排序</span></span><br><span class="line">  List&lt;Partition&gt; partitions = topic.getPartitions();</span><br><span class="line">  sort(partitions);</span><br><span class="line">  sort(consumers);</span><br><span class="line">  <span class="comment">// 计算每个Consumer分配的分区数</span></span><br><span class="line">  <span class="keyword">int</span> numPartitionsPerConsumer = partition.size() / consumers.size();</span><br><span class="line">  <span class="comment">// 额外有一些Consumer会多分配到分区</span></span><br><span class="line">  <span class="keyword">int</span> consumersWithExtraPartition = partition.size() % consumers.size();</span><br><span class="line">  <span class="comment">// 计算分配结果</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>, n = consumers.size(); i &lt; n; i++) &#123;</span><br><span class="line">    <span class="comment">// 第i个Consumer分配到的分区的index</span></span><br><span class="line">        <span class="keyword">int</span> start = numPartitionsPerConsumer * i + Math.min(i, consumersWithExtraPartition);</span><br><span class="line">        <span class="comment">// 第i个Consumer分配到的分区数</span></span><br><span class="line">        <span class="keyword">int</span> length = numPartitionsPerConsumer + (i + <span class="number">1</span> &gt; consumersWithExtraPartition ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">        <span class="comment">// 分装分配结果</span></span><br><span class="line">        assignment.get(consumersForTopic.get(i)).addAll(partitions.subList(start, start + length));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>RangeAssignor策略的原理是按照消费者总数和分区总数进行整除运算来获得一个跨度，然后将分区按照跨度进行平均分配，以保证分区尽可能均匀地分配给所有的消费者。对于每一个Topic，RangeAssignor策略会将消费组内所有订阅这个Topic的消费者按照名称的字典序排序，然后为每个消费者划分固定的分区范围，如果<strong>不够平均分配，那么字典序靠前的消费者会被多分配一个分区</strong>。</p>
<p>这种分配方式明显的一个问题是随着消费者订阅的Topic的数量的增加，不均衡的问题会越来越严重，比如上图中4个分区3个消费者的场景，C0会多分配一个分区。如果此时再订阅一个分区数为4的Topic，那么C0又会比C1、C2多分配一个分区，这样C0总共就比C1、C2多分配两个分区了，而且随着Topic的增加，这个情况会越来越严重。</p>
<p>分配结果：</p>
<p>订阅2个Topic，每个Topic4个分区，共3个Consumer</p>
<p>C0：[T0P0，T0P1，T1P0，T1P1]</p>
<p>C1：[T0P2，T1P2]</p>
<p>C2：[T0P3，T1P3]</p>
<h5 id="RoundRobinAssignor"><a href="#RoundRobinAssignor" class="headerlink" title="RoundRobinAssignor"></a>RoundRobinAssignor</h5><p>RoundRobinAssignor的分配策略是将消费组内订阅的所有Topic的分区及所有消费者进行排序后尽量均衡的分配（RangeAssignor是针对单个Topic的分区进行排序分配的）。如果消费组内，消费者订阅的Topic列表是相同的（每个消费者都订阅了相同的Topic），那么分配结果是尽量均衡的（消费者之间分配到的分区数的差值不会超过1）。如果订阅的Topic列表是不同的，那么分配结果是不保证“尽量均衡”的，因为某些消费者不参与一些Topic的分配。</p>
<p>RangeAssignor和RoundRobinAssignor(消费相同的topic)对比：</p>
<p><img src="/Kafka/%E6%88%AA%E5%B1%8F2021-12-28%20%E4%B8%8B%E5%8D%882.39.32.png" alt="截屏2021-12-28 下午2.39.32"></p>
<p>相对于RangeAssignor，在订阅多个Topic的情况下，RoundRobinAssignor的方式能使消费者之间尽量均衡的分配到分区（分配到的分区数的差值不会超过1——RangeAssignor的分配策略可能随着订阅的Topic越来越多，差值越来越大）。</p>
<p>对于组内消费者订阅Topic不一致的情况：假设有三个消费者分别为C0、C1、C2，有3个Topic T0、T1、T2，分别拥有1、2、3个分区，并且C0订阅T0，C1订阅T0和T1，C2订阅T0、T1、T0，那么RoundRobinAssignor的分配结果如下：</p>
<p><img src="/Kafka/%E6%88%AA%E5%B1%8F2021-12-28%20%E4%B8%8B%E5%8D%882.53.13.png" alt="截屏2021-12-28 下午2.53.13"></p>
<h5 id="StickyAssignor"><a href="#StickyAssignor" class="headerlink" title="StickyAssignor"></a>StickyAssignor</h5><p>尽管RoundRobinAssignor已经在RangeAssignor上做了一些优化来更均衡的分配分区，但是在一些情况下依旧会产生严重的分配偏差，<strong>比如消费组中订阅的Topic列表不相同的情况下</strong>（这个情况可能更多的发生在发布阶段，但是这真的是一个问题吗？——可以参照Kafka官方的说明：KIP-49 Fair Partition Assignment Strategy）。更核心的问题是<strong>无论是RangeAssignor，还是RoundRobinAssignor，当前的分区分配算法都没有考虑上一次的分配结果</strong>。显然，在执行一次新的分配之前，如果能考虑到上一次分配的结果，尽量少的调整分区分配的变动，显然是能节省很多开销的。</p>
<p>从字面意义上看，Sticky是“粘性的”，可以理解为分配结果是带“粘性的”——每一次分配变更相对上一次分配做最少的变动（上一次的结果是有粘性的），其目标有两点：</p>
<ol>
<li><p><strong>分区的分配尽量的均衡</strong></p>
</li>
<li><p><strong>每一次重分配的结果尽量与上一次分配结果保持一致</strong></p>
</li>
</ol>
<p>当这两个目标发生冲突时，优先保证第一个目标。第一个目标是每个分配算法都尽量尝试去完成的，而第二个目标才真正体现出StickyAssignor特性的。 </p>
<p>我们先来看预期分配的结构，后续再具体分析StickyAssignor的算法实现。</p>
<p>例如：</p>
<ul>
<li>有3个Consumer：C0、C1、C2</li>
<li>有4个Topic：T0、T1、T2、T3，每个Topic有2个分区</li>
<li>所有Consumer都订阅了这4个分区</li>
</ul>
<p>StickyAssignor的分配结果如下图所示（增加RoundRobinAssignor分配作为对比）：</p>
<p><img src="/Kafka/%E6%88%AA%E5%B1%8F2021-12-28%20%E4%B8%8B%E5%8D%882.58.53.png" alt="截屏2021-12-28 下午2.58.53"></p>
<p>上面的例子中，Sticky模式原来分配给C0、C2的分区都没有发生变动，且最终C0、C1达到的均衡的目的。</p>
<p>再举一个例子：</p>
<ul>
<li>有3个Consumer：C0、C1、C2</li>
<li>3个Topic：T0、T1、T2，它们分别有1、2、3个分区</li>
<li>C0订阅T0；C1订阅T0、T1；C2订阅T0、T1、T2</li>
</ul>
<p>分配结果如下图所示：</p>
<p><img src="/Kafka/%E6%88%AA%E5%B1%8F2021-12-28%20%E4%B8%8B%E5%8D%883.01.35.png" alt="截屏2021-12-28 下午3.01.35"></p>
<h4 id="offset的维护"><a href="#offset的维护" class="headerlink" title="offset的维护"></a>offset的维护</h4><p>由于consumer在消费过程中可能会出现断电宕机等故障，consumer恢复后，需要从故障前的位置的继续消费，所以<strong>consumer需要实时记录自己消费到了哪个offset，以便故障恢复后继续消费</strong>。</p>
<p>Kafka 0.9版本之前，consumer默认将offset保存在Zookeeper中，从0.9版本开始，consumer默认将offset保存在Kafka一个内置的topic中，该topic为<strong>consumer_offsets</strong>。</p>
<h3 id="Kafka高效读写数据"><a href="#Kafka高效读写数据" class="headerlink" title="Kafka高效读写数据"></a>Kafka高效读写数据</h3><p><strong>1）顺序写磁盘</strong></p>
<p>Kafka的producer生产数据，要写入到log文件中，写的过程是一直追加到文件末端，为顺序写。官网有数据表明，同样的磁盘，顺序写能到到600M/s，而随机写只有100k/s。这与磁盘的机械机构有关，顺序写之所以快，是因为其省去了大量磁头寻址的时间。</p>
<p><strong>2）应用Pagecache</strong></p>
<p>Kafka数据持久化是直接持久化到Pagecache中，这样会产生以下几个好处： </p>
<p>I/O Scheduler 会将连续的小块写组装成大块的物理写从而提高性能</p>
<p>I/O Scheduler 会尝试将一些写操作重新按顺序排好，从而减少磁盘头的移动时间</p>
<p>充分利用所有空闲内存（非 JVM 内存）。如果使用应用层 Cache（即 JVM 堆内存），会增加 GC 负担</p>
<p>读操作可直接在 Page Cache 内进行。如果消费和生产速度相当，甚至不需要通过物理磁盘（直接通过 Page Cache）交换数据</p>
<p>如果进程重启，JVM 内的 Cache 会失效，但 Page Cache 仍然可用</p>
<p>尽管持久化到Pagecache上可能会造成宕机丢失数据的情况，但这可以被Kafka的Replication机制解决。如果为了保证这种情况下数据不丢失而强制将 Page Cache 中的数据 Flush 到磁盘，反而会降低性能。</p>
<p><strong>3）零复制技术</strong></p>
<p><img src="/Kafka/162.png" alt="162"></p>
<h3 id="Zookeeper在Kafka中的作用"><a href="#Zookeeper在Kafka中的作用" class="headerlink" title="Zookeeper在Kafka中的作用"></a>Zookeeper在Kafka中的作用</h3><p>Kafka集群中有一个broker会被选举为Controller，负责管理集群broker的上下线，所有topic的分区副本分配和leader选举等工作。</p>
<p>Controller的管理工作都是依赖于Zookeeper的。</p>
<p>以下为partition的leader选举过程：</p>
<p><img src="/Kafka/163.png" alt="163"></p>
<h3 id="Kafka事务"><a href="#Kafka事务" class="headerlink" title="Kafka事务"></a>Kafka事务</h3><p>Kafka从0.11版本开始引入了事务支持。事务可以保证Kafka在Exactly Once语义的基础上，生产和消费可以跨分区和会话，要么全部成功，要么全部失败。</p>
<h4 id="Producer事务"><a href="#Producer事务" class="headerlink" title="Producer事务"></a>Producer事务</h4><p>为了实现跨分区跨会话的事务，需要引入一个<strong>全局唯一的Transaction ID</strong>，并ack。这样当Producer重启后就可以通过正在进行的Transaction ID获得原来的PID。</p>
<p>为了管理Transaction，Kafka引入了一个新的组件<strong>Transaction Coordinator</strong>。Producer就是通过和Transaction Coordinator交互获得Transaction ID对应的任务状态。Transaction Coordinator还负责将所有事务写入Kafka的一个内部Topic，这样即使整个服务重启，由于事务状态得到保存，进行中的事务状态可以得到恢复，从而继续进行。</p>
<h4 id="Consumer事务（精准一次性消费）"><a href="#Consumer事务（精准一次性消费）" class="headerlink" title="Consumer事务（精准一次性消费）"></a>Consumer事务（精准一次性消费）</h4><p>上述事务机制主要是从Producer方面考虑，对于Consumer而言，事务的保证就会相对较弱，尤其是无法保证Commit的信息被精确消费。这是由于Consumer可以通过offset访问任意信息，而且不同的Segment File生命周期不同，同一事务的消息可能会出现重启后被删除的情况。</p>
<p>如果想完成Consumer端的精准一次性消费，那么需要<strong>kafka消费端将消费过程和提交offset过程做原子绑定</strong>。此时我们需<strong>要将kafka的offset保存到支持事务的自定义介质中（比如mysql）</strong>。这部分知识会在后续项目部分涉及。</p>
<h2 id="Kafka-API"><a href="#Kafka-API" class="headerlink" title="Kafka API"></a>Kafka API</h2><h3 id="Producer-API"><a href="#Producer-API" class="headerlink" title="Producer API"></a>Producer API</h3><h4 id="消息发送流程"><a href="#消息发送流程" class="headerlink" title="消息发送流程"></a>消息发送流程</h4><p>Kafka的Producer发送消息采用的是<strong>异步发送</strong>的方式。在消息发送的过程中，涉及到了两个线程——main线程和Sender线程，以及一个线程共享变量——RecordAccumulator。main线程将消息发送给RecordAccumulator，Sender线程不断从RecordAccumulator中拉取消息发送到Kafka broker。</p>
<p><img src="/Kafka/164.png" alt="164"></p>
<p>相关参数：</p>
<p><strong>batch.size</strong>: 只有数据积累到batch.size之后，sender才会发送数据。</p>
<p><strong>linger.ms</strong>: 如果数据迟迟未达到batch.size，sender等待linger.time之后就会发送数据。</p>
<h4 id="异步发送API"><a href="#异步发送API" class="headerlink" title="异步发送API"></a>异步发送API</h4><p>1）导入依赖</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>2）编写代码</p>
<p>需要用到的类：</p>
<p><strong>KafkaProducer</strong>：需要创建一个生产者对象，用来发送数据</p>
<p><strong>ProducerConfig</strong>：获取所需的一系列配置参数</p>
<p><strong>ProducerRecord</strong>：每条数据都要封装成一个ProducerRecord对象</p>
<p>（1）不带回调函数的API</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.*;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ExecutionException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducer</span> </span>&#123;    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ExecutionException, InterruptedException </span>&#123;  </span><br><span class="line"></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();     </span><br><span class="line"></span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;linux1:9092&quot;</span>);    <span class="comment">//kafka集群，broker-list        </span></span><br><span class="line">        props.put(<span class="string">&quot;acks&quot;</span>, <span class="string">&quot;all&quot;</span>);                         <span class="comment">//-1        </span></span><br><span class="line">        props.put(<span class="string">&quot;retries&quot;</span>, <span class="number">1</span>);                          <span class="comment">//重试次数        </span></span><br><span class="line">        props.put(<span class="string">&quot;batch.size&quot;</span>, <span class="number">16384</span>);                   <span class="comment">//批次大小 16Kb        </span></span><br><span class="line">        props.put(<span class="string">&quot;linger.ms&quot;</span>, <span class="number">1</span>);                        <span class="comment">//等待时间        </span></span><br><span class="line">        props.put(<span class="string">&quot;buffer.memory&quot;</span>, <span class="number">33554432</span>);             <span class="comment">//RecordAccumulator缓冲区大小,缓冲区存放RecordBatch    </span></span><br><span class="line">        props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        Producer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;            </span><br><span class="line">            producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(<span class="string">&quot;first&quot;</span>, Integer.toString(i), Integer.toString(i)));            </span><br><span class="line">            <span class="comment">// topic, Partition, k, v                </span></span><br><span class="line">            <span class="comment">// Partition 和 k 可以省略        </span></span><br><span class="line">        &#125;      </span><br><span class="line"></span><br><span class="line">        producer.close();    </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（2）带回调函数的API</p>
<p>回调函数会在producer收到ack时调用，为异步调用，该方法有两个参数，分别是RecordMetadata和Exception，如果Exception为null，说明消息发送成功，如果Exception不为null，说明消息发送失败。</p>
<p>注意：消息发送失败会自动重试，不需要我们在回调函数中手动重试。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.*;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ExecutionException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducer</span> </span>&#123;    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ExecutionException, InterruptedException </span>&#123;        </span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();        </span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;linux1:9092&quot;</span>);<span class="comment">//kafka集群，broker-list        </span></span><br><span class="line">        props.put(<span class="string">&quot;acks&quot;</span>, <span class="string">&quot;all&quot;</span>);        </span><br><span class="line">        props.put(<span class="string">&quot;retries&quot;</span>, <span class="number">1</span>);<span class="comment">//重试次数        </span></span><br><span class="line">        props.put(<span class="string">&quot;batch.size&quot;</span>, <span class="number">16384</span>);<span class="comment">//批次大小        </span></span><br><span class="line">        props.put(<span class="string">&quot;linger.ms&quot;</span>, <span class="number">1</span>);<span class="comment">//等待时间        </span></span><br><span class="line">        props.put(<span class="string">&quot;buffer.memory&quot;</span>, <span class="number">33554432</span>);<span class="comment">//RecordAccumulator缓冲区大小        </span></span><br><span class="line">        props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);        </span><br><span class="line">        props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);        </span><br><span class="line"></span><br><span class="line">        Producer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);        </span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;            </span><br><span class="line">            producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(<span class="string">&quot;first&quot;</span>, Integer.toString(i), Integer.toString(i)), <span class="keyword">new</span> Callback() &#123;                </span><br><span class="line">                <span class="comment">//回调函数，该方法会在Producer收到ack时调用，为异步调用                </span></span><br><span class="line">                <span class="meta">@Override</span>                </span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCompletion</span><span class="params">(RecordMetadata metadata, Exception exception)</span> </span>&#123;                    </span><br><span class="line">                    <span class="keyword">if</span> (exception == <span class="keyword">null</span>) &#123;                        </span><br><span class="line">                        System.out.println(<span class="string">&quot;success-&gt;&quot;</span> + metadata.offset());                    </span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;                        </span><br><span class="line">                        exception.printStackTrace();                    </span><br><span class="line">                    &#125;                </span><br><span class="line">                &#125;            </span><br><span class="line">            &#125;);        </span><br><span class="line">        &#125;        </span><br><span class="line"></span><br><span class="line">        producer.close();    </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="同步发送API"><a href="#同步发送API" class="headerlink" title="同步发送API"></a>同步发送API</h4><p><strong>同步发送的意思就是，一条消息发送之后，会阻塞当前线程，直至返回ack。</strong></p>
<p>由于send方法返回的是一个Future对象，根据Futrue对象的特点，我们也可以实现同步发送的效果，<strong>只需调用Future对象的get方法即可</strong>。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.Producer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties; </span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ExecutionException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducer</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ExecutionException, InterruptedException </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();        </span><br><span class="line"></span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;linux1:9092&quot;</span>);                  <span class="comment">//kafka集群，broker-list        </span></span><br><span class="line">        props.put(<span class="string">&quot;acks&quot;</span>, <span class="string">&quot;all&quot;</span>);        props.put(<span class="string">&quot;retries&quot;</span>, <span class="number">1</span>);       <span class="comment">//重试次数        </span></span><br><span class="line">        props.put(<span class="string">&quot;batch.size&quot;</span>, <span class="number">16384</span>);                                 <span class="comment">//批次大小        </span></span><br><span class="line">        props.put(<span class="string">&quot;linger.ms&quot;</span>, <span class="number">1</span>);                                      <span class="comment">//等待时间        </span></span><br><span class="line">        props.put(<span class="string">&quot;buffer.memory&quot;</span>, <span class="number">33554432</span>);                           <span class="comment">//RecordAccumulator缓冲区大小        </span></span><br><span class="line">        props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);        </span><br><span class="line">        props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);        </span><br><span class="line">        Producer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);        </span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;            </span><br><span class="line">            producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(<span class="string">&quot;first&quot;</span>, Integer.toString(i), Integer.toString(i))).get();        </span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        producer.close();    </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Consumer-API"><a href="#Consumer-API" class="headerlink" title="Consumer API"></a>Consumer API</h3><p>Consumer消费数据时的可靠性是很容易保证的，因为数据在Kafka中是持久化的，故不用担心数据丢失问题。</p>
<p>由于consumer在消费过程中可能会出现断电宕机等故障，consumer恢复后，需要从故障前的位置继续消费，<strong>所以consumer需要实时记录自己消费到了哪个offset</strong>，以便故障恢复后继续消费。</p>
<p>所以offset的维护是Consumer消费数据是必须考虑的问题。</p>
<h4 id="自动提交offset"><a href="#自动提交offset" class="headerlink" title="自动提交offset"></a>自动提交offset</h4><p><strong>1）导入依赖</strong></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka&lt;/groupId &gt;</span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><strong>2）编写代码</strong></p>
<p>需要用到的类：</p>
<p><strong>KafkaConsumer</strong>：需要创建一个消费者对象，用来消费数据</p>
<p><strong>ConsumerConfig</strong>：获取所需的一系列配置参数</p>
<p><strong>ConsuemrRecord</strong>：每条数据都要封装成一个ConsumerRecord对象</p>
<p>为了使我们能够专注于自己的业务逻辑，Kafka提供了自动提交offset的功能。 </p>
<p><strong>自动提交offset的相关参数</strong>：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">enable.auto.commit：是否开启自动提交offset功能</span><br><span class="line">auto.commit.interval.ms：自动提交offset的时间间隔</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;<span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomConsumer</span> </span>&#123;    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;        </span><br><span class="line"></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();        </span><br><span class="line"></span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;linux1:9092&quot;</span>);        </span><br><span class="line">        props.put(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;test&quot;</span>);        </span><br><span class="line">        props.put(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;true&quot;</span>);        </span><br><span class="line">        props.put(<span class="string">&quot;auto.commit.interval.ms&quot;</span>, <span class="string">&quot;1000&quot;</span>);        </span><br><span class="line">        props.put(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);        </span><br><span class="line">        props.put(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);              </span><br><span class="line"></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);        </span><br><span class="line"></span><br><span class="line">        consumer.subscribe(Arrays.asList(<span class="string">&quot;first&quot;</span>));        </span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;            </span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);            </span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records)                </span><br><span class="line">                System.out.printf(<span class="string">&quot;offset = %d, key = %s, value = %s%n&quot;</span>, record.offset(), record.key(), record.value());        </span><br><span class="line">        &#125;    </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="手动提交offset"><a href="#手动提交offset" class="headerlink" title="手动提交offset"></a>手动提交offset</h4><p>虽然自动提交offset十分简介便利，但由于其是基于时间提交的，开发人员难以把握offset提交的时机。因此Kafka还提供了手动提交offset的API。</p>
<p>手动提交offset的方法有两种：分别是<strong>commitSync（同步提交）</strong>和<strong>commitAsync（异步提交）</strong>。两者的相同点是，都会将本次poll的一批数据最高的偏移量提交；不同点是，commitSync阻塞当前线程，一直到提交成功，并且会自动失败重试（由不可控因素导致，也会出现提交失败）；而commitAsync则没有失败重试机制，故有可能提交失败。</p>
<h5 id="1）同步提交offset"><a href="#1）同步提交offset" class="headerlink" title="1）同步提交offset"></a>1）同步提交offset</h5><p>由于同步提交offset有<strong>失败重试机制</strong>，故更加可靠，以下为同步提交offset的示例。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomComsumer</span> </span>&#123;    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;        </span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();        </span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;linux1:9092&quot;</span>);<span class="comment">//Kafka集群        </span></span><br><span class="line">        props.put(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;test&quot;</span>);<span class="comment">//消费者组，只要group.id相同，就属于同一个消费者组        </span></span><br><span class="line">        props.put(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;false&quot;</span>);<span class="comment">//关闭自动提交offset        </span></span><br><span class="line">        props.put(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);        </span><br><span class="line">        props.put(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);        </span><br><span class="line">        </span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);  </span><br><span class="line"></span><br><span class="line">        consumer.subscribe(Arrays.asList(<span class="string">&quot;first&quot;</span>));<span class="comment">//消费者订阅主题        </span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;      </span><br><span class="line"></span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);<span class="comment">//消费者拉取数据            </span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;                </span><br><span class="line">                System.out.printf(<span class="string">&quot;offset = %d, key = %s, value = %s%n&quot;</span>, record.offset(), record.key(), record.value());            </span><br><span class="line">            &#125;            </span><br><span class="line"></span><br><span class="line">            consumer.commitSync();<span class="comment">//同步提交，当前线程会阻塞直到offset提交成功        </span></span><br><span class="line">        &#125;    </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="2）异步提交offset"><a href="#2）异步提交offset" class="headerlink" title="2）异步提交offset"></a>2）异步提交offset</h5><p>虽然同步提交offset更可靠一些，但是由于其会阻塞当前线程，直到提交成功。因此吞吐量会收到很大的影响。因此更多的情况下，会选用异步提交offset的方式。</p>
<p>以下为异步提交offset的示例：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.TopicPartition;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays; </span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomConsumer</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();        </span><br><span class="line"></span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;linux1:9092&quot;</span>);<span class="comment">//Kafka集群        </span></span><br><span class="line">        props.put(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;test&quot;</span>);<span class="comment">//消费者组，只要group.id相同，就属于同一个消费者组        </span></span><br><span class="line">        props.put(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;false&quot;</span>);<span class="comment">//关闭自动提交offset        </span></span><br><span class="line">        props.put(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);        </span><br><span class="line">        props.put(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);        </span><br><span class="line"></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);        </span><br><span class="line"></span><br><span class="line">        consumer.subscribe(Arrays.asList(<span class="string">&quot;first&quot;</span>));<span class="comment">//消费者订阅主题        </span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;            </span><br><span class="line"></span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);<span class="comment">//消费者拉取数据            </span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;  </span><br><span class="line">                System.out.printf(<span class="string">&quot;offset = %d, key = %s, value = %s%n&quot;</span>, record.offset(), record.key(), record.value());            </span><br><span class="line">            &#125;            </span><br><span class="line"></span><br><span class="line">            consumer.commitAsync(<span class="keyword">new</span> OffsetCommitCallback() &#123;                </span><br><span class="line">                <span class="meta">@Override</span>                </span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onComplete</span><span class="params">(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, Exception exception)</span> </span>&#123;                    </span><br><span class="line">                    <span class="keyword">if</span> (exception != <span class="keyword">null</span>) &#123;                        </span><br><span class="line">                        System.err.println(<span class="string">&quot;Commit failed for&quot;</span> + offsets);                    </span><br><span class="line">                    &#125;                </span><br><span class="line">                &#125;            </span><br><span class="line">            &#125;);<span class="comment">//异步提交        </span></span><br><span class="line">        &#125;    </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="数据漏消费和重复消费"><a href="#数据漏消费和重复消费" class="headerlink" title="数据漏消费和重复消费"></a>数据漏消费和重复消费</h5><p>无论是同步提交还是异步提交offset，都有可能会造成数据的漏消费或者重复消费。<strong>先提交offset后消费，有可能造成数据的漏消费；而先消费后提交offset，有可能会造成数据的重复消费。</strong></p>
<h2 id="安装部署"><a href="#安装部署" class="headerlink" title="安装部署"></a>安装部署</h2><h3 id="jar包下载"><a href="#jar包下载" class="headerlink" title="jar包下载"></a>jar包下载</h3><p><a href="http://kafka.apache.org/downloads">http://kafka.apache.org/downloads</a></p>
<h3 id="集群部署"><a href="#集群部署" class="headerlink" title="集群部署"></a>集群部署</h3><p>1）解压安装包</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 software]$ tar -zxvf kafka_2.11-2.4.1.tgz -C /opt/module/</span><br></pre></td></tr></table></figure>

<p>2）修改解压后的文件名称</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 module]$ mv kafka_2.11-2.4.1 kafka</span><br></pre></td></tr></table></figure>

<p>3）在/opt/module/kafka目录下创建logs文件夹</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 kafka]$ mkdir logs</span><br></pre></td></tr></table></figure>

<p>4）修改配置文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 kafka]$ cd config</span><br><span class="line">[vincent@linux1 config]$ vim server.properties</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">broker的全局唯一编号，不能重复</span></span><br><span class="line">broker.id=0</span><br><span class="line"><span class="meta">#</span><span class="bash">删除topic功能使能</span></span><br><span class="line">delete.topic.enable=true</span><br><span class="line"><span class="meta">#</span><span class="bash">处理网络请求的线程数量</span></span><br><span class="line">num.network.threads=3</span><br><span class="line"><span class="meta">#</span><span class="bash">用来处理磁盘IO的线程数量</span></span><br><span class="line">num.io.threads=8</span><br><span class="line"><span class="meta">#</span><span class="bash">发送套接字的缓冲区大小</span></span><br><span class="line">socket.send.buffer.bytes=102400</span><br><span class="line"><span class="meta">#</span><span class="bash">接收套接字的缓冲区大小</span></span><br><span class="line">socket.receive.buffer.bytes=102400</span><br><span class="line"><span class="meta">#</span><span class="bash">请求套接字的缓冲区大小</span></span><br><span class="line">socket.request.max.bytes=104857600</span><br><span class="line"><span class="meta">#</span><span class="bash">kafka运行日志存放的路径</span></span><br><span class="line">log.dirs=/opt/module/kafka/logs</span><br><span class="line"><span class="meta">#</span><span class="bash">topic在当前broker上的分区个数</span></span><br><span class="line">num.partitions=1</span><br><span class="line"><span class="meta">#</span><span class="bash">用来恢复和清理data下数据的线程数量</span></span><br><span class="line">num.recovery.threads.per.data.dir=1</span><br><span class="line"><span class="meta">#</span><span class="bash">segment文件保留的最长时间，超时将被删除</span></span><br><span class="line">log.retention.hours=168</span><br><span class="line"><span class="meta">#</span><span class="bash">配置连接Zookeeper集群地址</span></span><br><span class="line">zookeeper.connect=linux1:2181,linux2:2181,linux3:2181/kafka</span><br></pre></td></tr></table></figure>

<p>5）配置环境变量</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 module]$ sudo vim /etc/profile.d/my_env.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">KAFKA_HOME</span></span><br><span class="line">export KAFKA_HOME=/opt/module/kafka</span><br><span class="line">export PATH=$PATH:$KAFKA_HOME/bin</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 module]$ source /etc/profile</span><br></pre></td></tr></table></figure>

<p>6）分发安装包</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 module]$ xsync kafka</span><br></pre></td></tr></table></figure>

<p>注意：分发之后记得配置其他机器的环境变量</p>
<p>7）分别在linux2和linux3上修改配置文件/opt/module/kafka/config/server.properties中的broker.id=1、broker.id=2</p>
<p>注：broker.id不得重复</p>
<p>8）启动集群</p>
<p>依次在linux1、linux2、linux3节点上启动kafka</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 kafka]$ kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties</span><br><span class="line">[vincent@linux2 kafka]$ kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties</span><br><span class="line">[vincent@linux3 kafka]$ kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties</span><br></pre></td></tr></table></figure>

<p>9）关闭集群</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 kafka]$ bin/kafka-server-stop.sh</span><br><span class="line">[vincent@linux2 kafka]$ bin/kafka-server-stop.sh</span><br><span class="line">[vincent@linux3 kafka]$ bin/kafka-server-stop.sh</span><br></pre></td></tr></table></figure>

<p>10）kafka群起脚本</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">if [ $# -lt 1 ] then        </span><br><span class="line">    echo &quot;No Args Input!!!!&quot;        </span><br><span class="line">    exit;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">for i in linux1 linux2 linux3</span><br><span class="line">do</span><br><span class="line">    case $1 in</span><br><span class="line">    &quot;start&quot;)        </span><br><span class="line">        echo &quot;=============== start $i kafka ==============&quot;        </span><br><span class="line">        ssh $i /opt/module/kafka/bin/kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties</span><br><span class="line">        ;;</span><br><span class="line">    &quot;stop&quot;)        </span><br><span class="line">        echo &quot;=============== stop $i kafka ===============&quot;        </span><br><span class="line">        ssh $i /opt/module/kafka/bin/kafka-server-stop.sh</span><br><span class="line">        ;;</span><br><span class="line">    *)        </span><br><span class="line">        echo &quot;Input Args Error!!!!&quot;</span><br><span class="line">        ;;</span><br><span class="line">    esac</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<h3 id="kafka命令行操作"><a href="#kafka命令行操作" class="headerlink" title="kafka命令行操作"></a>kafka命令行操作</h3><p>1）查看当前服务器中的所有topic</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 kafka]$ bin/kafka-topics.sh --zookeeper linux1:2181/kafka --list</span><br></pre></td></tr></table></figure>

<p>2）创建topic</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 kafka]$ bin/kafka-topics.sh --zookeeper linux1:2181/kafka --create --topic second --replication-factor 2 --partitions 2</span><br></pre></td></tr></table></figure>

<p>选项说明：</p>
<p>–topic 定义topic名</p>
<p>–replication-factor  定义副本数</p>
<p>–partitions  定义分区数</p>
<p>3）删除topic</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 kafka]$ bin/kafka-topics.sh --zookeeper linux1:2181/kafka --delete --topic first</span><br></pre></td></tr></table></figure>

<p>需要server.properties中设置delete.topic.enable=true否则只是标记删除。</p>
<p>4）发送消息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 kafka]$ bin/kafka-console-producer.sh --broker-list linux1:9092 --topic first</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">hello world</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">vincent  vincent</span></span><br></pre></td></tr></table></figure>

<p>5）消费消息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 kafka]$ bin/kafka-console-consumer.sh --bootstrap-server linux1:9092 --from-beginning --topic first</span><br></pre></td></tr></table></figure>

<p>–from-beginning：会把主题中以往所有的数据都读取出来。</p>
<p>6）查看某个Topic的详情</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 kafka]$ bin/kafka-topics.sh --zookeeper linux1:2181/kafka --describe --topic first</span><br></pre></td></tr></table></figure>

<p>7）修改分区数</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 kafka]$bin/kafka-topics.sh --zookeeper linux1:2181/kafka --alter --topic first --partitions 6</span><br></pre></td></tr></table></figure>

<h2 id="Kafka监控（Kafka-Eagle）"><a href="#Kafka监控（Kafka-Eagle）" class="headerlink" title="Kafka监控（Kafka Eagle）"></a>Kafka监控（Kafka Eagle）</h2><p>1）修改kafka启动命令</p>
<p>修改kafka-server-start.sh命令中</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">if [ &quot;x$KAFKA_HEAP_OPTS&quot; = &quot;x&quot; ]; </span><br><span class="line">    then export KAFKA_HEAP_OPTS=&quot;-Xmx1G -Xms1G&quot;</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<p>为</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">if [ &quot;x$KAFKA_HEAP_OPTS&quot; = &quot;x&quot; ]; then</span><br><span class="line">  export KAFKA_HEAP_OPTS=&quot;-server -Xms2G -Xmx2G -XX:PermSize=128m -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:ParallelGCThreads=8 -XX:ConcGCThreads=5 -XX:InitiatingHeapOccupancyPercent=70&quot;</span><br><span class="line">  export JMX_PORT=&quot;9999&quot;</span><br><span class="line"><span class="meta">  #</span><span class="bash"><span class="built_in">export</span> KAFKA_HEAP_OPTS=<span class="string">&quot;-Xmx1G -Xms1G&quot;</span></span></span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<p>注意：修改之后在启动Kafka之前要分发至其他节点</p>
<p>2）上传压缩包kafka-eagle-bin-1.4.5.tar.gz到集群/opt/software目录</p>
<p>3）解压到本地</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 software]$ tar -zxvf kafka-eagle-bin-1.4.5.tar.gz</span><br></pre></td></tr></table></figure>

<p>4）进入刚才解压的目录</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 kafka-eagle-bin-1.4.5]$ ll</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">总用量 82932</span><br><span class="line">-rw-rw-r--. 1 vincent vincent 84920710 8月  13 23:00 kafka-eagle-web-1.4.5-bin.tar.gz</span><br></pre></td></tr></table></figure>

<p>5）将kafka-eagle-web-1.3.7-bin.tar.gz解压至/opt/module</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 kafka-eagle-bin-1.3.7]$ tar -zxvf kafka-eagle-web-1.4.5-bin.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>

<p>6）修改名称</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 module]$ mv kafka-eagle-web-1.4.5 eagle</span><br></pre></td></tr></table></figure>

<p>7）给启动文件执行权限</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 eagle]$ cd bin</span><br><span class="line">[vincent@linux1 bin]$ ll</span><br><span class="line"></span><br><span class="line">总用量 12</span><br><span class="line">-rw-r--r--. 1 vincent vincent 1848 8月  22 2017 ke.bat</span><br><span class="line">-rw-r--r--. 1 vincent vincent 7190 7月  30 20:12 ke.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 bin]$ chmod 777 ke.sh</span><br></pre></td></tr></table></figure>

<p>8）修改配置文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#####################################</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> multi zookeeper&amp;kafka cluster list</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#####################################</span></span></span><br><span class="line"></span><br><span class="line">kafka.eagle.zk.cluster.alias=cluster1</span><br><span class="line">cluster1.zk.list=linux1:2181,linux2:2181,linux3:2181</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#####################################</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> kafka offset storage</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#####################################</span></span></span><br><span class="line"></span><br><span class="line">cluster1.kafka.eagle.offset.storage=kafka</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#####################################</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">enable</span> kafka metrics</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#####################################</span></span></span><br><span class="line"></span><br><span class="line">kafka.eagle.metrics.charts=true</span><br><span class="line">kafka.eagle.sql.fix.error=false</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#####################################</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> kafka jdbc driver address</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#####################################</span></span></span><br><span class="line"></span><br><span class="line">kafka.eagle.driver=com.mysql.jdbc.Driver</span><br><span class="line">kafka.eagle.url=jdbc:mysql://linux1:3306/ke?useUnicode=true&amp;characterEncoding=UTF-8&amp;zeroDateTimeBehavior=convertToNull</span><br><span class="line">kafka.eagle.username=root</span><br><span class="line">kafka.eagle.password=211819</span><br></pre></td></tr></table></figure>

<p>9）添加环境变量</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export KE_HOME=/opt/module/eagle</span><br><span class="line">export PATH=$PATH:$KE_HOME/bin</span><br></pre></td></tr></table></figure>

<p>注意：source /etc/profile</p>
<p>10）启动</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 eagle]$ bin/ke.sh start</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">... ...</span><br><span class="line">... ...</span><br><span class="line"></span><br><span class="line">*******************************************************************</span><br><span class="line"></span><br><span class="line">* Kafka Eagle Service has started success.</span><br><span class="line">* Welcome, Now you can visit &#x27;http://10.211.55.10:8048/ke&#x27;</span><br><span class="line">* Account:admin ,Password:211819</span><br><span class="line">*******************************************************************</span><br><span class="line">* &lt;Usage&gt; ke.sh [start|status|stop|restart|stats] &lt;/Usage&gt;</span><br><span class="line">* &lt;Usage&gt; https://www.kafka-eagle.org/ &lt;/Usage&gt;</span><br><span class="line">*******************************************************************</span><br><span class="line"></span><br><span class="line">[vincent@linux1 eagle]$</span><br></pre></td></tr></table></figure>

<p>注意：启动之前需要先启动ZK以及KAFKA</p>
<p>11）登录页面查看监控数据</p>
<p><a href="http://10.211.55.10:8048/ke">http://10.211.55.10:8048/ke</a></p>
]]></content>
      <categories>
        <category>实时计算</category>
      </categories>
  </entry>
  <entry>
    <title>Linux</title>
    <url>/Linux/</url>
    <content><![CDATA[<h2 id="VI-VIM编辑器"><a href="#VI-VIM编辑器" class="headerlink" title="VI/VIM编辑器"></a>VI/VIM编辑器</h2><p>一般模式：</p>
<table>
<thead>
<tr>
<th>语法</th>
<th>功能描述</th>
</tr>
</thead>
<tbody><tr>
<td>yy</td>
<td>复制光标当前一行</td>
</tr>
<tr>
<td>y数字y</td>
<td>复制一段（从第几行到第几行）</td>
</tr>
<tr>
<td>p</td>
<td>箭头移动到目的行粘贴</td>
</tr>
<tr>
<td>u</td>
<td>撤销上一步</td>
</tr>
<tr>
<td>dd</td>
<td>删除光标当前行</td>
</tr>
<tr>
<td>d数字d</td>
<td>删除光标（含）后多少行</td>
</tr>
<tr>
<td>x</td>
<td>删除一个字母，相当于del</td>
</tr>
<tr>
<td>X</td>
<td>删除一个字母，相当于Backspace</td>
</tr>
<tr>
<td>yw</td>
<td>复制一个词</td>
</tr>
<tr>
<td>dw</td>
<td>删除一个词</td>
</tr>
<tr>
<td>shift+^</td>
<td>移动到行头</td>
</tr>
<tr>
<td>shift+$</td>
<td>移动到<strong>行尾</strong></td>
</tr>
<tr>
<td>1+shift+g</td>
<td>移动到页头，数字</td>
</tr>
<tr>
<td>shift+g</td>
<td>移动到<strong>页尾</strong></td>
</tr>
<tr>
<td>数字N+shift+g</td>
<td>移动到<strong>目标行</strong></td>
</tr>
</tbody></table>
<p>编辑模式：</p>
<table>
<thead>
<tr>
<th>按键</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>i</td>
<td>当前光标前</td>
</tr>
<tr>
<td>a</td>
<td>当前光标后</td>
</tr>
<tr>
<td>o</td>
<td>当前光标行的下一行</td>
</tr>
<tr>
<td>I</td>
<td>光标所在行最前</td>
</tr>
<tr>
<td>A</td>
<td>光标所在行最后</td>
</tr>
<tr>
<td>O</td>
<td>当前光标行的上一行</td>
</tr>
</tbody></table>
<p>指令模式：</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>:w</td>
<td>保存</td>
</tr>
<tr>
<td>:q</td>
<td>退出</td>
</tr>
<tr>
<td>:!</td>
<td>强制执行</td>
</tr>
<tr>
<td>/要查找的词</td>
<td>n 查找下一个，N 往上查找</td>
</tr>
<tr>
<td>? 要查找的词</td>
<td>n是查找上一个，shift+n是往下查找</td>
</tr>
<tr>
<td>:set nu</td>
<td><strong>显示行号</strong></td>
</tr>
<tr>
<td>:set nonu</td>
<td><strong>关闭行号</strong></td>
</tr>
<tr>
<td>:%s/old/new/g</td>
<td>替换内容</td>
</tr>
</tbody></table>
<h2 id="网络配置和系统管理操作"><a href="#网络配置和系统管理操作" class="headerlink" title="网络配置和系统管理操作"></a>网络配置和系统管理操作</h2><h3 id="查看当前网络ip"><a href="#查看当前网络ip" class="headerlink" title="查看当前网络ip"></a>查看当前网络ip</h3><p>ifconfig</p>
<h3 id="测试主机之间网络连通性"><a href="#测试主机之间网络连通性" class="headerlink" title="测试主机之间网络连通性"></a>测试主机之间网络连通性</h3><p>ping 目的主机</p>
<p>ping <a href="http://www.baidu.com/">www.baidu.com</a></p>
<h3 id="修改IP地址"><a href="#修改IP地址" class="headerlink" title="修改IP地址"></a>修改IP地址</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim /etc/sysconfig/network-scripts/ifcfg-ens33</span><br><span class="line">vim /etc/sysconfig/network-scripts/ifcfg-eth0</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">TYPE=&quot;Ethernet&quot;</span><br><span class="line">PROXY_METHOD=&quot;none&quot;</span><br><span class="line">BROWSER_ONLY=&quot;no&quot;</span><br><span class="line">BOOTPROTO=static</span><br><span class="line">DEFROUTE=&quot;yes&quot;</span><br><span class="line">IPV4_FAILURE_FATAL=&quot;no&quot;</span><br><span class="line">IPV6INIT=&quot;yes&quot;</span><br><span class="line">IPV6_AUTOCONF=&quot;yes&quot;</span><br><span class="line">IPV6_DEFROUTE=&quot;yes&quot;</span><br><span class="line">IPV6_FAILURE_FATAL=&quot;no&quot;</span><br><span class="line">IPV6_ADDR_GEN_MODE=&quot;stable-privacy&quot;</span><br><span class="line">NAME=&quot;eth0&quot;</span><br><span class="line">UUID=&quot;19c440dc-9a12-4fd7-bc46-0a121ef6af96&quot;</span><br><span class="line">DEVICE=&quot;eth0&quot;</span><br><span class="line">ONBOOT=&quot;yes&quot;</span><br><span class="line">IPADDR=10.211.55.10</span><br><span class="line">GATEWAY=10.211.55.1</span><br><span class="line">DNS1=10.211.55.1</span><br></pre></td></tr></table></figure>



<h3 id="重启网络"><a href="#重启网络" class="headerlink" title="重启网络"></a>重启网络</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">service network restart</span><br></pre></td></tr></table></figure>



<h2 id="配置主机名"><a href="#配置主机名" class="headerlink" title="配置主机名"></a>配置主机名</h2><h3 id="修改主机名称"><a href="#修改主机名称" class="headerlink" title="修改主机名称"></a>修改主机名称</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo vim /etc/hostname</span><br></pre></td></tr></table></figure>

<h3 id="修改hosts映射文件"><a href="#修改hosts映射文件" class="headerlink" title="修改hosts映射文件"></a>修改hosts映射文件</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo vim /etc/hosts</span><br></pre></td></tr></table></figure>

<figure class="highlight accesslog"><table><tr><td class="code"><pre><span class="line"><span class="number">10.211.55.10</span>	linux1</span><br><span class="line"><span class="number">10.211.55.11</span>	linux2</span><br><span class="line"><span class="number">10.211.55.12</span>	linux3</span><br></pre></td></tr></table></figure>

<p>Windows 主机映射文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">C:\Windows\System32\drivers\etc</span><br></pre></td></tr></table></figure>

<p>Mac 主机映射文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/etc/hosts</span><br></pre></td></tr></table></figure>



<h2 id="关闭防火墙"><a href="#关闭防火墙" class="headerlink" title="关闭防火墙"></a>关闭防火墙</h2><h3 id="service"><a href="#service" class="headerlink" title="service"></a>service</h3><p>service  服务名 start | stop | restart | status</p>
<p>service network status</p>
<h3 id="chkconfig"><a href="#chkconfig" class="headerlink" title="chkconfig"></a>chkconfig</h3><p>chkconfig 设置后台服务的自启配置</p>
<p>chkconfig               （功能描述：查看所有服务器自启配置）</p>
<p>chkconfig 服务名 off  （功能描述：关掉指定服务的自动启动）</p>
<p>chkconfig 服务名 on  （功能描述：开启指定服务的自动启动）</p>
<p>chkconfig 服务名 –list     （功能描述：查看服务开机启动状态）</p>
<h4 id="启-关闭iptables-防火墙-服务的自动启动"><a href="#启-关闭iptables-防火墙-服务的自动启动" class="headerlink" title="启/关闭iptables(防火墙)服务的自动启动"></a>启/关闭iptables(防火墙)服务的自动启动</h4><p>chkconfig iptables on</p>
<p>chkconfig iptables off</p>
<h4 id="开启-关闭-iptables服务指定级别的自动启动"><a href="#开启-关闭-iptables服务指定级别的自动启动" class="headerlink" title="开启/关闭 iptables服务指定级别的自动启动"></a>开启/关闭 iptables服务指定级别的自动启动</h4><p>chkconfig –level 指定级别 iptables on</p>
<p>chkconfig –level 指定级别 iptables off</p>
<h3 id="systemctl"><a href="#systemctl" class="headerlink" title="systemctl"></a>systemctl</h3><p>systemctl  start | stop | restart | status     服务名</p>
<p>systemctl status firewalld</p>
<h4 id="systemctl-设置后台服务的自启配置"><a href="#systemctl-设置后台服务的自启配置" class="headerlink" title="systemctl 设置后台服务的自启配置"></a>systemctl 设置后台服务的自启配置</h4><p>systemctl list-unit-files     （功能描述：查看服务开机启动状态）</p>
<p>systemctl disable service_name  （功能描述：关掉指定服务的自动启动）</p>
<p>systemctl enable service_name  （功能描述：开启指定服务的自动启动）</p>
<h4 id="开启-关闭iptables-防火墙-服务的自动启动"><a href="#开启-关闭iptables-防火墙-服务的自动启动" class="headerlink" title="开启/关闭iptables(防火墙)服务的自动启动"></a>开启/关闭iptables(防火墙)服务的自动启动</h4><p>systemctl enable firewalld.service</p>
<p>systemctl disable firewalld.service</p>
<h4 id="临时关闭防火墙"><a href="#临时关闭防火墙" class="headerlink" title="临时关闭防火墙"></a>临时关闭防火墙</h4><p>systemctl stop firewalld</p>
<h4 id="开机启动时关闭防火墙"><a href="#开机启动时关闭防火墙" class="headerlink" title="开机启动时关闭防火墙"></a>开机启动时关闭防火墙</h4><p>systemctl disable firewalld.service</p>
<h2 id="关机重启命令"><a href="#关机重启命令" class="headerlink" title="关机重启命令"></a>关机重启命令</h2><p>sync &gt; shutdown &gt; reboot &gt; halt</p>
<p>sync          （功能描述：将数据由内存同步到硬盘中）</p>
<p>halt             （功能描述：关闭系统，等同于shutdown -h now 和 poweroff）</p>
<p>reboot         （功能描述：就是重启，等同于 shutdown -r now）</p>
<p>shutdown [选项] 时间    </p>
<p>powerpff</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-h</td>
<td>-h=halt关机</td>
</tr>
<tr>
<td>-r</td>
<td>-r=reboot重启</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>参数</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>now</td>
<td>立刻关机</td>
</tr>
<tr>
<td>时间</td>
<td>等待多久后关机（时间单位是分钟）。</td>
</tr>
</tbody></table>
<h2 id="帮助命令"><a href="#帮助命令" class="headerlink" title="帮助命令"></a>帮助命令</h2><p>man [命令或配置文件]        （功能描述：获得帮助信息）</p>
<p>man ls</p>
<p>help 命令    （功能描述：获得shell内置命令的帮助信息）</p>
<p>help cd</p>
<h2 id="文件目录类命令"><a href="#文件目录类命令" class="headerlink" title="文件目录类命令"></a>文件目录类命令</h2><h3 id="pwd"><a href="#pwd" class="headerlink" title="pwd"></a>pwd</h3><p>功能描述：显示当前工作目录的绝对路径</p>
<h3 id="ls"><a href="#ls" class="headerlink" title="ls"></a>ls</h3><p>ls [选项] [目录或是文件]</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-a</td>
<td>全部的文件，连同隐藏档( 开头为 . 的文件) 一起列出来(常用)</td>
</tr>
<tr>
<td>-l</td>
<td>长数据串列出，包含文件的属性与权限等等数据；(常用)</td>
</tr>
</tbody></table>
<h3 id="cd"><a href="#cd" class="headerlink" title="cd"></a>cd</h3><p>切换路径</p>
<h3 id="mkdir"><a href="#mkdir" class="headerlink" title="mkdir"></a>mkdir</h3><p>创建新目录</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-p</td>
<td>创建多层目录</td>
</tr>
</tbody></table>
<p>mkdir -p xiyou/dssz/meihouwang</p>
<h3 id="rmdir"><a href="#rmdir" class="headerlink" title="rmdir"></a>rmdir</h3><p>删除空目录</p>
<h3 id="touch"><a href="#touch" class="headerlink" title="touch"></a>touch</h3><p>创建空文件</p>
<h3 id="cp"><a href="#cp" class="headerlink" title="cp"></a>cp</h3><p>复制文件或者目录</p>
<p>cp [选项] source dest</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-r</td>
<td>递归复制整个文件夹</td>
</tr>
</tbody></table>
<p>cp -r xiyou/dssz/ ./</p>
<h3 id="rm"><a href="#rm" class="headerlink" title="rm"></a>rm</h3><p>移除文件或目录</p>
<p>rm [选项] deleteFile</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-r</td>
<td>递归删除目录中所有内容</td>
</tr>
<tr>
<td>-f</td>
<td>强制执行删除操作，而不提示用于进行确认。</td>
</tr>
<tr>
<td>-v</td>
<td>显示指令的详细执行过程</td>
</tr>
</tbody></table>
<p>rm -rf dssz/</p>
<h3 id="mv"><a href="#mv" class="headerlink" title="mv"></a>mv</h3><p>移动文件与目录或重命名</p>
<p>mv oldNameFile newNameFile    （功能描述：重命名）</p>
<p>mv /temp/movefile /targetFolder    （功能描述：移动文件）</p>
<h3 id="cat"><a href="#cat" class="headerlink" title="cat"></a>cat</h3><p>cat  [选项] 要查看的文件</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能描述</th>
</tr>
</thead>
<tbody><tr>
<td>-n</td>
<td>显示所有行的行号，包括空行。</td>
</tr>
</tbody></table>
<p>cat -n houge.txt</p>
<h3 id="more"><a href="#more" class="headerlink" title="more"></a>more</h3><p>文件内容分屏查看器</p>
<p>more 要查看的文件</p>
<table>
<thead>
<tr>
<th>操作</th>
<th>功能说明</th>
</tr>
</thead>
<tbody><tr>
<td>空白键 (space)</td>
<td>代表向下翻一页；</td>
</tr>
<tr>
<td>Enter</td>
<td>代表向下翻『一行』；</td>
</tr>
<tr>
<td>q</td>
<td>代表立刻离开 more ，不再显示该文件内容。</td>
</tr>
<tr>
<td>Ctrl+F</td>
<td>向下滚动一屏</td>
</tr>
<tr>
<td>Ctrl+B</td>
<td>返回上一屏</td>
</tr>
<tr>
<td>=</td>
<td>输出当前行的行号</td>
</tr>
<tr>
<td>:f</td>
<td>输出文件名和当前行的行号</td>
</tr>
</tbody></table>
<h3 id="less"><a href="#less" class="headerlink" title="less"></a>less</h3><p>分屏显示文件内容</p>
<table>
<thead>
<tr>
<th>操作</th>
<th>功能说明</th>
</tr>
</thead>
<tbody><tr>
<td>空白键</td>
<td>向下翻动一页；</td>
</tr>
<tr>
<td>[pagedown]</td>
<td>向下翻动一页</td>
</tr>
<tr>
<td>[pageup]</td>
<td>向上翻动一页；</td>
</tr>
<tr>
<td>/字串</td>
<td>向下搜寻『字串』的功能；n：向下查找；N：向上查找；</td>
</tr>
<tr>
<td>?字串</td>
<td>向上搜寻『字串』的功能；n：向上查找；N：向下查找；</td>
</tr>
<tr>
<td>q</td>
<td>离开 less 这个程序；</td>
</tr>
</tbody></table>
<h3 id="echo"><a href="#echo" class="headerlink" title="echo"></a>echo</h3><p>输出内容到控制台</p>
<p>echo [选项] [输出内容]</p>
<p>选项： </p>
<p> -e：  支持反斜线控制的字符转换</p>
<table>
<thead>
<tr>
<th>控制字符</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>\</td>
<td>输出\本身</td>
</tr>
<tr>
<td>\n</td>
<td>换行符</td>
</tr>
<tr>
<td>\t</td>
<td>制表符，也就是Tab键</td>
</tr>
</tbody></table>
<h3 id="head"><a href="#head" class="headerlink" title="head"></a>head</h3><p>head用于显示文件的开头部分内容，默认情况下head指令显示文件的前10行内容。</p>
<p>head 文件       （功能描述：查看文件头10行内容）</p>
<p>head -n 5 文件    （功能描述：查看文件头5行内容，5可以是任意行数）</p>
<p>head -n 2 smartd.conf</p>
<h3 id="tail"><a href="#tail" class="headerlink" title="tail"></a>tail</h3><p>tail用于输出文件中尾部的内容，默认情况下tail指令显示文件的后10行内容。</p>
<p>tail  文件             （功能描述：查看文件尾部10行内容）</p>
<p>tail  -n  5 文件     （功能描述：查看文件尾部5行内容，5可以是任意行数）</p>
<p>tail  -f  文件        （功能描述：实时追踪该文档的所有更新）</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-n&lt;行数&gt;</td>
<td>输出文件尾部n行内容</td>
</tr>
<tr>
<td>-f</td>
<td>显示文件最新追加的内容，监视文件变化</td>
</tr>
</tbody></table>
<h3 id="gt-输出重定向和-gt-gt-追加"><a href="#gt-输出重定向和-gt-gt-追加" class="headerlink" title="&gt; 输出重定向和 &gt;&gt; 追加"></a>&gt; 输出重定向和 &gt;&gt; 追加</h3><p>ls -l &gt; 文件                 （功能描述：列表的内容写入文件中（****覆盖写****））</p>
<p>ls -al  &gt;&gt; 文件            （功能描述：列表的内容****追加****到文件的末尾）</p>
<p>cat 文件1 &gt; 文件2      （功能描述：将文件1的内容覆盖到文件2）</p>
<p>echo “内容” &gt;&gt; 文件</p>
<p>将ls查看信息写入到文件中</p>
<p>ls -l&gt;houge.txt</p>
<p>将ls查看信息追加到文件中</p>
<p>ls -l&gt;&gt;houge.txt</p>
<p>采用echo将hello单词追加到文件中</p>
<p>echo hello&gt;&gt;houge.txt</p>
<h3 id="ln"><a href="#ln" class="headerlink" title="ln"></a>ln</h3><p>软链接</p>
<p>ln -s [原文件或目录] [软链接名]        （功能描述：给原文件创建一个软链接）</p>
<p>创建软连接</p>
<p>ln -s xiyou/dssz/houge.txt ./houzi</p>
<p>删除软连接</p>
<p>rm -rf houzi</p>
<p>进入软连接实际物理路径</p>
<p>ln -s xiyou/dssz/ ./dssz</p>
<p>cd -P dssz/</p>
<h3 id="history"><a href="#history" class="headerlink" title="history"></a>history</h3><p>history                        （功能描述：查看已经执行过历史命令）</p>
<h2 id="时间日期类命令"><a href="#时间日期类命令" class="headerlink" title="时间日期类命令"></a>时间日期类命令</h2><p>date [OPTION]… [+FORMAT]</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-d&lt;时间字符串&gt;</td>
<td>显示指定的”时间字符串”表示的时间，而非当前时间</td>
</tr>
<tr>
<td>-s&lt;日期时间&gt;</td>
<td>设置系统日期时间</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>参数</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>&lt;+日期时间格式&gt;</td>
<td>指定显示时使用的日期时间格式</td>
</tr>
</tbody></table>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">%</span><span class="bash">H 小时(以00-23来表示)</span></span><br><span class="line"><span class="meta">%</span><span class="bash">I 小时(以01-12来表示)</span></span><br><span class="line"><span class="meta">%</span><span class="bash">K 小时(以0-23来表示)</span></span><br><span class="line"><span class="meta">%</span><span class="bash">l 小时(以0-12来表示)</span></span><br><span class="line"><span class="meta">%</span><span class="bash">M 分钟(以00-59来表示)</span></span><br><span class="line"><span class="meta">%</span><span class="bash">P AM或PM</span></span><br><span class="line"><span class="meta">%</span><span class="bash">r 时间(含时分秒，小时以12小时AM/PM来表示)</span></span><br><span class="line"><span class="meta">%</span><span class="bash">s 总秒数。起算时间为1970-01-01 00:00:00 UTC</span></span><br><span class="line"><span class="meta">%</span><span class="bash">S 秒(以本地的惯用法来表示)</span></span><br><span class="line"><span class="meta">%</span><span class="bash">T 时间(含时分秒，小时以24小时制来表示)</span></span><br><span class="line"><span class="meta">%</span><span class="bash">X 时间(以本地的惯用法来表示)</span></span><br><span class="line"><span class="meta">%</span><span class="bash">Z 市区</span></span><br><span class="line"><span class="meta">%</span><span class="bash">a 星期的缩写</span></span><br><span class="line"><span class="meta">%</span><span class="bash">A 星期的完整名称</span></span><br><span class="line"><span class="meta">%</span><span class="bash">b 月份英文名的缩写</span></span><br><span class="line"><span class="meta">%</span><span class="bash">B 月份的完整英文名称</span></span><br><span class="line"><span class="meta">%</span><span class="bash">c 日期与时间。只输入date指令也会显示同样的结果</span></span><br><span class="line"><span class="meta">%</span><span class="bash">d 日期(以01-31来表示)</span></span><br><span class="line"><span class="meta">%</span><span class="bash">D 日期(含年月日)</span></span><br><span class="line"><span class="meta">%</span><span class="bash">j 该年中的第几天</span></span><br><span class="line"><span class="meta">%</span><span class="bash">m 月份(以01-12来表示)</span></span><br><span class="line"><span class="meta">%</span><span class="bash">U 该年中的周数</span></span><br><span class="line"><span class="meta">%</span><span class="bash">w 该周的天数，0代表周日，1代表周一，异词类推</span></span><br><span class="line"><span class="meta">%</span><span class="bash">x 日期(以本地的惯用法来表示)</span></span><br><span class="line"><span class="meta">%</span><span class="bash">y 年份(以00-99来表示)</span></span><br><span class="line"><span class="meta">%</span><span class="bash">Y 年份(以四位数来表示)</span></span><br><span class="line"><span class="meta">%</span><span class="bash">n 在显示时，插入新的一行</span></span><br><span class="line"><span class="meta">%</span><span class="bash">t 在显示时，插入tab</span></span><br><span class="line">MM 月份(必要) </span><br><span class="line">DD 日期(必要) </span><br><span class="line">hh 小时(必要) </span><br><span class="line">mm 分钟(必要)</span><br><span class="line">ss 秒(选择性) </span><br></pre></td></tr></table></figure>

<h3 id="date-显示当前时间"><a href="#date-显示当前时间" class="headerlink" title="date 显示当前时间"></a>date 显示当前时间</h3><p>date                                    （功能描述：显示当前时间）</p>
<p>date +%Y                            （功能描述：显示当前年份）</p>
<p>date +%m                            （功能描述：显示当前月份）</p>
<p>date +%d                            （功能描述：显示当前是哪一天）</p>
<p>date “+%Y-%m-%d %H:%M:%S”        （功能描述：显示年月日时分秒）</p>
<h3 id="date-显示非当前时间"><a href="#date-显示非当前时间" class="headerlink" title="date 显示非当前时间"></a>date 显示非当前时间</h3><figure class="highlight mel"><table><tr><td class="code"><pre><span class="line"><span class="keyword">date</span> -d <span class="string">&#x27;1 days ago&#x27;</span>			（功能描述：显示前一天时间）</span><br><span class="line"><span class="keyword">date</span> -d <span class="string">&#x27;-1 days ago&#x27;</span>			（功能描述：显示明天时间）</span><br><span class="line"><span class="keyword">date</span> -d <span class="string">&quot;2 days&quot;</span> +<span class="string">&quot;%Y-%m-%d %H:%M:%S&quot;</span>   （显示两天后时间）</span><br><span class="line"><span class="keyword">date</span> -d <span class="string">&quot;1970-01-01 2 days&quot;</span> +<span class="string">&quot;%Y-%m-%d&quot;</span>   （<span class="number">1970</span><span class="number">-01</span><span class="number">-03</span>）</span><br><span class="line"><span class="keyword">date</span> +%Y%m%d               #显示前天年月日 </span><br><span class="line"><span class="keyword">date</span> -d <span class="string">&quot;+1 day&quot;</span> +%Y%m%d   #显示前一天的日期 </span><br><span class="line"><span class="keyword">date</span> -d <span class="string">&quot;-1 day&quot;</span> +%Y%m%d   #显示后一天的日期 </span><br><span class="line"><span class="keyword">date</span> -d <span class="string">&quot;-1 month&quot;</span> +%Y%m%d #显示上一月的日期 </span><br><span class="line"><span class="keyword">date</span> -d <span class="string">&quot;+1 month&quot;</span> +%Y%m%d #显示下一月的日期 </span><br><span class="line"><span class="keyword">date</span> -d <span class="string">&quot;-1 year&quot;</span> +%Y%m%d  #显示前一年的日期 </span><br><span class="line"><span class="keyword">date</span> -d <span class="string">&quot;+1 year&quot;</span> +%Y%m%d  #显示下一年的日期</span><br></pre></td></tr></table></figure>

<h3 id="date-设置系统时间"><a href="#date-设置系统时间" class="headerlink" title="date 设置系统时间"></a>date 设置系统时间</h3><p>date -s 字符串时间</p>
<p>date -s “2017-06-19 20:52:18”</p>
<h3 id="cal-查看日历"><a href="#cal-查看日历" class="headerlink" title="cal 查看日历"></a>cal 查看日历</h3><p>cal [选项]            （功能描述：不加选项，显示本月日历）</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>具体某一年</td>
<td>显示这一年的日历</td>
</tr>
</tbody></table>
<p>cal 2020</p>
<h2 id="用户管理命令"><a href="#用户管理命令" class="headerlink" title="用户管理命令"></a>用户管理命令</h2><h3 id="useradd"><a href="#useradd" class="headerlink" title="useradd"></a>useradd</h3><p>useradd 用户名            （功能描述：添加新用户）</p>
<p>useradd -g 组名 用户名    （功能描述：添加新用户到某个组）</p>
<h3 id="passwd"><a href="#passwd" class="headerlink" title="passwd"></a>passwd</h3><p>设置用户密码</p>
<p>passwd 用户名    （功能描述：设置用户密码）</p>
<h3 id="id"><a href="#id" class="headerlink" title="id"></a>id</h3><p>查看用户是否存在</p>
<p>id 用户名</p>
<h3 id="cat-etc-passwd"><a href="#cat-etc-passwd" class="headerlink" title="cat  /etc/passwd"></a>cat  /etc/passwd</h3><p>查看创建了哪些用户</p>
<h3 id="su"><a href="#su" class="headerlink" title="su"></a>su</h3><p>切换用户</p>
<p>su 用户名称  （功能描述：切换用户，只能获得用户的执行权限，不能获得环境变量）</p>
<p>su - 用户名称（功能描述：切换到用户并获得该用户的环境变量及执行权限）</p>
<h3 id="userdel"><a href="#userdel" class="headerlink" title="userdel"></a>userdel</h3><p>删除用户</p>
<p>userdel  用户名        （功能描述：删除用户但保存用户主目录）</p>
<p>userdel -r 用户名      （功能描述：用户和用户主目录，都删除）</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-r</td>
<td>删除用户的同时，删除与用户相关的所有文件。</td>
</tr>
</tbody></table>
<h3 id="who"><a href="#who" class="headerlink" title="who"></a>who</h3><p>查看登录用户信息</p>
<p>whoami            （功能描述：显示自身用户名称）</p>
<p>who am i          （功能描述：显示登录用户的用户名)</p>
<h3 id="sudo"><a href="#sudo" class="headerlink" title="sudo"></a>sudo</h3><p>设置普通用户具有root权限</p>
<p>先修改配置文件</p>
<p>vim /etc/sudoers</p>
<p>修改 /etc/sudoers 文件，找到下面一行(91行)，在root下面添加一行，如下所示：</p>
<p>## Allow root to run any commands anywhere</p>
<p>root   ALL=(ALL)   ALL</p>
<p>vincent  ALL=(ALL)   ALL</p>
<h3 id="usermod"><a href="#usermod" class="headerlink" title="usermod"></a>usermod</h3><p>修改用户</p>
<p>usermod -g 用户组 用户名</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-g</td>
<td>修改用户的初始登录组，给定的组必须存在。默认组id是1。</td>
</tr>
</tbody></table>
<h3 id="groupadd"><a href="#groupadd" class="headerlink" title="groupadd"></a>groupadd</h3><p>groupadd 组名</p>
<h3 id="groupdel"><a href="#groupdel" class="headerlink" title="groupdel"></a>groupdel</h3><p>groupdel 组名</p>
<h3 id="groupmod"><a href="#groupmod" class="headerlink" title="groupmod"></a>groupmod</h3><p>groupmod -n 新组名 老组名</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能描述</th>
</tr>
</thead>
<tbody><tr>
<td>-n&lt;新组名&gt;</td>
<td>指定工作组的新组名</td>
</tr>
</tbody></table>
<h3 id="cat-etc-group"><a href="#cat-etc-group" class="headerlink" title="cat  /etc/group"></a>cat  /etc/group</h3><p>查看创建了哪些组</p>
<h2 id="文件权限类命令"><a href="#文件权限类命令" class="headerlink" title="文件权限类命令"></a>文件权限类命令</h2><h3 id="文件属性"><a href="#文件属性" class="headerlink" title="文件属性"></a>文件属性</h3><p>从左到右的10个字符</p>
<p>0首位表示类型</p>
<p>在Linux中第一个字符代表这个文件是目录、文件或链接文件等等</p>
<p>- 代表文件</p>
<p> d 代表目录</p>
<p> l 链接文档(link file)</p>
<p>第1-3位确定属主（该文件的所有者）拥有该文件的权限。—User</p>
<p>第4-6位确定属组（所有者的同组用户）拥有该文件的权限，—Group</p>
<p>第7-9位确定其他用户拥有该文件的权限 —Other</p>
<h3 id="rxw作用文件和目录的不同解释"><a href="#rxw作用文件和目录的不同解释" class="headerlink" title="rxw作用文件和目录的不同解释"></a>rxw作用文件和目录的不同解释</h3><p>作用到文件：</p>
<p>[ r ]代表可读(read): 可以读取，查看</p>
<p>[ w ]代表可写(write): 可以修改，但是不代表可以删除该文件，删除一个文件的前提条件是对该文件所在的目录有写权限，才能删除该文件.</p>
<p>[ x ]代表可执行(execute):可以被系统执行</p>
<p>作用到目录：</p>
<p>[ r ]代表可读(read): 可以读取，ls查看目录内容</p>
<p>[ w ]代表可写(write): 可以修改，目录内创建+删除+重命名目录</p>
<p>[ x ]代表可执行(execute):可以进入该目录</p>
<h3 id="chmod"><a href="#chmod" class="headerlink" title="chmod"></a>chmod</h3><p>改变权限</p>
<p>第一种方式变更权限</p>
<p>chmod  [{ugoa}{+-=}{rwx}] 文件或目录</p>
<p>chmod u-x,o+x houge.txt</p>
<p>第二种方式变更权限</p>
<p>chmod  [mode=421 ]  [文件或目录]</p>
<p>chmod 777 houge.txt</p>
<p>chmod -R 777 xiyou/</p>
<p>u:所有者  g:所有组  o:其他人  a:所有人(u、g、o的总和)</p>
<p>r=4 w=2 x=1     rwx=4+2+1=7</p>
<h3 id="chown"><a href="#chown" class="headerlink" title="chown"></a>chown</h3><p>改变所有者</p>
<p>chown [选项] [最终用户] [文件或目录]        （功能描述：改变文件或者目录的所有者）</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-R</td>
<td>递归操作</td>
</tr>
</tbody></table>
<p>chown -R vincent:vincent xiyou/</p>
<h3 id="chgrp"><a href="#chgrp" class="headerlink" title="chgrp"></a>chgrp</h3><p>chgrp [最终用户组] [文件或目录]    （功能描述：改变文件或者目录的所属组）</p>
<p>chgrp root houge.txt</p>
<h2 id="搜索查找类命令"><a href="#搜索查找类命令" class="headerlink" title="搜索查找类命令"></a>搜索查找类命令</h2><h3 id="find"><a href="#find" class="headerlink" title="find"></a>find</h3><p>ind指令将从指定目录向下递归地遍历其各个子目录，将满足条件的文件显示在终端。</p>
<p>find [搜索范围] [选项]</p>
<table>
<thead>
<tr>
<th>选项</th>
<th align="left">功能</th>
</tr>
</thead>
<tbody><tr>
<td>-name&lt;查询方式&gt;</td>
<td align="left">按照指定的文件名查找模式查找文件</td>
</tr>
<tr>
<td>-user&lt;用户名&gt;</td>
<td align="left">查找属于指定用户名所有文件</td>
</tr>
<tr>
<td>-size&lt;文件大小&gt;</td>
<td align="left">按照指定的文件大小查找文件,单位为: <em><strong>*b*</strong></em> —— 块（512字节）<em><strong>*c*</strong></em> —— 字节<em><strong>*w*</strong></em> —— 字（2字节）<em><strong>*k*</strong></em> —— 千字节<em><strong>*M*</strong></em> —— 兆字节<em><strong>*G*</strong></em> —— 吉字节</td>
</tr>
</tbody></table>
<p>find xiyou/ -name *.txt</p>
<p>find xiyou/ -user vincent</p>
<p>find /home -size +204800</p>
<h3 id="locate"><a href="#locate" class="headerlink" title="locate"></a>locate</h3><p>快速定位文件路径</p>
<p>locate 搜索文件</p>
<h3 id="grep"><a href="#grep" class="headerlink" title="grep"></a>grep</h3><p>过滤查找及“|”管道符</p>
<p>管道符，“|”，表示<strong>将前一个命令的处理结果输出传递给后面的命令处理</strong></p>
<p>grep 选项 查找内容 源文件</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-n</td>
<td>显示匹配行及行号。</td>
</tr>
</tbody></table>
<p>找test在第几行</p>
<p>ls | grep -n test</p>
<h2 id="压缩和解压类命令"><a href="#压缩和解压类命令" class="headerlink" title="压缩和解压类命令"></a>压缩和解压类命令</h2><h3 id="gzip-gunzip"><a href="#gzip-gunzip" class="headerlink" title="gzip/gunzip"></a>gzip/gunzip</h3><p>gzip 文件        （功能描述：压缩文件，只能将文件压缩为*.gz文件）</p>
<p>gunzip 文件.gz    （功能描述：解压缩文件命令）</p>
<p>只能压缩文件不能压缩目录。</p>
<p>不保留原来的文件。</p>
<h3 id="zip-unzip"><a href="#zip-unzip" class="headerlink" title="zip/unzip"></a>zip/unzip</h3><p>zip  [选项] XXX.zip  将要压缩的内容         （功能描述：压缩文件和目录的命令）</p>
<p>unzip [选项] XXX.zip                        （功能描述：解压缩文件）</p>
<table>
<thead>
<tr>
<th>zip选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-r</td>
<td>压缩目录</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>unzip选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-d&lt;目录&gt;</td>
<td>指定解压后文件的存放目录</td>
</tr>
</tbody></table>
<p>zip 压缩命令在window/linux都通用，可以压缩目录且保留源文件。</p>
<h3 id="tar"><a href="#tar" class="headerlink" title="tar"></a>tar</h3><p>tar  [选项]  XXX.tar.gz  将要打包进去的内容        （功能描述：打包目录，压缩后的文件格式.tar.gz）</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-c</td>
<td>产生.tar打包文件</td>
</tr>
<tr>
<td>-v</td>
<td>显示详细信息</td>
</tr>
<tr>
<td>-f</td>
<td>指定压缩后的文件名</td>
</tr>
<tr>
<td>-z</td>
<td>打包同时压缩</td>
</tr>
<tr>
<td>-x</td>
<td>解包.tar文件</td>
</tr>
</tbody></table>
<p>压缩多个文件</p>
<p>tar -zcvf houma.tar.gz houge.txt bailongma.txt</p>
<p>压缩目录</p>
<p>tar -zcvf xiyou.tar.gz xiyou/</p>
<p>解压到当前目录</p>
<p>tar -zxvf houma.tar.gz</p>
<p>解压到指定目录</p>
<p>tar -zxvf xiyou.tar.gz -C /opt</p>
<h2 id="磁盘分区类命令"><a href="#磁盘分区类命令" class="headerlink" title="磁盘分区类命令"></a>磁盘分区类命令</h2><h3 id="df"><a href="#df" class="headerlink" title="df"></a>df</h3><p>df: disk free 空余硬盘</p>
<p>查看磁盘空间使用情况</p>
<p>df  选项    （功能描述：列出文件系统的整体磁盘使用量，检查文件系统的磁盘空间占用情况）</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-h</td>
<td>以人们较易阅读的 GBytes, MBytes, KBytes 等格式自行显示；</td>
</tr>
</tbody></table>
<p>df -h</p>
<h3 id="fdisk"><a href="#fdisk" class="headerlink" title="fdisk"></a>fdisk</h3><p>查看分区</p>
<p>fdisk -l            （功能描述：查看磁盘分区详情）</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-l</td>
<td>显示所有硬盘的分区列表</td>
</tr>
</tbody></table>
<p>该命令必须在root用户下才能使用。</p>
<h3 id="lsblk"><a href="#lsblk" class="headerlink" title="lsblk"></a>lsblk</h3><p>查看设备挂载情况</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-f</td>
<td>查看详细的设备挂载情况，显示文件系统信息</td>
</tr>
</tbody></table>
<h3 id="mount-umount"><a href="#mount-umount" class="headerlink" title="mount/umount"></a>mount/umount</h3><p>mount [-t vfstype] [-o options] device dir    （功能描述：挂载设备）</p>
<p>umount 设备文件名或挂载点            （功能描述：卸载设备）</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-t vfstype</td>
<td>指定文件系统的类型，通常不必指定。mount 会自动选择正确的类型。常用类型有：光盘或光盘镜像：iso9660DOS fat16文件系统：msdos<a href="http://blog.csdn.net/hancunai0017/article/details/6995284">Windows</a> 9x fat32文件系统：vfatWindows NT ntfs文件系统：ntfsMount Windows文件<a href="http://blog.csdn.net/hancunai0017/article/details/6995284">网络</a>共享：smbfs<a href="http://blog.csdn.net/hancunai0017/article/details/6995284">UNIX</a>(LINUX) 文件网络共享：nfs</td>
</tr>
<tr>
<td>-o options</td>
<td>主要用来描述设备或档案的挂接方式。常用的参数有：loop：用来把一个文件当成硬盘分区挂接上系统ro：采用只读方式挂接设备rw：采用读写方式挂接设备　  iocharset：指定访问文件系统所用字符集</td>
</tr>
<tr>
<td>device</td>
<td>要挂接(mount)的设备</td>
</tr>
<tr>
<td>dir</td>
<td>设备在系统上的挂接点(mount point)</td>
</tr>
</tbody></table>
<h2 id="进程线程类命令"><a href="#进程线程类命令" class="headerlink" title="进程线程类命令"></a>进程线程类命令</h2><h3 id="ps"><a href="#ps" class="headerlink" title="ps"></a>ps</h3><p>查看当前系统进程状态</p>
<p>ps -aux | grep xxx        （功能描述：查看系统中所有进程）</p>
<p>ps -ef | grep xxx        （功能描述：可以查看子父进程之间的关系）</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-a</td>
<td>选择所有进程</td>
</tr>
<tr>
<td>-u</td>
<td>显示所有用户的所有进程</td>
</tr>
<tr>
<td>-x</td>
<td>显示没有终端的进程</td>
</tr>
</tbody></table>
<h4 id="功能说明"><a href="#功能说明" class="headerlink" title="功能说明"></a>功能说明</h4><p>（1）ps -aux显示信息说明</p>
<p>​    USER：该进程是由哪个用户产生的</p>
<p>​    PID：进程的ID号</p>
<p>%CPU：该进程占用CPU资源的百分比，占用越高，进程越耗费资源；</p>
<p>%MEM：该进程占用物理内存的百分比，占用越高，进程越耗费资源；</p>
<p>VSZ：该进程占用虚拟内存的大小，单位KB；</p>
<p>RSS：该进程占用实际物理内存的大小，单位KB；</p>
<p>TTY：该进程是在哪个终端中运行的。其中tty1-tty7代表本地控制台终端，tty1-tty6是本地的字符界面终端，tty7是图形终端。pts/0-255代表虚拟终端。</p>
<p>STAT：进程状态。常见的状态有：R：运行、S：睡眠、T：停止状态、s：包含子进程、+：位于后台</p>
<p>START：该进程的启动时间</p>
<p>TIME：该进程占用CPU的运算时间，注意不是系统时间</p>
<p>COMMAND：产生此进程的命令名</p>
<p>（2）ps -ef显示信息说明</p>
<p>UID：用户ID </p>
<p>PID：进程ID </p>
<p>PPID：父进程ID </p>
<p>C：CPU用于计算执行优先级的因子。数值越大，表明进程是CPU密集型运算，执行优先级会降低；数值越小，表明进程是I/O密集型运算，执行优先级会提高 </p>
<p>STIME：进程启动的时间 </p>
<p>TTY：完整的终端名称 </p>
<p>TIME：CPU时间 </p>
<p>CMD：启动进程所用的命令和参数</p>
<h4 id="经验技巧"><a href="#经验技巧" class="headerlink" title="经验技巧"></a>经验技巧</h4><p>如果想查看进程的CPU占用率和内存占用率，可以使用aux;</p>
<p>如果想查看进程的父进程ID可以使用ef;</p>
<h3 id="kill"><a href="#kill" class="headerlink" title="kill"></a>kill</h3><p>终止进程</p>
<p>kill  [选项] 进程号        （功能描述：通过进程号杀死进程）</p>
<p>killall 进程名称            （功能描述：通过进程名称杀死进程，也支持通配符，这在系统因负载过大而变得很慢时很有用）</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-9</td>
<td>表示强迫进程立即停止</td>
</tr>
</tbody></table>
<p>kill -9 5102</p>
<p>killall firefox</p>
<h3 id="pstree"><a href="#pstree" class="headerlink" title="pstree"></a>pstree</h3><p>查看进程树</p>
<p>pstree [选项]</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-p</td>
<td>显示进程的PID</td>
</tr>
<tr>
<td>-u</td>
<td>显示进程的所属用户</td>
</tr>
</tbody></table>
<h3 id="top"><a href="#top" class="headerlink" title="top"></a>top</h3><p>查看系统健康状态</p>
<p>top [选项]</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-d 秒数</td>
<td>指定top命令每隔几秒更新。默认是3秒在top命令的交互模式当中可以执行的命令：</td>
</tr>
<tr>
<td>-i</td>
<td>使top不显示任何闲置或者僵死进程。</td>
</tr>
<tr>
<td>-p</td>
<td>通过指定监控进程ID来仅仅监控某个进程的状态。</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>操作</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>P</td>
<td>以CPU使用率排序，默认就是此项</td>
</tr>
<tr>
<td>M</td>
<td>以内存的使用率排序</td>
</tr>
<tr>
<td>N</td>
<td>以PID排序</td>
</tr>
<tr>
<td>q</td>
<td>退出top</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>内容</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>12:26:46</td>
<td>系统当前时间</td>
</tr>
<tr>
<td>up 1 day, 13:32</td>
<td>系统的运行时间，本机已经运行1天13小时32分钟</td>
</tr>
<tr>
<td>2 users</td>
<td>当前登录了两个用户</td>
</tr>
<tr>
<td>load  average:  0.00, 0.00, 0.00</td>
<td>系统在之前1分钟，5分钟，15分钟的平均负载。一般认为小于1时，负载较小。如果大于1，系统已经超出负荷。</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>Tasks:  95 total</th>
<th>系统中的进程总数</th>
</tr>
</thead>
<tbody><tr>
<td>1 running</td>
<td>正在运行的进程数</td>
</tr>
<tr>
<td>94 sleeping</td>
<td>睡眠的进程</td>
</tr>
<tr>
<td>0 stopped</td>
<td>正在停止的进程</td>
</tr>
<tr>
<td>0 zombie</td>
<td>僵尸进程。如果不是0，需要手工检查僵尸进程</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>Cpu(s):  0.1%us</th>
<th>用户模式占用的CPU百分比</th>
</tr>
</thead>
<tbody><tr>
<td>0.1%sy</td>
<td>系统模式占用的CPU百分比</td>
</tr>
<tr>
<td>0.0%ni</td>
<td>改变过优先级的用户进程占用的CPU百分比</td>
</tr>
<tr>
<td>99.7%id</td>
<td>空闲CPU的CPU百分比</td>
</tr>
<tr>
<td>0.1%wa</td>
<td>等待输入/输出的进程的占用CPU百分比</td>
</tr>
<tr>
<td>0.0%hi</td>
<td>硬中断请求服务占用的CPU百分比</td>
</tr>
<tr>
<td>0.1%si</td>
<td>软中断请求服务占用的CPU百分比</td>
</tr>
<tr>
<td>0.0%st</td>
<td>st（Steal  time）虚拟时间百分比。就是当有虚拟机时，虚拟CPU等待实际CPU的时间百分比。</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>Mem:   625344k total</th>
<th>物理内存的总量，单位KB</th>
</tr>
</thead>
<tbody><tr>
<td>571504k used</td>
<td>已经使用的物理内存数量</td>
</tr>
<tr>
<td>53840k free</td>
<td>空闲的物理内存数量，我们使用的是虚拟机，总共只分配了628MB内存，所以只有53MB的空闲内存了</td>
</tr>
<tr>
<td>65800k buffers</td>
<td>作为缓冲的内存数量</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>Swap:  524280k total</th>
<th>交换分区（虚拟内存）的总大小</th>
</tr>
</thead>
<tbody><tr>
<td>0k used</td>
<td>已经使用的交互分区的大小</td>
</tr>
<tr>
<td>524280k free</td>
<td>空闲交换分区的大小</td>
</tr>
<tr>
<td>409280k cached</td>
<td>作为缓存的交互分区的大小</td>
</tr>
</tbody></table>
<h3 id="netstat"><a href="#netstat" class="headerlink" title="netstat"></a>netstat</h3><p>显示网络统计信息和端口占用情况</p>
<p>netstat -anp | grep 进程号    （功能描述：查看该进程网络信息）</p>
<p>netstat -nlp | grep 端口号    （功能描述：查看网络端口号占用情况）</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-n</td>
<td>拒绝显示别名，能显示数字的全部转化成数字</td>
</tr>
<tr>
<td>-l</td>
<td>仅列出有在listen（监听）的服务状态</td>
</tr>
<tr>
<td>-p</td>
<td>表示显示哪个进程在调用</td>
</tr>
</tbody></table>
<h3 id="lsof"><a href="#lsof" class="headerlink" title="lsof"></a>lsof</h3><p>查看端口被哪个进程占用</p>
<p>lsof -i:44444</p>
<p>COMMAND  PID    USER   FD   TYPE DEVICE SIZE/OFF NODE NAME<br>java    7073 vincent  670u  IPv6  73392      0t0  TCP *:cognex-dataman (LISTEN)<br>java    7073 vincent  672u  IPv6  73395      0t0  TCP localhost:cognex-dataman-&gt;localhost:45638 (CLOSE_WAIT)<br>java    7073 vincent  729u  IPv6  68482      0t0  TCP linux1:cognex-dataman-&gt;linux1:56552 (CLOSE_WAIT)</p>
<h3 id="crontab"><a href="#crontab" class="headerlink" title="crontab"></a>crontab</h3><p>系统定时任务</p>
<p>crontab [选项]</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-e</td>
<td>编辑crontab定时任务</td>
</tr>
<tr>
<td>-l</td>
<td>查询crontab任务</td>
</tr>
<tr>
<td>-r</td>
<td>删除当前用户所有的crontab任务</td>
</tr>
</tbody></table>
<p>重新启动crond服务</p>
<p>service crond restart</p>
<p>crontab -e </p>
<table>
<thead>
<tr>
<th>项目</th>
<th>含义</th>
<th>范围</th>
</tr>
</thead>
<tbody><tr>
<td>第一个“*”</td>
<td>一小时当中的第几分钟</td>
<td>0-59</td>
</tr>
<tr>
<td>第二个“*”</td>
<td>一天当中的第几小时</td>
<td>0-23</td>
</tr>
<tr>
<td>第三个“*”</td>
<td>一个月当中的第几天</td>
<td>1-31</td>
</tr>
<tr>
<td>第四个“*”</td>
<td>一年当中的第几月</td>
<td>1-12</td>
</tr>
<tr>
<td>第五个“*”</td>
<td>一周当中的星期几</td>
<td>0-7（0和7都代表星期日）</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>特殊符号</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>*</td>
<td>代表任何时间。比如第一个“*”就代表一小时中每分钟都执行一次的意思。</td>
</tr>
<tr>
<td>，</td>
<td>代表不连续的时间。比如“0 8,12,16 * * * 命令”，就代表在每天的8点0分，12点0分，16点0分都执行一次命令</td>
</tr>
<tr>
<td>-</td>
<td>代表连续的时间范围。比如“0 5  *  *  1-6命令”，代表在周一到周六的凌晨5点0分执行命令</td>
</tr>
<tr>
<td>*/n</td>
<td>代表每隔多久执行一次。比如“*/10  *  *  *  *  命令”，代表每隔10分钟就执行一遍命令</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>时间</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>45 22 * * * 命令</td>
<td>在22点45分执行命令</td>
</tr>
<tr>
<td>0 17 * * 1 命令</td>
<td>每周1 的17点0分执行命令</td>
</tr>
<tr>
<td>0 5 1,15 * * 命令</td>
<td>每月1号和15号的凌晨5点0分执行命令</td>
</tr>
<tr>
<td>40 4 * * 1-5 命令</td>
<td>每周一到周五的凌晨4点40分执行命令</td>
</tr>
<tr>
<td>*/10 4 * * * 命令</td>
<td>每天的凌晨4点，每隔10分钟执行一次命令</td>
</tr>
<tr>
<td>0 0 1,15 * 1 命令</td>
<td>每月1号和15号，每周1的0点0分都会执行命令。注意：星期几和几号最好不要同时出现，因为他们定义的都是天。非常容易让管理员混乱。</td>
</tr>
</tbody></table>
<p>每隔1分钟，向/root/bailongma.txt文件中添加一个11的数字</p>
<p>*/1 * * * * /bin/echo ”11” &gt;&gt; /root/bailongma.txt</p>
<h2 id="软件包管理"><a href="#软件包管理" class="headerlink" title="软件包管理"></a>软件包管理</h2><h3 id="rpm"><a href="#rpm" class="headerlink" title="rpm"></a>rpm</h3><p>RedHat Package Manager</p>
<h4 id="rpm查询命令"><a href="#rpm查询命令" class="headerlink" title="rpm查询命令"></a>rpm查询命令</h4><p>rpm -qa                （功能描述：查询所安装的所有rpm软件包）</p>
<p>rpm -qa | grep rpm软件包</p>
<p>rpm -qa | grep firefox</p>
<h4 id="rpm卸载命令"><a href="#rpm卸载命令" class="headerlink" title="rpm卸载命令"></a>rpm卸载命令</h4><p>rpm -e RPM软件包</p>
<p>rpm -e –nodeps 软件包</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-e</td>
<td>卸载软件包</td>
</tr>
<tr>
<td>–nodeps</td>
<td>卸载软件时，不检查依赖。这样的话，那些使用该软件包的软件在此之后可能就不能正常工作了。</td>
</tr>
</tbody></table>
<h4 id="rpm安装命令"><a href="#rpm安装命令" class="headerlink" title="rpm安装命令"></a>rpm安装命令</h4><p>rpm -ivh RPM包全名</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-i</td>
<td>-i=install，安装</td>
</tr>
<tr>
<td>-v</td>
<td>-v=verbose，显示详细信息</td>
</tr>
<tr>
<td>-h</td>
<td>-h=hash，进度条</td>
</tr>
<tr>
<td>–nodeps</td>
<td>–nodeps，不检测依赖进度</td>
</tr>
</tbody></table>
<h3 id="yum"><a href="#yum" class="headerlink" title="yum"></a>yum</h3><p>Yellow dog Updater, Modified。是一个在Fedora和RedHat以及CentOS中的Shell前端软件包管理器。基于RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软件包，无须繁琐地一次次下载、安装。</p>
<p>yum [选项] [参数]</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-y</td>
<td>对所有提问都回答“yes”</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>参数</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>install</td>
<td>安装rpm软件包</td>
</tr>
<tr>
<td>update</td>
<td>更新rpm软件包</td>
</tr>
<tr>
<td>check-update</td>
<td>检查是否有可用的更新rpm软件包</td>
</tr>
<tr>
<td>remove</td>
<td>删除指定的rpm软件包</td>
</tr>
<tr>
<td>list</td>
<td>显示软件包信息</td>
</tr>
<tr>
<td>clean</td>
<td>清理yum过期的缓存</td>
</tr>
<tr>
<td>deplist</td>
<td>显示yum软件包的所有依赖关系</td>
</tr>
</tbody></table>
<p>yum -y install firefox.x86_64</p>
<h4 id="修改网络yum源"><a href="#修改网络yum源" class="headerlink" title="修改网络yum源"></a>修改网络yum源</h4><p>默认的系统YUM源，需要连接国外apache网站，网速比较慢，可以修改关联的网络YUM源为国内镜像的网站，比如网易163,aliyun等。</p>
<p>1）安装wget, wget用来从指定的URL下载文件</p>
<p>yum install wget</p>
<p>2）在/etc/yum.repos.d/目录下，备份默认的repos文件</p>
<p>cp CentOS-Base.repo  CentOS-Base.repo.backup</p>
<p>3）下载网易163或者是aliyun的repos文件,任选其一</p>
<p>wget <a href="http://mirrors.aliyun.com/repo/Centos-7.repo">http://mirrors.aliyun.com/repo/Centos-7.repo</a>  //阿里云</p>
<p>wget <a href="http://mirrors.163.com/.help/CentOS7-Base-163.repo">http://mirrors.163.com/.help/CentOS7-Base-163.repo</a> //网易163</p>
<p>4）使用下载好的repos文件替换默认的repos文件，例如:用CentOS7-Base-163.repo替换CentOS-Base.repo</p>
<p>mv CentOS7-Base-163.repo  CentOS-Base.repo</p>
<p>5）清理旧缓存数据，缓存新数据</p>
<p>yum clean all</p>
<p>yum makecache</p>
<p>6）测试</p>
<p>yum -y install firefox.x86_64</p>
<h2 id="安全拷贝和远程同步"><a href="#安全拷贝和远程同步" class="headerlink" title="安全拷贝和远程同步"></a>安全拷贝和远程同步</h2><h3 id="scp（secure-copy）安全拷贝"><a href="#scp（secure-copy）安全拷贝" class="headerlink" title="scp（secure copy）安全拷贝"></a>scp（secure copy）安全拷贝</h3><p>scp可以实现服务器与服务器之间的数据拷贝。</p>
<p>scp    -r        $pdir/$fname                   $user@linux$host:$pdir/$fname</p>
<p>命令  递归    要拷贝的文件路径/名称   目的用户@主机:目的路径/名称</p>
<p>在hadoop101上，将hadoop101中/opt/module目录下的软件拷贝到hadoop102上。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@hadoop101 /]$ scp -r /opt/module/applog/application.properties  vincent@linux2:/opt/module/applog/application.properties</span><br></pre></td></tr></table></figure>

<h3 id="rsync-远程同步工具"><a href="#rsync-远程同步工具" class="headerlink" title="rsync 远程同步工具"></a>rsync 远程同步工具</h3><p>rsync主要用于备份和镜像。具有速度快、避免复制相同内容和支持符号链接的优点。</p>
<p>rsync和scp区别：用rsync做文件的复制要比scp的速度快，rsync只对差异文件做更新。scp是把所有文件都复制过去。</p>
<p>rsync   -av              $pdir/$fname                     $user@hadoop$host:$pdir/$fname</p>
<p>命令     选项参数    要拷贝的文件路径/名称     目的用户@主机:目的路径/名称</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-a</td>
<td>归档拷贝</td>
</tr>
<tr>
<td>-v</td>
<td>显示复制过程</td>
</tr>
</tbody></table>
<p>把hadoop101机器上的/opt/software目录同步到hadoop102服务器的/opt/software目录下。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@hadoop101 opt]$ rsync -av /opt/software vincent@hadoop102:/opt /software</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>系统</category>
      </categories>
  </entry>
  <entry>
    <title>Shell</title>
    <url>/Shell/</url>
    <content><![CDATA[<h1 id="Shell解析器"><a href="#Shell解析器" class="headerlink" title="Shell解析器"></a>Shell解析器</h1><p>（1）Linux提供的Shell解析器有：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 ~]$ cat /etc/shells </span><br><span class="line">/bin/sh</span><br><span class="line">/bin/bash</span><br><span class="line">/sbin/nologin</span><br><span class="line">/usr/bin/sh</span><br><span class="line">/usr/bin/bash</span><br><span class="line">/usr/sbin/nologin</span><br><span class="line">/bin/tcsh</span><br><span class="line">/bin/csh</span><br></pre></td></tr></table></figure>

<p>（2）bash和sh的关系</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 bin]$ ll | grep bash</span><br><span class="line">-rwxr-xr-x. 1 root root 941880  5月  11 2016 bash</span><br><span class="line">lrwxrwxrwx. 1 root root       4  5月  27 2017 sh -&gt; bash</span><br></pre></td></tr></table></figure>

<p>sh是bash的一种特殊的模式，也就是 /bin/sh 相当于 /bin/bash –posix。说白了sh就是 开启了POSIX(可移植操作系统接口)标准的bash 。sh一般设成bash的软链。</p>
<p>（3）Centos默认的解析器是bash</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 bin]$ echo $SHELL</span><br><span class="line">/bin/bash</span><br></pre></td></tr></table></figure>

<h1 id="Shell脚本入门"><a href="#Shell脚本入门" class="headerlink" title="Shell脚本入门"></a>Shell脚本入门</h1><p>1．脚本格式</p>
<p>脚本以#!/bin/bash开头（指定解析器）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br></pre></td></tr></table></figure>

<p>2．脚本执行方式</p>
<p>第一种：采用 bash+脚本 或  sh+脚本 的相对路径或绝对路径（不用赋予脚本+x权限）</p>
<p>第二种：采用输入脚本的绝对路径或相对路径执行脚本（必须具有可执行权限+x）</p>
<p>注意：第一种执行方法，本质是bash解析器帮你执行脚本，所以脚本本身不需要执行权限。第二种执行方法，本质是脚本需要自己执行，所以需要执行权限。</p>
<p>第三种：在脚本的路径前加上“.”</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">(a) 有以下脚本</span><br><span class="line">[root@0725pc shells]# cat test1.sh </span><br><span class="line"><span class="meta">#</span><span class="bash">! /bin/bash</span></span><br><span class="line">A=&quot;hello&quot;</span><br><span class="line"></span><br><span class="line">(b) 分别使用sh,bash,./ 和 . 的方式来执行，结果如下：</span><br><span class="line">[root@0725pc shells]#  bash test1.sh </span><br><span class="line">[root@0725pc shells]# echo $A</span><br><span class="line"></span><br><span class="line">[root@0725pc shells]# sh test1.sh </span><br><span class="line">[root@0725pc shells]# echo $A</span><br><span class="line"></span><br><span class="line">[root@0725pc shells]# ./test1.sh </span><br><span class="line">[root@0725pc shells]# echo $A</span><br><span class="line"></span><br><span class="line">[root@0725pc shells]# . test1.sh </span><br><span class="line">[root@0725pc shells]# echo $A</span><br><span class="line">hello</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>原因：</p>
<p>前三种方式都是在当前shell中打开一个子shell来执行脚本内容，当脚本内容结束，则子shell关闭，回到父shell中。</p>
<p>第四种，也就是使用在脚本路径前加.的方式，可以使脚本内容在当前shell里执行，而无需打开子shell！</p>
<p>开子shell与不开子shell的区别就在于，环境变量的继承关系，如<strong>在子shell中设置的当前变量，父shell是不可见的</strong>。</p>
<h1 id="Shell中的变量"><a href="#Shell中的变量" class="headerlink" title="Shell中的变量"></a>Shell中的变量</h1><h2 id="系统变量"><a href="#系统变量" class="headerlink" title="系统变量"></a>系统变量</h2><ol>
<li>常用系统变量</li>
</ol>
<p>$HOME、$PWD、$SHELL等</p>
<ol start="2">
<li>案例实操</li>
</ol>
<p>1）查看系统变量的值</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ echo $HOME</span><br><span class="line">/home/vincent</span><br></pre></td></tr></table></figure>

<p>（2）显示当前Shell中所有变量：set</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ set</span><br><span class="line">BASH=/bin/bash</span><br><span class="line">BASH_ALIASES=()</span><br><span class="line">BASH_ARGC=()</span><br><span class="line">BASH_ARGV=()</span><br></pre></td></tr></table></figure>

<h2 id="自定义变量"><a href="#自定义变量" class="headerlink" title="自定义变量"></a>自定义变量</h2><p>1．基本语法</p>
<p>（1）定义变量：变量=值 </p>
<p>（2）撤销变量：unset 变量</p>
<p>（3）声明静态变量：readonly变量，注意：不能unset</p>
<p>2．变量定义规则</p>
<p>​    （1）变量名称可以由字母、数字和下划线组成，但是不能以数字开头，环境变量名建议大写。</p>
<p>​    （2）<strong>等号两侧不能有空格</strong></p>
<p>​    （3）在bash中，变量默认类型都是字符串类型，无法直接进行数值运算。</p>
<p>​    （4）变量的值如果有空格，需要使用双引号或单引号括起来。</p>
<p>3．案例实操</p>
<p>（1）定义变量A</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ A=5</span><br><span class="line">[vincent@linux1 datas]$ echo $A</span><br><span class="line">5</span><br></pre></td></tr></table></figure>

<p>（2）给变量A重新赋值</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ A=6</span><br><span class="line">[vincent@linux1 datas]$ echo $A</span><br><span class="line">6</span><br></pre></td></tr></table></figure>

<p>（3）撤销变量A</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ unset A</span><br><span class="line">[vincent@linux1 datas]$ echo $A</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>（4）声明静态的变量B=2，不能unset</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ readonly B=2</span><br><span class="line">[vincent@linux1 datas]$ echo $B</span><br><span class="line">2</span><br><span class="line">[vincent@linux1 datas]$ B=9</span><br><span class="line">-bash: B: readonly variable</span><br></pre></td></tr></table></figure>

<p>（5）在bash中，<strong>变量默认类型都是字符串类型</strong>，无法直接进行数值运算</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 ~]$ C=1+2</span><br><span class="line">[vincent@linux1 ~]$ echo $C</span><br><span class="line">1+2</span><br></pre></td></tr></table></figure>

<p>（6）变量的值如果有空格，需要使用双引号或单引号括起来</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 ~]$ D=I love banzhang</span><br><span class="line">-bash: world: command not found</span><br><span class="line">[vincent@linux1 ~]$ D=&quot;I love banzhang&quot;</span><br><span class="line">[vincent@linux1 ~]$ echo $A</span><br><span class="line">I love banzhang</span><br></pre></td></tr></table></figure>

<p>（7）可把变量提升为全局环境变量，可供其他Shell程序使用</p>
<p>export 变量名</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ vim helloworld.sh </span><br><span class="line"></span><br><span class="line">在helloworld.sh文件中增加echo $B</span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">echo &quot;helloworld&quot;</span><br><span class="line">echo $B</span><br><span class="line"></span><br><span class="line">[vincent@linux1 datas]$ ./helloworld.sh </span><br><span class="line">Helloworld</span><br></pre></td></tr></table></figure>

<p>发现并没有打印输出变量B的值。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ export B</span><br><span class="line">[vincent@linux1 datas]$ ./helloworld.sh </span><br><span class="line">helloworld</span><br><span class="line">2</span><br></pre></td></tr></table></figure>

<h2 id="特殊变量：-n"><a href="#特殊变量：-n" class="headerlink" title="特殊变量：$n"></a>特殊变量：$n</h2><p>1．基本语法</p>
<p>​    $n  （功能描述：n为数字，**$0代表该脚本名称**，$1-$9代表第一到第九个参数，十以上的参数，十以上的参数需要用大括号包含，如${10}）</p>
<p>2．案例实操</p>
<p>（1）输出该脚本文件名称、输入参数1和输入参数2的值</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ touch parameter.sh </span><br><span class="line">[vincent@linux1 datas]$ vim parameter.sh</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">echo &quot;$0 $1 $2&quot;</span><br><span class="line"></span><br><span class="line">[vincent@linux1 datas]$ chmod 777 parameter.sh</span><br><span class="line"></span><br><span class="line">[vincent@linux1 datas]$ ./parameter.sh cls  xz</span><br><span class="line">./parameter.sh  cls   xz</span><br></pre></td></tr></table></figure>

<h2 id="特殊变量："><a href="#特殊变量：" class="headerlink" title="特殊变量：$#"></a>特殊变量：$#</h2><p>1．基本语法</p>
<p>​    $#  （功能描述：获取所有<strong>输入参数个数</strong>，常用于循环）。</p>
<p>2．案例实操</p>
<p>（1）获取输入参数的个数</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ vim parameter.sh</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">echo &quot;$0 $1 $2&quot;</span><br><span class="line">echo $#</span><br><span class="line"></span><br><span class="line">[vincent@linux1 datas]$ chmod 777 parameter.sh</span><br><span class="line"></span><br><span class="line">[vincent@linux1 datas]$ ./parameter.sh cls  xz</span><br><span class="line">parameter.sh cls xz </span><br><span class="line">2</span><br></pre></td></tr></table></figure>

<h2 id="特殊变量：-、"><a href="#特殊变量：-、" class="headerlink" title="特殊变量：$*、$@"></a>特殊变量：$*、$@</h2><p>1．基本语法</p>
<p>​    $*  （功能描述：这个变量代表命令行中所有的参数，$*把所有的参数看成<strong>一个整体</strong>）</p>
<p>​    $@ （功能描述：这个变量也代表命令行中所有的参数，不过$@把每个参数<strong>区分对待</strong>）</p>
<p>2．案例实操</p>
<p>（1）打印输入的所有参数</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ vim parameter.sh</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">echo &quot;$0 $1 $2&quot;</span><br><span class="line">echo $#</span><br><span class="line">echo $*</span><br><span class="line">echo $@</span><br><span class="line"></span><br><span class="line">[vincent@linux1 datas]$ bash parameter.sh 1 2 3</span><br><span class="line">parameter.sh  1   2</span><br><span class="line">3</span><br><span class="line">1 2 3</span><br><span class="line">1 2 3</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="特殊变量：-？"><a href="#特殊变量：-？" class="headerlink" title="特殊变量：$？"></a>特殊变量：$？</h2><p>1．基本语法</p>
<p>$？ （功能描述：<strong>最后一次执行的命令的返回状态</strong>。如果这个<strong>变量的值为0，证明上一个命令正确执行</strong>；如果这个变量的值为非0（具体是哪个数，由命令自己来决定），则证明上一个命令执行不正确了。）</p>
<p>2．案例实操</p>
<p>​    （1）判断helloworld.sh脚本是否正确执行</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ ./helloworld.sh </span><br><span class="line">hello world</span><br><span class="line">[vincent@linux1 datas]$ echo $?</span><br><span class="line">0</span><br></pre></td></tr></table></figure>

<p>（2）删除目录下文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd mytmp</span><br><span class="line">if (( $? == 0 )); then rm * ; fi</span><br><span class="line">或者</span><br><span class="line">if cd mytmp; then rm * ; fi</span><br></pre></td></tr></table></figure>

<h1 id="运算符"><a href="#运算符" class="headerlink" title="运算符"></a>运算符</h1><p>1．基本语法</p>
<p>（1）“**$((运算式))<strong>”或</strong>“$[运算式]”**</p>
<p>（2）expr  + , - , *, /, %  加，减，乘，除，取余</p>
<p>   (3)用 <code>$(( ))</code> 或 <code>let</code> 进行整数运算</p>
<p>注意：expr运算符间要有<strong>空格</strong></p>
<p>2．案例实操：</p>
<p>（1）计算3+2的值</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ expr 2 + 3</span><br><span class="line">5</span><br></pre></td></tr></table></figure>

<p>（2）计算 (2+3)*4 的值</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">expr一步完成计算</span><br><span class="line">[vincent@linux1 datas]$ expr `expr 2 + 3` \* 4</span><br><span class="line">20</span><br><span class="line"></span><br><span class="line">采用$[运算式]方式</span><br><span class="line">[vincent@linux1 datas]$ S=$[(2+3)*4]</span><br><span class="line">[vincent@linux1 datas]$ echo $S</span><br><span class="line">[vincent@linux1 datas]$ COUNT=$((COUNT + 5 + MAX * 2))</span><br><span class="line">[vincent@linux1 datas]$ let COUNT+=&#x27;5+MAX*2&#x27;</span><br></pre></td></tr></table></figure>

<h1 id="条件判断"><a href="#条件判断" class="headerlink" title="条件判断"></a>条件判断</h1><ol>
<li>基本语法</li>
</ol>
<p>[ condition ]<strong>（注意condition前后要有空格）</strong></p>
<p>或者</p>
<p>((condition ))</p>
<p><strong>注意：条件非空即为true，[ vincent ]返回true，[] 返回false。</strong></p>
<ol start="2">
<li>常用判断条件</li>
</ol>
<p>（1）两个整数之间比较</p>
<figure class="highlight asciidoc"><table><tr><td class="code"><pre><span class="line"><span class="section">= 字符串比较</span></span><br><span class="line"></span><br><span class="line">-lt 小于（less than）         -le 小于等于（less equal）</span><br><span class="line"></span><br><span class="line">-eq 等于（equal）           -gt 大于（greater than）</span><br><span class="line"></span><br><span class="line">-ge 大于等于（greater equal）  -ne 不等于（Not equal）</span><br></pre></td></tr></table></figure>

<p>（2）按照文件权限进行判断</p>
<figure class="highlight lua"><table><tr><td class="code"><pre><span class="line">-r 有读的权限（<span class="built_in">read</span>）       -w 有写的权限（<span class="built_in">write</span>）</span><br><span class="line"></span><br><span class="line">-x 有执行的权限（<span class="built_in">execute</span>）</span><br></pre></td></tr></table></figure>

<p>（3）按照文件类型进行判断</p>
<figure class="highlight diff"><table><tr><td class="code"><pre><span class="line"><span class="deletion">-f 文件存在并且是一个常规的文件（file）</span></span><br><span class="line"></span><br><span class="line"><span class="deletion">-e 文件存在（existence）      -d 文件存在并是一个目录（directory）</span></span><br></pre></td></tr></table></figure>

<ol start="3">
<li>案例实操</li>
</ol>
<p>（1）23是否大于等于22</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ [ 23 -ge 22 ]</span><br><span class="line">[vincent@linux1 datas]$ echo $?</span><br><span class="line">0</span><br></pre></td></tr></table></figure>

<p>（2）helloworld.sh是否具有写权限</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ [ -w helloworld.sh ]</span><br><span class="line">[vincent@linux1 datas]$ echo $?</span><br><span class="line">0</span><br></pre></td></tr></table></figure>

<p>（3）/home/vincent/cls.txt目录中的文件是否存在</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ [ -e /home/vincent/cls.txt ]</span><br><span class="line">[vincent@linux1 datas]$ echo $?</span><br><span class="line">1</span><br></pre></td></tr></table></figure>

<p>（4）多条件判断（**&amp;&amp; 表示前一条命令执行成功时，才执行后一条命令，|| 表示上一条命令执行失败后，才执行下一条命令**）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 ~]$ [ condition ] &amp;&amp; echo OK || echo notok</span><br><span class="line">OK</span><br><span class="line">[vincent@linux1 datas]$ [ condition ] &amp;&amp; [ ] || echo notok</span><br><span class="line">notok</span><br></pre></td></tr></table></figure>

<h1 id="流程控制"><a href="#流程控制" class="headerlink" title="流程控制"></a>流程控制</h1><h2 id="if-判断"><a href="#if-判断" class="headerlink" title="if 判断"></a>if 判断</h2><p>1．基本语法</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">if condition</span><br><span class="line">then</span><br><span class="line">    command1 </span><br><span class="line">    command2</span><br><span class="line">    ...</span><br><span class="line">    commandN </span><br><span class="line">fi</span><br><span class="line">一行:	if [ $(ps -ef | grep -c &quot;ssh&quot;) -gt 1 ]; then echo &quot;true&quot;; fi</span><br><span class="line"></span><br><span class="line">if condition</span><br><span class="line">then</span><br><span class="line">    command1 </span><br><span class="line">    command2</span><br><span class="line">    ...</span><br><span class="line">    commandN</span><br><span class="line">else</span><br><span class="line">    command</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if condition1</span><br><span class="line">then</span><br><span class="line">    command1</span><br><span class="line">elif condition2 </span><br><span class="line">then </span><br><span class="line">    command2</span><br><span class="line">else</span><br><span class="line">    commandN</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<p>​    注意事项：</p>
<p>（1）**[ 条件判断式 ]<strong>，中括号和条件判断式之间必须有</strong>空格**</p>
<p>（2）<strong>if后要有空格</strong></p>
<p>2．案例实操</p>
<p>（1）输入一个数字，如果是1，则输出banzhang zhen shuai，如果是2，则输出 cls zhen mei，如果是其它，什么也不输出。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ touch if.sh</span><br><span class="line">[vincent@linux1 datas]$ vim if.sh</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">if [ $1 -eq &quot;1&quot; ]</span><br><span class="line">then</span><br><span class="line">        echo &quot;banzhang zhen shuai&quot;</span><br><span class="line">elif [ $1 -eq &quot;2&quot; ]</span><br><span class="line">then</span><br><span class="line">        echo &quot;cls zhen mei&quot;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">[vincent@linux1 datas]$ chmod 777 if.sh </span><br><span class="line">[vincent@linux1 datas]$ ./if.sh 1</span><br><span class="line">banzhang zhen shuai</span><br></pre></td></tr></table></figure>

<h2 id="case-语句"><a href="#case-语句" class="headerlink" title="case 语句"></a>case 语句</h2><p>1．基本语法</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">case $变量名 in </span><br><span class="line">  &quot;值1&quot;)</span><br><span class="line">    如果变量的值等于值1，则执行程序1 </span><br><span class="line">    ;; </span><br><span class="line">  &quot;值2&quot;)</span><br><span class="line">    如果变量的值等于值2，则执行程序2 </span><br><span class="line">    ;; </span><br><span class="line">  …省略其他分支… </span><br><span class="line">  *)</span><br><span class="line">    如果变量的值都不是以上的值，则执行此程序 </span><br><span class="line">    ;; </span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<p>注意事项：</p>
<ol>
<li><p>  case行尾必须为单词in，每一个模式匹配必须以右括号)结束。</p>
</li>
<li><p>  双分号;;表示命令序列结束，相当于java中的break。</p>
</li>
<li><p>  最后的*)表示默认模式，相当于java中的default。</p>
</li>
</ol>
<p>2．案例实操</p>
<p>（1）输入一个数字，如果是1，则输出banzhang，如果是2，则输出cls，如果是其它，输出renyao。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">&quot;1&quot;)</span><br><span class="line">	echo &quot;banzhang&quot;</span><br><span class="line">	;;</span><br><span class="line">&quot;2&quot;)</span><br><span class="line">	echo &quot;cls&quot;</span><br><span class="line">	;;</span><br><span class="line">*)</span><br><span class="line">	echo &quot;renyao&quot;</span><br><span class="line">	;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<h2 id="for-循环"><a href="#for-循环" class="headerlink" title="for 循环"></a>for 循环</h2><p>1．基本语法1</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for ((初始值;循环控制条件;变量变化)) </span><br><span class="line">  do </span><br><span class="line">    程序 </span><br><span class="line">  done</span><br></pre></td></tr></table></figure>

<p>2．案例实操</p>
<p>（1）从1加到100</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">s=0</span><br><span class="line">for ((i=1;i&lt;=100;i=i+1))</span><br><span class="line">	do</span><br><span class="line">		s=$[$i+$s]</span><br><span class="line">	done</span><br><span class="line">echo $s</span><br></pre></td></tr></table></figure>

<p>3．基本语法2</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for 变量 in 值1 值2 值3… </span><br><span class="line">  do </span><br><span class="line">    程序 </span><br><span class="line">  done</span><br></pre></td></tr></table></figure>

<p>4．案例实操</p>
<p>​    （1）打印所有输入参数</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ vim for.sh</span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span> </span><br><span class="line"></span><br><span class="line">for i in $*</span><br><span class="line">do</span><br><span class="line">      echo &quot;ban zhang love $i &quot;</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">for j in $@</span><br><span class="line">do      </span><br><span class="line">        echo &quot;ban zhang love $j&quot;</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">[vincent@linux1 datas]$ bash for.sh cls xz bd</span><br><span class="line">ban zhang love cls </span><br><span class="line">ban zhang love xz </span><br><span class="line">ban zhang love bd </span><br><span class="line">ban zhang love cls</span><br><span class="line">ban zhang love xz</span><br><span class="line">ban zhang love bd</span><br></pre></td></tr></table></figure>

<p>（2）比较$*和$@区别</p>
<p>（a）$*和$@都表示传递给函数或脚本的所有参数，不被双引号“”包含时，都以$1 $2 …$n的形式输出所有参数。</p>
<p>（b）当它们被双引号“”包含时，“$*”会将所有的参数作为一个整体，以“$1 $2 …$n”的形式输出所有参数；“$@”会将各个参数分开，以“$1” “$2”…”$n”的形式输出所有参数。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ vim for.sh</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span> </span><br><span class="line"></span><br><span class="line">for i in &quot;$*&quot; </span><br><span class="line"><span class="meta">#</span><span class="bash">$*中的所有参数看成是一个整体，所以这个<span class="keyword">for</span>循环只会循环一次</span> </span><br><span class="line">        do </span><br><span class="line">                echo &quot;ban zhang love $i&quot;</span><br><span class="line">        done </span><br><span class="line"></span><br><span class="line">for j in &quot;$@&quot; </span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="variable">$@</span>中的每个参数都看成是独立的，所以“<span class="variable">$@</span>”中有几个参数，就会循环几次</span> </span><br><span class="line">        do </span><br><span class="line">                echo &quot;ban zhang love $j&quot; </span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">[vincent@linux1 datas]$ chmod 777 for.sh</span><br><span class="line">[vincent@linux1 datas]$ bash for.sh cls xz bd</span><br><span class="line">ban zhang love cls xz bd</span><br><span class="line">ban zhang love cls</span><br><span class="line">ban zhang love xz</span><br><span class="line">ban zhang love bd</span><br></pre></td></tr></table></figure>

<h2 id="while-循环"><a href="#while-循环" class="headerlink" title="while 循环"></a>while 循环</h2><p>1．基本语法</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">while [ 条件判断式 ] 或者 ((条件判断式))</span><br><span class="line">  do </span><br><span class="line">    程序</span><br><span class="line">  done</span><br><span class="line">注意: while 后面需要有空格</span><br></pre></td></tr></table></figure>

<p>2．案例实操</p>
<p>​    （1）从1加到100</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">s=0</span><br><span class="line">i=1</span><br><span class="line">while [ i -le 100 ]</span><br><span class="line">	do</span><br><span class="line">   s=$[$s+$i]</span><br><span class="line">   i=$[$i+1]</span><br><span class="line">  done</span><br><span class="line">echo $s</span><br></pre></td></tr></table></figure>

<h1 id="read读取控制台输入"><a href="#read读取控制台输入" class="headerlink" title="read读取控制台输入"></a>read读取控制台输入</h1><p>1．基本语法</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">read(选项)(参数)   </span><br><span class="line">	选项：</span><br><span class="line">-p：指定读取值时的提示符；</span><br><span class="line">-t：指定读取值时等待的时间（秒）。</span><br><span class="line">参数</span><br><span class="line">	变量：指定读取值的变量名</span><br></pre></td></tr></table></figure>

<p>2．案例实操</p>
<p>​    （1）提示7秒内，读取控制台输入的名称</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ vim read.sh</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">read -t 7 -p &quot;Enter your name in 7 seconds &quot; NAME</span><br><span class="line">echo $NAME</span><br><span class="line"></span><br><span class="line">[vincent@linux1 datas]$ ./read.sh </span><br><span class="line">Enter your name in 7 seconds xiaoze</span><br><span class="line">xiaoze</span><br></pre></td></tr></table></figure>

<h1 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h1><h2 id="系统函数"><a href="#系统函数" class="headerlink" title="系统函数"></a>系统函数</h2><ol>
<li>basename基本语法</li>
</ol>
<figure class="highlight mel"><table><tr><td class="code"><pre><span class="line"><span class="keyword">basename</span> [<span class="keyword">string</span> / pathname] [suffix]  	（功能描述：<span class="keyword">basename</span>命令会删掉所有的前缀包括最后一个（‘/’）字符，然后将字符串显示出来。</span><br><span class="line">选项：</span><br><span class="line">suffix为后缀，如果suffix被指定了，<span class="keyword">basename</span>会将pathname或<span class="keyword">string</span>中的suffix去掉。</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>案例实操</li>
</ol>
<p>（1）截取该/home/vincent/banzhang.txt路径的文件名称</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ basename /home/vincent/banzhang.txt </span><br><span class="line">banzhang.txt</span><br><span class="line">[vincent@linux1 datas]$ basename /home/vincent/banzhang.txt .txt</span><br><span class="line">banzhang</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>dirname基本语法</li>
</ol>
<figure class="highlight mel"><table><tr><td class="code"><pre><span class="line"><span class="keyword">dirname</span> 文件绝对路径		（功能描述：从给定的包含绝对路径的文件名中去除文件名（非目录的部分），然后返回剩下的路径（目录的部分））</span><br></pre></td></tr></table></figure>

<p>4．案例实操</p>
<p>（1）获取banzhang.txt文件的路径</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 ~]$ dirname /home/vincent/banzhang.txt </span><br><span class="line">/home/vincent</span><br></pre></td></tr></table></figure>

<h2 id="自定义函数"><a href="#自定义函数" class="headerlink" title="自定义函数"></a>自定义函数</h2><p>1．基本语法</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[ function ] funname[()]</span><br><span class="line">&#123;</span><br><span class="line">	Action;</span><br><span class="line">	[return int;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>2．经验技巧</p>
<p>​    （1）必须在调用函数地方之前，<strong>先声明函数</strong>，shell脚本是<strong>逐行运行</strong>。不会像其它语言一样先编译。</p>
<p>​    （2）函数返回值，只能通过$?系统变量获得，可以显示加：return返回，如果不加，将以最后一条命令运行结果，作为返回值。</p>
<p>3．案例实操</p>
<p>​    （1）计算两个输入参数的和</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ touch fun.sh</span><br><span class="line">[vincent@linux1 datas]$ vim fun.sh</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">function sum()</span><br><span class="line">&#123;</span><br><span class="line">    s=0</span><br><span class="line">    s=$[$1+$2]</span><br><span class="line">    echo &quot;$s&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">read -p &quot;Please input the number1: &quot; n1;</span><br><span class="line">read -p &quot;Please input the number2: &quot; n2;</span><br><span class="line">sum $n1 $n2;</span><br><span class="line"></span><br><span class="line">[vincent@linux1 datas]$ chmod 777 fun.sh</span><br><span class="line">[vincent@linux1 datas]$ ./fun.sh </span><br><span class="line">Please input the number1: 2</span><br><span class="line">Please input the number2: 5</span><br><span class="line">7</span><br><span class="line"></span><br><span class="line">funWithReturn()&#123;</span><br><span class="line">    echo &quot;这个函数会对输入的两个数字进行相加运算...&quot;</span><br><span class="line">    echo &quot;输入第一个数字: &quot;</span><br><span class="line">    read aNum</span><br><span class="line">    echo &quot;输入第二个数字: &quot;</span><br><span class="line">    read anotherNum</span><br><span class="line">    echo &quot;两个数字分别为 $aNum 和 $anotherNum !&quot;</span><br><span class="line">    return $(($aNum+$anotherNum))</span><br><span class="line">&#125;</span><br><span class="line">funWithReturn</span><br><span class="line">echo &quot;输入的两个数字之和为 $? !&quot;</span><br><span class="line"></span><br><span class="line">这个函数会对输入的两个数字进行相加运算...</span><br><span class="line">输入第一个数字: </span><br><span class="line">1</span><br><span class="line">输入第二个数字: </span><br><span class="line">2</span><br><span class="line">两个数字分别为 1 和 2 !</span><br><span class="line">输入的两个数字之和为 3 !</span><br></pre></td></tr></table></figure>

<h1 id="Shell-输入-输出重定向"><a href="#Shell-输入-输出重定向" class="headerlink" title="Shell 输入/输出重定向"></a>Shell 输入/输出重定向</h1><table>
<thead>
<tr>
<th align="left">命令</th>
<th align="left">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="left">command &gt; file</td>
<td align="left">将输出重定向到 file。</td>
</tr>
<tr>
<td align="left">command &lt; file</td>
<td align="left">将输入重定向到 file。</td>
</tr>
<tr>
<td align="left">command &gt;&gt; file</td>
<td align="left">将输出以追加的方式重定向到 file。</td>
</tr>
<tr>
<td align="left">n &gt; file</td>
<td align="left">将文件描述符为 n 的文件重定向到 file。</td>
</tr>
<tr>
<td align="left">n &gt;&gt; file</td>
<td align="left">将文件描述符为 n 的文件以追加的方式重定向到 file。</td>
</tr>
<tr>
<td align="left">n &gt;&amp; m</td>
<td align="left">将输出文件 m 和 n 合并。</td>
</tr>
<tr>
<td align="left">n &lt;&amp; m</td>
<td align="left">将输入文件 m 和 n 合并。</td>
</tr>
<tr>
<td align="left">&lt;&lt; tag</td>
<td align="left">将开始标记 tag 和结束标记 tag 之间的内容作为输入。</td>
</tr>
</tbody></table>
<p><strong>需要注意的是文件描述符 0 通常是标准输入（STDIN），1 是标准输出（STDOUT），2 是标准错误输出（STDERR）</strong></p>
<p>一般情况下，每个 Unix/Linux 命令运行时都会打开三个文件：</p>
<ul>
<li>标准输入文件(stdin)：stdin的文件描述符为0，Unix程序默认从stdin读取数据。</li>
<li>标准输出文件(stdout)：stdout 的文件描述符为1，Unix程序默认向stdout输出数据。</li>
<li>标准错误文件(stderr)：stderr的文件描述符为2，Unix程序会向stderr流中写入错误信息。</li>
</ul>
<p>默认情况下，command &gt; file 将 stdout 重定向到 file，command &lt; file 将stdin 重定向到 file。</p>
<p>如果希望 stderr 重定向到 file，可以这样写：</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line">$ <span class="keyword">command</span> <span class="number">2</span>&gt;<span class="keyword">file</span></span><br></pre></td></tr></table></figure>

<p>如果希望 stderr 追加到 file 文件末尾，可以这样写：</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line">$ <span class="keyword">command</span> <span class="number">2</span>&gt;&gt;<span class="keyword">file</span></span><br></pre></td></tr></table></figure>

<p>如果希望将 stdout 和 stderr 合并后重定向到 file，可以这样写：</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line">$ <span class="keyword">command</span> &gt; <span class="keyword">file</span> <span class="number">2</span>&gt;&amp;<span class="number">1</span></span><br><span class="line"></span><br><span class="line">或者</span><br><span class="line"></span><br><span class="line">$ <span class="keyword">command</span> &gt;&gt; <span class="keyword">file</span> <span class="number">2</span>&gt;&amp;<span class="number">1</span></span><br></pre></td></tr></table></figure>

<p>如果希望对 stdin 和 stdout 都重定向，可以这样写：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">command</span> &lt; file1 &gt;file2</span></span><br></pre></td></tr></table></figure>

<p><strong>/dev/null 文件</strong></p>
<p>如果希望执行某个命令，但又不希望在屏幕上显示输出结果，那么可以将输出重定向到 /dev/null：</p>
<figure class="highlight arcade"><table><tr><td class="code"><pre><span class="line">$ command &gt; <span class="regexp">/dev/</span><span class="literal">null</span></span><br></pre></td></tr></table></figure>

<p>/dev/null 是一个特殊的文件，写入到它的内容都会被丢弃；如果尝试从该文件读取内容，那么什么也读不到。但是 /dev/null 文件非常有用，将命令的输出重定向到它，会起到”禁止输出”的效果。</p>
<p>如果希望屏蔽 stdout 和 stderr，可以这样写：</p>
<figure class="highlight arcade"><table><tr><td class="code"><pre><span class="line">$ command &gt; <span class="regexp">/dev/</span><span class="literal">null</span> <span class="number">2</span>&gt;&amp;<span class="number">1</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p><strong>注意：</strong>0 是标准输入（STDIN），1 是标准输出（STDOUT），2 是标准错误输出（STDERR）。</p>
<p>这里的 <strong>2</strong> 和 <strong>&gt;</strong> 之间不可以有空格，<strong>2&gt;</strong> 是一体的时候才表示错误输出。</p>
</blockquote>
<p><strong>Here Document</strong></p>
<p>Here Document 是 Shell 中的一种特殊的重定向方式，用来将输入重定向到一个交互式 Shell 脚本或程序。</p>
<p>它的基本的形式如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">command</span> &lt;&lt; <span class="string">delimiter</span></span><br><span class="line"><span class="string">    document</span></span><br><span class="line"><span class="string">delimiter</span></span><br></pre></td></tr></table></figure>

<p>它的作用是将两个 delimiter 之间的内容(document) 作为输入传递给 command。</p>
<p>注意：</p>
<p>结尾的delimiter 一定要顶格写，前面不能有任何字符，后面也不能有任何字符，包括空格和 tab 缩进。</p>
<p>开始的delimiter前后的空格会被忽略掉。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> wc -l &lt;&lt; <span class="string">EOF</span></span></span><br><span class="line">    欢迎来到</span><br><span class="line">    菜鸟教程</span><br><span class="line">    www.runoob.com</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">3          # 输出结果为 3 行</span><br></pre></td></tr></table></figure>

<h1 id="Shell工具"><a href="#Shell工具" class="headerlink" title="Shell工具"></a>Shell工具</h1><h2 id="cut"><a href="#cut" class="headerlink" title="cut"></a>cut</h2><p>cut的工作就是“剪”，具体的说就是在文件中负责剪切数据用的。cut 命令从文件的每一行剪切字节、字符和字段并将这些字节、字符和字段输出。</p>
<p>1.基本用法</p>
<figure class="highlight coq"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cut</span> [选项参数]  filename</span><br><span class="line">说明：默认分隔符是制表符</span><br></pre></td></tr></table></figure>

<p>2.选项参数说明</p>
<table>
<thead>
<tr>
<th>选项参数</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-f</td>
<td>列号，提取第几列  （-n：1到n列，n-：n到最后一列）</td>
</tr>
<tr>
<td>-d</td>
<td>分隔符，按照指定分隔符分割列</td>
</tr>
</tbody></table>
<p>3.案例实操</p>
<p>（0）数据准备</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ vim cut.txt</span><br><span class="line">dong shen</span><br><span class="line">guan zhen</span><br><span class="line">wo  wo</span><br><span class="line">lai  lai</span><br><span class="line">le  le</span><br></pre></td></tr></table></figure>

<p>（1）切割cut.txt第一列</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ cut -d &quot; &quot; -f 1 cut.txt </span><br><span class="line">dong</span><br><span class="line">guan</span><br><span class="line">wo</span><br><span class="line">lai</span><br><span class="line">le</span><br></pre></td></tr></table></figure>

<p>（2）切割cut.txt第二、三列</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ cut -d &quot; &quot; -f 2,3 cut.txt </span><br><span class="line">shen</span><br><span class="line">zhen</span><br><span class="line"> wo</span><br><span class="line"> lai</span><br><span class="line"> le</span><br></pre></td></tr></table></figure>

<p>（3）在cut.txt文件中切割出guan</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ cat cut.txt | grep &quot;guan&quot; | cut -d &quot; &quot; -f 1</span><br><span class="line">guan</span><br></pre></td></tr></table></figure>

<p>（4）选取系统PATH变量值，第2个“：”开始后的所有路径：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ echo $PATH</span><br><span class="line">/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/vincent/bin</span><br><span class="line"></span><br><span class="line">[vincent@linux1 datas]$ echo $PATH | cut -d : -f 2-</span><br><span class="line">/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/vincent/bin</span><br></pre></td></tr></table></figure>

<h2 id="sed"><a href="#sed" class="headerlink" title="sed"></a>sed</h2><p>sed是一种<strong>流编辑器，它一次处理一行内容</strong>。处理时，把当前处理的行存储在临时缓冲区中，称为“模式空间”，接着用sed命令处理缓冲区中的内容，处理完成后，把缓冲区的内容送往屏幕。接着处理下一行，这样不断重复，直到文件末尾。文件内容并没有改变，除非你使用重定向存储输出。</p>
<ol>
<li>基本用法</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sed [选项参数]  ‘<span class="built_in">command</span>’ filename</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>选项参数说明</li>
</ol>
<figure class="highlight diff"><table><tr><td class="code"><pre><span class="line"><span class="deletion">-e : 直接在指令列模式上进行sed的动作编辑。</span></span><br><span class="line"><span class="deletion">-n ：使用安静(silent)模式。在一般 sed 的用法中，所有来自 STDIN 的数据一般都会被列出到终端上。但如果加上 -n 参数后，则只有经过sed 特殊处理的那一行(或者动作)才会被列出来。</span></span><br><span class="line"><span class="deletion">-f ：直接将 sed 的动作写在一个文件内， -f filename 则可以运行 filename 内的 sed 动作。</span></span><br><span class="line"><span class="deletion">-r ：sed 的动作支持的是延伸型正规表示法的语法。(默认是基础正规表示法语法)</span></span><br><span class="line"><span class="deletion">-i ：直接修改读取的文件内容，而不是输出到终端。</span></span><br></pre></td></tr></table></figure>

<ol start="3">
<li>命令功能描述</li>
</ol>
<p>表10-3</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>功能描述</th>
</tr>
</thead>
<tbody><tr>
<td><em>a</em></td>
<td>新增，a的后面可以接字串，在下一行出现</td>
</tr>
<tr>
<td>d</td>
<td>删除</td>
</tr>
<tr>
<td>s</td>
<td>查找并替换</td>
</tr>
</tbody></table>
<ol start="4">
<li>案例实操</li>
</ol>
<p>（0）数据准备</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ vim sed.txt</span><br><span class="line">dong shen</span><br><span class="line">guan zhen</span><br><span class="line">wo  wo</span><br><span class="line">lai  lai</span><br><span class="line"></span><br><span class="line">le  le</span><br></pre></td></tr></table></figure>

<p>（1）将“mei nv”这个单词插入到sed.txt第二行下，打印</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ sed &#x27;2a mei nv&#x27; sed.txt </span><br><span class="line">dong shen</span><br><span class="line">guan zhen</span><br><span class="line">mei nv</span><br><span class="line">wo  wo</span><br><span class="line">lai  lai</span><br><span class="line"></span><br><span class="line">le  le</span><br><span class="line">[vincent@linux1 datas]$ cat sed.txt </span><br><span class="line">dong shen</span><br><span class="line">guan zhen</span><br><span class="line">wo  wo</span><br><span class="line">lai  lai</span><br><span class="line"></span><br><span class="line">le  le</span><br></pre></td></tr></table></figure>

<p>注意：文件并没有改变</p>
<p>（2）删除sed.txt文件所有包含wo的行</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ sed &#x27;/wo/d&#x27; sed.txt</span><br><span class="line">dong shen</span><br><span class="line">guan zhen</span><br><span class="line">lai  lai</span><br><span class="line"></span><br><span class="line">le  le</span><br></pre></td></tr></table></figure>

<p>（3）将sed.txt文件中wo替换为ni</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ sed &#x27;s/wo/ni/g&#x27; sed.txt </span><br><span class="line">dong shen</span><br><span class="line">guan zhen</span><br><span class="line">ni  ni</span><br><span class="line">lai  lai</span><br><span class="line"></span><br><span class="line">le  le</span><br></pre></td></tr></table></figure>

<p><strong>注意：‘g’表示global，全部替换</strong></p>
<p>（4）将sed.txt文件中的第二行删除并将wo替换为ni</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ sed -e &#x27;2d&#x27; -e &#x27;s/wo/ni/g&#x27; sed.txt </span><br><span class="line">dong shen</span><br><span class="line">ni  ni </span><br><span class="line">lai  lai</span><br><span class="line"></span><br><span class="line">le  le</span><br></pre></td></tr></table></figure>

<h2 id="awk"><a href="#awk" class="headerlink" title="awk"></a>awk</h2><p><a href="https://www.w3cschool.cn/awk/">https://www.w3cschool.cn/awk/</a></p>
<p>一个强大的文本分析工具，<strong>把文件逐行的读入，以空格为默认分隔符将每行切片</strong>，切开的部分再进行分析处理。</p>
<h3 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">命令行</span><br><span class="line">awk [options] file ...</span><br><span class="line"></span><br><span class="line">awk程序文件</span><br><span class="line">awk [option] -f file ....</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk --help</span><br><span class="line"></span><br><span class="line">Usage: awk [POSIX or GNU style options] -f progfile [--] file ...</span><br><span class="line">Usage: awk [POSIX or GNU style options] [--] &#x27;program&#x27; file ...</span><br><span class="line">POSIX options:          GNU long options: (standard)</span><br><span class="line">        -f progfile             --file=progfile</span><br><span class="line">        -F fs                   --field-separator=fs</span><br><span class="line">        -v var=val              --assign=var=val</span><br><span class="line">Short options:          GNU long options: (extensions)</span><br><span class="line">        -b                      --characters-as-bytes</span><br><span class="line">        -c                      --traditional</span><br><span class="line">        -C                      --copyright</span><br><span class="line">        -d[file]                --dump-variables[=file]</span><br><span class="line">        -e &#x27;program-text&#x27;       --source=&#x27;program-text&#x27;</span><br><span class="line">        -E file                 --exec=file</span><br><span class="line">        -g                      --gen-pot</span><br><span class="line">        -h                      --help</span><br><span class="line">        -L [fatal]              --lint[=fatal]</span><br><span class="line">        -n                      --non-decimal-data</span><br><span class="line">        -N                      --use-lc-numeric</span><br><span class="line">        -O                      --optimize</span><br><span class="line">        -p[file]                --profile[=file]</span><br><span class="line">        -P                      --posix</span><br><span class="line">        -r                      --re-interval</span><br><span class="line">        -S                      --sandbox</span><br><span class="line">        -t                      --lint-old</span><br><span class="line">        -V                      --version</span><br><span class="line"></span><br><span class="line">To report bugs, see node `Bugs&#x27; in `gawk.info&#x27;, which is</span><br><span class="line">section `Reporting Problems and Bugs&#x27; in the printed version.</span><br><span class="line"></span><br><span class="line">gawk is a pattern scanning and processing language.</span><br><span class="line">By default it reads standard input and writes standard output.</span><br><span class="line"></span><br><span class="line">Examples:</span><br><span class="line">        gawk &#x27;&#123; sum += $1 &#125;; END &#123; print sum &#125;&#x27; file</span><br><span class="line">        gawk -F: &#x27;&#123; print $1 &#125;&#x27; /etc/passwd</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">-v 选项</span><br><span class="line">在程序执行之前为变量赋值</span><br><span class="line"></span><br><span class="line">awk -v name=Jerry &#x27;BEGIN&#123;printf &quot;Name = %s\n&quot;, name&#125;&#x27;</span><br><span class="line">Name = Jerry</span><br></pre></td></tr></table></figure>

<h3 id="内置变量"><a href="#内置变量" class="headerlink" title="内置变量"></a>内置变量</h3><table>
<thead>
<tr>
<th>变量</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>ARGC</td>
<td>命令行参数个数</td>
</tr>
<tr>
<td>ARGV</td>
<td>命令行参数排列</td>
</tr>
<tr>
<td>CONVFMT</td>
<td>数据转换为字符串的格式，其默认值为 %.6g</td>
</tr>
<tr>
<td>OFMT</td>
<td>表示数值输出的格式，它的默认值为 %.6g</td>
</tr>
<tr>
<td>ENVIRON</td>
<td>支持队列中系统环境变量的使用</td>
</tr>
<tr>
<td>FILENAME</td>
<td>awk浏览的文件名</td>
</tr>
<tr>
<td>FNR</td>
<td>浏览文件的记录数</td>
</tr>
<tr>
<td>RLENGTH</td>
<td>表示 match 函数匹配的字符串长度</td>
</tr>
<tr>
<td>RSTART</td>
<td>表示由 match 函数匹配的字符串的第一个字符的位置</td>
</tr>
<tr>
<td>FS</td>
<td>FS command contains the field separator character which is used to divide fields on the input line. The default is “white space”, meaning space and tab characters. FS can be reassigned to another character (typically in BEGIN) to change the field separator.</td>
</tr>
<tr>
<td>NF</td>
<td>NF command keeps a count of the number of fields within the current input record.</td>
</tr>
<tr>
<td>NR</td>
<td>NR command keeps a current count of the number of input records. Remember that records are usually lines. Awk command performs the pattern/action statements once for each record in a file.</td>
</tr>
<tr>
<td>OFS</td>
<td>OFS command stores the output field separator, which separates the fields when Awk prints them. The default is a blank space. Whenever print has several parameters separated with commas, it will print the value of OFS in between each parameter.</td>
</tr>
<tr>
<td>ORS</td>
<td>ORS command stores the output record separator, which separates the output lines when Awk prints them. The default is a newline character. print automatically outputs the contents of ORS at the end of whatever it is given to print.</td>
</tr>
<tr>
<td>RS</td>
<td>RS command stores the current record separator character. Since, by default, an input line is the input record, the default record separator character is a newline.</td>
</tr>
</tbody></table>
<h4 id="ARGC"><a href="#ARGC" class="headerlink" title="ARGC"></a>ARGC</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;BEGIN &#123;print &quot;Arguments =&quot;, ARGC&#125;&#x27; One Two Three Four</span><br><span class="line">Arguments = 5</span><br></pre></td></tr></table></figure>

<h4 id="ARGV"><a href="#ARGV" class="headerlink" title="ARGV"></a>ARGV</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;BEGIN &#123; for (i = 0; i &lt; ARGC; ++i)</span><br><span class="line">      &#123; printf &quot;ARGV[%d] = %s\n&quot;, i, ARGV[i] &#125; </span><br><span class="line">                    &#125;&#x27; one two three four</span><br><span class="line">ARGV[0] = awk</span><br><span class="line">ARGV[1] = one</span><br><span class="line">ARGV[2] = two</span><br><span class="line">ARGV[3] = three</span><br><span class="line">ARGV[4] = four</span><br></pre></td></tr></table></figure>

<h4 id="CONVFMT"><a href="#CONVFMT" class="headerlink" title="CONVFMT"></a>CONVFMT</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;BEGIN &#123; print &quot;Conversion Format =&quot;, CONVFMT &#125;&#x27;</span><br><span class="line">Conversion Format = %.6g</span><br></pre></td></tr></table></figure>

<h4 id="ENVIRON"><a href="#ENVIRON" class="headerlink" title="ENVIRON"></a>ENVIRON</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;BEGIN &#123; print ENVIRON[&quot;USER&quot;] &#125;&#x27;</span><br><span class="line">vincent</span><br></pre></td></tr></table></figure>

<h4 id="FILENAME"><a href="#FILENAME" class="headerlink" title="FILENAME"></a>FILENAME</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;END &#123;print FILENAME&#125;&#x27; marks.txt</span><br><span class="line">marks.txt</span><br></pre></td></tr></table></figure>

<h4 id="FS"><a href="#FS" class="headerlink" title="FS"></a>FS</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;BEGIN &#123;print &quot;FS = &quot; FS&#125;&#x27; | cat -vte</span><br><span class="line">FS =  $</span><br></pre></td></tr></table></figure>

<h4 id="NF"><a href="#NF" class="headerlink" title="NF"></a>NF</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo -e &quot;One Two\nOne Two Three\nOne Two Three Four&quot; | awk &#x27;NF &gt; 2&#x27;</span><br><span class="line">One Two Three</span><br><span class="line">One Two Three Four</span><br></pre></td></tr></table></figure>

<h4 id="NR"><a href="#NR" class="headerlink" title="NR"></a>NR</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo -e &quot;One Two\nOne Two Three\nOne Two Three Four&quot; | awk &#x27;NR &lt; 3&#x27;</span><br><span class="line">One Two</span><br><span class="line">One Two Three</span><br></pre></td></tr></table></figure>

<h4 id="RLENGTH"><a href="#RLENGTH" class="headerlink" title="RLENGTH"></a>RLENGTH</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;BEGIN &#123; if (match(&quot;One Two Three&quot;, &quot;ree&quot;)) &#123; print RLENGTH &#125; &#125;&#x27;</span><br></pre></td></tr></table></figure>

<h4 id="RSTART"><a href="#RSTART" class="headerlink" title="RSTART"></a>RSTART</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;BEGIN &#123; if (match(&quot;One Two Three&quot;, &quot;Thre&quot;)) &#123; print RSTART &#125; &#125;&#x27;</span><br><span class="line">9</span><br></pre></td></tr></table></figure>

<h3 id="操作符"><a href="#操作符" class="headerlink" title="操作符"></a>操作符</h3><h4 id="算术运算符"><a href="#算术运算符" class="headerlink" title="算术运算符"></a>算术运算符</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;BEGIN &#123; a = 50; b = 20; print &quot;(a + b) = &quot; (a + b) &#125;&#x27;</span><br><span class="line">(a + b) = 70</span><br><span class="line"></span><br><span class="line">awk &#x27;BEGIN &#123; a = 50; b = 20; print &quot;(a / b) = &quot;, (a / b) &#125;&#x27;</span><br><span class="line">(a / b) =  2.5</span><br><span class="line"></span><br><span class="line">awk &#x27;BEGIN &#123; a = 10; a = a ^ 2; print &quot;a =&quot;, a &#125;&#x27;</span><br><span class="line">a = 100</span><br><span class="line"></span><br><span class="line">awk &#x27;BEGIN &#123; a = 10; a = a ** 2; print &quot;a =&quot;, a &#125;&#x27;</span><br><span class="line">a = 100</span><br></pre></td></tr></table></figure>

<h4 id="递增运算符与递减运算符"><a href="#递增运算符与递减运算符" class="headerlink" title="递增运算符与递减运算符"></a>递增运算符与递减运算符</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;BEGIN &#123; a = 10; b = ++a; printf &quot;a = %d, b = %d\n&quot;, a, b &#125;&#x27;</span><br><span class="line">a = 11, b = 11</span><br><span class="line"></span><br><span class="line">awk &#x27;BEGIN &#123; a = 10; b = a++; printf &quot;a = %d, b = %d\n&quot;, a, b &#125;&#x27;</span><br><span class="line">a = 11, b = 10</span><br></pre></td></tr></table></figure>

<h4 id="赋值操作符"><a href="#赋值操作符" class="headerlink" title="赋值操作符"></a>赋值操作符</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;BEGIN &#123; name = &quot;Jerry&quot;; print &quot;My name is&quot;, name &#125;&#x27;</span><br><span class="line">My name is Jerry</span><br><span class="line"></span><br><span class="line">awk &#x27;BEGIN &#123; cnt=10; cnt += 10; print &quot;Counter =&quot;, cnt &#125;&#x27;</span><br><span class="line">Counter = 20</span><br></pre></td></tr></table></figure>

<h4 id="关系运算符"><a href="#关系运算符" class="headerlink" title="关系运算符"></a>关系运算符</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;BEGIN &#123; a = 10; b = 10; if (a == b) print &quot;a == b&quot; &#125;&#x27;</span><br></pre></td></tr></table></figure>

<h4 id="逻辑运算符"><a href="#逻辑运算符" class="headerlink" title="逻辑运算符"></a>逻辑运算符</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;BEGIN &#123;num = 5; if (num &gt;= 0 &amp;&amp; num &lt;= 7) printf &quot;%d is in octal format\n&quot;, num &#125;&#x27;</span><br><span class="line">awk &#x27;BEGIN &#123;ch = &quot;\n&quot;; if (ch == &quot; &quot; || ch == &quot;\t&quot; || ch == &quot;\n&quot;) print &quot;Current character is whitespace.&quot; &#125;&#x27;</span><br><span class="line">awk &#x27;BEGIN &#123; name = &quot;&quot;; if (! length(name)) print &quot;name is empty string.&quot; &#125;&#x27;</span><br><span class="line">awk &#x27;BEGIN &#123; a = 10; b = 20; (a &gt; b) ? max = a : max = b; print &quot;Max =&quot;, max&#125;&#x27;</span><br></pre></td></tr></table></figure>

<h4 id="一元运算符"><a href="#一元运算符" class="headerlink" title="一元运算符"></a>一元运算符</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;BEGIN &#123; a = -10; a = +a; print &quot;a =&quot;, a &#125;&#x27;</span><br><span class="line">a = -10</span><br><span class="line">awk &#x27;BEGIN &#123; a = -10; a = -a; print &quot;a =&quot;, a &#125;&#x27;</span><br><span class="line">a = 10</span><br></pre></td></tr></table></figure>

<h4 id="字符串连接操作符"><a href="#字符串连接操作符" class="headerlink" title="字符串连接操作符"></a>字符串连接操作符</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;BEGIN &#123; str1=&quot;Hello, &quot;; str2=&quot;World&quot;; str3 = str1 str2; print str3 &#125;&#x27;</span><br><span class="line">Hello, World</span><br></pre></td></tr></table></figure>

<h4 id="数组成员操作符"><a href="#数组成员操作符" class="headerlink" title="数组成员操作符"></a>数组成员操作符</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;BEGIN &#123; arr[0] = 1; arr[1] = 2; arr[2] = 3; for (i in arr) printf &quot;arr[%d] = %d\n&quot;, i, arr[i] &#125;&#x27;</span><br><span class="line">arr[2] = 3</span><br><span class="line">arr[0] = 1</span><br><span class="line">arr[1] = 2</span><br></pre></td></tr></table></figure>

<h4 id="正则表达式操作符"><a href="#正则表达式操作符" class="headerlink" title="正则表达式操作符"></a>正则表达式操作符</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;$0 ~ 9&#x27; marks.txt</span><br><span class="line">2)    Rahul    Maths      90</span><br><span class="line">5)    Hari     History    89</span><br><span class="line"></span><br><span class="line">awk &#x27;$0 !~ 9&#x27; marks.txt</span><br><span class="line">1)  Amit    Physics 80</span><br><span class="line">3)  Shyam   Biology 87</span><br><span class="line">4)  Kedar   English 85</span><br></pre></td></tr></table></figure>

<h3 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h3><p>（1）统计passwd文件名，每行的行号，每行的列数</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ awk -F: &#x27;&#123;print &quot;filename:&quot;  FILENAME &quot;, linenumber:&quot; NR  &quot;,columns:&quot; NF&#125;&#x27; passwd </span><br><span class="line">filename:passwd, linenumber:1,columns:7</span><br><span class="line">filename:passwd, linenumber:2,columns:7</span><br><span class="line">filename:passwd, linenumber:3,columns:7</span><br></pre></td></tr></table></figure>

<p>（2）切割IP</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ ifconfig eth0 | grep &quot;netmask&quot; | awk -F &#x27; &#x27; &#x27;&#123;print $2&#125;&#x27; </span><br><span class="line">10.211.55.10</span><br></pre></td></tr></table></figure>

<p>（3）查询sed.txt中空行所在的行号</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ awk &#x27;/^$/&#123;print NR&#125;&#x27; sed.txt </span><br><span class="line">5</span><br></pre></td></tr></table></figure>

<p>（4）其他实例</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">who am i | awk &#x27;&#123;print $0&#125;&#x27;</span><br><span class="line">vincent  ttys000  Jan  4 11:18</span><br><span class="line"></span><br><span class="line">who am i | awk &#x27;&#123;print $1&#125;&#x27;</span><br><span class="line">vincent</span><br><span class="line"></span><br><span class="line">who am i | awk &#x27;&#123;print $1,$5&#125;&#x27;</span><br><span class="line">vincent 11:18</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">dennis_ritchie.txt</span><br><span class="line">UNIX is basically a simple operating system, but you hve to be genius to understand the simplicity.</span><br><span class="line"></span><br><span class="line">awk &#x27;&#123;print $1,$2,$NF&#125;&#x27; dennis_ritchie.txt</span><br><span class="line">UNIX is simplicity.</span><br><span class="line"></span><br><span class="line">awk &#x27;BEGIN &#123;print &quot;Dennis Ritchie&quot;&#125; &#123;print $0&#125;&#x27; dennis_ritchie.txt</span><br><span class="line">Dennis Ritchie</span><br><span class="line">UNIX is basically a simple operating system, but you hve to be genius to understand the simplicity.</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">date | awk &#x27;&#123;print $2,$3,$6&#125;&#x27;</span><br><span class="line">Jan 4 2022</span><br><span class="line"></span><br><span class="line">date | awk &#x27;OFS=&quot;/&quot; &#123;print$2,$3,$6&#125;&#x27;</span><br><span class="line">Jan/4/2022</span><br><span class="line"></span><br><span class="line">date | awk &#x27;OFS=&quot;-&quot; &#123;print$2,$3,$6&#125;&#x27;</span><br><span class="line">Jan-4-2022</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">password.txt</span><br><span class="line">root:x:0:0:root:/root:/bin/bash</span><br><span class="line">bin:x:1:1:bin:/bin:/sbin/nologin</span><br><span class="line">vincent:x:1000:1000:vincent:/home/vincent:/bin/bash</span><br><span class="line">mysql:x:27:27:MySQL Server:/var/lib/mysql:/bin/false</span><br><span class="line"></span><br><span class="line">awk -F: &#x27;&#123;print $1,$6&#125;&#x27; ./password.txt</span><br><span class="line">root /root</span><br><span class="line">bin /bin</span><br><span class="line">vincent /home/vincent</span><br><span class="line">mysql /var/lib/mysql</span><br><span class="line"></span><br><span class="line">awk -F: &#x27;$3 &gt;= 1000 &#123;print $1,$6&#125;&#x27; ./password.txt</span><br><span class="line">vincent /home/vincent</span><br><span class="line"></span><br><span class="line">awk -F: &#x27;BEGIN &#123;print &quot;User Accounts\n-------------&quot;&#125; $3 &gt;= 1000 &#123;print $1,$6&#125;&#x27; ./password.txt</span><br><span class="line">User Accounts</span><br><span class="line">-------------</span><br><span class="line">vincent /home/vincent</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">fstab.txt</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="bash"><span class="comment"># /etc/fstab</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Created by anaconda on Thu Jun 18 08:00:40 2020</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="bash"><span class="comment"># Accessible filesystems, by reference, are maintained under &#x27;/dev/disk&#x27;</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) <span class="keyword">for</span> more info</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="bash">UUID=a707eb5e-c386-49c1-ae34-3bbfa02a5550 /                       ext4    defaults        1 1</span></span><br><span class="line">UUID=f7665600-7d3b-442e-a2cc-93ed374db403 /boot                   ext4    defaults        1 2</span><br><span class="line">UUID=4b94cc3e-3bf3-4654-9772-e1e7d290b8de swap                    swap    defaults        0 0</span><br><span class="line"></span><br><span class="line">awk &#x27;/^UUID/ &#123;print $0&#125;&#x27; ./fstab.txt</span><br><span class="line">UUID=a707eb5e-c386-49c1-ae34-3bbfa02a5550 /                       ext4    defaults        1 1</span><br><span class="line">UUID=f7665600-7d3b-442e-a2cc-93ed374db403 /boot                   ext4    defaults        1 2</span><br><span class="line">UUID=4b94cc3e-3bf3-4654-9772-e1e7d290b8de swap                    swap    defaults        0 0</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;BEGIN &#123; print sqrt((2+3)*5)&#125;&#x27;</span><br><span class="line">25</span><br><span class="line"></span><br><span class="line">awk &#x27;BEGIN &#123;print atan2(0, -1)&#125;&#x27;</span><br><span class="line">3.14159</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">employee.txt</span><br><span class="line">ajay manager account 45000</span><br><span class="line">sunil clerk account 25000</span><br><span class="line">varun manager sales 50000</span><br><span class="line">amit manager account 47000</span><br><span class="line">tarun peon sales 15000</span><br><span class="line">deepak clerk sales 23000</span><br><span class="line">sunil peon sales 13000</span><br><span class="line">satvik director purchase 80000</span><br><span class="line"></span><br><span class="line">awk &#x27;&#123;print NR,$0&#125;&#x27; employee.txt </span><br><span class="line">1 ajay manager account 45000</span><br><span class="line">2 sunil clerk account 25000</span><br><span class="line">3 varun manager sales 50000</span><br><span class="line">4 amit manager account 47000</span><br><span class="line">5 tarun peon sales 15000</span><br><span class="line">6 deepak clerk sales 23000</span><br><span class="line">7 sunil peon sales 13000</span><br><span class="line">8 satvik director purchase 80000 </span><br><span class="line"></span><br><span class="line">awk &#x27;&#123;print $1,$NF&#125;&#x27; employee.txt </span><br><span class="line">ajay 45000</span><br><span class="line">sunil 25000</span><br><span class="line">varun 50000</span><br><span class="line">amit 47000</span><br><span class="line">tarun 15000</span><br><span class="line">deepak 23000</span><br><span class="line">sunil 13000</span><br><span class="line">satvik 80000</span><br><span class="line"></span><br><span class="line">awk &#x27;&#123;print $1 &quot;\t&quot; $NF&#125;&#x27; employee.txt</span><br><span class="line">ajay    45000</span><br><span class="line">sunil   25000</span><br><span class="line">varun   50000</span><br><span class="line">amit    47000</span><br><span class="line">tarun   15000</span><br><span class="line">deepak  23000</span><br><span class="line">sunil   13000</span><br><span class="line">satvik  80000</span><br><span class="line"></span><br><span class="line">Display Line From 3 to 6</span><br><span class="line">awk &#x27;NR==3, NR==6 &#123;print NR,$0&#125;&#x27; employee.txt </span><br><span class="line">3 varun manager sales 50000</span><br><span class="line">4 amit manager account 47000</span><br><span class="line">5 tarun peon sales 15000</span><br><span class="line">6 deepak clerk sales 23000</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">geeksforgeeks.txt</span><br><span class="line"></span><br><span class="line">A    B    C</span><br><span class="line">Tarun    A12    1</span><br><span class="line">Man    B6    2</span><br><span class="line">Praveen    M42    3</span><br><span class="line"></span><br><span class="line">awk &#x27;&#123;print NR &quot; - &quot; $1 &#125;&#x27; geeksforgeeks.txt </span><br><span class="line">1 -</span><br><span class="line">2 - A</span><br><span class="line">3 - Tarun</span><br><span class="line">4 - Man</span><br><span class="line">5 - Praveen</span><br><span class="line"></span><br><span class="line">awk &#x27;NF &gt; 0 &#123;print NR &quot; - &quot; $1 &#125;&#x27; geeksforgeeks.txt </span><br><span class="line">2 - A</span><br><span class="line">3 - Tarun</span><br><span class="line">4 - Man</span><br><span class="line">5 - Praveen</span><br><span class="line"></span><br><span class="line">awk &#x27;NF &lt;= 0 &#123;print NR&#125;&#x27;  geeksforgeeks.txt</span><br><span class="line">1</span><br><span class="line"></span><br><span class="line">To find the length of the longest line present in the file:</span><br><span class="line">awk &#x27;BEGIN&#123;max=0&#125; &#123; if (length($0) &gt; max) max = length($0) &#125; END &#123; print max &#125;&#x27; geeksforgeeks.txt</span><br><span class="line">19</span><br><span class="line"></span><br><span class="line">Printing lines with more than 15 characters:  </span><br><span class="line">awk &#x27;length($0) &gt; 15&#x27; geeksforgeeks.txt</span><br><span class="line">Tarun    A12    1</span><br><span class="line">Praveen    M42    3</span><br><span class="line"></span><br><span class="line">To find/check for any string in any specific column:  </span><br><span class="line">awk &#x27;&#123; if($2 == &quot;B6&quot;) print $0;&#125;&#x27; geeksforgeeks.txt</span><br><span class="line">Man    B6    2</span><br><span class="line"></span><br><span class="line">To print the squares of first numbers from 1 to n say 6:  </span><br><span class="line">awk &#x27;BEGIN &#123; for(i=1;i&lt;=6;i++) print &quot;square of&quot;, i, &quot;is&quot;,i*i&#125;&#x27;</span><br><span class="line">square of 1 is 1</span><br><span class="line">square of 2 is 4</span><br><span class="line">square of 3 is 9</span><br><span class="line">square of 4 is 16</span><br><span class="line">square of 5 is 25</span><br><span class="line">square of 6 is 36</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">information.txt</span><br><span class="line">fristName       lastName        age     city       ID</span><br><span class="line"></span><br><span class="line">Thomas          Shelby          30      Rio        400</span><br><span class="line">Omega           Night           45      Ontario    600</span><br><span class="line">Wood            Tinker          54      Lisbon     N/A</span><br><span class="line">Giorgos         Georgiou        35      London     300</span><br><span class="line">Timmy           Turner          32      Berlin     N/A</span><br><span class="line"></span><br><span class="line">awk &#x27;&#123;print $1&#125;&#x27; information.txt | head -3</span><br><span class="line">fristName</span><br><span class="line"></span><br><span class="line">awk &#x27;!/0$/&#x27; information.txt  </span><br><span class="line">fristName       lastName        age     city       ID</span><br><span class="line"></span><br><span class="line">Wood            Tinker          54      Lisbon     N/A</span><br><span class="line">Timmy           Turner          32      Berlin     N/A</span><br><span class="line"></span><br><span class="line">awk &#x27; /io/ &#123;print $0&#125;&#x27; information.txt </span><br><span class="line">Thomas          Shelby          30      Rio        400</span><br><span class="line">Omega           Night           45      Ontario    600</span><br><span class="line">Giorgos         Georgiou        35      London     300</span><br><span class="line"></span><br><span class="line">awk &#x27;/N\/A$/&#x27; information.txt </span><br><span class="line">Wood            Tinker          54      Lisbon     N/A</span><br><span class="line">Timmy           Turner          32      Berlin     N/A</span><br><span class="line"></span><br><span class="line">awk &#x27;NF &gt; 0 &amp;&amp; $3 &lt; 40  &#123; print $0 &#125;&#x27; information.txt</span><br><span class="line">Thomas          Shelby          30      Rio        400</span><br><span class="line">Giorgos         Georgiou        35      London     300</span><br><span class="line">Timmy           Turner          32      Berlin     N/A</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">marks.txt</span><br><span class="line">1)    Amit     Physics    80</span><br><span class="line">2)    Rahul    Maths      90</span><br><span class="line">3)    Shyam    Biology    87</span><br><span class="line">4)    Kedar    English    85</span><br><span class="line">5)    Hari     History    89</span><br><span class="line"></span><br><span class="line">awk &#x27;/a/&#123;++cnt&#125; END &#123;print &quot;Count = &quot; &quot;&quot; cnt&#125;&#x27; marks.txt</span><br><span class="line">Count = 4</span><br><span class="line">AWK 在使用一个变量前不需要特意地声明这个变量</span><br><span class="line"></span><br><span class="line">awk &#x27;length($0) &gt; 18&#x27; marks.txt</span><br></pre></td></tr></table></figure>

<h2 id="sort"><a href="#sort" class="headerlink" title="sort"></a>sort</h2><p>sort命令是在Linux里非常有用，它将文件进行排序，并将排序结果标准输出。</p>
<ol>
<li>基本语法</li>
</ol>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="title">sort</span><span class="params">(选项)</span><span class="params">(参数)</span></span></span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>选项</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>-n</td>
<td>依照数值的大小排序</td>
</tr>
<tr>
<td>-r</td>
<td>以相反的顺序来排序</td>
</tr>
<tr>
<td>-t</td>
<td>设置排序时所用的分隔字符</td>
</tr>
<tr>
<td>-k</td>
<td>指定需要排序的列</td>
</tr>
</tbody></table>
<p>参数：指定待排序的文件列表</p>
<ol start="2">
<li>案例实操</li>
</ol>
<p>（0）数据准备</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ vim sort.sh </span><br><span class="line">bb:40:5.4</span><br><span class="line">bd:20:4.2</span><br><span class="line">xz:50:2.3</span><br><span class="line">cls:10:3.5</span><br><span class="line">ss:30:1.6</span><br></pre></td></tr></table></figure>

<p>（1）按照“：”分割后的第三列倒序排序。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 datas]$ sort -t : -nrk 3 sort.sh </span><br><span class="line">bb:40:5.4</span><br><span class="line">bd:20:4.2</span><br><span class="line">cls:10:3.5</span><br><span class="line">xz:50:2.3</span><br><span class="line">ss:30:1.6</span><br></pre></td></tr></table></figure>

<h2 id="wc"><a href="#wc" class="headerlink" title="wc"></a>wc</h2><p>wc命令用来计算数字。利用wc指令我们可以计算文件的<strong>Byte数、字数或是列数</strong>，若不指定文件名称，或是所给予的文件名为“-”，则wc指令会从标准输入设备读取数据。</p>
<ol>
<li>基本用法</li>
</ol>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line">wc <span class="selector-attr">[选项参数]</span> filename</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>参数说明</li>
</ol>
<table>
<thead>
<tr>
<th>选项参数</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-l</td>
<td>统计文件行数</td>
</tr>
<tr>
<td>-w</td>
<td>统计文件的单词数</td>
</tr>
<tr>
<td>-m</td>
<td>统计文件的字符数</td>
</tr>
<tr>
<td>-c</td>
<td>统计文件的字节数</td>
</tr>
</tbody></table>
<ol start="3">
<li>案例实操</li>
</ol>
<p> 统计redis_6379.conf文件的行数、单词数、字节数。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@0725pc myredis]$ ll | grep redis_6379.conf</span><br><span class="line">-rw-r--r--. 1 root root      188 10月  9 15:33 redis_6379.conf</span><br><span class="line">[root@0725pc myredis]$ cat -n redis_6379.conf </span><br><span class="line">     1	include /root/myredis/redis.conf</span><br><span class="line">     2	pidfile &quot;/var/run/redis_6379.pid&quot;</span><br><span class="line">     3	port 6379</span><br><span class="line">     4	dbfilename &quot;dump_6379.rdb&quot;</span><br><span class="line">     5	cluster-enabled yes</span><br><span class="line">     6	cluster-config-file nodes-6379.conf</span><br><span class="line">     7	cluster-node-timeout 15000</span><br><span class="line">     8	</span><br><span class="line">[root@0725pc myredis]$ wc -w redis_6379.conf </span><br><span class="line">14 redis_6379.conf</span><br><span class="line">[root@0725pc myredis]$ wc -l redis_6379.conf </span><br><span class="line">8 redis_6379.conf</span><br><span class="line">[root@0725pc myredis]$ wc -m redis_6379.conf </span><br><span class="line">188 redis_6379.conf</span><br></pre></td></tr></table></figure>

<h2 id="zcat"><a href="#zcat" class="headerlink" title="zcat"></a>zcat</h2><p>显示压缩包中文件的内容</p>
<p><strong>zcat命令</strong> 用于不真正解压缩文件，就能显示压缩包中文件的内容的场合。</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="title">zcat</span><span class="params">(选项)</span><span class="params">(参数)</span></span></span><br></pre></td></tr></table></figure>

<figure class="highlight diff"><table><tr><td class="code"><pre><span class="line"><span class="deletion">-S：指定gzip格式的压缩包的后缀。当后缀不是标准压缩包后缀时使用此选项；</span></span><br><span class="line"><span class="deletion">-c：将文件内容写到标注输出；</span></span><br><span class="line"><span class="deletion">-d：执行解压缩操作；</span></span><br><span class="line"><span class="deletion">-l：显示压缩包中文件的列表；</span></span><br><span class="line"><span class="deletion">-L：显示软件许可信息；</span></span><br><span class="line"><span class="deletion">-q：禁用警告信息；</span></span><br><span class="line"><span class="deletion">-r：在目录上执行递归操作；</span></span><br><span class="line"><span class="deletion">-t：测试压缩文件的完整性；</span></span><br><span class="line"><span class="deletion">-V：显示指令的版本信息；</span></span><br><span class="line"><span class="deletion">-l：更快的压缩速度；</span></span><br><span class="line"><span class="deletion">-9：更高的压缩比。</span></span><br></pre></td></tr></table></figure>

<h1 id="案例-1"><a href="#案例-1" class="headerlink" title="案例"></a>案例</h1><p>1.使用Linux命令查询file1中空行所在的行号</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">awk &#x27;/^$/&#123;print NR&#125;&#x27; file1</span><br></pre></td></tr></table></figure>

<p>2.有文件chengji.txt内容如下:</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">张三 40</span><br><span class="line">李四 50</span><br><span class="line">王五 60</span><br></pre></td></tr></table></figure>

<p>使用Linux命令计算第二列的和并输出</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cat chengji.txt | awk -F &quot; &quot; -v sum=0 &#x27;&#123;sum+=$2&#125; END&#123;print sum&#125;&#x27;</span><br></pre></td></tr></table></figure>

<p>3.Shell脚本里如何检查一个文件是否存在？如果不存在该如何处理？</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">if [ -f file.txt ]; then</span><br><span class="line">   echo &quot;文件存在!&quot;</span><br><span class="line">else</span><br><span class="line">   echo &quot;文件不存在!&quot;</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<p>4.用shell写一个脚本，对文本中无序的一列数字排序</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sort -n test.txt | awk &#x27;&#123;a+=$1; print $1&#125; END&#123;print &quot;SUM=&quot;a&#125;&#x27;</span><br></pre></td></tr></table></figure>

<p>5.请用shell脚本写出查找当前文件夹（/home）下所有的文本文件内容中包含有字符”shen”的文件名称</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">grep -r &quot;shen&quot; /home | cut -d &quot;:&quot; -f 1</span><br></pre></td></tr></table></figure>





<figure class="highlight dos"><table><tr><td class="code"><pre><span class="line">ARGC               命令行参数个数</span><br><span class="line">ARGV               命令行参数排列</span><br><span class="line">ENVIRON            支持队列中系统环境变量的使用</span><br><span class="line">FILENAME           awk浏览的文件名</span><br><span class="line">FNR                浏览文件的记录数</span><br><span class="line"><span class="built_in">FS</span>                 设置输入域分隔符，等价于命令行 -F选项</span><br><span class="line">NF                 浏览记录的域的个数</span><br><span class="line">NR                 已读的记录数</span><br><span class="line">OFS                输出域分隔符</span><br><span class="line">ORS                输出记录分隔符</span><br><span class="line">RS                 控制记录分隔符</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>语言</category>
      </categories>
  </entry>
  <entry>
    <title>Yarn</title>
    <url>/Yarn/</url>
    <content><![CDATA[<p>Yarn是一个资源调度平台，<strong>负责为运算程序提供服务器运算资源</strong>，相当于一个分布式的操作系统平台，而MapReduce等运算程序则相当于运行于操作系统之上的应用程序。</p>
<h1 id="Yarn基本架构"><a href="#Yarn基本架构" class="headerlink" title="Yarn基本架构"></a>Yarn基本架构</h1><p>YARN主要由ResourceManager、NodeManager、ApplicationMaster和Container等组件构成。</p>
<p><img src="/Yarn/121.png" alt="121"></p>
<h1 id="Yarn工作机制"><a href="#Yarn工作机制" class="headerlink" title="Yarn工作机制"></a>Yarn工作机制</h1><p><img src="/Yarn/122.png" alt="122"></p>
<p>工作机制详解</p>
<p>​    （1）MR程序提交到<strong>客户端所在的节点</strong>。</p>
<p>​    （2）YarnRunner向ResourceManager申请一个Application。</p>
<p>​    （3）RM将该应用程序的资源路径返回给YarnRunner。</p>
<p>​    （4）该程序将运行所需资源提交到HDFS上。</p>
<p>​    （5）程序资源提交完毕后，申请运行mrAppMaster。</p>
<p>​    （6）RM将用户的请求初始化成一个Task。</p>
<p>​    （7）其中一个NodeManager领取到Task任务。</p>
<p>​    （8）该NodeManager创建容器Container，并产生MRAppmaster。</p>
<p>​    （9）Container从HDFS上拷贝资源到本地。</p>
<p>​    （10）MRAppmaster向RM 申请运行MapTask资源。</p>
<p>​    （11）RM将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。</p>
<p>​    （12）MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序。</p>
<p>​    （13）<strong>MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。</strong></p>
<p>​    （14）ReduceTask向MapTask获取相应分区的数据。</p>
<p>​    （15）程序运行完毕后，MR会向RM申请注销自己。</p>
<h1 id="作业提交全过程"><a href="#作业提交全过程" class="headerlink" title="作业提交全过程"></a>作业提交全过程</h1><h2 id="作业提交过程之YARN"><a href="#作业提交过程之YARN" class="headerlink" title="作业提交过程之YARN"></a>作业提交过程之YARN</h2><p><img src="/Yarn/123.png" alt="123"></p>
<p>（1）作业提交</p>
<p>第1步：Client调用job.waitForCompletion方法，向整个集群提交MapReduce作业。</p>
<p>第2步：Client向RM申请一个作业id。</p>
<p>第3步：<strong>RM给Client返回该job资源的提交路径和作业id</strong>。</p>
<p>第4步：<strong>Client提交jar包、切片信息和配置文件到指定的资源提交路径</strong>。</p>
<p>第5步：Client提交完资源后，向RM申请运行MrAppMaster。</p>
<p>（2）作业初始化</p>
<p>第6步：当RM收到Client的请求后，将该job添加到<strong>容量调度器</strong>中。</p>
<p>第7步：某一个空闲的NM领取到该Job。</p>
<p>第8步：<strong>该NM创建Container，并产生MRAppmaster</strong>。</p>
<p>第9步：<strong>下载Client提交的资源到本地</strong>。</p>
<p>（3）任务分配</p>
<p>第10步：MrAppMaster向RM申请运行多个MapTask任务资源。</p>
<p>第11步：RM将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。</p>
<p>（4）任务运行</p>
<p>第12步：MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序。</p>
<p>第13步：MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。</p>
<p>第14步：ReduceTask向MapTask获取相应分区的数据。</p>
<p>第15步：程序运行完毕后，MR会向RM申请注销自己。</p>
<p>（5）进度和状态更新</p>
<p>YARN中的任务将其进度和状态(包括counter)返回给应用管理器, 客户端每秒(通过mapreduce.client.progressmonitor.pollinterval设置)向应用管理器请求进度更新, 展示给用户。</p>
<p>（6）作业完成</p>
<p>除了向应用管理器请求作业进度外, 客户端每5秒都会通过调用waitForCompletion()来检查作业是否完成。时间间隔可以通过mapreduce.client.completion.pollinterval来设置。作业完成之后, 应用管理器和Container会清理工作状态。作业的信息会被作业历史服务器存储以备之后用户核查。</p>
<h2 id="作业提交过程之MapReduce"><a href="#作业提交过程之MapReduce" class="headerlink" title="作业提交过程之MapReduce"></a>作业提交过程之MapReduce</h2><p><img src="/Yarn/124.png" alt="124"></p>
<h1 id="资源调度器"><a href="#资源调度器" class="headerlink" title="资源调度器"></a>资源调度器</h1><p>Hadoop作业调度器主要有三种：FIFO、Capacity Scheduler和Fair Scheduler。Hadoop3.1.3默认的资源调度器是Capacity Scheduler。</p>
<p>具体设置详见：yarn-default.xml文件</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>The class to use as the resource scheduler.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>FIFO：</p>
<p><img src="/Yarn/125.png" alt="125"></p>
<p><strong>FIFO调度器以集群资源独占的方式来运行作业，这样的好处是一个作业可以充分利用所有的集群资源，但是对于运行时间短，重要性高或者交互式查询类的MR作业就要等待排在序列前的作业完成才能被执行，这也就导致了如果有一个非常大的Job在运行，那么后面的作业将会被阻塞。</strong></p>
<p>Capacity Scheduler：</p>
<p><img src="/Yarn/126.png" alt="126"></p>
<p>可以将它理解成一个个的资源队列。这个资源队列是用户自己去分配的。例如因为工作所需要把整个集群分成了AB两个队列，A队列下面还可以继续分，比如将A队列再分为1和2两个子队列。那么队列的分配就可以参考下面的树形结构：</p>
<p>—A[60%]</p>
<p>|—A.1[40%]</p>
<p>|—A.2[60%]</p>
<p>—B[40%]</p>
<p>上述的树形结构可以理解为A队列占用整个资源的60%，B队列占用整个资源的40%。A队列里面又分了两个子队列，A.1占据40%，A.2占据60%，也就是说此时A.1和A.2分别占用A队列的40%和60%的资源。虽然此时已经具体分配了集群的资源，但是并不是说A提交了任务之后只能使用它被分配到的60%的资源，而B队列的40%的资源就处于空闲。只要是其它队列中的资源处于空闲状态，那么有任务提交的队列可以使用空闲队列所分配到的资源，使用的多少是依据配来决定。</p>
<p>Capacity调度器具有以下的几个特性：</p>
<ol>
<li><p><strong>层次化的队列设计</strong>，这种层次化的队列设计保证了<strong>子队列可以使用父队列设置的全部资源</strong>。这样通过层次化的管理，更容易合理分配和限制资源的使用。</p>
</li>
<li><p>容量保证，队列上都会设置一个资源的占比，这样可以<strong>保证每个队列都不会占用整个集群的资源</strong>。</p>
</li>
<li><p>安全，<strong>每个队列又严格的访问控制</strong>。用户只能向自己的队列里面提交任务，而且不能修改或者访问其他队列的任务。</p>
</li>
<li><p>弹性分配，空闲的资源可以被分配给任何队列。当多个队列出现争用的时候，则会按照比例进行平衡。</p>
</li>
<li><p>多租户租用，通过队列的容量限制，多个用户就可以共享同一个集群，同事保证每个队列分配到自己的容量，提高利用率。</p>
</li>
<li><p>操作性，Yarn支持动态修改调整容量、权限等的分配，可以在运行时直接修改。还提供给管理员界面，来显示当前的队列状况。管理员可以在运行时，添加一个队列；但是不能删除一个队列。管理员还可以在运行时暂停某个队列，这样可以保证当前的队列在执行过程中，集群不会接收其他的任务。如果一个队列被设置成了stopped，那么就不能向他或者子队列上提交任务了。</p>
</li>
<li><p>基于资源的调度，协调不同资源需求的应用程序，比如内存、CPU、磁盘等等。</p>
</li>
</ol>
<p>相关参数的配置：</p>
<p>（1）capacity：队列的资源容量（百分比）。 当系统非常繁忙时，应保证每个队列的容量得到满足，而如果每个队列应用程序较少，可将剩余资源共享给其他队列。注意，所有队列的容量之和应小于100。</p>
<p>（2）maximum-capacity：队列的资源使用上限（百分比）。由于存在资源共享，因此一个队列使用的资源量可能超过其容量，而最多使用资源量可通过该参数限制。（这也是前文提到的关于有任务运行的队列可以占用的资源的最大百分比）</p>
<p>（3）user-limit-factor：每个用户最多可使用的资源量（百分比）。比如，假设该值为30，则任何时刻，每个用户使用的资源量不能超过该队列容量的30%。</p>
<p>（4）maximum-applications ：集群或者队列中同时处于等待和运行状态的应用程序数目上限，这是一个强限制，一旦集群中应用程序数目超过该上限，后续提交的应用程序将被拒绝，默认值为 10000。所有队列的数目上限可通过参数yarn.scheduler.capacity.maximum-applications设置（可看做默认值），而单个队列可通过参数yarn.scheduler.capacity.<queue-path>.maximum- applications设置适合自己的值。</queue-path></p>
<p>（5）maximum-am-resource-percent：集群中用于运行应用程序<br>ApplicationMaster的资源比例上限，该参数通常用于限制处于活动状态的应用程序数目。该参数类型为浮点型，默认是0.1，表示10%。所有队列的ApplicationMaster资源比例上限可通过参数yarn.scheduler.capacity. maximum-am-resource-percent设置（可看做默认值），而单个队列可通过参数<br>yarn.scheduler.capacity.<queue-path>.<br>maximum-am-resource-percent设置适合自己的值。</queue-path></p>
<p>（6）state ：队列状态可以为STOPPED或者 RUNNING，如果一个队列处于STOPPED状态，用户不可以将应用程序提交到该队列或者它的子队列中，类似的，如果ROOT队列处于STOPPED 状态，用户不可以向集群中提交应用程序，但正在运行的应用程序仍可以正常运行结束，以便队列可以优雅地退出。</p>
<p>（7）acl_submit_applications：限定哪些Linux用户/用户组可向给定队列中提交应用程序。需要注意的是，该属性具有继承性，即如果一个用户可以向某个队列中提交应用程序，则它可以向它的所有子队列中提交应用程序。配置该属性时，用户之间或用户组之间用“，”分割，用户和用户组之间用空格分割，比如“user1, user2 group1,group2”。</p>
<p>（8）acl_administer_queue：为队列指定一个管理员，该管理员可控制该队列的所有应用程序，比如杀死任意一个应用程序等。同样，该属性具有继承性，如果一个用户可以向某个队列中提交应用程序，则它可以向它的所有子队列中提交应用程序。</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><span class="line">如果用idea，可以在用</span><br><span class="line">conf.set(<span class="string">&quot;mapred.job.queue.name&quot;</span>, <span class="string">&quot;a&quot;</span>);	<span class="regexp">//</span>a 是队列名称</span><br><span class="line"></span><br><span class="line">如果在Linux上运行jar包，则可以用</span><br><span class="line">hadoop jar hadoop-mapreduce-examples-<span class="number">2.7</span>.<span class="number">2</span>.jar  wordcount -D mapreduce.job.queuename=a <span class="regexp">/mapjoin /</span>output3</span><br></pre></td></tr></table></figure>

<p>Fair Scheduler：</p>
<p><img src="/Yarn/127.png" alt="127"></p>
<p><img src="/Yarn/128.png" alt="128"></p>
<p>Fair调度器是一个队列资源分配方式，在整个时间线上，所有的Job平均的获取资源。默认情况下，Fair调度器只是对内存资源做公平的调度和分配。当集群中只有一个任务在运行时，那么此任务会占用整个集群的资源。当其他的任务提交后，那些释放的资源将会被分配给新的Job，所以每个任务最终都能获取几乎一样多的资源。</p>
<p>例如有两个用户A和B，他们分别拥有一个队列。当A启动一个Job而B没有任务提交时，A会获得全部集群资源；当B启动一个Job后，A的任务会继续运行，不过队列A会慢慢释放它的一些资源，一会儿之后两个任务会各自获得一半的集群资源。如果此时B再启动第二个Job并且其它任务也还在运行时，那么它将会和B队列中的的第一个Job共享队列B的资源，也就是队列B的两个Job会分别使用集群四分之一的资源，而队列A的Job仍然会使用集群一半的资源，结果就是<strong>集群的资源最终在两个用户之间平等的共享</strong>。</p>
<p>相关参数的配置：</p>
<p>（1）yarn.scheduler.fair.allocation.file： “allocation”文件的位置，“allocation”文件是一个用来描述queue以及它们的属性的配置文件。这个文件必须为格式严格的xml文件。如果为相对路径，那么将会在classpath下查找此文件(conf目录下)。默认值为“fair-scheduler.xml”。</p>
<p>（2）yarn.scheduler.fair.user-as-default-queue：是否将与allocation有关的username作为默认的queue name，当queue name没有指定的时候。如果设置成false(且没有指定queue name) 或者没有设定，所有的jobs将共享“default”<br>queue。默认值为true。</p>
<p>（3）yarn.scheduler.fair.preemption：是否使用“preemption”(优先权，抢占)，默认为fasle，在此版本中此功能为测试性的。</p>
<p>（4）yarn.scheduler.fair.assignmultiple：是在允许在一个心跳中，发送多个container分配信息。默认值为false。</p>
<p>（5）yarn.scheduler.fair.max.assign：如果assignmultuple为true，那么在一次心跳中，最多发送分配container的个数。默认为-1，无限制。</p>
<p>（6）yarn.scheduler.fair.locality.threshold.node：一个float值，在0~1之间，表示在等待获取满足node-local条件的containers时，最多放弃不满足node-local的container的机会次数，放弃的nodes个数为集群的大小的比例。默认值为-1.0表示不放弃任何调度的机会。</p>
<p>（7）yarn.scheduler.fair.locality.threashod.rack：同上，满足rack-local。</p>
<p>（8）yarn.scheduler.fair.sizebaseweight：是否根据application的大小(Job的个数)作为权重。默认为false，如果为true，那么复杂的application将获取更多的资源。</p>
<p><strong>如果业务逻辑比较简单或者刚接触Hadoop的时候建议使用FIFO调度器；如果需要控制部分应用的优先级同时又想要充分利用集群资源的情况下，建议使用Capacity调度器；如果想要多用户或者多队列公平的共享集群资源，那么就选用Fair调度器。希望大家能够根据业务所需选择合适的调度器。</strong></p>
<h1 id="容量调度器多队列提交案例"><a href="#容量调度器多队列提交案例" class="headerlink" title="容量调度器多队列提交案例"></a>容量调度器多队列提交案例</h1><h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><p><strong>Yarn默认的容量调度器是一条单队列的调度器</strong>，在实际使用中会出现单个任务阻塞整个队列的情况。同时，随着业务的增长，公司需要分业务限制集群使用率。这就<strong>需要我们按照业务种类配置多条任务队列</strong>。</p>
<h2 id="配置多队列的容量调度器"><a href="#配置多队列的容量调度器" class="headerlink" title="配置多队列的容量调度器"></a>配置多队列的容量调度器</h2><p>默认Yarn的配置下，容量调度器只有一条Default队列。在capacity-scheduler.xml中可以配置多条队列，并降低default队列资源占比：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.queues<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>default,hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">      The queues at the this level (root is the root queue).</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.default.capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>40<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--队列目标资源百分比，所有队列相加必须等于100--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--队列最大资源百分比--&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.maximum-capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>100<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--单用户可用队列资源占比--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.user-limit-factor<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--队列状态（RUNNING或STOPPING）--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.state<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>RUNNING<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--队列允许哪些用户提交--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.acl_submit_applications<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--队列允许哪些用户管理--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.acl_administer_queue<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.maximum-application-lifetime<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>-1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>hive队列中任务的最大生命时长<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>在配置完成后，重启Yarn，就可以看到两条队列。</p>
<h2 id="向Hive队列提交任务"><a href="#向Hive队列提交任务" class="headerlink" title="向Hive队列提交任务"></a>向Hive队列提交任务</h2><p>默认的任务提交都是提交到default队列的。如果希望向其他队列提交任务，需要在Driver中声明：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WcDrvier</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line"></span><br><span class="line">        configuration.set(<span class="string">&quot;mapred.job.queue.name&quot;</span>, <span class="string">&quot;hive&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//1. 获取一个Job实例</span></span><br><span class="line">        Job job = Job.getInstance(configuration);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2. 设置类路径</span></span><br><span class="line">        job.setJarByClass(WcDrvier.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3. 设置Mapper和Reducer</span></span><br><span class="line">        job.setMapperClass(WcMapper.class);</span><br><span class="line">        job.setReducerClass(WcReducer.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4. 设置Mapper和Reducer的输出类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        job.setCombinerClass(WcReducer.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//5. 设置输入输出文件</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//6. 提交Job</span></span><br><span class="line">        <span class="keyword">boolean</span> b = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="任务的推测执行"><a href="#任务的推测执行" class="headerlink" title="任务的推测执行"></a>任务的推测执行</h1><p>1．作业完成时间取决于最慢的任务完成时间</p>
<p>一个作业由若干个Map任务和Reduce任务构成。因硬件老化、软件Bug等，某些任务可能运行非常慢。</p>
<p>思考：系统中有99%的Map任务都完成了，只有少数几个Map老是进度很慢，完不成，怎么办？</p>
<p>2．推测执行机制</p>
<p>发现拖后腿的任务，比如某个任务运行速度远慢于任务平均速度。<strong>为拖后腿任务启动一个备份任务</strong>，同时运行。谁先运行完，则采用谁的结果。</p>
<p>3．执行推测任务的前提条件</p>
<p>（1）每个Task只能有一个备份任务</p>
<p>（2）当前Job已完成的Task必须不小于0.05（5%）</p>
<p>（3）开启推测执行参数设置。mapred-site.xml文件中默认是打开的。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, then multiple instances of some map tasks may be executed in parallel.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, then multiple instances of some reduce tasks may be executed in parallel.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>4．不能启用推测执行机制情况</p>
<p>  （1）任务间存在严重的负载倾斜；</p>
<p>  （2）特殊任务，比如任务向数据库中写数据。</p>
<h1 id="查看YARN任务日志的几种方式"><a href="#查看YARN任务日志的几种方式" class="headerlink" title="查看YARN任务日志的几种方式"></a>查看YARN任务日志的几种方式</h1><h2 id="1、通过history-server"><a href="#1、通过history-server" class="headerlink" title="1、通过history server"></a>1、通过history server</h2><h2 id="2、通过yarn命令-用户要和提交任务的用户一致）"><a href="#2、通过yarn命令-用户要和提交任务的用户一致）" class="headerlink" title="2、通过yarn命令(用户要和提交任务的用户一致）"></a>2、通过yarn命令(用户要和提交任务的用户一致）</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yarn application -list -appStates ALL（这个不显示时间信息）</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yarn logs -applicationId application_1493700892407_0007</span><br></pre></td></tr></table></figure>

<h2 id="3、直接查看hdfs路径的log"><a href="#3、直接查看hdfs路径的log" class="headerlink" title="3、直接查看hdfs路径的log"></a>3、直接查看hdfs路径的log</h2><p>1）查看yarn-site.xml，确定log配置目录</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.remote-app-log-dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>/app-logs<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>2）查看日志文件信息（注意日期和时间）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hdfs@node1 root]$ hdfs dfs -ls /app-logs/hdfs/logs</span><br><span class="line">Found 1 items</span><br><span class="line">drwxrwx---   - hdfs hadoop          0 2017-05-02 04:18 /app-logs/hdfs/logs/application_1493700892407_0007</span><br></pre></td></tr></table></figure>

<p>3）查看日志详情（注意查看节点重启前的几个敏感app）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yarn logs -applicationId application_1493700892407_0007</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title>Flume</title>
    <url>/Flume/</url>
    <content><![CDATA[<h2 id="Flume定义"><a href="#Flume定义" class="headerlink" title="Flume定义"></a>Flume定义</h2><p>Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。Flume基于流式架构，灵活简单。</p>
<p><img src="/Flume/3.png" alt="3"></p>
<h2 id="Flume基础架构"><a href="#Flume基础架构" class="headerlink" title="Flume基础架构"></a>Flume基础架构</h2><p><img src="/Flume/4.png" alt="4"></p>
<h3 id="Agent"><a href="#Agent" class="headerlink" title="Agent"></a>Agent</h3><p>Agent是一个JVM进程，它以事件的形式将数据从源头送至目的。</p>
<p>Agent主要有3个部分组成，Source、Channel、Sink。</p>
<h3 id="Source"><a href="#Source" class="headerlink" title="Source"></a>Source</h3><p>Source是负责接收数据到Flume Agent的组件。Source组件可以处理各种类型、各种格式的日志数据，包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy。</p>
<h3 id="Sink"><a href="#Sink" class="headerlink" title="Sink"></a>Sink</h3><p><strong>Sink不断地轮询Channel中的事件且批量地移除它们</strong>，并将这些事件批量写入到存储或索引系统、或者被发送到另一个Flume Agent。</p>
<p>Sink组件目的地包括<strong>hdfs</strong>、logger、avro、thrift、ipc、file、<strong>HBase</strong>、solr、自定义。</p>
<h3 id="Channel"><a href="#Channel" class="headerlink" title="Channel"></a>Channel</h3><p>channels是一个Agent上存储events的仓库，Source向其中添加events，而Sink从中取走移除events。</p>
<p>Channel是位于Source和Sink之间的缓冲区。因此，<strong>Channel允许Source和Sink运作在不同的速率上</strong>。<strong>Channel是线程安全的，可以同时处理几个Source的写入操作和几个Sink的读取操作。</strong></p>
<p>Flume自带两种Channel：Memory Channel和File Channel。</p>
<p>Memory Channel是内存中的队列。Memory Channel在不需要关心数据丢失的情景下适用。如果需要关心数据丢失，那么Memory Channel就不应该使用，因为程序死亡、机器宕机或者重启都会导致数据丢失。</p>
<p>File Channel将所有事件写到磁盘。因此在程序关闭或机器宕机的情况下不会丢失数据。</p>
<p>kafka channel的parseAsFlumeEvent解析event</p>
<p>Kafka Channel的parseAsFlumeEvent的默认值为true，即会为对source来的数据进行解析，解析完会对数据前加前缀，前缀为topic名，因此这种情况，下游会需要做额外的截取工作，所以，当不需要前缀名时，将该属性设置为false.</p>
<p>实际中使用kafkachannel, 使用header中时间戳滚动, 会有问题, 建议值使用本地时间来滚动, 并且parseAsFlumeEvent设置为false</p>
<h3 id="Event"><a href="#Event" class="headerlink" title="Event"></a>Event</h3><p>传输单元，Flume数据传输的基本单元，以Event的形式将数据从源头送至目的地。<strong>Event由Header和Body两部分组成</strong>，Header用来存放该event的一些属性，为<strong>K-V结构</strong>，Body用来存放该条数据，形式为<strong>字节数组</strong>。</p>
<h2 id="Flume入门"><a href="#Flume入门" class="headerlink" title="Flume入门"></a>Flume入门</h2><h3 id="Flume安装部署"><a href="#Flume安装部署" class="headerlink" title="Flume安装部署"></a>Flume安装部署</h3><p>Flume官网地址：<a href="http://flume.apache.org/">http://flume.apache.org/</a></p>
<p>文档查看地址：<a href="http://flume.apache.org/FlumeUserGuide.html">http://flume.apache.org/FlumeUserGuide.html</a></p>
<p>下载地址：<a href="http://archive.apache.org/dist/flume/">http://archive.apache.org/dist/flume/</a></p>
<p>将apache-flume-1.9.0-bin.tar.gz上传到linux的/opt/software目录下</p>
<p>解压apache-flume-1.9.0-bin.tar.gz到/opt/module/目录下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -zxf /opt/software/apache-flume-1.9.0-bin.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>

<p>修改apache-flume-1.9.0-bin的名称为flume</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mv /opt/module/apache-flume-1.9.0-bin /opt/module/flume</span><br></pre></td></tr></table></figure>

<p>将lib文件夹下的guava-11.0.2.jar删除以兼容Hadoop 3.1.3</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">rm /opt/module/flume/lib/guava-11.0.2.jar</span><br></pre></td></tr></table></figure>

<h3 id="监控端口数据官方案例"><a href="#监控端口数据官方案例" class="headerlink" title="监控端口数据官方案例"></a>监控端口数据官方案例</h3><p>使用Flume监听一个端口，收集该端口数据，并打印到控制台。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">netcat</span><br><span class="line"></span><br><span class="line">client/server模式</span><br><span class="line"><span class="meta">#</span><span class="bash"> linux1作为服务器监听4444端口</span></span><br><span class="line">[vincent@linux1 ~]$ nc -l 4444</span><br><span class="line"><span class="meta">#</span><span class="bash"> linux2作为客户端向服务器4444端口发送数据</span></span><br><span class="line">[vincent@linux2 ~]$ nc -linux1 4444</span><br><span class="line"></span><br><span class="line">data transfer模式</span><br><span class="line"><span class="meta">#</span><span class="bash"> filename.in的数据写入filename.out</span></span><br><span class="line">[vincent@linux1 ~]$ nc -l 1234 &gt; filename.out</span><br><span class="line">[vincent@linux1 ~]$ nc host.example 1234 &lt; filename.in</span><br></pre></td></tr></table></figure>

<p>（1）安装netcat工具</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 software]$ sudo yum install -y nc</span><br></pre></td></tr></table></figure>

<p>（2）判断6666端口是否被占用</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume-telnet]$ sudo netstat -tunlp | grep 6666</span><br></pre></td></tr></table></figure>

<p>（3）创建Flume Agent配置文件flume-netcat-logger.conf</p>
<p>在flume目录下创建job文件夹并进入job文件夹。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ mkdir job</span><br><span class="line"></span><br><span class="line">[vincent@linux1 flume]$ cd job/</span><br></pre></td></tr></table></figure>

<p>在job文件夹下创建Flume Agent配置文件netcat-logger.conf。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 job]$ vim netcat-logger.conf</span><br></pre></td></tr></table></figure>

<p>在flume-netcat-logger.conf文件中添加如下内容。</p>
<p>添加内容如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinks = k1</span><br><span class="line"></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = 0.0.0.0</span><br><span class="line">a1.sources.r1.port = 6666</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line"></span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line"></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line"><span class="meta">#</span><span class="bash"> the maximum number of events stored <span class="keyword">in</span> the channel</span></span><br><span class="line">a1.channels.c1.capacity = 10000</span><br><span class="line"><span class="meta">#</span><span class="bash"> the maximum number of events the channel will take from</span> </span><br><span class="line"><span class="meta">#</span><span class="bash"> a <span class="built_in">source</span> or give to a sink per transaction</span> </span><br><span class="line">a1.channels.c1.transactionCapacity = 10000</span><br><span class="line">a1.channels.c1.byteCapacityBufferPercentage = 20</span><br><span class="line">a1.channels.c1.byteCapacity = 800000</span><br></pre></td></tr></table></figure>

<p>注：配置文件来源于官方手册<a href="http://flume.apache.org/FlumeUserGuide.html">http://flume.apache.org/FlumeUserGuide.html</a></p>
<p>4）修改log4j.properties，将数据输出到控制台</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim /opt/module/hive/conf/log4j.properties</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">flume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<p>5）先开启flume监听端口</p>
<p>第一种写法：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/netcat-logger.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<p>第二种写法：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ bin/flume-ng agent -c conf/ -n a1 -f job/netcat-logger.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<p>参数说明：</p>
<p>​    –conf/-c：表示配置文件存储在conf/目录</p>
<p>​    –name/-n：表示给agent起名为a1</p>
<p>​    –conf-file/-f：flume本次启动读取的配置文件是在job文件夹下的flume-telnet.conf文件。</p>
<p>​    -Dflume.root.logger=INFO,console ：-D表示flume运行时动态修改flume.root.logger参数属性值，并将控制台日志打印级别设置为INFO级别。日志级别包括:log、info、warn、error。</p>
<p>（6）使用netcat工具向本机的6666端口发送内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 ~]$ nc localhost 6666</span><br></pre></td></tr></table></figure>

<p>（7）在Flume监听页面观察接收数据情况</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="number">2020</span>-<span class="number">06</span>-<span class="number">16</span> <span class="number">10</span>:<span class="number">18</span>:<span class="number">22</span>,<span class="number">393</span> (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.<span class="built_in">process</span>(LoggerSink.java:<span class="number">95</span>)] Event: &#123; headers:&#123;&#125; <span class="selector-tag">body</span>: <span class="number">61</span> <span class="number">61</span> <span class="number">61</span> <span class="number">61</span>                                     aaaa &#125;</span><br><span class="line"><span class="number">2020</span>-<span class="number">06</span>-<span class="number">16</span> <span class="number">10</span>:<span class="number">18</span>:<span class="number">32</span>,<span class="number">305</span> (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.<span class="built_in">process</span>(LoggerSink.java:<span class="number">95</span>)] Event: &#123; headers:&#123;&#125; <span class="selector-tag">body</span>: <span class="number">73</span> <span class="number">73</span> <span class="number">73</span> <span class="number">73</span>                                     ssss &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="实时监控单个追加文件"><a href="#实时监控单个追加文件" class="headerlink" title="实时监控单个追加文件"></a>实时监控单个追加文件</h3><p>实时监控Hive日志，并上传到HDFS中</p>
<p><img src="/Flume/5.png" alt="5"></p>
<p>（1）Flume要想将数据输出到HDFS，依赖Hadoop相关jar包</p>
<p>检查/etc/profile.d/my_env.sh文件，确认Hadoop和Java环境变量配置正确</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">JAVA_HOME</span></span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_212</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"> </span></span><br><span class="line"><span class="meta">#</span><span class="bash">HADOOP_HOME</span></span><br><span class="line">export HADOOP_HOME=/opt/module/hadoop-3.1.3</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure>

<p>（2）创建exec-hdfs.conf文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 job]$ vim exec-hdfs.conf</span><br></pre></td></tr></table></figure>

<p>注：要想读取Linux系统中的文件，就得按照Linux命令的规则执行命令。由于Hive日志在Linux系统中所以读取文件的类型选择：exec即execute执行的意思，表示执行Linux命令来读取文件。</p>
<p>添加如下内容:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a2.sources = r2</span><br><span class="line">a2.sinks = k2</span><br><span class="line">a2.channels = c2</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a2.sources.r2.type = exec</span><br><span class="line"><span class="meta">#</span><span class="bash"> 不能实现断点续传，可能会在flume挂掉的时候丢失数据</span></span><br><span class="line">a2.sources.r2.command = tail -F /opt/module/hive/logs/hive.log</span><br><span class="line"><span class="meta">#</span><span class="bash"> 下面一行配置可以使上面一行执行更复杂的命令，如增加管道符运算等</span></span><br><span class="line">a2.sources.r2.shell = /bin/bash -c</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a2.sinks.k2.type = hdfs</span><br><span class="line">a2.sinks.k2.hdfs.path = hdfs://linux1:9820/flume/%Y%m%d/%H</span><br><span class="line"><span class="meta">#</span><span class="bash">上传文件的前缀</span></span><br><span class="line">a2.sinks.k2.hdfs.filePrefix = logs-</span><br><span class="line"><span class="meta">#</span><span class="bash">是否对时间戳取整</span></span><br><span class="line">a2.sinks.k2.hdfs.round = true</span><br><span class="line"><span class="meta">#</span><span class="bash">多少时间单位创建一个新的文件夹</span></span><br><span class="line">a2.sinks.k2.hdfs.roundValue = 2</span><br><span class="line"><span class="meta">#</span><span class="bash">重新定义时间单位</span></span><br><span class="line">a2.sinks.k2.hdfs.roundUnit = hour</span><br><span class="line"><span class="meta">#</span><span class="bash">是否使用本地时间戳</span></span><br><span class="line">a2.sinks.k2.hdfs.useLocalTimeStamp = true</span><br><span class="line"><span class="meta">#</span><span class="bash">积攒多少个Event才flush到HDFS一次，小于等于transactionCapacity</span></span><br><span class="line">a2.sinks.k2.hdfs.batchSize = 100</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">设置文件类型，可支持压缩</span></span><br><span class="line">a2.sinks.k2.hdfs.fileType = DataStream</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">多久生成一个新的目录（60秒钟滚动一次）</span></span><br><span class="line">a2.sinks.k2.hdfs.rollInterval = 60</span><br><span class="line"><span class="meta">#</span><span class="bash">设置每个目录的滚动大小（128M滚动一次）</span></span><br><span class="line">a2.sinks.k2.hdfs.rollSize = 134217700</span><br><span class="line"><span class="meta">#</span><span class="bash">目录的滚动与Event数量无关(多少个event写到一个文件)</span></span><br><span class="line">a2.sinks.k2.hdfs.rollCount = 0</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a2.channels.c2.type = memory</span><br><span class="line">a2.channels.c2.capacity = 1000</span><br><span class="line">a2.channels.c2.transactionCapacity = 100</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a2.sources.r2.channels = c2</span><br><span class="line">a2.sinks.k2.channel = c2</span><br></pre></td></tr></table></figure>

<p>注意：</p>
<p>对于所有与时间相关的转义序列，Event Header中必须存在以 “timestamp”的key（除非hdfs.useLocalTimeStamp设置为true，此方法会使用TimestampInterceptor自动添加timestamp）。</p>
<p>a3.sinks.k3.hdfs.useLocalTimeStamp = true</p>
<p>（3）启动HDFS并等待安全模式退出</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ mycluster start[vincent@linux1 flume]$ hdfs dfsadmin -safemode wait</span><br></pre></td></tr></table></figure>

<p>（4）运行Flume</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/exec-hdfs.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<p>（5）开启Hadoop和Hive并操作Hive产生日志</p>
<figure class="highlight elixir"><table><tr><td class="code"><pre><span class="line">[vincent<span class="variable">@linux1</span> hadoop<span class="number">-3.1</span>.<span class="number">3</span>]<span class="variable">$ </span>sbin/start-dfs.sh[vincent<span class="variable">@linux1</span> hadoop<span class="number">-3.1</span>.<span class="number">3</span>]<span class="variable">$ </span>sbin/start-yarn.sh[vincent<span class="variable">@linux1</span> hive]<span class="variable">$ </span>bin/hivehive (default)&gt;</span><br></pre></td></tr></table></figure>

<p>（6）在HDFS上查看文件</p>
<p>linux1:9820</p>
<h3 id="实时监控目录下多个新文件"><a href="#实时监控目录下多个新文件" class="headerlink" title="实时监控目录下多个新文件"></a>实时监控目录下多个新文件</h3><p>使用Flume监听整个目录的文件，并上传至HDFS。</p>
<p><img src="/Flume/6.png" alt="6"></p>
<p>（1）创建配置文件spooldir-hdfs.conf</p>
<p>创建一个文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 job]$ vim spooldir-hdfs.conf</span><br></pre></td></tr></table></figure>

<p>添加如下内容：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a3.sources = r3</span><br><span class="line">a3.sinks = k3</span><br><span class="line">a3.channels = c3</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 监控一个目录下的多个文件</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 断点续传</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 延迟高，不能进行实时采集</span></span><br><span class="line">a3.sources.r3.type = spooldir</span><br><span class="line">a3.sources.r3.spoolDir = /opt/module/flume/upload</span><br><span class="line">a3.sources.r3.fileSuffix = .COMPLETED</span><br><span class="line">a3.sources.r3.fileHeader = true</span><br><span class="line"><span class="meta">#</span><span class="bash">忽略所有以.tmp结尾的文件，不上传</span></span><br><span class="line">a3.sources.r3.ignorePattern = \\S*\\.tmp</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a3.sinks.k3.type = hdfs</span><br><span class="line">a3.sinks.k3.hdfs.path = hdfs://linux1:9820/flume/upload/%Y%m%d/%H</span><br><span class="line"><span class="meta">#</span><span class="bash">上传文件的前缀</span></span><br><span class="line">a3.sinks.k3.hdfs.filePrefix = upload-</span><br><span class="line"><span class="meta">#</span><span class="bash">是否按照时间滚动文件夹</span></span><br><span class="line">a3.sinks.k3.hdfs.round = true</span><br><span class="line"><span class="meta">#</span><span class="bash">多少时间单位创建一个新的文件夹</span></span><br><span class="line">a3.sinks.k3.hdfs.roundValue = 1</span><br><span class="line"><span class="meta">#</span><span class="bash">重新定义时间单位</span></span><br><span class="line">a3.sinks.k3.hdfs.roundUnit = hour</span><br><span class="line"><span class="meta">#</span><span class="bash">是否使用本地时间戳</span></span><br><span class="line">a3.sinks.k3.hdfs.useLocalTimeStamp = true</span><br><span class="line"><span class="meta">#</span><span class="bash">积攒多少个Event才flush到HDFS一次</span></span><br><span class="line">a3.sinks.k3.hdfs.batchSize = 100</span><br><span class="line"><span class="meta">#</span><span class="bash">设置文件类型，可支持压缩</span></span><br><span class="line">a3.sinks.k3.hdfs.fileType = DataStream</span><br><span class="line"><span class="meta">#</span><span class="bash">多久生成一个新的文件</span></span><br><span class="line">a3.sinks.k3.hdfs.rollInterval = 60</span><br><span class="line"><span class="meta">#</span><span class="bash">设置每个文件的滚动大小大概是128M</span></span><br><span class="line">a3.sinks.k3.hdfs.rollSize = 134217700</span><br><span class="line"><span class="meta">#</span><span class="bash">文件的滚动与Event数量无关</span></span><br><span class="line">a3.sinks.k3.hdfs.rollCount = 0</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a3.channels.c3.type = memory</span><br><span class="line">a3.channels.c3.capacity = 1000</span><br><span class="line">a3.channels.c3.transactionCapacity = 100</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a3.sources.r3.channels = c3</span><br><span class="line">a3.sinks.k3.channel = c3</span><br></pre></td></tr></table></figure>

<p>（2）启动监控文件夹命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/spooldir-hdfs.conf</span><br></pre></td></tr></table></figure>

<p>说明：在使用Spooling Directory Source时，不要在监控目录中创建并持续修改文件；上传完成的文件会以.COMPLETED结尾；被监控文件夹每500毫秒扫描一次文件变动。</p>
<p>（3）向upload文件夹中添加文件</p>
<p>在/opt/module/flume目录下创建upload文件夹</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ mkdir upload</span><br></pre></td></tr></table></figure>

<p>向upload文件夹中添加文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 upload]$ touch vincent.txt</span><br><span class="line">[vincent@linux1 upload]$ touch vincent.tmp</span><br><span class="line">[vincent@linux1 upload]$ touch vincent.log</span><br></pre></td></tr></table></figure>

<p>（4）查看HDFS上的数据</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><span class="line"><span class="number">2020</span>-<span class="number">06</span>-<span class="number">17</span> <span class="number">11</span>:<span class="number">05</span>:<span class="number">23</span>,<span class="number">957</span> (hdfs-k3-roll-timer-<span class="number">0</span>) [INFO - org.apache.flume.sink.hdfs.HDFSEventSink<span class="variable">$1</span>.run(HDFSEventSink.java:<span class="number">393</span>)] Writer callback called.</span><br><span class="line"><span class="number">2020</span>-<span class="number">06</span>-<span class="number">17</span> <span class="number">11</span>:<span class="number">05</span>:<span class="number">23</span>,<span class="number">958</span> (hdfs-k3-roll-timer-<span class="number">0</span>) [INFO - org.apache.flume.sink.hdfs.BucketWriter.doClose(BucketWriter.java:<span class="number">438</span>)] Closing hdfs:<span class="regexp">//</span>linux1:<span class="number">9820</span><span class="regexp">/flume/u</span>pload<span class="regexp">/20200617/</span><span class="number">11</span>/upload-.<span class="number">1592362973087</span>.tmp</span><br><span class="line"><span class="number">2020</span>-<span class="number">06</span>-<span class="number">17</span> <span class="number">11</span>:<span class="number">05</span>:<span class="number">23</span>,<span class="number">967</span> (hdfs-k3-call-runner-<span class="number">5</span>) [INFO - org.apache.flume.sink.hdfs.BucketWriter<span class="variable">$7</span>.call(BucketWriter.java:<span class="number">681</span>)] Renaming hdfs:<span class="regexp">//</span>linux1:<span class="number">9820</span><span class="regexp">/flume/u</span>pload<span class="regexp">/20200617/</span><span class="number">11</span><span class="regexp">/upload-.1592362973087.tmp to hdfs:/</span><span class="regexp">/linux1:9820/</span>flume<span class="regexp">/upload/</span><span class="number">20200617</span><span class="regexp">/11/u</span>pload-.<span class="number">1592362973087</span></span><br><span class="line"><span class="number">2020</span>-<span class="number">06</span>-<span class="number">17</span> <span class="number">11</span>:<span class="number">05</span>:<span class="number">27</span>,<span class="number">909</span> (pool-<span class="number">5</span>-thread-<span class="number">1</span>) [INFO - org.apache.flume.client.avro.ReliableSpoolingFileEventReader.readEvents(ReliableSpoolingFileEventReader.java:<span class="number">384</span>)] Last read took us just up to a file boundary. Rolling to the <span class="keyword">next</span> file, <span class="keyword">if</span> there is one.</span><br><span class="line"><span class="number">2020</span>-<span class="number">06</span>-<span class="number">17</span> <span class="number">11</span>:<span class="number">05</span>:<span class="number">27</span>,<span class="number">909</span> (pool-<span class="number">5</span>-thread-<span class="number">1</span>) [INFO - org.apache.flume.client.avro.ReliableSpoolingFileEventReader.rollCurrentFile(ReliableSpoolingFileEventReader.java:<span class="number">497</span>)] Preparing to move file <span class="regexp">/opt/m</span>odule<span class="regexp">/flume/u</span>pload<span class="regexp">/d.txt to /</span>opt<span class="regexp">/module/</span>flume<span class="regexp">/upload/</span>d.txt.COMPLETED</span><br><span class="line"><span class="number">2020</span>-<span class="number">06</span>-<span class="number">17</span> <span class="number">11</span>:<span class="number">05</span>:<span class="number">28</span>,<span class="number">982</span> (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.hdfs.HDFSDataStream.configure(HDFSDataStream.java:<span class="number">57</span>)] Serializer = TEXT, UseRawLocalFileSystem = false</span><br><span class="line"><span class="number">2020</span>-<span class="number">06</span>-<span class="number">17</span> <span class="number">11</span>:<span class="number">05</span>:<span class="number">28</span>,<span class="number">992</span> (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.hdfs.BucketWriter.open(BucketWriter.java:<span class="number">246</span>)] Creating hdfs:<span class="regexp">//</span>linux1:<span class="number">9820</span><span class="regexp">/flume/u</span>pload<span class="regexp">/20200617/</span><span class="number">11</span>/upload-.<span class="number">1592363128983</span>.tmp</span><br><span class="line"><span class="number">2020</span>-<span class="number">06</span>-<span class="number">17</span> <span class="number">11</span>:<span class="number">05</span>:<span class="number">32</span>,<span class="number">011</span> (Thread-<span class="number">20</span>) [INFO - org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:<span class="number">239</span>)] SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false</span><br></pre></td></tr></table></figure>

<h3 id="实时监控目录下的多个追加文件"><a href="#实时监控目录下的多个追加文件" class="headerlink" title="实时监控目录下的多个追加文件"></a>实时监控目录下的多个追加文件</h3><p><strong>Exec source适用于监控一个实时追加的文件，不能实现断点续传；Spooldir Source适合用于同步新文件，但不适合对实时追加日志的文件进行监听并同步；而Taildir Source适合用于监听多个实时追加的文件，并且能够实现断点续传。</strong></p>
<p>使用Flume监听整个目录的实时追加文件，并上传至HDFS。</p>
<p><img src="/Flume/7.png" alt="7"></p>
<p>（1）创建配置文件taildir-hdfs.conf</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 job]$ vim taildir-hdfs.conf</span><br></pre></td></tr></table></figure>

<p>添加如下内容：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a3.sources = r3</span><br><span class="line">a3.sinks = k3</span><br><span class="line">a3.channels = c3</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a3.sources.r3.type = TAILDIR</span><br><span class="line">a3.sources.r3.positionFile = /opt/module/flume/tail_dir.json</span><br><span class="line">a3.sources.r3.filegroups = f1 f2</span><br><span class="line">a3.sources.r3.filegroups.f1 = /opt/module/flume/files/.*file.*</span><br><span class="line">a3.sources.r3.filegroups.f2 = /opt/module/flume/files2/.*</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a3.sinks.k3.type = hdfs</span><br><span class="line">a3.sinks.k3.hdfs.path = hdfs://linux1:9820/flume/upload2/%Y%m%d/%H</span><br><span class="line"><span class="meta">#</span><span class="bash">上传文件的前缀</span></span><br><span class="line">a3.sinks.k3.hdfs.filePrefix = upload-</span><br><span class="line"><span class="meta">#</span><span class="bash">是否按照时间滚动文件夹</span></span><br><span class="line">a3.sinks.k3.hdfs.round = true</span><br><span class="line"><span class="meta">#</span><span class="bash">多少时间单位创建一个新的文件夹</span></span><br><span class="line">a3.sinks.k3.hdfs.roundValue = 1</span><br><span class="line"><span class="meta">#</span><span class="bash">重新定义时间单位</span></span><br><span class="line">a3.sinks.k3.hdfs.roundUnit = hour</span><br><span class="line"><span class="meta">#</span><span class="bash">是否使用本地时间戳</span></span><br><span class="line">a3.sinks.k3.hdfs.useLocalTimeStamp = true</span><br><span class="line"><span class="meta">#</span><span class="bash">积攒多少个Event才flush到HDFS一次</span></span><br><span class="line">a3.sinks.k3.hdfs.batchSize = 100</span><br><span class="line"><span class="meta">#</span><span class="bash">设置文件类型，可支持压缩</span></span><br><span class="line">a3.sinks.k3.hdfs.fileType = DataStream</span><br><span class="line"><span class="meta">#</span><span class="bash">多久生成一个新的文件</span></span><br><span class="line">a3.sinks.k3.hdfs.rollInterval = 60</span><br><span class="line"><span class="meta">#</span><span class="bash">设置每个文件的滚动大小大概是128M</span></span><br><span class="line">a3.sinks.k3.hdfs.rollSize = 134217700</span><br><span class="line"><span class="meta">#</span><span class="bash">文件的滚动与Event数量无关</span></span><br><span class="line">a3.sinks.k3.hdfs.rollCount = 0</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a3.channels.c3.type = memory</span><br><span class="line">a3.channels.c3.capacity = 1000</span><br><span class="line">a3.channels.c3.transactionCapacity = 100</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a3.sources.r3.channels = c3</span><br><span class="line">a3.sinks.k3.channel = c3</span><br></pre></td></tr></table></figure>

<p>（2）创建目录files和files2</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ mkdir files files2</span><br></pre></td></tr></table></figure>

<p>（2）启动监控文件夹命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file jobs/taildir-hdfs.conf</span><br></pre></td></tr></table></figure>

<p>（3）向files文件夹中追加内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 files]$ echo hello &gt;&gt; file1.txt</span><br><span class="line">[vincent@linux1 files]$ echo world &gt;&gt; file2.txt</span><br></pre></td></tr></table></figure>

<p>（4）查看HDFS上的数据</p>
<p>Taildir说明：</p>
<p>Taildir Source维护了一个json格式的position File，其会定期的往position File中更新每个文件读取到的最新的位置，因此能够实现断点续传。Position File的格式如下：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;<span class="attr">&quot;inode&quot;</span>:<span class="number">2496272</span>,<span class="attr">&quot;pos&quot;</span>:<span class="number">12</span>,<span class="attr">&quot;file&quot;</span>:<span class="string">&quot;/opt/module/flume/files/file1.txt&quot;</span>&#125;&#123;<span class="attr">&quot;inode&quot;</span>:<span class="number">2496275</span>,<span class="attr">&quot;pos&quot;</span>:<span class="number">12</span>,<span class="attr">&quot;file&quot;</span>:<span class="string">&quot;/opt/module/flume/files/file2.txt&quot;</span>&#125;</span><br></pre></td></tr></table></figure>

<p>注：Linux中储存文件元数据的区域就叫做inode，每个inode都有一个号码，操作系统用inode号码来识别不同的文件，Unix/Linux系统内部不使用文件名，而使用inode号码来识别文件。</p>
<h2 id="Flume事务"><a href="#Flume事务" class="headerlink" title="Flume事务"></a>Flume事务</h2><p><img src="/Flume/142.png" alt="142"></p>
<h2 id="Flume-Agent内部原理"><a href="#Flume-Agent内部原理" class="headerlink" title="Flume Agent内部原理"></a>Flume Agent内部原理</h2><p><img src="/Flume/143.png" alt="143"></p>
<h3 id="1）ChannelSelector"><a href="#1）ChannelSelector" class="headerlink" title="1）ChannelSelector"></a>1）ChannelSelector</h3><p>ChannelSelector的作用就是选出Event将要被发往哪个Channel。其共有两种类型，分别是<strong>Replicating（复制）</strong>和<strong>Multiplexing（多路复用）</strong>。</p>
<p><strong>ReplicatingSelector会将同一个Event发往所有的Channel，Multiplexing会根据相应的原则，将不同的Event发往不同的Channel。</strong></p>
<h3 id="2）SinkProcessor"><a href="#2）SinkProcessor" class="headerlink" title="2）SinkProcessor"></a>2）SinkProcessor</h3><p>SinkProcessor共有三种类型，分别是<strong>DefaultSinkProcessor、LoadBalancingSinkProcessor和FailoverSinkProcessor</strong></p>
<p><strong>DefaultSinkProcessor对应的是单个的Sink，LoadBalancingSinkProcessor和FailoverSinkProcessor对应的是Sink Group，LoadBalancingSinkProcessor可以实现负载均衡的功能，FailoverSinkProcessor具有错误恢复的功能。</strong></p>
<h2 id="Flume拓扑结构"><a href="#Flume拓扑结构" class="headerlink" title="Flume拓扑结构"></a>Flume拓扑结构</h2><h3 id="简单串联"><a href="#简单串联" class="headerlink" title="简单串联"></a>简单串联</h3><p><img src="/Flume/144.png" alt="144"></p>
<p>这种模式是将多个flume顺序连接起来了，从最初的source开始到最终sink传送的目的存储系统。此模式不建议桥接过多的flume数量， flume数量过多不仅会影响传输速率，而且一旦传输过程中某个节点flume宕机，会影响整个传输系统。</p>
<h3 id="复制和多路复用"><a href="#复制和多路复用" class="headerlink" title="复制和多路复用"></a>复制和多路复用</h3><p><img src="/Flume/145.png" alt="145"></p>
<p>Flume支持将事件流向一个或者多个目的地。这种模式可以将相同数据复制到多个channel中，或者将不同数据分发到不同的channel中，sink可以选择传送到不同的目的地。</p>
<h3 id="负载均衡和故障转移"><a href="#负载均衡和故障转移" class="headerlink" title="负载均衡和故障转移"></a>负载均衡和故障转移</h3><p><img src="/Flume/146.png" alt="146"></p>
<p>Flume支持使用将多个sink逻辑上分到一个sink组，sink组配合不同的SinkProcessor可以实现负载均衡和错误恢复的功能。</p>
<h3 id="聚合"><a href="#聚合" class="headerlink" title="聚合"></a>聚合</h3><p><img src="/Flume/147.png" alt="147"></p>
<p>这种模式是我们最常见的，也非常实用，日常web应用通常分布在上百个服务器，大者甚至上千个、上万个服务器。产生的日志，处理起来也非常麻烦。用flume的这种组合方式能很好的解决这一问题，每台服务器部署一个flume采集日志，传送到一个集中收集日志的flume，再由此flume上传到hdfs、hive、hbase等，进行日志分析。</p>
<h2 id="Flume企业开发案例"><a href="#Flume企业开发案例" class="headerlink" title="Flume企业开发案例"></a>Flume企业开发案例</h2><h3 id="复制和多路复用-1"><a href="#复制和多路复用-1" class="headerlink" title="复制和多路复用"></a>复制和多路复用</h3><p>使用Flume-1监控文件变动，Flume-1将变动内容传递给Flume-2，Flume-2负责存储到HDFS。同时Flume-1将变动内容传递给Flume-3，Flume-3负责输出到Local FileSystem。</p>
<p>注：先启动服务端flume（后面的）</p>
<p><img src="/Flume/8.png" alt="8"></p>
<p>（1）准备工作</p>
<p>配置f1在linux1，配置f2在linux2，配置f3在linux3</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 job]$ mkdir replicating</span><br><span class="line">[vincent@linux2 job]$ mkdir replicating</span><br><span class="line">[vincent@linux3 job]$ mkdir replicating</span><br></pre></td></tr></table></figure>

<p>（2）创建a1.conf</p>
<p>配置1个接收日志文件的source和两个channel、两个sink，分别输送给flume-hdfs和flume-dir。</p>
<p>编辑配置文件：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 replicating]$ vim taildir-avro.conf</span><br></pre></td></tr></table></figure>

<p>添加如下内容：</p>
<p>注：channel selector没配置默认使用复制模式</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line">a1.channels = c1 c2</span><br><span class="line"><span class="meta">#</span><span class="bash"> 将数据流复制给所有channel</span></span><br><span class="line">a1.sources.r1.selector.type = replicating</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = TAILDIR</span><br><span class="line">a1.sources.r1.positionFile = /opt/module/flume/t1.json</span><br><span class="line">a1.sources.r1.filegroups = f1</span><br><span class="line">a1.sources.r1.filegroups.f1 = /opt/module/hive/logs/hive.log</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> sink端的avro是一个数据发送者</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line"><span class="meta">#</span><span class="bash"> 服务端ip</span></span><br><span class="line">a1.sinks.k1.hostname = linux2</span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line"></span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line"><span class="meta">#</span><span class="bash"> 服务端ip</span></span><br><span class="line">a1.sinks.k2.hostname = linux3</span><br><span class="line">a1.sinks.k2.port = 4141</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line">a1.channels.c2.type = memory</span><br><span class="line">a1.channels.c2.capacity = 1000</span><br><span class="line">a1.channels.c2.transactionCapacity = 100</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1 c2</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c2</span><br></pre></td></tr></table></figure>

<p>（3）创建a2.conf</p>
<p>配置上级Flume输出的Source，输出是到HDFS的Sink。</p>
<p>编辑配置文件：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux2 replicating]$ vim avro-hdfs.conf</span><br></pre></td></tr></table></figure>

<p>添加如下内容：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.sinks = k1</span><br><span class="line">a2.channels = c1</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">source</span>端的avro是一个数据接收服务</span></span><br><span class="line">a2.sources.r1.type = avro</span><br><span class="line">a2.sources.r1.bind = linux2</span><br><span class="line">a2.sources.r1.port = 4141</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a2.sinks.k1.type = hdfs</span><br><span class="line">a2.sinks.k1.hdfs.path = hdfs://linux1:9820/flume2/%Y%m%d/%H</span><br><span class="line"><span class="meta">#</span><span class="bash">上传文件的前缀</span></span><br><span class="line">a2.sinks.k1.hdfs.filePrefix = flume2-</span><br><span class="line"><span class="meta">#</span><span class="bash">是否按照时间滚动文件夹</span></span><br><span class="line">a2.sinks.k1.hdfs.round = true</span><br><span class="line"><span class="meta">#</span><span class="bash">多少时间单位创建一个新的文件夹</span></span><br><span class="line">a2.sinks.k1.hdfs.roundValue = 1</span><br><span class="line"><span class="meta">#</span><span class="bash">重新定义时间单位</span></span><br><span class="line">a2.sinks.k1.hdfs.roundUnit = hour</span><br><span class="line"><span class="meta">#</span><span class="bash">是否使用本地时间戳</span></span><br><span class="line">a2.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line"><span class="meta">#</span><span class="bash">积攒多少个Event才flush到HDFS一次</span></span><br><span class="line">a2.sinks.k1.hdfs.batchSize = 100</span><br><span class="line"><span class="meta">#</span><span class="bash">设置文件类型，可支持压缩</span></span><br><span class="line">a2.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line"><span class="meta">#</span><span class="bash">多久生成一个新的文件</span></span><br><span class="line">a2.sinks.k1.hdfs.rollInterval = 600</span><br><span class="line"><span class="meta">#</span><span class="bash">设置每个文件的滚动大小大概是128M</span></span><br><span class="line">a2.sinks.k1.hdfs.rollSize = 134217700</span><br><span class="line"><span class="meta">#</span><span class="bash">文件的滚动与Event数量无关</span></span><br><span class="line">a2.sinks.k1.hdfs.rollCount = 0</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p>（4）创建a3.conf</p>
<p>配置上级Flume输出的Source，输出是到本地目录的Sink。</p>
<p>编辑配置文件：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux3 replicating]$ vim avro-file.conf</span><br></pre></td></tr></table></figure>

<p>添加如下内容：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.sinks = k1</span><br><span class="line">a3.channels = c2</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line">a3.sources.r1.bind = linux3</span><br><span class="line">a3.sources.r1.port = 4141</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a3.sinks.k1.type = file_roll</span><br><span class="line"><span class="meta">#</span><span class="bash"> 需要手动创建下面本地路径</span></span><br><span class="line">a3.sinks.k1.sink.directory = /opt/module/datas/flume3</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a3.channels.c2.type = memory</span><br><span class="line">a3.channels.c2.capacity = 1000</span><br><span class="line">a3.channels.c2.transactionCapacity = 100</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a3.sources.r1.channels = c2</span><br><span class="line">a3.sinks.k1.channel = c2</span><br></pre></td></tr></table></figure>

<p>提示：输出的本地目录必须是已经存在的目录，如果该目录不存在，并不会创建新的目录。</p>
<p>（6）启动Hadoop和Hive</p>
<p>（7）先后启动后两个flume，最后启动第一个flume（a3、a2、a1）</p>
<p>（8）检查HDFS上数据</p>
<p>（9）检查/opt/module/datas/flume3目录中数据</p>
<h3 id="负载均衡和故障转移-1"><a href="#负载均衡和故障转移-1" class="headerlink" title="负载均衡和故障转移"></a>负载均衡和故障转移</h3><p>使用Flume1监控一个端口，其sink组中的sink分别对接Flume2和Flume3，采用FailoverSinkProcessor，实现故障转移的功能。</p>
<p>注：一个channel对应一个sink group</p>
<p><img src="/Flume/9.png" alt="9"></p>
<p>（1）准备工作</p>
<p>在/opt/module/flume/job目录下创建group2文件夹</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 job]$ mkdir failover</span><br></pre></td></tr></table></figure>

<p>（2）创建a1.conf.conf</p>
<p>配置1个netcat source和1个channel、1个sink group（2个sink），分别输送给a2.conf和a3.conf。</p>
<p>编辑配置文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 failover]$ vim a1.conf</span><br></pre></td></tr></table></figure>

<p>添加如下内容:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line"></span><br><span class="line">a1.sinkgroups = g1</span><br><span class="line">a1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line">a1.sinkgroups.g1.processor.type = failover</span><br><span class="line"><span class="meta">#</span><span class="bash"> 优先级高的工作，断了之后低的工作，连上之后高的继续工作</span></span><br><span class="line">a1.sinkgroups.g1.processor.priority.k1 = 5</span><br><span class="line">a1.sinkgroups.g1.processor.priority.k2 = 10</span><br><span class="line">a1.sinkgroups.g1.processor.maxpenalty = 10000</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = 0.0.0.0</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = linux2</span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line">a1.sinks.k2.hostname = linux3</span><br><span class="line">a1.sinks.k2.port = 4141</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c1</span><br></pre></td></tr></table></figure>

<p>（3）创建a2.conf</p>
<p>配置上级Flume输出的Source，输出是到本地控制台。</p>
<p>编辑配置文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux2 failover]$ vim a2.conf</span><br></pre></td></tr></table></figure>

<p>添加如下内容:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.sinks = k1</span><br><span class="line">a2.channels = c1</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a2.sources.r1.type = avro</span><br><span class="line">a2.sources.r1.bind = linux2</span><br><span class="line">a2.sources.r1.port = 4141</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a2.sinks.k1.type = logger</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="meta"> </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p>（4）创建a3.conf</p>
<p>配置上级Flume输出的Source，输出是到本地控制台。</p>
<p>编辑配置文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux3 failover]$ vim a3.conf</span><br></pre></td></tr></table></figure>

<p>添加如下内容:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.sinks = k1</span><br><span class="line">a3.channels = c2</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line">a3.sources.r1.bind = linux3</span><br><span class="line">a3.sources.r1.port = 4141</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a3.sinks.k1.type = logger</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a3.channels.c2.type = memory</span><br><span class="line">a3.channels.c2.capacity = 1000</span><br><span class="line">a3.channels.c2.transactionCapacity = 100</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a3.sources.r1.channels = c2</span><br><span class="line">a3.sinks.k1.channel = c2</span><br></pre></td></tr></table></figure>

<p>（5）执行配置文件</p>
<p>分别开启对应配置文件：a3.conf，a2.conf，a1.conf。</p>
<p>（6）使用netcat工具向本机的44444端口发送内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> nc localhost 44444</span></span><br></pre></td></tr></table></figure>

<p>（7）查看Flume2及Flume3的控制台打印日志</p>
<p>（8）将Flume2 kill，观察Flume3的控制台打印情况。</p>
<p>注：使用jps -ml查看Flume进程。</p>
<h3 id="聚合-1"><a href="#聚合-1" class="headerlink" title="聚合"></a>聚合</h3><p>linux1上的Flume-1监控文件/opt/module/group.log，</p>
<p>linux2上的Flume-2监控某一个端口的数据流，</p>
<p>Flume-1与Flume-2将数据发送给linux3上的Flume-3，Flume-3将最终数据打印到控制台。</p>
<p><img src="/Flume/10.png" alt="10"></p>
<p>（1）准备工作</p>
<p>分发Flume</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 module]$ xsync flume</span><br></pre></td></tr></table></figure>

<p>在linux1、linux2以及linux3的/opt/module/flume/job目录下创建一个consolidation文件夹。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 job]$ mkdir consolidation</span><br><span class="line">[vincent@linux2 job]$ mkdir consolidation</span><br><span class="line">[vincent@linux3 job]$ mkdir consolidation</span><br></pre></td></tr></table></figure>

<p>（2）创建a1.conf </p>
<p>配置Source用于监控hive.log文件，配置Sink输出数据到下一级Flume。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 consolidation]$ vim exec-avro.conf </span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">a1.sources.r1.command = tail -F /opt/module/hive/logs/hive.log</span><br><span class="line">a1.sources.r1.shell = /bin/bash -c</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = linux3</span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p>（3）创建a2.conf</p>
<p>配置Source监控端口44444数据流，配置Sink数据到下一级Flume：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux2 consolidation]$ vim netcat-avro.conf</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.sinks = k1</span><br><span class="line">a2.channels = c1</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a2.sources.r1.type = netcat</span><br><span class="line">a2.sources.r1.bind = linux2</span><br><span class="line">a2.sources.r1.port = 4444</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a2.sinks.k1.type = avro</span><br><span class="line">a2.sinks.k1.hostname = linux3</span><br><span class="line">a2.sinks.k1.port = 4141</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p>（4）创建a3.conf</p>
<p>配置source用于接收flume1与flume2发送过来的数据流，最终合并后sink到控制台。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux3 consolidation]$ vim avro-logger.conf</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.sinks = k1</span><br><span class="line">a3.channels = c1</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line">a3.sources.r1.bind = linux3</span><br><span class="line">a3.sources.r1.port = 4141</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a3.sinks.k1.type = logger</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the channel</span></span><br><span class="line">a3.channels.c1.type = memory</span><br><span class="line">a3.channels.c1.capacity = 1000</span><br><span class="line">a3.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a3.sources.r1.channels = c1</span><br><span class="line">a3.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p>（5）分发配置文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume] xsync ../flume</span><br></pre></td></tr></table></figure>

<p>（6）执行配置文件</p>
<p>分别开启对应配置文件：avro-logger.conf，netcat-avro.conf，exec-avro.conf 。</p>
<p>（7）在linux1上向/opt/module目录下的hive.log追加内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 module]$ echo &#x27;hello&#x27; &gt; hive.log</span><br></pre></td></tr></table></figure>

<p>（7）向linux2的4444端口发送数据</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux2 flume]$ nc linux2 4444</span><br></pre></td></tr></table></figure>

<p>（8）检查linux3上数据</p>
<h3 id="自定义Interceptor"><a href="#自定义Interceptor" class="headerlink" title="自定义Interceptor"></a>自定义Interceptor</h3><p>1）案例需求</p>
<p>使用Flume采集服务器本地日志，需要按照日志类型的不同，将不同种类的日志发往不同的分析系统。</p>
<p>2）需求分析</p>
<p>在实际的开发中，一台服务器产生的日志类型可能有很多种，不同类型的日志可能需要发送到不同的分析系统。此时会用到Flume拓扑结构中的Multiplexing结构，Multiplexing的原理是，根据event中Header的某个key的值，将不同的event发送到不同的Channel中，所以我们需要自定义一个Interceptor，为不同类型的event的Header中的key赋予不同的值。</p>
<p>在该案例中，我们以端口数据模拟日志，以数字（单个）和字母（单个）模拟不同类型的日志，我们需要自定义interceptor区分数字和字母，将其分别发往不同的分析系统（Channel）。</p>
<p><img src="/Flume/11.png" alt="11"></p>
<p>3）实现步骤</p>
<p>（1）创建一个maven项目，并引入以下依赖。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flume<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flume-ng-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.9.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>（2）定义MyInterceptor类并实现Interceptor接口。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.vincent.flume.interceptor;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Context;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Event;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.interceptor.Interceptor;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 根据输入数据的首字符不同，添加不同的header来让ChannelSelector处理</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> Vincent</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span> 2020-06-17</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyInterceptor</span> <span class="keyword">implements</span> <span class="title">Interceptor</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 初始化方法，新建Interceptor时候用</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 更改方法 对event进行处理</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> event 传入数据</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span>  处理好的数据</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Event <span class="title">intercept</span><span class="params">(Event event)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">byte</span>[] body = event.getBody();</span><br><span class="line">        <span class="keyword">if</span> (body[<span class="number">0</span>] &lt; <span class="string">&#x27;z&#x27;</span> &amp;&amp; body[<span class="number">0</span>] &gt; <span class="string">&#x27;a&#x27;</span>) &#123;</span><br><span class="line">            event.getHeaders().put(<span class="string">&quot;type&quot;</span>, <span class="string">&quot;letter&quot;</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (body[<span class="number">0</span>] &gt; <span class="string">&#x27;0&#x27;</span> &amp;&amp; body[<span class="number">0</span>] &lt; <span class="string">&#x27;9&#x27;</span>) &#123;</span><br><span class="line">            event.getHeaders().put(<span class="string">&quot;type&quot;</span>, <span class="string">&quot;number&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> event;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 批处理方法，对传入的一批数据进行处理，source设置了batchSize</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> events</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span>  处理好的数据</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;Event&gt; <span class="title">intercept</span><span class="params">(List&lt;Event&gt; events)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (Event event : events) &#123;</span><br><span class="line">            intercept(event);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> events;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 如果有需要关闭的资源，在这个方法中关闭</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 框架会调用Builder来创建Interceptor实例</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyBuilder</span> <span class="keyword">implements</span> <span class="title">Interceptor</span>.<span class="title">Builder</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 创建实例的方法</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@return</span>  新的Interceptor</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Interceptor <span class="title">build</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> MyInterceptor();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 读取配置文件的方法</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> context   配置文件</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（3）编辑flume配置文件</p>
<p>为linux1上的Flume-1配置1个netcat source，1个sink group（2个avro sink），并配置相应的ChannelSelector和interceptor。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line">a1.channels = c1 c2</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = linux1</span><br><span class="line">a1.sources.r1.port = 4444</span><br><span class="line"></span><br><span class="line">a1.sources.r1.interceptors = i1</span><br><span class="line">a1.sources.r1.interceptors.i1.type = com.vincent.flume.interceptor.MyInterceptor$MyBuilder</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 多路复用模式</span></span><br><span class="line">a1.sources.r1.selector.type = multiplexing</span><br><span class="line">a1.sources.r1.selector.header = type</span><br><span class="line">a1.sources.r1.selector.mapping.letter = c1</span><br><span class="line">a1.sources.r1.selector.mapping.number = c2</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = linux2</span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line"></span><br><span class="line">a1.sinks.k2.type = avro</span><br><span class="line">a1.sinks.k2.hostname = linux3</span><br><span class="line">a1.sinks.k2.port = 4141</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c2.type = memory</span><br><span class="line">a1.channels.c2.capacity = 1000</span><br><span class="line">a1.channels.c2.transactionCapacity = 100</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1 c2</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c2</span><br></pre></td></tr></table></figure>

<p>为linux2上的Flume-2配置一个avro source和一个logger sink。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">a2.sources = r1</span><br><span class="line">a2.sinks = k1</span><br><span class="line">a2.channels = c1</span><br><span class="line"></span><br><span class="line">a2.sources.r1.type = avro</span><br><span class="line">a2.sources.r1.bind = linux2</span><br><span class="line">a2.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line">a2.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line">a2.channels.c1.type = memory</span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line">a2.sinks.k1.channel = c1</span><br><span class="line">a2.sources.r1.channels = c1</span><br></pre></td></tr></table></figure>

<p>为linux3上的Flume-3配置一个avro source和一个logger sink。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">a3.sources = r1</span><br><span class="line">a3.sinks = k1</span><br><span class="line">a3.channels = c1</span><br><span class="line"></span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line">a3.sources.r1.bind = linux3</span><br><span class="line">a3.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line">a3.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line">a3.channels.c1.type = memory</span><br><span class="line">a3.channels.c1.capacity = 1000</span><br><span class="line">a3.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line">a3.sinks.k1.channel = c1</span><br><span class="line">a3.sources.r1.channels = c1</span><br></pre></td></tr></table></figure>

<p>（4）分别在linux3，linux2，linux1上启动flume进程，注意先后顺序。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux3 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/interceptor/a3.conf -Dflume.root.logger=INFO,console</span><br><span class="line">[vincent@linux2 flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/interceptor/a2.conf -Dflume.root.logger=INFO,console</span><br><span class="line">[vincent@linux1 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/interceptor/a1.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<p>（5）像linux1的44444端口发送字母和数字。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 ~]$ nc linux1 4444</span><br></pre></td></tr></table></figure>

<p>（6）观察linux2和linux3打印的日志。</p>
<h3 id="自定义Source"><a href="#自定义Source" class="headerlink" title="自定义Source"></a>自定义Source</h3><p>Source是负责接收数据到Flume Agent的组件。Source组件可以处理各种类型、各种格式的日志数据，包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy。官方提供的source类型已经很多，但是有时候并不能满足实际开发当中的需求，此时我们就需要根据实际需求自定义某些source。</p>
<p>官方也提供了自定义source的接口：</p>
<p><a href="#source">https://flume.apache.org/FlumeDeveloperGuide.html#source</a> </p>
<p>根据官方说明自定义MySource需要继承AbstractSource类并实现Configurable和PollableSource接口。</p>
<p>实现相应方法：</p>
<p>getBackOffSleepIncrement()            //暂不用</p>
<p>getMaxBackOffSleepInterval()         //暂不用</p>
<p>configure(Context context)             //初始化context（读取配置文件内容）</p>
<p>process()                                         //获取数据封装成event并写入channel，这个方法将被循环调用。</p>
<p>使用场景：读取MySQL数据或者其他文件系统。</p>
<p>需求：使用flume接收数据，并给每条数据添加前缀，输出到控制台。前缀可从flume配置文件中配置。</p>
<p><img src="/Flume/12.png" alt="12"></p>
<p><img src="/Flume/14.png" alt="14"></p>
<p>1）导入pom依赖</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flume<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flume-ng-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.9.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>2）编写代码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.vincent.flume.source;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flume.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.channel.ChannelProcessor;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.conf.Configurable;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.event.SimpleEvent;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.source.AbstractSource;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> Vincent</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span> 2020-06-17</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MySource</span> <span class="keyword">extends</span> <span class="title">AbstractSource</span> <span class="keyword">implements</span> <span class="title">Configurable</span>, <span class="title">PollableSource</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String prefff;</span><br><span class="line">    <span class="keyword">private</span> Long interval;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 拉取事件并交给ChannelProcessor处理的方法，循环调用</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> EventDeliveryException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Status <span class="title">process</span><span class="params">()</span> <span class="keyword">throws</span> EventDeliveryException </span>&#123;</span><br><span class="line">        Status status = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">//通过外部方法拉取数据</span></span><br><span class="line">            Event e = getSomeData();</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Store the Event into this Source&#x27;s associated Channel(s)</span></span><br><span class="line">            <span class="comment">//getChannelProcessor().processEvent(e);</span></span><br><span class="line">            ChannelProcessor channelProcessor = getChannelProcessor();</span><br><span class="line">            channelProcessor.processEvent(e);</span><br><span class="line"></span><br><span class="line">            status = Status.READY;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">            <span class="comment">// Log exception, handle individual exceptions as needed</span></span><br><span class="line"></span><br><span class="line">            status = Status.BACKOFF;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// re-throw all Errors</span></span><br><span class="line">            <span class="keyword">if</span> (t <span class="keyword">instanceof</span> Error) &#123;</span><br><span class="line">                <span class="keyword">throw</span> (Error)t;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> status;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 拉取数据并包装成Event的过程</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span>  拉取到的Event</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> Event <span class="title">getSomeData</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">//通过随机数模拟拉取到的数据</span></span><br><span class="line">        <span class="keyword">int</span> i = (<span class="keyword">int</span>) (Math.random() * <span class="number">1000</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//添加前缀</span></span><br><span class="line">        String message = prefix + i;</span><br><span class="line"></span><br><span class="line">        Thread.sleep(interval);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//包装成Event</span></span><br><span class="line">        Event event = <span class="keyword">new</span> SimpleEvent();</span><br><span class="line">        event.setBody(message.getBytes());</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> event;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 如果拉取不到数据，Backoff时间的增长速度</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span>  增长量</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getBackOffSleepIncrement</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1000</span>;    <span class="comment">//1s</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 最大等待时间</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span>  时间</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getMaxBackOffSleepInterval</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">10000</span>;   <span class="comment">//10s</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 来自configurable，可以定义我们的自定义Source</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context   配置文件</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line">        prefix = context.getString(<span class="string">&quot;prefff&quot;</span>,<span class="string">&quot;xxxx&quot;</span>);</span><br><span class="line">        interval = context.getLong(<span class="string">&quot;interval&quot;</span>, <span class="number">500L</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>3）测试</p>
<p>（1）打包</p>
<p>将写好的代码打包，并放到flume的lib目录（/opt/module/flume）下。</p>
<p>（2）配置文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ vim job/custom-logger.conf </span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = com.vincent.flume.source.MySource</span><br><span class="line">a1.sources.r1.interval = 100</span><br><span class="line">a1.sources.r1.prefff = ABCD-</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p>3）开启任务</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ bin/flume-ng agent -c conf/ -f job/custom-logger.conf -n a1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<h3 id="自定义Sink"><a href="#自定义Sink" class="headerlink" title="自定义Sink"></a>自定义Sink</h3><p>1）介绍</p>
<p>Sink不断地轮询Channel中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另一个Flume Agent。</p>
<p>Sink是完全事务性的。在从Channel批量删除数据之前，每个Sink用Channel启动一个事务。批量事件一旦成功写出到存储系统或下一个Flume Agent，Sink就利用Channel提交事务。事务一旦被提交，该Channel从自己的内部缓冲区删除事件。</p>
<p>Sink组件目的地包括hdfs、logger、avro、thrift、ipc、file、null、HBase、solr、自定义。官方提供的Sink类型已经很多，但是有时候并不能满足实际开发当中的需求，此时我们就需要根据实际需求自定义某些Sink。</p>
<p>官方也提供了自定义sink的接口：</p>
<p><a href="https://flume.apache.org/FlumeDeveloperGuide.html#sink">https://flume.apache.org/FlumeDeveloperGuide.html#sink</a></p>
<p>根据官方说明自定义MySink需要继承AbstractSink类并实现Configurable接口。</p>
<p>实现相应方法：</p>
<p>configure(Context context)//初始化context（读取配置文件内容）</p>
<p>process()//从Channel读取获取数据（event），这个方法将被循环调用。</p>
<p>使用场景：读取Channel数据写入MySQL或者其他文件系统。</p>
<p>2）需求</p>
<p>使用flume接收数据，并在Sink端给每条数据添加前缀和后缀，输出到控制台。前后缀可在flume任务配置文件中配置。</p>
<p><img src="/Flume/15.png" alt="15"></p>
<p>3）编码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.vincent.flume.sink;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flume.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.conf.Configurable;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.sink.AbstractSink;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> Vincent</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span> 2020-06-17</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MySink</span> <span class="keyword">extends</span> <span class="title">AbstractSink</span> <span class="keyword">implements</span> <span class="title">Configurable</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">  	<span class="comment">//创建Logger对象</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOG = LoggerFactory.getLogger(MySink.class);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String prefix;</span><br><span class="line">    <span class="keyword">private</span> String suffix;</span><br><span class="line">  </span><br><span class="line">  	<span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 改方法被调用时，会从Channel中拉取数据并处理</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span>  处理的状态</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> EventDeliveryException   处理失败时候会抛出异常</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Status <span class="title">process</span><span class="params">()</span> <span class="keyword">throws</span> EventDeliveryException </span>&#123;</span><br><span class="line">        <span class="comment">//声明返回值状态信息</span></span><br><span class="line">        Status status;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取当前Sink绑定的Channel</span></span><br><span class="line">        Channel ch = getChannel();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取事务</span></span><br><span class="line">        Transaction txn = ch.getTransaction();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//声明事件</span></span><br><span class="line">        Event event;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//开启事务</span></span><br><span class="line">        txn.begin();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//读取Channel中的事件，直到读取到事件结束循环</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            event = ch.take();</span><br><span class="line">            <span class="keyword">if</span> (event != <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">//处理事件（打印）</span></span><br><span class="line">            LOG.info(prefix + <span class="keyword">new</span> String(event.getBody()) + suffix);</span><br><span class="line"></span><br><span class="line">            <span class="comment">//事务提交</span></span><br><span class="line">            txn.commit();</span><br><span class="line">            status = Status.READY;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">//遇到异常，事务回滚</span></span><br><span class="line">            txn.rollback();</span><br><span class="line">            status = Status.BACKOFF;</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">//关闭事务</span></span><br><span class="line">            txn.close();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> status;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 配置方法</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context   配置文件</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line">				<span class="comment">//读取配置文件内容，有默认值</span></span><br><span class="line">        prefix = context.getString(<span class="string">&quot;prefix&quot;</span>, <span class="string">&quot;hello:&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//读取配置文件内容，无默认值</span></span><br><span class="line">        suffix = context.getString(<span class="string">&quot;suffix&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>4）测试</p>
<p>（1）打包</p>
<p>将写好的代码打包，并放到flume的lib目录（/opt/module/flume）下。</p>
<p>（2）配置文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe/configure the <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = 0.0.0.0</span><br><span class="line">a1.sources.r1.port = 4444</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = com.vincent.flume.sink.MySink</span><br><span class="line">a1.sinks.k1.prefix = xsh:</span><br><span class="line">a1.sinks.k1.suffix = :xsh</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p>（3）开启任务并向linux1的44444端口发送数据</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ bin/flume-ng agent -c conf/ -f job/mysink.conf -n a1 -Dflume.root.logger=INFO,console</span><br><span class="line">[vincent@linux1 ~]$ nc linux1 4444</span><br></pre></td></tr></table></figure>

<h2 id="Flume数据流监控"><a href="#Flume数据流监控" class="headerlink" title="Flume数据流监控"></a>Flume数据流监控</h2><h3 id="Ganglia的安装与部署"><a href="#Ganglia的安装与部署" class="headerlink" title="Ganglia的安装与部署"></a>Ganglia的安装与部署</h3><p>1）安装httpd服务与php</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ sudo yum -y install httpd php</span><br></pre></td></tr></table></figure>

<p>2）安装其他依赖</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ sudo yum -y install rrdtool perl-rrdtool rrdtool-devel</span><br><span class="line">[vincent@linux1 flume]$ sudo yum -y install apr-devel</span><br></pre></td></tr></table></figure>

<p>3）安装ganglia</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ sudo rpm -Uvh http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ sudo yum -y install ganglia-gmetad ganglia-web ganglia-gmond</span><br></pre></td></tr></table></figure>

<p>Ganglia由gmond、gmetad和gweb三部分组成。</p>
<p>gmond（Ganglia Monitoring Daemon）是一种轻量级服务，安装在每台需要收集指标数据的节点主机上。使用gmond，你可以很容易收集很多系统指标数据，如CPU、内存、磁盘、网络和活跃进程的数据等。</p>
<p>gmetad（Ganglia Meta Daemon）整合所有信息，并将其以RRD格式存储至磁盘的服务。</p>
<p>gweb（Ganglia Web）Ganglia可视化工具，gweb是一种利用浏览器显示gmetad所存储数据的PHP前端。在Web界面中以图表方式展现集群的运行状态下收集的多种不同指标数据。</p>
<p>4）修改配置文件/etc/httpd/conf.d/ganglia.conf</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ sudo vim /etc/httpd/conf.d/ganglia.conf</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Ganglia monitoring system php web frontend</span></span><br><span class="line">Alias /ganglia /usr/share/ganglia</span><br><span class="line">&lt;Location /ganglia&gt;</span><br><span class="line"><span class="meta">	#</span><span class="bash"> Require <span class="built_in">local</span></span></span><br><span class="line">  Require ip linux1</span><br><span class="line">&lt;/Location&gt;</span><br></pre></td></tr></table></figure>

<p>5）修改配置文件/etc/ganglia/gmetad.conf</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ sudo vim /etc/ganglia/gmetad.conf</span><br></pre></td></tr></table></figure>

<p>修改为：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">data_source &quot;flume_cluster&quot; linux1 linux2 linux3</span><br></pre></td></tr></table></figure>

<p>6）修改配置文件/etc/ganglia/gmond.conf</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ sudo vim /etc/ganglia/gmond.conf</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cluster &#123;</span><br><span class="line">  name = &quot;flume_cluster&quot;</span><br><span class="line">  owner = &quot;unspecified&quot;</span><br><span class="line">  latlong = &quot;unspecified&quot;</span><br><span class="line">  url = &quot;unspecified&quot;</span><br><span class="line">&#125;</span><br><span class="line">udp_send_channel &#123;</span><br><span class="line"><span class="meta">  #</span><span class="bash">bind_hostname = yes <span class="comment"># Highly recommended, soon to be default.</span></span></span><br><span class="line">                       # This option tells gmond to use a source address</span><br><span class="line">                       # that resolves to the machine&#x27;s hostname.  Without</span><br><span class="line">                       # this, the metrics may appear to come from any</span><br><span class="line">                       # interface and the DNS names associated with</span><br><span class="line">                       # those IPs will be used to create the RRDs.</span><br><span class="line"><span class="meta">  #</span><span class="bash"> mcast_join = 239.2.11.71</span></span><br><span class="line">  host = linux1</span><br><span class="line">  port = 8649</span><br><span class="line">  ttl = 1</span><br><span class="line">&#125;</span><br><span class="line">udp_recv_channel &#123;</span><br><span class="line"><span class="meta">  #</span><span class="bash"> mcast_join = 239.2.11.71</span></span><br><span class="line">  bind = linux1</span><br><span class="line">  port = 8649</span><br><span class="line">  retry_bind = true</span><br><span class="line"><span class="meta">  #</span><span class="bash"> Size of the UDP buffer. If you are handling lots of metrics you really</span></span><br><span class="line"><span class="meta">  #</span><span class="bash"> should bump it up to e.g. 10MB or even higher.</span></span><br><span class="line"><span class="meta">  #</span><span class="bash"> buffer = 10485760</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ xsync.sh /etc/ganglia/gmond.conf</span><br></pre></td></tr></table></figure>

<p>7）修改配置文件/etc/selinux/config</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ sudo vim /etc/selinux/config</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> This file controls the state of SELinux on the system.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> SELINUX= can take one of these three values:</span></span><br><span class="line"><span class="meta">#</span><span class="bash">     enforcing - SELinux security policy is enforced.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">     permissive - SELinux prints warnings instead of enforcing.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">     disabled - No SELinux policy is loaded.</span></span><br><span class="line">SELINUX=disabled</span><br><span class="line"><span class="meta">#</span><span class="bash"> SELINUXTYPE= can take one of these two values:</span></span><br><span class="line"><span class="meta">#</span><span class="bash">     targeted - Targeted processes are protected,</span></span><br><span class="line"><span class="meta">#</span><span class="bash">     mls - Multi Level Security protection.</span></span><br><span class="line">SELINUXTYPE=targeted</span><br></pre></td></tr></table></figure>

<p>尖叫提示：selinux本次生效关闭必须重启，如果此时不想重启，可以临时生效之：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ sudo setenforce 0</span><br></pre></td></tr></table></figure>

<p>8）启动ganglia</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ sudo systemctl start httpd</span><br><span class="line">[vincent@linux1 flume]$ sudo systemctl start gmetad</span><br><span class="line">[vincent@linux1 flume]$ sudo systemctl start gmond</span><br></pre></td></tr></table></figure>

<p>9）打开网页浏览ganglia页面</p>
<p><a href="http://linux3/ganglia">http://linux3/ganglia</a></p>
<p>尖叫提示：如果完成以上操作依然出现权限不足错误，请修改/var/lib/ganglia目录的权限：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ sudo chmod -R 777 /var/lib/ganglia</span><br></pre></td></tr></table></figure>

<h3 id="Ganglia-Reporting"><a href="#Ganglia-Reporting" class="headerlink" title="Ganglia Reporting"></a>Ganglia Reporting</h3><p>1）启动Flume任务</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ bin/flume-ng agent \</span><br><span class="line">--conf conf/ \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf-file job/flume-netcat-logger.conf \</span><br><span class="line">-Dflume.root.logger==INFO,console \</span><br><span class="line">-Dflume.monitoring.type=ganglia \</span><br><span class="line">-Dflume.monitoring.hosts=linux1:8649</span><br></pre></td></tr></table></figure>

<p>2）发送数据观察ganglia监测图</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 flume]$ nc localhost 6666</span><br></pre></td></tr></table></figure>

<p>3）图例说明：</p>
<table>
<thead>
<tr>
<th>字段（图表名称）</th>
<th>字段含义</th>
</tr>
</thead>
<tbody><tr>
<td>EventPutAttemptCount</td>
<td>source尝试写入channel的事件总数量</td>
</tr>
<tr>
<td>EventPutSuccessCount</td>
<td>成功写入channel且提交的事件总数量</td>
</tr>
<tr>
<td>EventTakeAttemptCount</td>
<td>sink尝试从channel拉取事件的总数量。</td>
</tr>
<tr>
<td>EventTakeSuccessCount</td>
<td>sink成功读取的事件的总数量</td>
</tr>
<tr>
<td>StartTime</td>
<td>channel启动的时间（毫秒）</td>
</tr>
<tr>
<td>StopTime</td>
<td>channel停止的时间（毫秒）</td>
</tr>
<tr>
<td>ChannelSize</td>
<td>目前channel中事件的总数量</td>
</tr>
<tr>
<td>ChannelFillPercentage</td>
<td>channel占用百分比</td>
</tr>
<tr>
<td>ChannelCapacity</td>
<td>channel的容量</td>
</tr>
</tbody></table>
<h3 id="JSON-Reporting"><a href="#JSON-Reporting" class="headerlink" title="JSON Reporting"></a>JSON Reporting</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/flume-ng agent --conf-file example.conf --name a1 -Dflume.monitoring.type=http -Dflume.monitoring.port=34545</span><br></pre></td></tr></table></figure>

<figure class="highlight awk"><table><tr><td class="code"><pre><span class="line">http:<span class="regexp">//</span>&lt;hostname&gt;:&lt;port&gt;/metrics</span><br><span class="line">http:<span class="regexp">//</span>linux1:<span class="number">34545</span>/metrics</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>离线计算</category>
      </categories>
  </entry>
  <entry>
    <title>Kylin</title>
    <url>/Kylin/</url>
    <content><![CDATA[<h1 id="Kylin定义"><a href="#Kylin定义" class="headerlink" title="Kylin定义"></a>Kylin定义</h1><p>Apache Kylin是一个开源的分布式分析引擎，提供Hadoop/Spark之上的SQL查询接口及多维分析（OLAP）能力以支持超大规模数据，最初由eBay Inc开发并贡献至开源社区。它能在亚秒内查询巨大的Hive表。</p>
<h1 id="Kylin架构"><a href="#Kylin架构" class="headerlink" title="Kylin架构"></a>Kylin架构</h1><p><img src="/Kylin/148.png" alt="148"></p>
<p>1）REST Server</p>
<p>REST Server是一套面向应用程序开发的入口点，旨在实现针对Kylin平台的应用开发工作。 此类应用程序可以提供查询、获取结果、触发cube构建任务、获取元数据以及获取用户权限等等。另外可以通过Restful接口实现SQL查询。</p>
<p>2）查询引擎（Query Engine）</p>
<p>当cube准备就绪后，查询引擎就能够获取并解析用户查询。它随后会与系统中的其它组件进行交互，从而向用户返回对应的结果。 </p>
<p>3）路由器（Routing）</p>
<p>在最初设计时曾考虑过将Kylin不能执行的查询引导去Hive中继续执行，但在实践后发现Hive与Kylin的速度差异过大，导致用户无法对查询的速度有一致的期望，很可能大多数查询几秒内就返回结果了，而有些查询则要等几分钟到几十分钟，因此体验非常糟糕。最后这个路由功能在发行版中默认关闭。</p>
<p>4）元数据管理工具（Metadata）</p>
<p>Kylin是一款元数据驱动型应用程序。元数据管理工具是一大关键性组件，用于对保存在Kylin当中的所有元数据进行管理，其中包括最为重要的cube元数据。其它全部组件的正常运作都需以元数据管理工具为基础。 Kylin的元数据存储在hbase中。 </p>
<p>5）任务引擎（Cube Build Engine）</p>
<p>这套引擎的设计目的在于处理所有离线任务，其中包括shell脚本、Java API以及Map Reduce任务等等。任务引擎对Kylin当中的全部任务加以管理与协调，从而确保每一项任务都能得到切实执行并解决其间出现的故障。</p>
<h1 id="Kylin特点"><a href="#Kylin特点" class="headerlink" title="Kylin特点"></a>Kylin特点</h1><p>Kylin的主要特点包括支持SQL接口、支持超大规模数据集、亚秒级响应、可伸缩性、高吞吐率、BI工具集成等。</p>
<p>1）标准SQL接口：Kylin是以标准的SQL作为对外服务的接口。</p>
<p>2）支持超大数据集：Kylin对于大数据的支撑能力可能是目前所有技术中最为领先的。早在2015年eBay的生产环境中就能支持百亿记录的秒级查询，之后在移动的应用场景中又有了千亿记录秒级查询的案例。</p>
<p>3）亚秒级响应：Kylin拥有优异的查询相应速度，这点得益于预计算，很多复杂的计算，比如连接、聚合，在离线的预计算过程中就已经完成，这大大降低了查询时刻所需的计算量，提高了响应速度。</p>
<p>4）可伸缩性和高吞吐率：单节点Kylin可实现每秒70个查询，还可以搭建Kylin的集群。</p>
<p>5）BI工具集成</p>
<p>Kylin可以与现有的BI工具集成，具体包括如下内容。</p>
<p>ODBC：与Tableau、Excel、PowerBI等工具集成</p>
<p>JDBC：与Saiku、BIRT等Java工具集成</p>
<p>RestAPI：与JavaScript、Web网页集成</p>
<p>Kylin开发团队还贡献了<strong>Zepplin</strong>的插件，也可以使用Zepplin来访问Kylin服务。</p>
<h1 id="Kylin安装"><a href="#Kylin安装" class="headerlink" title="Kylin安装"></a>Kylin安装</h1><p>安装Kylin前需先部署好Hadoop、Hive、Zookeeper、HBase，并且需要在/etc/profile中配置以下环境变量HADOOP_HOME，HIVE_HOME，HBASE_HOME，记得source使其生效。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vinent@linux1 sorfware]$ tar -zxvf apache-kylin-3.0.2-bin.tar.gz -C /opt/module/</span><br><span class="line">[vinent@linux1 module]$ mv /opt/module/apache-kylin-3.0.2-bin /opt/module/kylin</span><br></pre></td></tr></table></figure>

<p>（1）启动Kylin之前，需先启动Hadoop（hdfs，yarn，jobhistoryserver）、Zookeeper、Hbase</p>
<p>（2）启动Kylin</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vinent@linux1 kylin]$ bin/kylin.sh start</span><br></pre></td></tr></table></figure>

<p>在<a href="http://linux1:7070/kylin%E6%9F%A5%E7%9C%8BWeb%E9%A1%B5%E9%9D%A2">http://linux1:7070/kylin查看Web页面</a></p>
<p>用户名为：ADMIN，密码为：KYLIN</p>
<p>（3）关闭Kylin</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vinent@linux1 kylin]$ bin/kylin.sh stop</span><br></pre></td></tr></table></figure>

<h1 id="使用进阶"><a href="#使用进阶" class="headerlink" title="使用进阶"></a>使用进阶</h1><p>1）每日全量维度表及拉链维度表<strong>重复Key问题</strong>如何处理</p>
<p>按照上述流程，会发现，在cube构建流程中出现以下错误</p>
<p>上述错误原因是model中的维度表dwd_dim_user_info_his为拉链表、dwd_dim_sku_info为每日全量表，故使用整张表作为维度表，必然会出现订单表中同一个user_id或者sku_id对应多条数据的问题，针对上述问题，有以下两种解决方案。</p>
<p>方案一：在hive中创建维度表的临时表，该临时表中只存放维度表最新的一份完整的数据，在kylin中创建模型时选择该临时表作为维度表。</p>
<p>方案二：与方案一思路相同，但不使用物理临时表，而选用视图（view）实现相同的功能。</p>
<p>此处采用方案二：</p>
<h2 id="创建维度表视图"><a href="#创建维度表视图" class="headerlink" title="创建维度表视图"></a>创建维度表视图</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create view dwd_dim_user_info_his_view as select * from dwd_dim_user_info_his where end_date=&#x27;9999-99-99&#x27;;</span><br><span class="line"></span><br><span class="line">--全量维度表视图</span><br><span class="line">create view dwd_dim_sku_info_view as select * from dwd_dim_sku_info where dt=date_add(current_date,-1);</span><br><span class="line"></span><br><span class="line">--当前情形我们先创建一个2020-06-25的视图</span><br><span class="line">create view dwd_dim_sku_info_view as select * from dwd_dim_sku_info where dt=&#x27;2020-06-25&#x27;;</span><br></pre></td></tr></table></figure>

<h2 id="查询结果"><a href="#查询结果" class="headerlink" title="查询结果"></a>查询结果</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select</span><br><span class="line">    ui.gender,</span><br><span class="line">    si.category3_id,</span><br><span class="line">    dp.region_id,</span><br><span class="line">    sum(od.final_amount.d)</span><br><span class="line">from</span><br><span class="line">    dwd_fact_order_detail od</span><br><span class="line">join</span><br><span class="line">    dwd_dim_user_info_his_view ui</span><br><span class="line">on</span><br><span class="line">    od.user_id=ui.id</span><br><span class="line">join</span><br><span class="line">    dwd_dim_sku_info_view si</span><br><span class="line">on</span><br><span class="line">    od.sku_id=si.id</span><br><span class="line">join</span><br><span class="line">    dwd_dim_base_province dp</span><br><span class="line">on</span><br><span class="line">    od.province_id=dp.id</span><br><span class="line">group by</span><br><span class="line">    ui.gender,si.category3_id,dp.region_id;</span><br></pre></td></tr></table></figure>

<h2 id="自动构建cube"><a href="#自动构建cube" class="headerlink" title="自动构建cube"></a>自动构建cube</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">cube_name=order_cube</span><br><span class="line">do_date=`date -d &#x27;-1 day&#x27; +%F`</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">获取00:00时间戳</span></span><br><span class="line">start_date_unix=`date -d &quot;$do_date 08:00:00&quot; +%s`</span><br><span class="line">start_date=$(($start_date_unix*1000))</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">获取24:00的时间戳</span></span><br><span class="line">stop_date=$(($start_date+86400000))</span><br><span class="line"></span><br><span class="line">curl -X PUT -H &quot;Authorization: Basic QURNSU46S1lMSU4=&quot; -H &#x27;Content-Type: application/json&#x27; -d &#x27;&#123;&quot;startTime&quot;:&#x27;$start_date&#x27;, &quot;endTime&quot;:&#x27;$stop_date&#x27;, &quot;buildType&quot;:&quot;BUILD&quot;&#125;&#x27; http://linux1:7070/kylin/api/cubes/$cube_name/build</span><br></pre></td></tr></table></figure>

<h1 id="Kylin-Cube构建原理"><a href="#Kylin-Cube构建原理" class="headerlink" title="Kylin Cube构建原理"></a>Kylin Cube构建原理</h1><h2 id="维度和度量"><a href="#维度和度量" class="headerlink" title="维度和度量"></a>维度和度量</h2><p>维度：即观察数据的角度。比如员工数据，可以从性别角度来分析，也可以更加细化，从入职时间或者地区的维度来观察。维度是一组离散的值，比如说性别中的男和女，或者时间维度上的每一个独立的日期。因此在统计时可以将维度值相同的记录聚合在一起，然后应用聚合函数做累加、平均、最大和最小值等聚合计算。</p>
<p>度量：即被聚合（观察）的统计值，也就是聚合运算的结果。比如说员工数据中不同性别员工的人数，又或者说在同一年入职的员工有多少。</p>
<h2 id="Cube和Cuboid"><a href="#Cube和Cuboid" class="headerlink" title="Cube和Cuboid"></a>Cube和Cuboid</h2><p>有了维度跟度量，一个数据表或者数据模型上的所有字段就可以分类了，它们要么是维度，要么是度量（可以被聚合）。于是就有了根据维度和度量做预计算的Cube理论。</p>
<p>给定一个数据模型，我们可以对其上的所有维度进行聚合，对于N个维度来说，组合的所有可能性共有2n种。对于每一种维度的组合，将度量值做聚合计算，然后将结果保存为一个物化视图，称为Cuboid。所有维度组合的Cuboid作为一个整体，称为Cube。</p>
<p>下面举一个简单的例子说明，假设有一个电商的销售数据集，其中<strong>维度包括时间[time]、商品[item]、地区[location]和供应商[supplier]，度量为销售额。那么所有维度的组合就有2^4 = 16种</strong>。</p>
<h2 id="Cube存储原理"><a href="#Cube存储原理" class="headerlink" title="Cube存储原理"></a>Cube存储原理</h2><p><img src="/Kylin/149.png" alt="149"></p>
<p><img src="/Kylin/150.png" alt="150"></p>
<h2 id="Cube构建算法"><a href="#Cube构建算法" class="headerlink" title="Cube构建算法"></a>Cube构建算法</h2><h3 id="1）逐层构建算法（layer）"><a href="#1）逐层构建算法（layer）" class="headerlink" title="1）逐层构建算法（layer）"></a>1）逐层构建算法（layer）</h3><p>我们知道，一个N维的Cube，是由1个N维子立方体、N个(N-1)维子立方体、N*(N-1)/2个(N-2)维子立方体、……、N个1维子立方体和1个0维子立方体构成，总共有2^N个子立方体组成，在逐层算法中，按维度数逐层减少来计算，每个层级的计算（除了第一层，它是从原始数据聚合而来），是基于它上一层级的结果来计算的。比如，**[Group by A, B]的结果，可以基于[Group by A, B, C]的结果，通过去掉C后聚合得来的**；这样可以减少重复计算；当 0维度Cuboid计算出来的时候，整个Cube的计算也就完成了。</p>
<p>每一轮的计算都是一个MapReduce任务，且串行执行；一个N维的Cube，至少需要N次MapReduce Job。</p>
<p><img src="/Kylin/151.png" alt="151"></p>
<p>算法优点：</p>
<p>1）此算法充分利用了MapReduce的优点，处理了中间复杂的排序和shuffle工作，故而算法代码清晰简单，易于维护；</p>
<p>2）受益于Hadoop的日趋成熟，此算法非常稳定，即便是集群资源紧张时，也能保证最终能够完成。</p>
<p>算法缺点：</p>
<p>1）当Cube有比较多维度的时候，所需要的MapReduce任务也相应增加；由于Hadoop的任务调度需要耗费额外资源，特别是集群较庞大的时候，反复递交任务造成的额外开销会相当可观；</p>
<p>2）由于Mapper逻辑中并未进行聚合操作，所以每轮MR的shuffle工作量都很大，导致效率低下。</p>
<p>3）对HDFS的读写操作较多：由于每一层计算的输出会用做下一层计算的输入，这些Key-Value需要写到HDFS上；当所有计算都完成后，Kylin还需要额外的一轮任务将这些文件转成HBase的HFile格式，以导入到HBase中去；</p>
<p>总体而言，该算法的效率较低，尤其是当Cube维度数较大的时候。</p>
<h3 id="2）快速构建算法（inmem）"><a href="#2）快速构建算法（inmem）" class="headerlink" title="2）快速构建算法（inmem）"></a>2）快速构建算法（inmem）</h3><p>也被称作“逐段”(By Segment) 或“逐块”(By Split) 算法，从1.5.x开始引入该算法，该算法的主要思想是，每个Mapper将其所分配到的数据块，计算成一个完整的小Cube 段（包含所有Cuboid）。每个Mapper将计算完的Cube段输出给Reducer做合并，生成大Cube，也就是最终结果。如图所示解释了此流程。</p>
<p><img src="/Kylin/152.png" alt="152"></p>
<p>与旧算法相比，快速算法主要有两点不同：</p>
<p>1） Mapper会利用内存做预聚合，算出所有组合；Mapper输出的每个Key都是不同的，这样会减少输出到Hadoop MapReduce的数据量，Combiner也不再需要；</p>
<p>2）一轮MapReduce便会完成所有层次的计算，减少Hadoop任务的调配。</p>
<h1 id="Kylin-Cube构建优化"><a href="#Kylin-Cube构建优化" class="headerlink" title="Kylin Cube构建优化"></a>Kylin Cube构建优化</h1><h2 id="使用衍生维度（derived-dimension）"><a href="#使用衍生维度（derived-dimension）" class="headerlink" title="使用衍生维度（derived dimension）"></a>使用衍生维度（derived dimension）</h2><p>衍生维度用于在有效维度内将维度表上的非主键维度排除掉，并使用维度表的主键（其实是事实表上相应的外键）来替代它们。Kylin会在底层记录维度表主键与维度表其他维度之间的映射关系，以便在查询时能够动态地将维度表的主键“翻译”成这些非主键维度，并进行实时聚合。</p>
<p><img src="/Kylin/153.png" alt="153"></p>
<p>虽然衍生维度具有非常大的吸引力，但这也并不是说所有维度表上的维度都得变成衍生维度，如果从维度表主键到某个维度表维度所需要的聚合工作量非常大，则不建议使用衍生维度。</p>
<h2 id="使用聚合组（Aggregation-group）"><a href="#使用聚合组（Aggregation-group）" class="headerlink" title="使用聚合组（Aggregation group）"></a>使用聚合组（Aggregation group）</h2><p>聚合组（Aggregation Group）是一种强大的剪枝工具。聚合组假设一个Cube的所有维度均可以根据业务需求划分成若干组（当然也可以是一个组），由于同一个组内的维度更可能同时被同一个查询用到，因此会表现出更加紧密的内在关联。每个分组的维度集合均是Cube所有维度的一个子集，不同的分组各自拥有一套维度集合，它们可能与其他分组有相同的维度，也可能没有相同的维度。每个分组各自独立地根据自身的规则贡献出一批需要被物化的Cuboid，所有分组贡献的Cuboid的并集就成为了当前Cube中所有需要物化的Cuboid的集合。不同的分组有可能会贡献出相同的Cuboid，构建引擎会察觉到这点，并且保证每一个Cuboid无论在多少个分组中出现，它都只会被物化一次。</p>
<p>对于每个分组内部的维度，用户可以使用如下三种可选的方式定义，它们之间的关系，具体如下。</p>
<p>1）强制维度（Mandatory），如果一个维度被定义为强制维度，那么这个分组产生的所有Cuboid中每一个Cuboid都会包含该维度。每个分组中都可以有0个、1个或多个强制维度。如果根据这个分组的业务逻辑，则相关的查询一定会在过滤条件或分组条件中，因此可以在该分组中把该维度设置为强制维度。</p>
<p><img src="/Kylin/154.png" alt="154"></p>
<p>2）层级维度（Hierarchy），每个层级包含两个或更多个维度。假设一个层级中包含D1，D2…Dn这n个维度，那么在该分组产生的任何Cuboid中， 这n个维度只会以（），（D1），（D1，D2）…（D1，D2…Dn）这n+1种形式中的一种出现。每个分组中可以有0个、1个或多个层级，不同的层级之间不应当有共享的维度。如果根据这个分组的业务逻辑，则多个维度直接存在层级关系，因此可以在该分组中把这些维度设置为层级维度。</p>
<p><img src="/Kylin/155.png" alt="155"></p>
<p>3）联合维度（Joint），每个联合中包含两个或更多个维度，如果某些列形成一个联合，那么在该分组产生的任何Cuboid中，这些联合维度要么一起出现，要么都不出现。每个分组中可以有0个或多个联合，但是不同的联合之间不应当有共享的维度（否则它们可以合并成一个联合）。如果根据这个分组的业务逻辑，多个维度在查询中总是同时出现，则可以在该分组中把这些维度设置为联合维度。</p>
<p><img src="/Kylin/156.png" alt="156"></p>
<p>这些操作可以在Cube Designer的Advanced Setting中的Aggregation Groups区域完成。</p>
<h2 id="Row-Key优化"><a href="#Row-Key优化" class="headerlink" title="Row Key优化"></a>Row Key优化</h2><p>1）被用作过滤的维度放在前边</p>
<p><img src="/Kylin/157.png" alt="157"></p>
<p>2）基数大的维度放在基数小的维度前边</p>
<p><img src="/Kylin/158.png" alt="158"></p>
<h2 id="并发粒度优化"><a href="#并发粒度优化" class="headerlink" title="并发粒度优化"></a>并发粒度优化</h2><p>当Segment中某一个Cuboid的大小超出一定的阈值时，系统会将该Cuboid的数据分片到多个分区中，以实现Cuboid数据读取的并行化，从而优化Cube的查询速度。具体的实现方式如下：构建引擎根据Segment估计的大小，以及参数“kylin.hbase.region.cut”的设置决定Segment在存储引擎中总共需要几个分区来存储，如果存储引擎是HBase，那么分区的数量就对应于HBase中的Region数量。kylin.hbase.region.cut的默认值是5.0，单位是GB，也就是说对于一个大小估计是50GB的Segment，构建引擎会给它分配10个分区。用户还可以通过设置kylin.hbase.region.count.min（默认为1）和kylin.hbase.region.count.max（默认为500）两个配置来决定每个Segment最少或最多被划分成多少个分区。</p>
<h1 id="Kylin-BI工具集成"><a href="#Kylin-BI工具集成" class="headerlink" title="Kylin BI工具集成"></a>Kylin BI工具集成</h1><p>可以与Kylin结合使用的可视化工具很多，例如：</p>
<p>ODBC：与Tableau、Excel、PowerBI等工具集成</p>
<p>JDBC：与Saiku、BIRT等Java工具集成</p>
<p>RestAPI：与JavaScript、Web网页集成</p>
<p>Kylin开发团队还贡献了Zepplin的插件，也可以使用Zepplin来访问Kylin服务。</p>
<h2 id="JDBC"><a href="#JDBC" class="headerlink" title="JDBC"></a>JDBC</h2><p>1）新建项目并导入依赖</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kylin<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kylin-jdbc<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.5.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>2）编码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.vincent;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestKylin</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//Kylin_JDBC 驱动</span></span><br><span class="line">        String KYLIN_DRIVER = <span class="string">&quot;org.apache.kylin.jdbc.Driver&quot;</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//Kylin_URL</span></span><br><span class="line">        String KYLIN_URL = <span class="string">&quot;jdbc:kylin://linux1:7070/FirstProject&quot;</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//Kylin的用户名</span></span><br><span class="line">        String KYLIN_USER = <span class="string">&quot;ADMIN&quot;</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//Kylin的密码</span></span><br><span class="line">        String KYLIN_PASSWD = <span class="string">&quot;KYLIN&quot;</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//添加驱动信息</span></span><br><span class="line">        Class.forName(KYLIN_DRIVER);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取连接</span></span><br><span class="line">        Connection connection = DriverManager.getConnection(KYLIN_URL, KYLIN_USER, KYLIN_PASSWD);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//预编译SQL</span></span><br><span class="line">        PreparedStatement ps = connection.prepareStatement(<span class="string">&quot;SELECT sum(sal) FROM emp group by deptno&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//执行查询</span></span><br><span class="line">        ResultSet resultSet = ps.executeQuery();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//遍历打印</span></span><br><span class="line">        <span class="keyword">while</span> (resultSet.next()) &#123;</span><br><span class="line">            System.out.println(resultSet.getInt(<span class="number">1</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Zepplin"><a href="#Zepplin" class="headerlink" title="Zepplin"></a>Zepplin</h2><p>1）Zepplin安装与启动</p>
<p>（1）将zeppelin-0.8.0-bin-all.tgz上传至Linux</p>
<p>（2）解压zeppelin-0.8.0-bin-all.tgz之/opt/module</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 sorfware]$ tar -zxvf zeppelin-0.8.0-bin-all.tgz -C /opt/module/</span><br></pre></td></tr></table></figure>

<p>（3）修改名称</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 module]$ mv zeppelin-0.8.0-bin-all/ zeppelin</span><br></pre></td></tr></table></figure>

<p>（4）启动</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 zeppelin]$ bin/zeppelin-daemon.sh start</span><br></pre></td></tr></table></figure>

<p>可登录网页查看，web默认端口号为8080，可以设置为7980</p>
<p>2）配置Zepplin支持Kylin</p>
<p>（1）点击右上角anonymous选择Interpreter</p>
]]></content>
      <categories>
        <category>离线计算</category>
      </categories>
  </entry>
  <entry>
    <title>Azkaban</title>
    <url>/Azkaban/</url>
    <content><![CDATA[<h1 id="Azkaban完整配置"><a href="#Azkaban完整配置" class="headerlink" title="Azkaban完整配置"></a>Azkaban完整配置</h1><p>见官网文档：<a href="https://azkaban.readthedocs.io/en/latest/configuration.html">https://azkaban.readthedocs.io/en/latest/configuration.html</a></p>
<h1 id="调度工具对比"><a href="#调度工具对比" class="headerlink" title="调度工具对比"></a>调度工具对比</h1><table>
<thead>
<tr>
<th>特性</th>
<th>Hamake</th>
<th>Oozie</th>
<th>Azkaban</th>
<th>Cascading</th>
</tr>
</thead>
<tbody><tr>
<td>工作流描述语言</td>
<td>XML</td>
<td>XML (xPDL based)</td>
<td>text file with key/value pairs</td>
<td>Java API</td>
</tr>
<tr>
<td>依赖机制</td>
<td>data-driven</td>
<td>explicit</td>
<td>explicit</td>
<td>explicit</td>
</tr>
<tr>
<td>是否要web容器</td>
<td>No</td>
<td>Yes</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>进度跟踪</td>
<td>console/log messages</td>
<td>web page</td>
<td>web page</td>
<td>Java API</td>
</tr>
<tr>
<td>Hadoop job调度支持</td>
<td>no</td>
<td>yes</td>
<td>yes</td>
<td>yes</td>
</tr>
<tr>
<td>运行模式</td>
<td>command line utility</td>
<td>daemon</td>
<td>daemon</td>
<td>API</td>
</tr>
<tr>
<td>Pig支持</td>
<td>yes</td>
<td>yes</td>
<td>yes</td>
<td>yes</td>
</tr>
<tr>
<td>事件通知</td>
<td>no</td>
<td>no</td>
<td>no</td>
<td>yes</td>
</tr>
<tr>
<td>需要安装</td>
<td>no</td>
<td>yes</td>
<td>yes</td>
<td>no</td>
</tr>
<tr>
<td>支持的hadoop版本</td>
<td>0.18+</td>
<td>0.20+</td>
<td>currently unknown</td>
<td>0.18+</td>
</tr>
<tr>
<td>重试支持</td>
<td>no</td>
<td>workflownode evel</td>
<td>yes</td>
<td>yes</td>
</tr>
<tr>
<td>运行任意命令</td>
<td>yes</td>
<td>yes</td>
<td>yes</td>
<td>yes</td>
</tr>
<tr>
<td>Amazon EMR支持</td>
<td>yes</td>
<td>no</td>
<td>currently unknown</td>
<td>yes</td>
</tr>
</tbody></table>
<h2 id="Azkaban与Oozie对比"><a href="#Azkaban与Oozie对比" class="headerlink" title="Azkaban与Oozie对比"></a>Azkaban与Oozie对比</h2><p>对市面上最流行的两种调度器，给出以下详细对比，以供技术选型参考。总体来说，ooize相比azkaban是一个重量级的任务调度系统，功能全面，但配置使用也更复杂。如果可以不在意某些功能的缺失，轻量级调度器azkaban是很不错的候选对象。</p>
<p>详情如下：</p>
<p>1）功能</p>
<p>两者均可以调度mapreduce，pig，java，脚本工作流任务</p>
<p>两者均可以定时执行工作流任务</p>
<p>2）工作流定义</p>
<p>Azkaban使用Properties文件定义工作流</p>
<p>Oozie使用XML文件定义工作流</p>
<p>3）工作流传参</p>
<p>Azkaban支持直接传参，例如${input}</p>
<p>Oozie支持参数和EL表达式，例如${fs:dirSize(myInputDir)}</p>
<p>4）定时执行</p>
<p>Azkaban的定时执行任务是基于时间的</p>
<p>Oozie的定时执行任务基于时间和输入数据</p>
<p>5）资源管理</p>
<p>Azkaban有较严格的权限控制，如用户对工作流进行读/写/执行等操作</p>
<p>Oozie暂无严格的权限控制</p>
<p>6）工作流执行</p>
<p>Azkaban有两种运行模式，分别是solo server mode(executor server和web server部署在同一台节点)和multi server mode(executor server和web server可以部署在不同节点)</p>
<p>Oozie作为工作流服务器运行，支持多用户和多工作流</p>
<p>7）工作流管理</p>
<p>Azkaban支持浏览器以及ajax方式操作工作流</p>
<p>Oozie支持命令行、HTTP REST、Java API、浏览器操作工作流</p>
<h2 id="Azkaban特点"><a href="#Azkaban特点" class="headerlink" title="Azkaban特点"></a>Azkaban特点</h2><p>Azkaban是由Linkedin开源的一个批量工作流任务调度器。用于在一个工作流内以一个特定的顺序运行一组工作和流程。Azkaban定义了一种KV文件格式来建立任务之间的依赖关系，并提供一个易于使用的web用户界面维护和跟踪你的工作流。</p>
<p>它有如下功能特点：</p>
<p>1）Web用户界面</p>
<p>2）方便上传工作流</p>
<p>3）方便设置任务之间的关系</p>
<p>4）调度工作流</p>
<p>5）认证/授权(权限的工作)</p>
<p>6）能够杀死并重新启动工作流</p>
<p>7）模块化和可插拔的插件机制</p>
<p>8）项目工作区</p>
<p>9）工作流和任务的日志记录和审计</p>
<h1 id="集群模式"><a href="#集群模式" class="headerlink" title="集群模式"></a>集群模式</h1><p>1.上传tar包到集群</p>
<p>2.设置mysql</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; CREATE DATABASE azkaban;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mysql&gt; CREATE USER &#x27;azkaban&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;211819&#x27;;</span><br><span class="line">mysql&gt; GRANT SELECT,INSERT,UPDATE,DELETE ON azkaban.* to &#x27;azkaban&#x27;@&#x27;%&#x27; WITH GRANT OPTION;</span><br><span class="line"></span><br><span class="line">mysql&gt; use azkaban;</span><br><span class="line">-- 创建azkaban表</span><br><span class="line">mysql&gt; source /opt/module/azkaban/azkaban-db-3.84.4/create-all-sql-3.84.4.sql</span><br><span class="line">mysql&gt; quit;</span><br><span class="line"></span><br><span class="line">-- 更改mysql表大小</span><br><span class="line">sudo vim /etc/my.cnf</span><br><span class="line"></span><br><span class="line">[mysqld]</span><br><span class="line">...</span><br><span class="line">max_allowed_packet=1024M</span><br><span class="line"></span><br><span class="line">-- 重启</span><br><span class="line">sudo systemctl restart mysqld</span><br></pre></td></tr></table></figure>

<p>3.设置Executor Server并同步节点</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim /opt/module/azkaban/azkaban-exec-server-3.84.4/conf/azkaban.properties</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Azkaban Personalization Settings</span></span><br><span class="line">azkaban.name=vincent</span><br><span class="line">azkaban.label=My Local Azkaban</span><br><span class="line">azkaban.color=#FF3601</span><br><span class="line">azkaban.default.servlet.path=/index</span><br><span class="line">web.resource.dir=web/</span><br><span class="line">default.timezone.id=Asia/Shanghai</span><br><span class="line"><span class="meta">#</span><span class="bash"> Azkaban UserManager class</span></span><br><span class="line">user.manager.class=azkaban.user.XmlUserManager</span><br><span class="line">user.manager.xml.file=conf/azkaban-users.xml</span><br><span class="line"><span class="meta">#</span><span class="bash"> Loader <span class="keyword">for</span> projects</span></span><br><span class="line">executor.global.properties=conf/global.properties</span><br><span class="line">azkaban.project.dir=projects</span><br><span class="line"><span class="meta">#</span><span class="bash"> Velocity dev mode</span></span><br><span class="line">velocity.dev.mode=false</span><br><span class="line"><span class="meta">#</span><span class="bash"> Azkaban Jetty server properties.</span></span><br><span class="line">jetty.use.ssl=false</span><br><span class="line">jetty.maxThreads=25</span><br><span class="line">jetty.port=8081</span><br><span class="line"><span class="meta">#</span><span class="bash"> Where the Azkaban web server is located</span></span><br><span class="line">azkaban.webserver.url=http://linux1:8081</span><br><span class="line"><span class="meta">#</span><span class="bash"> mail settings</span></span><br><span class="line">mail.sender=</span><br><span class="line">mail.host=</span><br><span class="line"><span class="meta">#</span><span class="bash"> User facing web server configurations used to construct the user facing server URLs. They are useful when there is a reverse proxy between Azkaban web servers and users.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> enduser -&gt; myazkabanhost:443 -&gt; proxy -&gt; localhost:8081</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> when this parameters <span class="built_in">set</span> <span class="keyword">then</span> these parameters are used to generate email links.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">if</span> these parameters are not <span class="built_in">set</span> <span class="keyword">then</span> jetty.hostname, and jetty.port(<span class="keyword">if</span> ssl configured jetty.ssl.port) are used.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> azkaban.webserver.external_hostname=myazkabanhost.com</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> azkaban.webserver.external_ssl_port=443</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> azkaban.webserver.external_port=8081</span></span><br><span class="line">job.failure.email=</span><br><span class="line">job.success.email=</span><br><span class="line">lockdown.create.projects=false</span><br><span class="line">cache.directory=cache</span><br><span class="line"><span class="meta">#</span><span class="bash"> JMX stats</span></span><br><span class="line">jetty.connector.stats=true</span><br><span class="line">executor.connector.stats=true</span><br><span class="line"><span class="meta">#</span><span class="bash"> Azkaban plugin settings</span></span><br><span class="line">azkaban.jobtype.plugin.dir=plugins/jobtypes</span><br><span class="line"><span class="meta">#</span><span class="bash"> Azkaban mysql settings by default. Users should configure their own username and password.</span></span><br><span class="line">database.type=mysql</span><br><span class="line">mysql.port=3306</span><br><span class="line">mysql.host=linux1</span><br><span class="line">mysql.database=azkaban</span><br><span class="line">mysql.user=azkaban</span><br><span class="line">mysql.password=211819</span><br><span class="line">mysql.numconnections=100</span><br><span class="line"><span class="meta">#</span><span class="bash"> Azkaban Executor settings</span></span><br><span class="line">executor.maxThreads=50</span><br><span class="line">executor.flow.threads=30</span><br><span class="line">executor.metric.reports=true</span><br><span class="line">executor.metric.milisecinterval.default=60000</span><br></pre></td></tr></table></figure>

<p>4.三台机器上启动executor server</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd /opt/module/azkaban/azkaban-exec-server-3.84.4</span><br><span class="line">bin/start-exec.sh</span><br></pre></td></tr></table></figure>

<p>5.如果在目录下出现executor.port文件，说明启动成功，下面激活executor</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">curl -G &quot;localhost:$(&lt;./executor.port)/executor?action=activate&quot; &amp;&amp; echo</span><br></pre></td></tr></table></figure>

<p>6.设置web server</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim /opt/module/azkaban/azkaban-web-server-3.84.4/conf/azkaban.properties</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">default.timezone.id=Asia/Shanghai</span><br><span class="line">...</span><br><span class="line">database.type=mysql</span><br><span class="line">mysql.port=3306</span><br><span class="line">mysql.host=linux1</span><br><span class="line">mysql.database=azkaban</span><br><span class="line">mysql.user=azkaban</span><br><span class="line">mysql.password=211819</span><br><span class="line">mysql.numconnections=100</span><br><span class="line">...</span><br><span class="line">azkaban.executorselector.filters=StaticRemainingFlowSize,CpuStatus</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim /opt/module/azkaban/azkaban-web-server-3.84.4/conf/azkaban-users.xml</span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">azkaban-users</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">user</span> <span class="attr">groups</span>=<span class="string">&quot;azkaban&quot;</span> <span class="attr">password</span>=<span class="string">&quot;azkaban&quot;</span> <span class="attr">roles</span>=<span class="string">&quot;admin&quot;</span> <span class="attr">username</span>=<span class="string">&quot;azkaban&quot;</span>/&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">user</span> <span class="attr">password</span>=<span class="string">&quot;metrics&quot;</span> <span class="attr">roles</span>=<span class="string">&quot;metrics&quot;</span> <span class="attr">username</span>=<span class="string">&quot;metrics&quot;</span>/&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">user</span> <span class="attr">password</span>=<span class="string">&quot;211819&quot;</span> <span class="attr">roles</span>=<span class="string">&quot;metrics,admin&quot;</span> <span class="attr">username</span>=<span class="string">&quot;vincent&quot;</span>/&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">role</span> <span class="attr">name</span>=<span class="string">&quot;admin&quot;</span> <span class="attr">permissions</span>=<span class="string">&quot;ADMIN&quot;</span>/&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">role</span> <span class="attr">name</span>=<span class="string">&quot;metrics&quot;</span> <span class="attr">permissions</span>=<span class="string">&quot;METRICS&quot;</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">azkaban-users</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>7.linux1上启动web server</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/start-web.sh</span><br></pre></td></tr></table></figure>

<h1 id="workflow"><a href="#workflow" class="headerlink" title="workflow"></a>workflow</h1><h2 id="入门"><a href="#入门" class="headerlink" title="入门"></a>入门</h2><p>1.新建 first.project 文件</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">azkaban</span>-flow-version: <span class="number">2</span>.<span class="number">0</span></span><br></pre></td></tr></table></figure>

<p>2.新建 basic.flow 文件</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">nodes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">jobA</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">echo</span> <span class="string">&quot;This is an echoed text.&quot;</span></span><br></pre></td></tr></table></figure>

<p>3.在webserver新建项目</p>
<p>4.将两个文件打包为zip并上传执行</p>
<h2 id="作业设置"><a href="#作业设置" class="headerlink" title="作业设置"></a>作业设置</h2><p>Azkaban作业以Key:Value形式定义，每个作业有如下配置：</p>
<ol>
<li><p>name：作业名称</p>
</li>
<li><p>type：作业类型（详细类型配置见第3章）</p>
</li>
<li><p>config：和作业类型相关的配置，也以KV值形式</p>
</li>
<li><p>dependsOn：作业依赖</p>
</li>
</ol>
<h2 id="作业依赖案例"><a href="#作业依赖案例" class="headerlink" title="作业依赖案例"></a>作业依赖案例</h2><p>1.修改basic.flow为如下内容</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">nodes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">jobC</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">    <span class="comment"># jobC 依赖 JobA和JobB</span></span><br><span class="line">    <span class="attr">dependsOn:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">jobA</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">jobB</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">echo</span> <span class="string">&quot;I’m JobC&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">jobA</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">echo</span> <span class="string">&quot;I’m JobA&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">jobB</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">echo</span> <span class="string">&quot;I’m JobB&quot;</span></span><br></pre></td></tr></table></figure>

<p>2.打包上传并执行</p>
<h2 id="工作流配置"><a href="#工作流配置" class="headerlink" title="工作流配置"></a>工作流配置</h2><p>可以在工作流配置文件中添加config标签，对工作流进行全局配置，这些配置会应用到该工作流的<strong>所有</strong>作业中。例如，在basic.flow前面添加config：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">config:</span></span><br><span class="line">  <span class="attr">words.to.print:</span> <span class="string">&quot;This is for test!&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">nodes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">jobA</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">echo</span> <span class="string">$&#123;words.to.print&#125;</span></span><br></pre></td></tr></table></figure>

<h2 id="失败重试"><a href="#失败重试" class="headerlink" title="失败重试"></a>失败重试</h2><p>在Job配置中添加两个参数，即可实现任务失败自动重试：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment">#任务重试次数</span></span><br><span class="line"><span class="attr">retries:</span> <span class="number">3</span></span><br><span class="line"><span class="comment">#任务重试延迟</span></span><br><span class="line"><span class="attr">retry.backoff:</span> <span class="number">10000</span></span><br></pre></td></tr></table></figure>

<p>案例：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">nodes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">JobA</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">sh</span> <span class="string">/not_exists.sh</span></span><br><span class="line">      <span class="attr">retries:</span> <span class="number">3</span></span><br><span class="line">      <span class="attr">retry.backoff:</span> <span class="number">10000</span></span><br></pre></td></tr></table></figure>

<p>也可以在Flow全局配置中添加任务失败重试配置，此时重试配置会应用到所有Job。</p>
<p>案例如下：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">config:</span></span><br><span class="line">  <span class="attr">retries:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">retry.backoff:</span> <span class="number">10000</span></span><br><span class="line">  </span><br><span class="line"><span class="attr">nodes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">JobA</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">sh</span> <span class="string">/not_exists.sh</span></span><br></pre></td></tr></table></figure>

<h2 id="内嵌工作流"><a href="#内嵌工作流" class="headerlink" title="内嵌工作流"></a>内嵌工作流</h2><p>工作流定义文件中可以添加子工作流，例如：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">nodes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">jobC</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">    <span class="comment"># jobC 依赖embedded_flow</span></span><br><span class="line">    <span class="attr">dependsOn:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">embedded_flow</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">echo</span> <span class="string">&quot;I’m JobC&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">embedded_flow</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">flow</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">    <span class="comment"># prop: value 是工作流的一个全局配置</span></span><br><span class="line">      <span class="attr">prop:</span> <span class="string">value</span></span><br><span class="line">    <span class="attr">nodes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">jobB</span></span><br><span class="line">      <span class="comment"># noop 什么都不干</span></span><br><span class="line">        <span class="attr">type:</span> <span class="string">noop</span>					</span><br><span class="line">        <span class="attr">dependsOn:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">jobA</span></span><br><span class="line"></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">jobA</span></span><br><span class="line">        <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">        <span class="attr">config:</span></span><br><span class="line">          <span class="attr">command:</span> <span class="string">echo</span> <span class="string">$&#123;prop&#125;</span></span><br></pre></td></tr></table></figure>

<h1 id="作业类型"><a href="#作业类型" class="headerlink" title="作业类型"></a>作业类型</h1><h2 id="命令行类型"><a href="#命令行类型" class="headerlink" title="命令行类型"></a>命令行类型</h2><p>命令行类型是最基本的内建类型，type类型为command，可用的配置为：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">nodes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">jobA</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">echo</span> <span class="string">&quot;Command 1&quot;</span></span><br></pre></td></tr></table></figure>

<h2 id="JavaProcess"><a href="#JavaProcess" class="headerlink" title="JavaProcess"></a>JavaProcess</h2><p>JavaProcess类型可以运行一个自定义主类方法，type类型为javaprocess，可用的配置为：</p>
<p><strong>1.</strong> Xms：最小堆</p>
<p><strong>2.</strong> Xmx：最大堆</p>
<p><strong>3.</strong> java.class：要运行的Java对象，其中必须包含Main方法</p>
<p>案例：</p>
<ol>
<li>首先新建一个Java项目，新建一个包含主方法的主类，内容如下：</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.vincent;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AzTest</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;This is for testing!&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>打包成jar包</p>
<ol>
<li>新建testJava.flow，内容如下</li>
</ol>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">nodes:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">test_java</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">javaprocess</span></span><br><span class="line">  <span class="attr">config:</span></span><br><span class="line">   <span class="attr">Xms:</span> <span class="string">96M</span></span><br><span class="line">   <span class="attr">Xmx:</span> <span class="string">200M</span></span><br><span class="line">   <span class="attr">java.class:</span> <span class="string">com.vincent.AzTest</span></span><br></pre></td></tr></table></figure>

<p>将Jar包、flow文件和project文件打包成zip，上传到集群并执行</p>
<h1 id="条件工作流"><a href="#条件工作流" class="headerlink" title="条件工作流"></a>条件工作流</h1><p>条件工作流功能允许用户根据条件指定是否运行某些作业。条件由先前作业的运行时参数（例如输出）和预定义宏组成。在这些条件下，用户可以在确定作业执行逻辑时获得更大的灵活性。例如，只要父作业之一成功，他们就可以运行当前作业。他们可以在工作流内部实现分支逻辑。</p>
<h2 id="运行时参数"><a href="#运行时参数" class="headerlink" title="运行时参数"></a>运行时参数</h2><p>运行时参数一般指作业的输出，使用时有以下几个条件：</p>
<ol>
<li><p>使用 ${jobName:param} 来定义作业运行时参数的条件</p>
</li>
<li><p>“:” 用于分隔jobName和参数</p>
</li>
<li><p>job运行时，使用参数与条件中的字符串或数字进行比较</p>
</li>
<li><p>用户需要事先将参数的值写入 $JOB_OUTPUT_PROP_FILE</p>
</li>
</ol>
<p>支持的运算符：</p>
<ol>
<li><p>==    等于</p>
</li>
<li><p>!=    不等于</p>
</li>
<li><blockquote>
<p>   大于</p>
</blockquote>
</li>
<li><blockquote>
<p>=    大于等于</p>
</blockquote>
</li>
<li><p>&lt;    小于</p>
</li>
<li><p>&lt;=    小于等于</p>
</li>
<li><p>&amp;&amp;    与</p>
</li>
<li><p>||    或</p>
</li>
<li><p>!    非</p>
</li>
</ol>
<p>案例：</p>
<ol>
<li>新建basic.flow</li>
</ol>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">nodes:</span></span><br><span class="line"> <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">JobA</span></span><br><span class="line">   <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">   <span class="attr">config:</span></span><br><span class="line">     <span class="attr">command:</span> <span class="string">sh</span> <span class="string">/opt/module/write_to_props.sh</span></span><br><span class="line"></span><br><span class="line"> <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">JobB</span></span><br><span class="line">   <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">   <span class="attr">dependsOn:</span></span><br><span class="line">     <span class="bullet">-</span> <span class="string">JobA</span></span><br><span class="line">   <span class="attr">config:</span></span><br><span class="line">     <span class="attr">command:</span> <span class="string">echo</span> <span class="string">&quot;This is JobB.&quot;</span></span><br><span class="line">   <span class="attr">condition:</span> <span class="string">$&#123;JobA:param1&#125;</span> <span class="string">==</span> <span class="string">&quot;AAA&quot;</span></span><br><span class="line"></span><br><span class="line"> <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">JobC</span></span><br><span class="line">   <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">   <span class="attr">dependsOn:</span></span><br><span class="line">     <span class="bullet">-</span> <span class="string">JobA</span></span><br><span class="line">   <span class="attr">config:</span></span><br><span class="line">     <span class="attr">command:</span> <span class="string">echo</span> <span class="string">&quot;This is JobC.&quot;</span></span><br><span class="line">   <span class="attr">condition:</span> <span class="string">$&#123;JobA:param1&#125;</span> <span class="string">==</span> <span class="string">&quot;BBB&quot;</span></span><br></pre></td></tr></table></figure>

<ol start="2">
<li>新建/opt/module/write_to_props.sh</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim /opt/module/write_to_props.sh</span><br><span class="line">内容为：</span><br><span class="line">echo &#x27;&#123;&quot;param1&quot;:&quot;AAA&quot;&#125;&#x27; &gt; $JOB_OUTPUT_PROP_FILE</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>按照我们设定的条件，由于JobA输出为AAA（单引号会将里边字符自动转义），所以会执行JobB分支。上传执行，注意观察分支条件：</li>
</ol>
<h2 id="预定义宏"><a href="#预定义宏" class="headerlink" title="预定义宏"></a>预定义宏</h2><p>预定义宏将会在所有父作业上评估，即YAML文件中的dependsOn部分。可用的预定义宏如下：</p>
<ol>
<li><p>all_success: 全部成功(默认)</p>
</li>
<li><p>all_done：全部完成</p>
</li>
<li><p>all_failed：全部失败</p>
</li>
<li><p>one_success：至少一个成功</p>
</li>
<li><p>one_failed：至少一个失败</p>
</li>
</ol>
<p>案例：</p>
<ol>
<li>修改上个案例的basic.flow:</li>
</ol>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">nodes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">JobA</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">sh</span> <span class="string">/opt/module/write_to_props.sh</span></span><br><span class="line"></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">JobB</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">    <span class="attr">dependsOn:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">JobA</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">echo</span> <span class="string">&quot;This is JobB.&quot;</span></span><br><span class="line">    <span class="attr">condition:</span> <span class="string">$&#123;JobA:param1&#125;</span> <span class="string">==</span> <span class="string">&quot;AAA&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">JobC</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">    <span class="attr">dependsOn:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">JobA</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">echo</span> <span class="string">&quot;This is JobC.&quot;</span></span><br><span class="line">    <span class="attr">condition:</span> <span class="string">$&#123;JobA:param1&#125;</span> <span class="string">==</span> <span class="string">&quot;BBB&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">JobD</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">    <span class="attr">dependsOn:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">JobB</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">JobC</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">echo</span> <span class="string">&quot;This is JobD.&quot;</span></span><br><span class="line">    <span class="attr">condition:</span> <span class="string">one_success</span></span><br><span class="line"></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">JobE</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">    <span class="attr">dependsOn:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">JobB</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">JobC</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">echo</span> <span class="string">&quot;This is JobE.&quot;</span></span><br><span class="line">    <span class="attr">condition:</span> <span class="string">all_success</span></span><br><span class="line"></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">JobF</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">    <span class="attr">dependsOn:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">JobB</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">JobC</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">JobD</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">JobE</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">echo</span> <span class="string">&quot;This is JobF.&quot;</span></span><br><span class="line"><span class="attr">condition:</span> <span class="string">all_done</span></span><br></pre></td></tr></table></figure>

<ol start="2">
<li>打包执行，注意观察任务的执行情况</li>
</ol>
<h1 id="失败报警"><a href="#失败报警" class="headerlink" title="失败报警"></a>失败报警</h1><h2 id="默认邮件报警"><a href="#默认邮件报警" class="headerlink" title="默认邮件报警"></a>默认邮件报警</h2><p>1.在web-server节点（linux1）编辑 /opt/module/azkaban/azkaban-web-server-3.84.4/conf/azkaban.properties，修改如下内容：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">这里设置邮件发送服务器，需要 申请邮箱，且开通stmp服务，以下只是例子</span></span><br><span class="line">mail.sender=vincent@126.com</span><br><span class="line">mail.host=smtp.126.com</span><br><span class="line">mail.user=vincent@126.com</span><br><span class="line">mail.passwd=password</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">这里设置工作流成功或者失败默认向哪里发送服务</span></span><br><span class="line">job.failure.email=vincent@126.com</span><br><span class="line">job.success.email=vincent@126.com</span><br></pre></td></tr></table></figure>

<p>保存并重启web-server。</p>
<p>2.编辑basic.flow，加入如下属性：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">nodes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">jobA</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">command</span></span><br><span class="line">    <span class="attr">config:</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">echo</span> <span class="string">&quot;This is an echoed text.&quot;</span></span><br><span class="line">      <span class="attr">failure.emails:</span> <span class="string">vincent@126.com</span></span><br><span class="line">      <span class="attr">success.emails:</span> <span class="string">vincent@126.com</span></span><br><span class="line">      <span class="attr">notify.emails:</span> <span class="string">vincent@126.com</span> </span><br></pre></td></tr></table></figure>

<h2 id="自定义报警"><a href="#自定义报警" class="headerlink" title="自定义报警"></a>自定义报警</h2><p>有时任务执行失败后邮件报警接收不及时，需要自定义报警装置，比如电话报警。此时需要首先找一个电话通知服务商，比如onealter.com，购买相应服务后，获取通知API。然后进行MailAlter二次开发。</p>
<p>1.新建一个普通的Java项目</p>
<p>2.在项目的lib里添加4个Jar包：</p>
<p>3.新建com.vincent.PhoneAlter类实现azkaban.alert.Alerter</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> azkaban.alert.Alerter;</span><br><span class="line"><span class="keyword">import</span> azkaban.executor.ExecutableFlow;</span><br><span class="line"><span class="keyword">import</span> azkaban.executor.Executor;</span><br><span class="line"><span class="keyword">import</span> azkaban.executor.ExecutorManagerException;</span><br><span class="line"><span class="keyword">import</span> azkaban.sla.SlaOption;</span><br><span class="line"><span class="keyword">import</span> azkaban.utils.Props;</span><br><span class="line"><span class="keyword">import</span> com.google.gson.JsonObject;</span><br><span class="line"><span class="keyword">import</span> org.apache.log4j.Logger;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PhoneAlterter</span> <span class="keyword">implements</span> <span class="title">Alerter</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger logger = Logger.getLogger(PhoneAlterter.class);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String appKey;</span><br><span class="line">    <span class="keyword">private</span> String url;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">PhoneAlterter</span><span class="params">(Props props)</span> </span>&#123;</span><br><span class="line">        appKey = props.getString(<span class="string">&quot;my.alert.appKey&quot;</span>, <span class="string">&quot;&quot;</span>);</span><br><span class="line">        url = props.getString(<span class="string">&quot;my.alert.url&quot;</span>, <span class="string">&quot;&quot;</span>);</span><br><span class="line">        logger.info(<span class="string">&quot;Appkey: &quot;</span> + appKey);</span><br><span class="line">        logger.info(<span class="string">&quot;URL: &quot;</span> + url);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 成功的通知</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> exflow</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">alertOnSuccess</span><span class="params">(ExecutableFlow exflow)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 出现问题的通知</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> exflow</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> extraReasons</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">alertOnError</span><span class="params">(ExecutableFlow exflow, String... extraReasons)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//一般来说网络电话服务都是通过HTTP请求发送的，这里可以调用shell发送HTTP请求</span></span><br><span class="line">        JsonObject alert = <span class="keyword">new</span> JsonObject();</span><br><span class="line">        alert.addProperty(<span class="string">&quot;app&quot;</span>, appKey);</span><br><span class="line">        alert.addProperty(<span class="string">&quot;eventId&quot;</span>, exflow.getId());</span><br><span class="line">        alert.addProperty(<span class="string">&quot;eventType&quot;</span>, <span class="string">&quot;trigger&quot;</span>);</span><br><span class="line">        alert.addProperty(<span class="string">&quot;alarmContent&quot;</span>, exflow.getId() + <span class="string">&quot; fails!&quot;</span>);</span><br><span class="line">        alert.addProperty(<span class="string">&quot;priority&quot;</span>, <span class="string">&quot;2&quot;</span>);</span><br><span class="line">        String[] cmd = <span class="keyword">new</span> String[<span class="number">8</span>];</span><br><span class="line">        cmd[<span class="number">0</span>] = <span class="string">&quot;curl&quot;</span>;</span><br><span class="line">        cmd[<span class="number">1</span>] = <span class="string">&quot;-H&quot;</span>;</span><br><span class="line">        cmd[<span class="number">2</span>] = <span class="string">&quot;Content-type: application/json&quot;</span>;</span><br><span class="line">        cmd[<span class="number">3</span>] = <span class="string">&quot;-X&quot;</span>;</span><br><span class="line">        cmd[<span class="number">4</span>] = <span class="string">&quot;POST&quot;</span>;</span><br><span class="line">        cmd[<span class="number">5</span>] = <span class="string">&quot;-d&quot;</span>;</span><br><span class="line">        cmd[<span class="number">6</span>] = alert.toString();</span><br><span class="line">        cmd[<span class="number">7</span>] = url;</span><br><span class="line">        logger.info(<span class="string">&quot;Sending phone alert!&quot;</span>);</span><br><span class="line">        Runtime.getRuntime().exec(cmd);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 首次出现问题的通知</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> exflow</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">alertOnFirstError</span><span class="params">(ExecutableFlow exflow)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">alertOnSla</span><span class="params">(SlaOption slaOption, String slaMessage)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">alertOnFailedUpdate</span><span class="params">(Executor executor, List&lt;ExecutableFlow&gt; executions, ExecutorManagerException e)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>4.新建/opt/module/azkaban/azkaban-web-server-3.84.4/plugin/alerter/phone-alerter文件夹，并在内部新建conf和lib两个目录</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mkdir -p /opt/module/azkaban/azkaban-web-server-3.84.4/plugins/alerter/phone-alerter/conf /opt/module/azkaban/azkaban-web-server-3.84.4/plugins/alerter/phone-alerter/lib</span><br></pre></td></tr></table></figure>

<p>5.在新建的conf目录里，新建plugin.properties</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">name一定要设置email，用以覆盖默认的邮件报警</span></span><br><span class="line">alerter.name=email</span><br><span class="line">alerter.external.classpaths=lib</span><br><span class="line">alerter.class=com.vincent.PhoneAlterter</span><br><span class="line"><span class="meta">#</span><span class="bash">这两个参数和你使用的AlertAPI有关系</span></span><br><span class="line">my.alert.appKey=cf3e2ce2-40ba-c3cd-1c74-xxxxxxxxxxxx</span><br><span class="line">my.alert.url=http://some.example.url</span><br></pre></td></tr></table></figure>

<p>6.将代码打包，并将Jar上传到/opt/module/azkaban/azkaban-web-server-3.84.4/lib文件夹，并重启web服务。</p>
]]></content>
      <categories>
        <category>工具</category>
      </categories>
  </entry>
  <entry>
    <title>Git</title>
    <url>/Git/</url>
    <content><![CDATA[<p>Git是目前世界上最先进的分布式版本控制系统</p>
<p>功能：版本还原、代码备份、分支管理、协同开发、历史追查、版本记录、权限管理</p>
<h2 id="集中式版本管理"><a href="#集中式版本管理" class="headerlink" title="集中式版本管理"></a>集中式版本管理</h2><p>经典产品： CVS、VSS、SVN</p>
<p>特点：由中央仓库统一管理，结构简单，上手容易！</p>
<p><img src="/Git/16.png" alt="16"></p>
<p>不足：</p>
<p>版本管理的服务器一旦崩溃，硬盘损坏，代码如何恢复？</p>
<p>程序员上传到服务器的代码要求是完整版本，但是程序员开发过程中想做小版本的管理，以便追溯查询，怎么破？</p>
<p>系统正在上线运行，时不时还要修改bug，要增加好几个功能要几个月，如何管理几个版本？</p>
<p>如何管理一个分布在世界各地、互不相识的大型开发团队？</p>
<h2 id="Git的安装"><a href="#Git的安装" class="headerlink" title="Git的安装"></a>Git的安装</h2><p>Git官网：<a href="https://git-scm.com/">https://git-scm.com/</a></p>
<p>GitHub网站:<a href="http://www.github.com/">http://www.github.com</a></p>
<p>命令行工具：Git for windows</p>
<p>下载地址：<a href="https://git-for-windows.github.io/">https://git-for-windows.github.io/</a></p>
<p>在 macOS 上安装：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git --version</span></span><br></pre></td></tr></table></figure>

<p>在 Linux 上安装：</p>
<p> 基于 RPM 的发行版，如 RHEL 或 CentOS</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo dnf install git-all</span></span><br></pre></td></tr></table></figure>

<p>基于 Debian 的发行版上，如 Ubuntu:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo apt install git-all</span></span><br></pre></td></tr></table></figure>

<p>从源代码安装：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo dnf install dh-autoreconf curl-devel expat-devel gettext-devel \</span></span><br><span class="line"><span class="bash">  openssl-devel perl-devel zlib-devel</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo apt-get install dh-autoreconf libcurl4-gnutls-dev libexpat1-dev \</span></span><br><span class="line"><span class="bash">  gettext libz-dev libssl-dev</span></span><br></pre></td></tr></table></figure>

<p>为了添加文档的多种格式（doc、html、info），需要以下附加的依赖：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo dnf install asciidoc xmlto docbook2X</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo apt-get install asciidoc xmlto docbook2x</span></span><br></pre></td></tr></table></figure>

<p>如果你使用基于 Debian 的发行版（Debian/Ubuntu/Ubuntu-derivatives），你也需要 install-info 包：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo apt-get install install-info</span></span><br></pre></td></tr></table></figure>

<p>如果你使用基于 RPM 的发行版（Fedora/RHEL/RHEL衍生版），你还需要 getopt 包 （它已经在基于 Debian</p>
<p>的发行版中预装了）：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo dnf install getopt</span></span><br></pre></td></tr></table></figure>

<p>此外，如果你使用 Fedora/RHEL/RHEL衍生版，那么你需要执行以下命令：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo ln -s /usr/bin/db2x_docbook2texi /usr/bin/docbook2x-texi</span></span><br></pre></td></tr></table></figure>

<p>完成后，你可以使用 Git 来获取 Git 的更新：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git <span class="built_in">clone</span> git://git.kernel.org/pub/scm/git/git.git</span></span><br></pre></td></tr></table></figure>

<h3 id="设置Git账户"><a href="#设置Git账户" class="headerlink" title="设置Git账户"></a>设置Git账户</h3><table>
<thead>
<tr>
<th>命令</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>git config –list</td>
<td>查看所有配置</td>
</tr>
<tr>
<td>git config –list –show-origin</td>
<td>查看所有配置以及所在文件位置</td>
</tr>
<tr>
<td>git config –global user.name xxx</td>
<td>设置git用户名</td>
</tr>
<tr>
<td>git config –global user.email xxx</td>
<td>设置git邮箱</td>
</tr>
<tr>
<td>git init</td>
<td>初始化本地库</td>
</tr>
<tr>
<td>git config core.autocrlf false</td>
<td>取消换行符转换的warning提醒</td>
</tr>
</tbody></table>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git config --global user.name <span class="string">&quot;Vincent&quot;</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> git config --global user.email 18635372669@163.com</span></span><br></pre></td></tr></table></figure>

<h3 id="本地仓库初始化"><a href="#本地仓库初始化" class="headerlink" title="本地仓库初始化"></a>本地仓库初始化</h3><p>1、新建一个本地仓库，也就是一个文件夹。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> /Users/vincent</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mkdir my_project</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> /Users/vincent/my_project</span></span><br></pre></td></tr></table></figure>

<p>2、在此文件夹目录下执行git init命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git init</span></span><br></pre></td></tr></table></figure>

<h2 id="常用Git命令"><a href="#常用Git命令" class="headerlink" title="常用Git命令"></a>常用Git命令</h2><table>
<thead>
<tr>
<th>命令</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>git status</td>
<td>查看本地库的状态(git status -s 简化输出结果)</td>
</tr>
<tr>
<td>git add [file]</td>
<td>多功能命令: 1. 开始跟踪新文件 2. 把已跟踪的文件添加到暂存区 3. 合并时把有冲突的文件标记为已解决状态</td>
</tr>
<tr>
<td>git commit -m “xxx” [file]</td>
<td>将暂存区的文件提交到本地库,-m 后面为修改的说明（注释）</td>
</tr>
</tbody></table>
<p>工作区—–&gt;暂存区—–&gt;本地库</p>
<p>工作区—–&gt;暂存区：git add [file]</p>
<p>暂存区—–&gt;工作区：git rm cached [file]</p>
<p>暂存区—–&gt;本地库：git commit –m “xxx” [file]</p>
<h3 id="忽略文件"><a href="#忽略文件" class="headerlink" title="忽略文件"></a>忽略文件</h3><p>一般我们总会有些文件无需纳入 Git 的管理，也不希望它们总出现在未跟踪文件列表。 </p>
<p>通常都是些自动生成的文 件，比如日志文件，或者编译过程中创建的临时文件等。 </p>
<p>在这种情况下，我们可以创建一个名为 .gitignore 的文件，列出要忽略的文件的模式。</p>
<p>一个.gitignore 的案例:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 忽略所有的 .a 文件</span> </span><br><span class="line">*.a</span><br><span class="line"><span class="meta">#</span><span class="bash"> 但跟踪所有的 lib.a，即便你在前面忽略了 .a 文件</span> </span><br><span class="line">!lib.a</span><br><span class="line"><span class="meta">#</span><span class="bash"> 只忽略当前目录下的 TODO 文件，而不忽略 subdir/TODO. 不递归的忽略</span></span><br><span class="line">/TODO</span><br><span class="line"><span class="meta">#</span><span class="bash"> 忽略任何目录下名为 build 的文件夹 递归的忽略</span></span><br><span class="line">build/</span><br><span class="line"><span class="meta">#</span><span class="bash"> 忽略 doc/notes.txt，但不忽略 doc/server/arch.txt</span> </span><br><span class="line">doc/*.txt</span><br><span class="line"><span class="meta">#</span><span class="bash"> 忽略 doc/ 目录及其所有子目录下的 .pdf 文件</span> </span><br><span class="line">doc/**/*.pdf</span><br></pre></td></tr></table></figure>

<h2 id="版本切换"><a href="#版本切换" class="headerlink" title="版本切换"></a>版本切换</h2><h3 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h3><table>
<thead>
<tr>
<th>命令</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>git log</td>
<td>以完整格式查看本地库状态(查看历史版本)</td>
</tr>
<tr>
<td>git log –pretty=oneline</td>
<td>以单行模式查看本地库状态</td>
</tr>
<tr>
<td>git reset –hard HEAD^</td>
<td>回退一个版本</td>
</tr>
<tr>
<td>git reset –hard HEAD~n</td>
<td>回退N个版本</td>
</tr>
<tr>
<td>git reflog</td>
<td>查看所有操作的历史记录</td>
</tr>
<tr>
<td>git reset –hard [具体版本号，例如：1f9a527等]</td>
<td>回到（回退和前进都行）指定版本号的版本，</td>
</tr>
<tr>
<td>git checkout – [file]</td>
<td>从本地库检出最新文件覆盖工作区的文件(文件还没有提交到暂存区, 否则无效)</td>
</tr>
<tr>
<td>git reset [file]  或者 git restore –staged [file]</td>
<td>从暂存区撤销文件</td>
</tr>
<tr>
<td>git restore <file></file></td>
<td>放弃在工作区的修改(还没有提交到暂存区)</td>
</tr>
<tr>
<td>git rm –cache [file]</td>
<td>撤销对文件的跟踪.</td>
</tr>
</tbody></table>
<h3 id="比较文件"><a href="#比较文件" class="headerlink" title="比较文件"></a>比较文件</h3><p>将工作区中的文件和暂存区进行比较:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git diff &lt;file&gt;</span></span><br></pre></td></tr></table></figure>

<p>将工作区中的文件和本地库当前版本进行比较:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git diff HEAD &lt;file&gt;</span></span><br></pre></td></tr></table></figure>

<p>查看暂存区和本地库最新提交版本的差别:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git diff --cached &lt;file&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="删除操作"><a href="#删除操作" class="headerlink" title="删除操作"></a>删除操作</h3><p>1、删除文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> rm a.txt</span></span><br></pre></td></tr></table></figure>

<p>2、更新删除操作</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git add/rm a.txt</span></span><br></pre></td></tr></table></figure>

<p>3、提交更新</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git commit -m <span class="string">&#x27;delete a.txt&#x27;</span></span></span><br></pre></td></tr></table></figure>

<h2 id="Git的工作机制"><a href="#Git的工作机制" class="headerlink" title="Git的工作机制"></a>Git的工作机制</h2><h3 id="三区"><a href="#三区" class="headerlink" title="三区"></a>三区</h3><p><strong>工作区(Working Directory):就是你电脑本地硬盘目录</strong></p>
<p><strong>本地库(Repository):工作区有个隐藏目录.git，它就是Git的本地版本库</strong></p>
<p><strong>暂存区(stage):一般存放在”git目录”下的index文件（.git/index）中，所以我们把暂存区有时也叫作索引（index）。</strong></p>
<h3 id="git目录"><a href="#git目录" class="headerlink" title=".git目录"></a>.git目录</h3><p>HEAD    git项目当前指针位置</p>
<p>index        git索引文件</p>
<p>config        git配置信息</p>
<p>refs/        标示git项目分支指向那次提交commitId</p>
<p>objects/    git本地仓库对象（commit tree blob类型）</p>
<p>logs/        每次refs的历史记录</p>
<h2 id="分支操作"><a href="#分支操作" class="headerlink" title="分支操作"></a>分支操作</h2><h3 id="分支的概念"><a href="#分支的概念" class="headerlink" title="分支的概念"></a>分支的概念</h3><p>不使用分支，就是人与人之间协作；</p>
<p>使用分支，就是小组与小组之间的协作；</p>
<p>从主干中拉取分支，开发完成，将工作，合并到主干。</p>
<h3 id="常用命令汇总"><a href="#常用命令汇总" class="headerlink" title="常用命令汇总"></a>常用命令汇总</h3><table>
<thead>
<tr>
<th>命令</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>git branch [分支名]</td>
<td>创建分支</td>
</tr>
<tr>
<td>git branch -v</td>
<td>查看分支,可以使用-v参数查看详细信息</td>
</tr>
<tr>
<td>git checkout [分支名]</td>
<td>切换分支</td>
</tr>
<tr>
<td>git merge [分支名]</td>
<td>合并分支；将merge命令中指定的分支合并到当前分支上例如：如果想将dev分支合并到master分支，那么必须在master分支上执行merge命令</td>
</tr>
<tr>
<td>git branch -d[分支名]</td>
<td>删除分支</td>
</tr>
<tr>
<td>git checkout -b [分支名]</td>
<td>新建并切换到当前分支</td>
</tr>
<tr>
<td>git log –oneline –decorate –graph –all</td>
<td>它会输出你的提交历史、各个分支的指向以及项目的分支分叉情况</td>
</tr>
<tr>
<td>git merge –abort</td>
<td>取消合并</td>
</tr>
</tbody></table>
<h2 id="GitHub"><a href="#GitHub" class="headerlink" title="GitHub"></a>GitHub</h2><p><a href="https://github.com/Vincent-ski/bigdata.git">https://github.com/Vincent-ski/bigdata.git</a></p>
<p><a href="mailto:&#x67;&#x69;&#x74;&#x40;&#103;&#105;&#116;&#x68;&#x75;&#98;&#46;&#99;&#111;&#109;">&#x67;&#x69;&#x74;&#x40;&#103;&#105;&#116;&#x68;&#x75;&#98;&#46;&#99;&#111;&#109;</a>:Vincent-ski/bigdata.git</p>
<p>GitHub是一个Git项目托管网站,主要提供基于Git的版本托管服务。</p>
<h3 id="本地库联通GitHub"><a href="#本地库联通GitHub" class="headerlink" title="本地库联通GitHub"></a>本地库联通GitHub</h3><p>1、查看本地是否配置了密钥</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> ~/.ssh</span></span><br></pre></td></tr></table></figure>

<p>2、生成密钥</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ssh-keygen</span></span><br></pre></td></tr></table></figure>

<p>3、将公钥告诉github账户，相当于约定一个接头暗号</p>
<p>4、测试本地仓库和github的联通性：ssh -T <a href="mailto:&#x67;&#105;&#116;&#x40;&#103;&#x69;&#116;&#104;&#x75;&#x62;&#x2e;&#x63;&#111;&#x6d;">&#x67;&#105;&#116;&#x40;&#103;&#x69;&#116;&#104;&#x75;&#x62;&#x2e;&#x63;&#111;&#x6d;</a></p>
<p>5、.ssh文件夹中会多一个文件known_hosts，其中记录了连接的github的ip账号</p>
<h3 id="push"><a href="#push" class="headerlink" title="push"></a>push</h3><p>本地库推送到GitHub</p>
<p>1、准备本地库</p>
<p>2、在GitHub上创建仓库</p>
<p>3、增加远程地址</p>
<p>git remote add  &lt;远端代号&gt;  &lt;远端地址&gt;</p>
<p>&lt;远端代号&gt; 是指远程链接的代号，一般直接用origin作代号，也可以自定义</p>
<p>&lt;远端地址&gt; 默认远程链接的url</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git remote add origin git@github.com:Vincent-ski/bigdata.git</span></span><br></pre></td></tr></table></figure>

<p>4、本地库推送到远程库</p>
<p>git  push  -u  &lt;远端代号&gt;   &lt;本地分支名称&gt;</p>
<p>&lt;远端代号&gt; 是指远程链接的代号</p>
<p>&lt;分支名称&gt;  是指要提交的分支名字，比如master</p>
<p>我们第一次推送master分支时，加上了-u参数，Git不但会把本地的master分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命令。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git push -u origin master</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> git push origin color</span></span><br></pre></td></tr></table></figure>

<p>5、查看远程分支</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git remote -v</span></span><br></pre></td></tr></table></figure>

<h3 id="pull"><a href="#pull" class="headerlink" title="pull"></a>pull</h3><p>如果远程库的版本新于当前库，那么此时为了使当前库和远程库保持一致，可以执行pull命令</p>
<p>git pull &lt;远端代号&gt;  &lt;远端分支名&gt;</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git pull origin master</span></span><br></pre></td></tr></table></figure>

<h3 id="fetch"><a href="#fetch" class="headerlink" title="fetch"></a>fetch</h3><p>从远程库获取更新，但是并不合并</p>
<h3 id="clone"><a href="#clone" class="headerlink" title="clone"></a>clone</h3><p>刚开始做项目的时候，需要从远程库将项目先整到本机</p>
<p>执行命令：</p>
<p>git  clone  &lt;远端地址&gt;  &lt;新项目目录名&gt;</p>
<p>&lt;远端地址&gt; 是指远程链接的地址;</p>
<p>&lt;项目目录名&gt;  是指为克隆的项目在本地新建的目录名称，可以不填，默认是GitHub的项目名;</p>
<p>命令执行完后，会自动为这个远端地址建一个名为origin的代号。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git <span class="built_in">clone</span> https://github.com/Vincent-ski/bigdata.git hahaha</span></span><br></pre></td></tr></table></figure>

<h3 id="fork"><a href="#fork" class="headerlink" title="fork"></a>fork</h3><p>如果其他人，搜索到了你的项目，想对其做一些编辑时，必须先执行fork操作。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="/Git/17.png" alt="17"></p>
]]></content>
      <categories>
        <category>工具</category>
      </categories>
  </entry>
  <entry>
    <title>HDFS</title>
    <url>/HDFS/</url>
    <content><![CDATA[<h1 id="HDFS概述"><a href="#HDFS概述" class="headerlink" title="HDFS概述"></a>HDFS概述</h1><p><img src="/HDFS/38.png" alt="38"></p>
<h1 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h1><p><img src="/HDFS/39.png" alt="39"></p>
<p><img src="/HDFS/40.png" alt="40"></p>
<h1 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h1><p><img src="/HDFS/41.png" alt="41"></p>
<p><img src="/HDFS/42.png" alt="42"></p>
<h1 id="文件块大小"><a href="#文件块大小" class="headerlink" title="文件块大小"></a>文件块大小</h1><p><img src="/HDFS/43.png" alt="43"></p>
<p><img src="/HDFS/44.png" alt="44"></p>
<h1 id="Shell操作"><a href="#Shell操作" class="headerlink" title="Shell操作"></a>Shell操作</h1><p>bin/hadoop fs 具体命令   </p>
<p>bin/hdfs dfs 具体命令</p>
<p>两个是完全相同的。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">//-help</span><br><span class="line">hadoop fs -help rm</span><br><span class="line"></span><br><span class="line">//-ls</span><br><span class="line">hadoop fs -ls /</span><br><span class="line"></span><br><span class="line">//-mkdir</span><br><span class="line">hadoop fs -mkdir -p /sanguo/shuguo</span><br><span class="line"></span><br><span class="line">//-moveFromLocal	从本地剪切粘贴到HDFS</span><br><span class="line">hadoop fs -moveFromLocal ./kongming.txt  /sanguo/shuguo</span><br><span class="line"></span><br><span class="line">//-appendToFile	追加一个文件到已经存在的文件末尾</span><br><span class="line">hadoop fs -appendToFile liubei.txt /sanguo/shuguo/kongming.txt</span><br><span class="line"></span><br><span class="line">//-cat 显示文件内容</span><br><span class="line">hadoop fs -cat /sanguo/shuguo/kongming.txt</span><br><span class="line"></span><br><span class="line">//-chgrp 、-chmod、-chown Linux文件系统中的用法一样，修改文件所属权限</span><br><span class="line">hadoop fs	-chmod	666	/sanguo/shuguo/kongming.txt</span><br><span class="line">hadoop fs	-chown	vincent:vincent	/sanguo/shuguo/kongming.txt</span><br><span class="line"></span><br><span class="line">//-copyFromLocal	从本地文件系统中拷贝文件到HDFS路径去</span><br><span class="line">hadoop fs -copyFromLocal README.txt /</span><br><span class="line"></span><br><span class="line">//-put 等同于copyFromLocal</span><br><span class="line">hadoop fs -put ./zaiyiqi.txt /user/vincent/test/</span><br><span class="line"></span><br><span class="line">//-copyToLocal 从HDFS拷贝到本地</span><br><span class="line">hadoop fs -copyToLocal /sanguo/shuguo/kongming.txt ./</span><br><span class="line"></span><br><span class="line">//-get 等同于copyToLocal，就是从HDFS下载文件到本地</span><br><span class="line">hadoop fs -get /sanguo/shuguo/kongming.txt ./</span><br><span class="line"></span><br><span class="line">//-cp 从HDFS的一个路径拷贝到HDFS的另一个路径</span><br><span class="line">hadoop fs -cp /sanguo/shuguo/kongming.txt /zhuge.txt</span><br><span class="line"></span><br><span class="line">//-mv 在HDFS目录中移动文件</span><br><span class="line">hadoop fs -mv /zhuge.txt /sanguo/shuguo/</span><br><span class="line"></span><br><span class="line">//-getmerge 合并下载多个文件，比如HDFS的目录 /user/vincent/test下有多个文件:log1, log2,log3,...</span><br><span class="line">hadoop fs -getmerge /user/vincent/test/* ./zaiyiqi.txt</span><br><span class="line"></span><br><span class="line">//-tail 显示一个文件的末尾</span><br><span class="line">hadoop fs -tail /sanguo/shuguo/kongming.txt</span><br><span class="line"></span><br><span class="line">//-rm 删除文件或文件夹</span><br><span class="line">hadoop fs -rm /user/vincent/test/jinlian2.txt</span><br><span class="line"></span><br><span class="line">//-rmdir 删除空目录</span><br><span class="line">hadoop fs -rmdir /test</span><br><span class="line"></span><br><span class="line">//-du 统计文件夹的大小信息</span><br><span class="line">hadoop fs -du -s -h /user/vincent/test</span><br><span class="line"></span><br><span class="line">//-setrep 设置HDFS中文件的副本数量</span><br><span class="line">hadoop fs -setrep 5 /README.txt			//README.txt 存了5份</span><br><span class="line"></span><br><span class="line">//判断文件、路径是否存在 参数有-e -f等</span><br><span class="line">hadoop fs -test /aaa/bbb</span><br></pre></td></tr></table></figure>

<h1 id="HDFS客户端权限"><a href="#HDFS客户端权限" class="headerlink" title="HDFS客户端权限"></a>HDFS客户端权限</h1><p>hadoop默认情况下开启了权限检查，且默认使用dir.who作为http访问的静态用户,因此可通过关闭权限检查或者配置http访问的静态用户为vincent，二选一即可。</p>
<p>在core-site.xml中修改http访问的静态用户为vincent：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.http.staticuser.user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>vincent<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>在hdfs-site.xml中关闭权限检查：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h1 id="HDFS客户端操作"><a href="#HDFS客户端操作" class="headerlink" title="HDFS客户端操作"></a>HDFS客户端操作</h1><h2 id="准备操作"><a href="#准备操作" class="headerlink" title="准备操作"></a>准备操作</h2><p>1）配置Hadoop环境变量</p>
<p>2）创建maven</p>
<p>3）导入依赖</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.12<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.logging.log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j-slf4j-impl<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.12.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>项目的src/main/resources目录下，新建一个文件，命名为“log4j2.xml”，在文件中填入：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">Configuration</span> <span class="attr">status</span>=<span class="string">&quot;error&quot;</span> <span class="attr">strict</span>=<span class="string">&quot;true&quot;</span> <span class="attr">name</span>=<span class="string">&quot;XMLConfig&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">Appenders</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 类型名为Console，名称为必须属性 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">Appender</span> <span class="attr">type</span>=<span class="string">&quot;Console&quot;</span> <span class="attr">name</span>=<span class="string">&quot;STDOUT&quot;</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!-- 布局为PatternLayout的方式，</span></span><br><span class="line"><span class="comment">            输出样式为[INFO] [2018-01-22 17:34:01][org.test.Console]I&#x27;m here --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">Layout</span> <span class="attr">type</span>=<span class="string">&quot;PatternLayout&quot;</span></span></span><br><span class="line"><span class="tag">                    <span class="attr">pattern</span>=<span class="string">&quot;[%p] [%d&#123;yyyy-MM-dd HH:mm:ss&#125;][%c&#123;10&#125;]%m%n&quot;</span> /&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">Appender</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;/<span class="name">Appenders</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">Loggers</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 可加性为false --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">Logger</span> <span class="attr">name</span>=<span class="string">&quot;test&quot;</span> <span class="attr">level</span>=<span class="string">&quot;info&quot;</span> <span class="attr">additivity</span>=<span class="string">&quot;false&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">AppenderRef</span> <span class="attr">ref</span>=<span class="string">&quot;STDOUT&quot;</span> /&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">Logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!-- root loggerConfig设置 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">Root</span> <span class="attr">level</span>=<span class="string">&quot;info&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">AppenderRef</span> <span class="attr">ref</span>=<span class="string">&quot;STDOUT&quot;</span> /&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">Root</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">Loggers</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">Configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>4）创建包名 com.vincent.hdfs</p>
<p>5）创建HdfsClient类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HdfsClient</span></span>&#123;	</span><br><span class="line">   <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testHdfsClient</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">//1. 创建HDFS客户端对象,传入uri， configuration , user</span></span><br><span class="line">        FileSystem fileSystem =</span><br><span class="line">                FileSystem.get(URI.create(<span class="string">&quot;hdfs://linux1:9820&quot;</span>), <span class="keyword">new</span> Configuration(), <span class="string">&quot;vincent&quot;</span>);</span><br><span class="line">        <span class="comment">//2. 操作集群</span></span><br><span class="line">        <span class="comment">// 例如：在集群的/目录下创建 testHDFS目录</span></span><br><span class="line">        fileSystem.mkdirs(<span class="keyword">new</span> Path(<span class="string">&quot;/testHDFS&quot;</span>));</span><br><span class="line">        <span class="comment">//3. 关闭资源</span></span><br><span class="line">        fileSystem.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="API操作"><a href="#API操作" class="headerlink" title="API操作"></a>API操作</h2><h3 id="HDFS文件上传（测试参数优先级）"><a href="#HDFS文件上传（测试参数优先级）" class="headerlink" title="HDFS文件上传（测试参数优先级）"></a>HDFS文件上传（测试参数优先级）</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testCopyFromLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 1 获取文件系统</span></span><br><span class="line">		Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    <span class="comment">// 设置副本数为2个</span></span><br><span class="line">		configuration.set(<span class="string">&quot;dfs.replication&quot;</span>, <span class="string">&quot;2&quot;</span>);</span><br><span class="line">		FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">&quot;hdfs://linux1:9820&quot;</span>), configuration, <span class="string">&quot;vincent&quot;</span>);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 2 上传文件</span></span><br><span class="line">		fs.copyFromLocalFile(<span class="keyword">new</span> Path(<span class="string">&quot;e:/banzhang.txt&quot;</span>), <span class="keyword">new</span> Path(<span class="string">&quot;/banzhang.txt&quot;</span>));</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 3 关闭资源</span></span><br><span class="line">		fs.close();</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在项目的resources中新建hdfs-site.xml文件，并将如下内容拷贝进去，再次测试</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>参数优先级：</p>
<p>（1）客户端代码中设置的值 &gt;（2）ClassPath下的用户自定义配置文件 &gt;（3）然后是服务器的自定义配置(xxx-site.xml) &gt;（4）服务器的默认配置(xxx-default.xml)</p>
<h3 id="HDFS文件下载"><a href="#HDFS文件下载" class="headerlink" title="HDFS文件下载"></a>HDFS文件下载</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testCopyToLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 1 获取文件系统</span></span><br><span class="line">		Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">		FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">&quot;hdfs://linux1:9820&quot;</span>), configuration, <span class="string">&quot;vincent&quot;</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 2 执行下载操作</span></span><br><span class="line">		<span class="comment">// boolean delSrc 指是否将原文件删除</span></span><br><span class="line">		<span class="comment">// Path src 指要下载的文件路径</span></span><br><span class="line">		<span class="comment">// Path dst 指将文件下载到的路径</span></span><br><span class="line">		<span class="comment">// boolean useRawLocalFileSystem 是否开启文件校验</span></span><br><span class="line">		fs.copyToLocalFile(<span class="keyword">false</span>, <span class="keyword">new</span> Path(<span class="string">&quot;/banzhang.txt&quot;</span>), <span class="keyword">new</span> Path(<span class="string">&quot;e:/banhua.txt&quot;</span>), <span class="keyword">true</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 3 关闭资源</span></span><br><span class="line">		fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="HDFS文件夹删除"><a href="#HDFS文件夹删除" class="headerlink" title="HDFS文件夹删除"></a>HDFS文件夹删除</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testDelete</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 1 获取文件系统</span></span><br><span class="line">	Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">	FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">&quot;hdfs://linux1:9820&quot;</span>), configuration, <span class="string">&quot;vincent&quot;</span>);</span><br><span class="line">		</span><br><span class="line">	<span class="comment">// 2 执行删除</span></span><br><span class="line">	fs.delete(<span class="keyword">new</span> Path(<span class="string">&quot;/0213/&quot;</span>), <span class="keyword">true</span>);</span><br><span class="line">		</span><br><span class="line">	<span class="comment">// 3 关闭资源</span></span><br><span class="line">	fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="HDFS文件名更改-移动"><a href="#HDFS文件名更改-移动" class="headerlink" title="HDFS文件名更改/移动"></a>HDFS文件名更改/移动</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testRename</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 1 获取文件系统</span></span><br><span class="line">	Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">	FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">&quot;hdfs://linux1:9820&quot;</span>), configuration, <span class="string">&quot;vincent&quot;</span>); </span><br><span class="line">		</span><br><span class="line">	<span class="comment">// 2 修改文件名称</span></span><br><span class="line">	fs.rename(<span class="keyword">new</span> Path(<span class="string">&quot;/banzhang.txt&quot;</span>), <span class="keyword">new</span> Path(<span class="string">&quot;/banhua.txt&quot;</span>));</span><br><span class="line">		</span><br><span class="line">	<span class="comment">// 3 关闭资源</span></span><br><span class="line">	fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="HDFS文件详情查看"><a href="#HDFS文件详情查看" class="headerlink" title="HDFS文件详情查看"></a>HDFS文件详情查看</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testListFiles</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 1获取文件系统</span></span><br><span class="line">	Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">	FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">&quot;hdfs://linux1:9820&quot;</span>), configuration, <span class="string">&quot;vincent&quot;</span>); </span><br><span class="line">		</span><br><span class="line">	<span class="comment">// 2 获取文件详情</span></span><br><span class="line">	RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(<span class="keyword">new</span> Path(<span class="string">&quot;/&quot;</span>), <span class="keyword">true</span>);</span><br><span class="line">		</span><br><span class="line">	<span class="keyword">while</span>(listFiles.hasNext())&#123;</span><br><span class="line">		LocatedFileStatus status = listFiles.next();</span><br><span class="line">			</span><br><span class="line">		<span class="comment">// 输出详情</span></span><br><span class="line">		<span class="comment">// 文件名称</span></span><br><span class="line">		System.out.println(status.getPath().getName());</span><br><span class="line">		<span class="comment">// 长度</span></span><br><span class="line">		System.out.println(status.getLen());</span><br><span class="line">		<span class="comment">// 权限</span></span><br><span class="line">		System.out.println(status.getPermission());</span><br><span class="line">		<span class="comment">// 分组</span></span><br><span class="line">		System.out.println(status.getGroup());</span><br><span class="line">			</span><br><span class="line">		<span class="comment">// 获取存储的块信息</span></span><br><span class="line">		BlockLocation[] blockLocations = status.getBlockLocations();</span><br><span class="line">			</span><br><span class="line">		<span class="keyword">for</span> (BlockLocation blockLocation : blockLocations) &#123;</span><br><span class="line">				</span><br><span class="line">			<span class="comment">// 获取块存储的主机节点</span></span><br><span class="line">			String[] hosts = blockLocation.getHosts();</span><br><span class="line">				</span><br><span class="line">			<span class="keyword">for</span> (String host : hosts) &#123;</span><br><span class="line">				System.out.println(host);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">			</span><br><span class="line">		System.out.println(<span class="string">&quot;-----------漂亮的分割线----------&quot;</span>);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3 关闭资源</span></span><br><span class="line">fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="HDFS文件和文件夹判断"><a href="#HDFS文件和文件夹判断" class="headerlink" title="HDFS文件和文件夹判断"></a>HDFS文件和文件夹判断</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testListStatus</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, URISyntaxException</span>&#123;</span><br><span class="line">        </span><br><span class="line">    <span class="comment">// 1 获取文件配置信息</span></span><br><span class="line">    Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">    FileSystem fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">&quot;hdfs://linux1:9820&quot;</span>), configuration, <span class="string">&quot;vincent&quot;</span>);</span><br><span class="line">        </span><br><span class="line">    <span class="comment">// 2 判断是文件还是文件夹</span></span><br><span class="line">    FileStatus[] listStatus = fs.listStatus(<span class="keyword">new</span> Path(<span class="string">&quot;/&quot;</span>));</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">for</span> (FileStatus fileStatus : listStatus) &#123;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 如果是文件</span></span><br><span class="line">        <span class="keyword">if</span> (fileStatus.isFile()) &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;f:&quot;</span>+fileStatus.getPath().getName());</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;d:&quot;</span>+fileStatus.getPath().getName());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">        </span><br><span class="line">    <span class="comment">// 3 关闭资源</span></span><br><span class="line">    fs.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="HDFS的数据流"><a href="#HDFS的数据流" class="headerlink" title="HDFS的数据流"></a>HDFS的数据流</h1><h2 id="写入数据"><a href="#写入数据" class="headerlink" title="写入数据"></a>写入数据</h2><p><img src="/HDFS/45.png" alt="45"></p>
<p>1）客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。</p>
<p>2）NameNode返回是否可以上传。</p>
<p>3）客户端请求第一个 Block上传到哪几个DataNode服务器上。</p>
<p>4）NameNode返回3个DataNode节点，分别为dn1、dn2、dn3。</p>
<p>5）客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。</p>
<p>6）dn1、dn2、dn3逐级应答客户端。</p>
<p>7）客户端开始往dn1上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。</p>
<p>8）当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器。（重复执行3-7步）。</p>
<h3 id="网络拓扑-节点距离计算"><a href="#网络拓扑-节点距离计算" class="headerlink" title="网络拓扑-节点距离计算"></a>网络拓扑-节点距离计算</h3><p><strong>在HDFS写数据的过程中，NameNode会选择距离待上传数据最近距离的DataNode接收数据</strong>。那么这个最近距离怎么计算呢？</p>
<p>节点距离：两个节点到达最近的共同祖先的距离总和。</p>
<p><img src="/HDFS/46.png" alt="46"></p>
<h3 id="副本节点选择"><a href="#副本节点选择" class="headerlink" title="副本节点选择"></a>副本节点选择</h3><p><img src="/HDFS/47.png" alt="47"></p>
<h3 id="机架感知"><a href="#机架感知" class="headerlink" title="机架感知"></a>机架感知</h3><p>分布式的集群通常包含非常多的机器，由于受到机架槽位和交换机网口的限制，通常大型的分布式集群都会跨好几个机架，由多个机架上的机器共同组成一个分布式集群。机架内的机器之间的网络速度通常都会高于跨机架机器之间的网络速度，并且机架之间机器的网络通信通常受到上层交换机间网络带宽的限制。</p>
<p><strong>Hadoop在设计时考虑到数据的安全与高效，数据文件默认在HDFS上存放三份，存储策略为:</strong></p>
<p>第一个block副本放在<strong>客户端所在的数据节点里</strong>（如果客户端不在集群范围内，则从整个集群中随机选择一个合适的数据节点来存放）。</p>
<p>第二个副本放置在<strong>与第一个副本所在节点相同机架内的其它数据节点上</strong></p>
<p>第三个副本放置在<strong>不同机架的节点上</strong></p>
<p>这样如果本地数据损坏，节点可以从同一机架内的相邻节点拿到数据，速度肯定比从跨机架节点上拿数据要快；<br>同时，如果整个机架的网络出现异常，也能保证在其它机架的节点上找到数据。<br>为了降低整体的带宽消耗和读取延时，HDFS会尽量让读取程序读取离它最近的副本。<br>如果在读取程序的同一个机架上有一个副本，那么就读取该副本。<br>如果一个HDFS集群跨越多个数据中心，那么客户端也将首先读本地数据中心的副本。<br>那么Hadoop是如何确定任意两个节点是位于同一机架，还是跨机架的呢？答案就是机架感知。</p>
<p><strong>默认情况下，hadoop的机架感知是没有被启用的</strong>。所有的机器hadoop都默认在同一个默认的机架下，名为 “/default-rack”，这种情况下，任何一台datanode机器，不管物理上是否属于同一个机架，都会被认为是在同一个机架下，此时，就很容易出现增添机架间网络负载的情况。因为此时hadoop集群的HDFS在选机器的时候，是随机选择的，也就是说，<br>很有可能在写数据时，hadoop将第一块数据block1写到了rack1上，然后随机的选择下将block2写入到了rack2下，<br>此时两个rack之间产生了数据传输的流量，再接下来，在随机的情况下，又将block3重新又写回了rack1，此时，两个rack之间又产生了一次数据流量。</p>
<p>在job处理的数据量非常的大，或者往hadoop推送的数据量非常大的时候，这种情况会造成rack之间的网络流量成倍的上升，成为性能的瓶颈，<br>进而影响作业的性能以至于整个集群的服务。</p>
<p><strong>配置</strong></p>
<p>默认情况下，namenode启动时候日志是这样的：</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">INFO org<span class="selector-class">.apache</span><span class="selector-class">.hadoop</span><span class="selector-class">.net</span><span class="selector-class">.NetworkTopology</span>: Adding <span class="selector-tag">a</span> new node: /default-rack/ <span class="number">172.16</span>.<span class="number">145.35</span>:<span class="number">50010</span></span><br></pre></td></tr></table></figure>

<p>每个IP 对应的机架ID都是 /default-rack ，说明hadoop的机架感知没有被启用。<br>要将hadoop机架感知的功能启用，配置非常简单，在 NameNode所在节点的/etc/hadoop/conf下的core-site.xml配置文件中配置一个选项:</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>topology.script.file.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>/etc/hadoop/conf/RackAware.py<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><strong>这个配置选项的value指定为一个可执行程序，通常为一个脚本，该脚本接受一个参数，输出一个值。</strong><br>接受的参数通常为某台datanode机器的ip地址，而输出的值通常为该ip地址对应的datanode所在的rack，例如”/rack1”。<br>Namenode启动时，会判断该配置选项是否为空，如果非空，则表示已经启用机架感知的配置，此时namenode会根据配置寻找该脚本，<br>并在接收到每一个datanode的heartbeat时，将该datanode的ip地址作为参数传给该脚本运行，并将得到的输出作为该datanode所属的机架ID，保存到内存的一个map中。</p>
<p>脚本：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/usr/bin/python</span></span><br><span class="line"><span class="meta">#</span><span class="bash">-*-coding:UTF-8 -*-</span></span><br><span class="line">import sys</span><br><span class="line"> </span><br><span class="line">rack = &#123;&quot;NN01&quot;:&quot;rack2&quot;,</span><br><span class="line">        &quot;NN02&quot;:&quot;rack3&quot;,</span><br><span class="line">        &quot;DN01&quot;:&quot;rack4&quot;,</span><br><span class="line">        &quot;DN02&quot;:&quot;rack4&quot;,</span><br><span class="line">        &quot;DN03&quot;:&quot;rack1&quot;,</span><br><span class="line">        &quot;DN04&quot;:&quot;rack3&quot;,</span><br><span class="line">        &quot;DN05&quot;:&quot;rack1&quot;,</span><br><span class="line">        &quot;DN06&quot;:&quot;rack4&quot;,</span><br><span class="line">        &quot;DN07&quot;:&quot;rack1&quot;,</span><br><span class="line">        &quot;DN08&quot;:&quot;rack2&quot;,</span><br><span class="line">        &quot;DN09&quot;:&quot;rack1&quot;,</span><br><span class="line">        &quot;DN10&quot;:&quot;rack2&quot;,</span><br><span class="line">        &quot;172.16.145.32&quot;:&quot;rack2&quot;,</span><br><span class="line">        &quot;172.16.145.33&quot;:&quot;rack3&quot;,</span><br><span class="line">        &quot;172.16.145.34&quot;:&quot;rack4&quot;,</span><br><span class="line">        &quot;172.16.145.35&quot;:&quot;rack4&quot;,</span><br><span class="line">        &quot;172.16.145.36&quot;:&quot;rack1&quot;,</span><br><span class="line">        &quot;172.16.145.37&quot;:&quot;rack3&quot;,</span><br><span class="line">        &quot;172.16.145.38&quot;:&quot;rack1&quot;,</span><br><span class="line">        &quot;172.16.145.39&quot;:&quot;rack4&quot;,</span><br><span class="line">        &quot;172.16.145.40&quot;:&quot;rack1&quot;,</span><br><span class="line">        &quot;172.16.145.41&quot;:&quot;rack2&quot;,</span><br><span class="line">        &quot;172.16.145.42&quot;:&quot;rack1&quot;,</span><br><span class="line">        &quot;172.16.145.43&quot;:&quot;rack2&quot;,</span><br><span class="line">        &#125;</span><br><span class="line"> </span><br><span class="line">if __name__==&quot;__main__&quot;:</span><br><span class="line">    print &quot;/&quot; + rack.get(sys.argv[1],&quot;rack0&quot;)</span><br></pre></td></tr></table></figure>

<p>这样配置后，namenode启动时候日志是这样的：</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">INFO org<span class="selector-class">.apache</span><span class="selector-class">.hadoop</span><span class="selector-class">.net</span><span class="selector-class">.NetworkTopology</span>: Adding <span class="selector-tag">a</span> new node: /rack4/ <span class="number">172.16</span>.<span class="number">145.35</span>:<span class="number">50010</span></span><br></pre></td></tr></table></figure>

<p>说明hadoop的机架感知已经被启用了。<br>查看HADOOP机架信息命令: <strong>hdfs  dfsadmin  -printTopology</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[hadoop@NN01 hadoop-hdfs]$ hdfs dfsadmin -printTopology</span><br><span class="line">Rack: /rack1</span><br><span class="line">   172.16.145.36:50010 (DN03)</span><br><span class="line">   172.16.145.38:50010 (DN05)</span><br><span class="line">   172.16.145.40:50010 (DN07)</span><br><span class="line">   172.16.145.42:50010 (DN09)</span><br><span class="line">   172.16.145.44:50010 (DN11)</span><br><span class="line">   172.16.145.54:50010 (DN17)</span><br><span class="line">   172.16.145.56:50010 (DN19)</span><br><span class="line">   172.16.145.58:50010 (DN21)</span><br><span class="line"> </span><br><span class="line">Rack: /rack2</span><br><span class="line">   172.16.145.41:50010 (DN08)</span><br><span class="line">   172.16.145.43:50010 (DN10)</span><br><span class="line">   172.16.145.45:50010 (DN12)</span><br><span class="line">   172.16.145.60:50010 (DN23)</span><br><span class="line">   172.16.145.62:50010 (DN25)</span><br><span class="line"> </span><br><span class="line">Rack: /rack3</span><br><span class="line">   172.16.145.37:50010 (DN04)</span><br><span class="line">   172.16.145.51:50010 (DN14)</span><br><span class="line">   172.16.145.53:50010 (DN16)</span><br><span class="line">   172.16.145.55:50010 (DN18)</span><br><span class="line">   172.16.145.57:50010 (DN20)</span><br><span class="line"> </span><br><span class="line">Rack: /rack4</span><br><span class="line">   172.16.145.34:50010 (DN01)</span><br><span class="line">   172.16.145.35:50010 (DN02)</span><br><span class="line">   172.16.145.39:50010 (DN06)</span><br><span class="line">   172.16.145.50:50010 (DN13)</span><br><span class="line">   172.16.145.52:50010 (DN15)</span><br><span class="line">   172.16.145.59:50010 (DN22)</span><br><span class="line">   172.16.145.61:50010 (DN24)</span><br></pre></td></tr></table></figure>

<p><strong>hdfs 三个副本的这种存放策略减少了机架间的数据传输，提高了写操作的效率。机架的错误远远比节点的错误少，所以这种策略不会影响到数据的可靠性和可用性。与此同时，因为数据块只存放在两个不同的机架上，所以此策略减少了读取数据时需要的网络传输总带宽。在这种策略下，副本并不是均匀的分布在不同的机架上，这种策略在不损害数据可靠性和读取性能的情况下改进了写的性能。</strong></p>
<h2 id="读出数据"><a href="#读出数据" class="headerlink" title="读出数据"></a>读出数据</h2><p><img src="/HDFS/48.png" alt="48"></p>
<p>1）客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。</p>
<p>2）挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。</p>
<p>3）DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）。</p>
<p>4）客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。</p>
<h1 id="NameNode和SecondaryNameNode"><a href="#NameNode和SecondaryNameNode" class="headerlink" title="NameNode和SecondaryNameNode"></a>NameNode和SecondaryNameNode</h1><h2 id="NN和2NN工作机制"><a href="#NN和2NN工作机制" class="headerlink" title="NN和2NN工作机制"></a>NN和2NN工作机制</h2><p>思考：<strong>NameNode中的元数据是存储在哪里的</strong>？</p>
<p>首先，我们做个假设，如果存储在NameNode节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。因此产生在磁盘中备份元数据的Fsimage。</p>
<p>这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新Fsimage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦NameNode节点断电，就会产生数据丢失。因此，引入<strong>Edits文件(只进行追加操作，效率很高)<strong>。</strong>每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中</strong>。这样，一旦NameNode节点断电，可以通过Fsimage和Edits的合并，合成元数据。</p>
<p>但是，如果长时间添加数据到Edits中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要<strong>定期进行Fsimage和Edits的合并</strong>，如果这个操作由NameNode节点完成，又会效率过低。因此，引入一个新的节点<strong>SecondaryNamenode，专门用于FsImage和Edits的合并</strong>。</p>
<p><img src="/HDFS/49.png" alt="49"></p>
<ol>
<li>第一阶段：NameNode启动</li>
</ol>
<p>（1）第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。</p>
<p>（2）客户端对元数据进行增删改的请求。</p>
<p>（3）NameNode记录操作日志，更新滚动日志。</p>
<p>（4）NameNode在内存中对元数据进行增删改。</p>
<ol start="2">
<li>第二阶段：Secondary NameNode工作</li>
</ol>
<p>（1）Secondary NameNode询问NameNode是否需要CheckPoint。直接带回NameNode是否检查结果。</p>
<p>（2）Secondary NameNode请求执行CheckPoint。</p>
<p>（3）NameNode滚动正在写的Edits日志。</p>
<p>（4）将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。</p>
<p>（5）Secondary NameNode加载编辑日志和镜像文件到内存，并合并。</p>
<p>（6）生成新的镜像文件fsimage.chkpoint。</p>
<p>（7）拷贝fsimage.chkpoint到NameNode。</p>
<p>（8）NameNode将fsimage.chkpoint重新命名成fsimage。</p>
<p>NN和2NN工作机制详解：</p>
<p><strong>Fsimage：NameNode内存中元数据序列化后形成的文件。</strong><br><strong>Edits：记录客户端更新元数据信息的每一步操作（可通过Edits运算出元数据）。</strong></p>
<p>NameNode启动时，先滚动Edits并生成一个空的edits.inprogress，然后加载Edits和Fsimage到内存中，此时NameNode内存就持有最新的元数据信息。Client开始对NameNode发送元数据的增删改的请求，这些请求的操作首先会被记录到edits.inprogress中（<strong>查询元数据的操作不会被记录在Edits中，因为查询操作不会更改元数据信息</strong>），如果此时NameNode挂掉，重启后会从Edits中读取元数据的信息。然后，NameNode会在内存中执行元数据的增删改的操作。<br>由于Edits中记录的操作会越来越多，Edits文件会越来越大，导致NameNode在启动加载Edits时会很慢，所以需要对Edits和Fsimage进行合并（所谓合并，就是将Edits和Fsimage加载到内存中，照着Edits中的操作一步步执行，最终形成新的Fsimage）。<strong>SecondaryNameNode的作用就是帮助NameNode进行Edits和Fsimage的合并工作。</strong><br>SecondaryNameNode首先会询问NameNode是否需要CheckPoint（触发CheckPoint需要满足两个条件中的任意一个，定时时间到和Edits中数据写满了）。直接带回NameNode是否检查结果。SecondaryNameNode执行CheckPoint操作，首先会让NameNode滚动Edits并生成一个空的edits.inprogress，滚动Edits的目的是给Edits打个标记，以后所有新的操作都写入edits.inprogress，其他未合并的Edits和Fsimage会拷贝到SecondaryNameNode的本地，然后将拷贝的Edits和Fsimage加载到内存中进行合并，生成fsimage.chkpoint，然后将fsimage.chkpoint拷贝给NameNode，重命名为Fsimage后替换掉原来的Fsimage。NameNode在启动时就只需要加载之前未合并的Edits和Fsimage即可，因为合并过的Edits中的元数据信息已经被记录在Fsimage中。</p>
<h2 id="Fsimage和Edits解析"><a href="#Fsimage和Edits解析" class="headerlink" title="Fsimage和Edits解析"></a>Fsimage和Edits解析</h2><p><img src="/HDFS/50.png" alt="50"></p>
<h3 id="oiv查看Fsimage文件"><a href="#oiv查看Fsimage文件" class="headerlink" title="oiv查看Fsimage文件"></a>oiv查看Fsimage文件</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs oiv -p 文件类型 -i 镜像文件 -o 转换后文件输出路径</span><br><span class="line"></span><br><span class="line">[vincent@linux1 current]$ pwd</span><br><span class="line">/opt/module/hadoop-3.1.3/data/tmp/dfs/name/current</span><br><span class="line"></span><br><span class="line">[vincent@linux1 current]$ hdfs oiv -p XML -i fsimage_0000000000000000025 -o /opt/module/hadoop-3.1.3/fsimage.xml</span><br><span class="line"></span><br><span class="line">[vincent@linux1 current]$ cat /opt/module/hadoop-3.1.3/fsimage.xml</span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">inode</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">id</span>&gt;</span>16386<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">type</span>&gt;</span>DIRECTORY<span class="tag">&lt;/<span class="name">type</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">mtime</span>&gt;</span>1512722284477<span class="tag">&lt;/<span class="name">mtime</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">permission</span>&gt;</span>vincent:supergroup:rwxr-xr-x<span class="tag">&lt;/<span class="name">permission</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">nsquota</span>&gt;</span>-1<span class="tag">&lt;/<span class="name">nsquota</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">dsquota</span>&gt;</span>-1<span class="tag">&lt;/<span class="name">dsquota</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">inode</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">inode</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">id</span>&gt;</span>16387<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">type</span>&gt;</span>DIRECTORY<span class="tag">&lt;/<span class="name">type</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>vincent<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">mtime</span>&gt;</span>1512790549080<span class="tag">&lt;/<span class="name">mtime</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">permission</span>&gt;</span>vincent:supergroup:rwxr-xr-x<span class="tag">&lt;/<span class="name">permission</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">nsquota</span>&gt;</span>-1<span class="tag">&lt;/<span class="name">nsquota</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">dsquota</span>&gt;</span>-1<span class="tag">&lt;/<span class="name">dsquota</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">inode</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">inode</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">id</span>&gt;</span>16389<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">type</span>&gt;</span>FILE<span class="tag">&lt;/<span class="name">type</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>wc.input<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">replication</span>&gt;</span>3<span class="tag">&lt;/<span class="name">replication</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">mtime</span>&gt;</span>1512722322219<span class="tag">&lt;/<span class="name">mtime</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">atime</span>&gt;</span>1512722321610<span class="tag">&lt;/<span class="name">atime</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">perferredBlockSize</span>&gt;</span>134217728<span class="tag">&lt;/<span class="name">perferredBlockSize</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">permission</span>&gt;</span>vincent:supergroup:rw-r--r--<span class="tag">&lt;/<span class="name">permission</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">blocks</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">block</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">id</span>&gt;</span>1073741825<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">genstamp</span>&gt;</span>1001<span class="tag">&lt;/<span class="name">genstamp</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">numBytes</span>&gt;</span>59<span class="tag">&lt;/<span class="name">numBytes</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">block</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">blocks</span>&gt;</span></span><br><span class="line">&lt;/inode &gt;</span><br></pre></td></tr></table></figure>

<p>Fsimage中没有记录块所对应DataNode，为什么？</p>
<p>在集群启动后，要求DataNode上报数据块信息，并间隔一段时间后再次上报。</p>
<h3 id="oev查看Edits文件"><a href="#oev查看Edits文件" class="headerlink" title="oev查看Edits文件"></a>oev查看Edits文件</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs oev -p 文件类型 -i 编辑日志 -o 转换后文件输出路径</span><br><span class="line"></span><br><span class="line">[vincent@hadoop102 current]$ hdfs oev -p XML -i edits_0000000000000000012-0000000000000000013 -o /opt/module/hadoop-3.1.3/edits.xml</span><br><span class="line"></span><br><span class="line">[vincent@linux1 current]$ cat /opt/module/hadoop-3.1.3/edits.xml</span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">EDITS</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">EDITS_VERSION</span>&gt;</span>-63<span class="tag">&lt;/<span class="name">EDITS_VERSION</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_START_LOG_SEGMENT<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">TXID</span>&gt;</span>129<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_ADD<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">TXID</span>&gt;</span>130<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">LENGTH</span>&gt;</span>0<span class="tag">&lt;/<span class="name">LENGTH</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">INODEID</span>&gt;</span>16407<span class="tag">&lt;/<span class="name">INODEID</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">PATH</span>&gt;</span>/hello7.txt<span class="tag">&lt;/<span class="name">PATH</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">REPLICATION</span>&gt;</span>2<span class="tag">&lt;/<span class="name">REPLICATION</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">MTIME</span>&gt;</span>1512943607866<span class="tag">&lt;/<span class="name">MTIME</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">ATIME</span>&gt;</span>1512943607866<span class="tag">&lt;/<span class="name">ATIME</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">BLOCKSIZE</span>&gt;</span>134217728<span class="tag">&lt;/<span class="name">BLOCKSIZE</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">CLIENT_NAME</span>&gt;</span>DFSClient_NONMAPREDUCE_-1544295051_1<span class="tag">&lt;/<span class="name">CLIENT_NAME</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">CLIENT_MACHINE</span>&gt;</span>192.168.1.5<span class="tag">&lt;/<span class="name">CLIENT_MACHINE</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">OVERWRITE</span>&gt;</span>true<span class="tag">&lt;/<span class="name">OVERWRITE</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">PERMISSION_STATUS</span>&gt;</span></span><br><span class="line">				<span class="tag">&lt;<span class="name">USERNAME</span>&gt;</span>vincent<span class="tag">&lt;/<span class="name">USERNAME</span>&gt;</span></span><br><span class="line">				<span class="tag">&lt;<span class="name">GROUPNAME</span>&gt;</span>supergroup<span class="tag">&lt;/<span class="name">GROUPNAME</span>&gt;</span></span><br><span class="line">				<span class="tag">&lt;<span class="name">MODE</span>&gt;</span>420<span class="tag">&lt;/<span class="name">MODE</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;/<span class="name">PERMISSION_STATUS</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">RPC_CLIENTID</span>&gt;</span>908eafd4-9aec-4288-96f1-e8011d181561<span class="tag">&lt;/<span class="name">RPC_CLIENTID</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">RPC_CALLID</span>&gt;</span>0<span class="tag">&lt;/<span class="name">RPC_CALLID</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_ALLOCATE_BLOCK_ID<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">TXID</span>&gt;</span>131<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">BLOCK_ID</span>&gt;</span>1073741839<span class="tag">&lt;/<span class="name">BLOCK_ID</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_SET_GENSTAMP_V2<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">TXID</span>&gt;</span>132<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">GENSTAMPV2</span>&gt;</span>1016<span class="tag">&lt;/<span class="name">GENSTAMPV2</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_ADD_BLOCK<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">TXID</span>&gt;</span>133<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">PATH</span>&gt;</span>/hello7.txt<span class="tag">&lt;/<span class="name">PATH</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">BLOCK</span>&gt;</span></span><br><span class="line">				<span class="tag">&lt;<span class="name">BLOCK_ID</span>&gt;</span>1073741839<span class="tag">&lt;/<span class="name">BLOCK_ID</span>&gt;</span></span><br><span class="line">				<span class="tag">&lt;<span class="name">NUM_BYTES</span>&gt;</span>0<span class="tag">&lt;/<span class="name">NUM_BYTES</span>&gt;</span></span><br><span class="line">				<span class="tag">&lt;<span class="name">GENSTAMP</span>&gt;</span>1016<span class="tag">&lt;/<span class="name">GENSTAMP</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;/<span class="name">BLOCK</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">RPC_CLIENTID</span>&gt;</span><span class="tag">&lt;/<span class="name">RPC_CLIENTID</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">RPC_CALLID</span>&gt;</span>-2<span class="tag">&lt;/<span class="name">RPC_CALLID</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_CLOSE<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">TXID</span>&gt;</span>134<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">LENGTH</span>&gt;</span>0<span class="tag">&lt;/<span class="name">LENGTH</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">INODEID</span>&gt;</span>0<span class="tag">&lt;/<span class="name">INODEID</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">PATH</span>&gt;</span>/hello7.txt<span class="tag">&lt;/<span class="name">PATH</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">REPLICATION</span>&gt;</span>2<span class="tag">&lt;/<span class="name">REPLICATION</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">MTIME</span>&gt;</span>1512943608761<span class="tag">&lt;/<span class="name">MTIME</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">ATIME</span>&gt;</span>1512943607866<span class="tag">&lt;/<span class="name">ATIME</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">BLOCKSIZE</span>&gt;</span>134217728<span class="tag">&lt;/<span class="name">BLOCKSIZE</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">CLIENT_NAME</span>&gt;</span><span class="tag">&lt;/<span class="name">CLIENT_NAME</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">CLIENT_MACHINE</span>&gt;</span><span class="tag">&lt;/<span class="name">CLIENT_MACHINE</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">OVERWRITE</span>&gt;</span>false<span class="tag">&lt;/<span class="name">OVERWRITE</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">BLOCK</span>&gt;</span></span><br><span class="line">				<span class="tag">&lt;<span class="name">BLOCK_ID</span>&gt;</span>1073741839<span class="tag">&lt;/<span class="name">BLOCK_ID</span>&gt;</span></span><br><span class="line">				<span class="tag">&lt;<span class="name">NUM_BYTES</span>&gt;</span>25<span class="tag">&lt;/<span class="name">NUM_BYTES</span>&gt;</span></span><br><span class="line">				<span class="tag">&lt;<span class="name">GENSTAMP</span>&gt;</span>1016<span class="tag">&lt;/<span class="name">GENSTAMP</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;/<span class="name">BLOCK</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">PERMISSION_STATUS</span>&gt;</span></span><br><span class="line">				<span class="tag">&lt;<span class="name">USERNAME</span>&gt;</span>vincent<span class="tag">&lt;/<span class="name">USERNAME</span>&gt;</span></span><br><span class="line">				<span class="tag">&lt;<span class="name">GROUPNAME</span>&gt;</span>supergroup<span class="tag">&lt;/<span class="name">GROUPNAME</span>&gt;</span></span><br><span class="line">				<span class="tag">&lt;<span class="name">MODE</span>&gt;</span>420<span class="tag">&lt;/<span class="name">MODE</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;/<span class="name">PERMISSION_STATUS</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">&lt;/EDITS &gt;</span><br></pre></td></tr></table></figure>

<h2 id="CheckPoint时间设置"><a href="#CheckPoint时间设置" class="headerlink" title="CheckPoint时间设置"></a>CheckPoint时间设置</h2><p>（1）通常情况下，SecondaryNameNode每隔一小时执行一次。</p>
<p>hdfs-default.xml</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>3600<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>（2）一分钟检查一次操作次数，3当操作次数达到1百万时，SecondaryNameNode执行一次。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.txns<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>操作动作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.check.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span> 1分钟检查一次操作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="NameNode故障处理"><a href="#NameNode故障处理" class="headerlink" title="NameNode故障处理"></a>NameNode故障处理</h2><p><strong>方法一：将SecondaryNameNode中数据拷贝到NameNode存储数据的目录</strong></p>
<ol>
<li><p>kill -9 NameNode进程</p>
</li>
<li><p>删除NameNode存储的数据（/opt/module/hadoop-3.1.3/data/name）</p>
</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">rm -rf /opt/module/hadoop-3.1.3/data/name/*</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>拷贝SecondaryNameNode中数据到原NameNode存储数据目录</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">scp -r vincent@linux3:/opt/module/hadoop-3.1.3/data/namesecondary/* /opt/module/hadoop-3.1.3/data/name/</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>重新启动NameNode</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs --daemon start namenode</span><br></pre></td></tr></table></figure>

<p><strong>方法二：使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中</strong></p>
<ol>
<li>修改hdfs-site.xml中的</li>
</ol>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>120<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.1.3/data/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ol start="2">
<li><p>kill -9 NameNode进程</p>
</li>
<li><p>删除NameNode存储的数据（/opt/module/hadoop-3.1.3/data/name）</p>
</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hadoop-3.1.3]$ rm -rf /opt/module/hadoop-3.1.3/data/name/*</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>如果SecondaryNameNode不和NameNode在一个主机节点上，需要将SecondaryNameNode存储数据的目录拷贝到NameNode存储数据的平级目录，并删除in_use.lock文件</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 data]$ scp -r vincent@linux3:/opt/module/hadoop-3.1.3/data/namesecondary ./</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在linux1中删除拷贝过来的in_use.lock</span></span><br><span class="line">[vincent@linux1 namesecondary]$ rm -rf in_use.lock</span><br><span class="line"></span><br><span class="line">[vincent@linux1 data]$ pwd</span><br><span class="line">/opt/module/hadoop-3.1.3/data</span><br><span class="line"></span><br><span class="line">[vincent@linux1 data]$ ls</span><br><span class="line">data  name  namesecondary</span><br></pre></td></tr></table></figure>

<ol start="5">
<li>   导入检查点数据（等待一会ctrl+c结束掉）</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hadoop-3.1.3]$ bin/hdfs namenode -importCheckpoint</span><br></pre></td></tr></table></figure>

<ol start="6">
<li>   启动NameNode</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hadoop-3.1.3]$ hdfs --daemon start namenode</span><br></pre></td></tr></table></figure>

<h2 id="集群安全模式"><a href="#集群安全模式" class="headerlink" title="集群安全模式"></a>集群安全模式</h2><p><img src="/HDFS/51.png" alt="51"></p>
<p>集群处于安全模式，不能执行重要操作（写操作）。集群启动完成后，自动退出安全模式。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/hdfs dfsadmin -safemode get		（功能描述：查看安全模式状态）</span><br><span class="line">bin/hdfs dfsadmin -safemode enter  （功能描述：进入安全模式状态）</span><br><span class="line">bin/hdfs dfsadmin -safemode leave	（功能描述：离开安全模式状态）</span><br><span class="line">bin/hdfs dfsadmin -safemode wait	（功能描述：等待安全模式状态）</span><br></pre></td></tr></table></figure>

<h2 id="NameNode多目录配置"><a href="#NameNode多目录配置" class="headerlink" title="NameNode多目录配置"></a>NameNode多目录配置</h2><ol>
<li><p>NameNode的本地目录可以配置成多个，且每个目录存放内容相同，增加了可靠性</p>
</li>
<li><p>具体配置如下</p>
<p>1）在hdfs-site.xml文件中修改如下内容</p>
</li>
</ol>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///$&#123;hadoop.tmp.dir&#125;/name1,file:///$&#123;hadoop.tmp.dir&#125;/name2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>​        2）停止集群，删除data和logs中所有数据。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hadoop-3.1.3]$ rm -rf data/ logs/</span><br><span class="line">[vincent@linux2 hadoop-3.1.3]$ rm -rf data/ logs/</span><br><span class="line">[vincent@linux3 hadoop-3.1.3]$ rm -rf data/ logs/</span><br></pre></td></tr></table></figure>

<p>​        3）格式化集群并启动</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hadoop-3.1.3]$ bin/hdfs namenode –format</span><br><span class="line">[vincent@linux1 hadoop-3.1.3]$ sbin/start-dfs.sh</span><br></pre></td></tr></table></figure>

<p>​        4）查看结果</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 data]$ ll</span><br><span class="line">总用量 12</span><br><span class="line">drwx------. 3 vincent vincent 4096 12月 11 08:03 data</span><br><span class="line">drwxrwxr-x. 3 vincent vincent 4096 12月 11 08:03 name1</span><br><span class="line">drwxrwxr-x. 3 vincent vincent 4096 12月 11 08:03 name2</span><br></pre></td></tr></table></figure>

<h1 id="DataNode"><a href="#DataNode" class="headerlink" title="DataNode"></a>DataNode</h1><h2 id="DataNode工作机制"><a href="#DataNode工作机制" class="headerlink" title="DataNode工作机制"></a>DataNode工作机制</h2><p><img src="/HDFS/52.png" alt="52"></p>
<p>1）<strong>一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据</strong>包括数据块的长度，块数据的校验和，以及时间戳。</p>
<p>2）DataNode启动后向NameNode注册，通过后，周期性（1小时）的向NameNode上报所有的块信息。</p>
<p>3）<strong>心跳是每3秒一次</strong>，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。<strong>如果超过10分钟30秒没有收到某个DataNode的心跳，则认为该节点不可用。</strong></p>
<p>4）集群运行中可以安全加入和退出一些机器。</p>
<h2 id="数据完整性"><a href="#数据完整性" class="headerlink" title="数据完整性"></a>数据完整性</h2><p><img src="/HDFS/53.png" alt="53"></p>
<p>思考：如果电脑磁盘里面存储的数据是控制高铁信号灯的红灯信号（1）和绿灯信号（0），但是存储该数据的磁盘坏了，一直显示是绿灯，是否很危险？同理DataNode节点上的数据损坏了，却没有发现，是否也很危险，那么如何解决呢？</p>
<p>如下是DataNode节点保证数据完整性的方法。</p>
<p>1）当DataNode读取Block的时候，它会计算CheckSum。</p>
<p>2）如果计算后的CheckSum，与Block创建时值不一样，说明Block已经损坏。</p>
<p>3）Client读取其他DataNode上的Block。</p>
<p>4）常见的校验算法 crc（32）  md5（128）  sha1（160）。</p>
<p>5）DataNode在其文件创建后周期验证CheckSum。</p>
<h2 id="掉线时限参数设置"><a href="#掉线时限参数设置" class="headerlink" title="掉线时限参数设置"></a>掉线时限参数设置</h2><p><img src="/HDFS/54.png" alt="54"></p>
<p>需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.heartbeat.recheck-interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>300000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.heartbeat.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="服役新数据节点"><a href="#服役新数据节点" class="headerlink" title="服役新数据节点"></a>服役新数据节点</h2><p>1.在新节点中进行操作系统配置，包括主机名、网络、防火墙和无密码登录等。</p>
<p>2.在所有节点/etc/hosts文件中添加新节点。</p>
<p>3.把namenode的有关配置文件复制到该节点。</p>
<p>4.修改master节点slaves/works文件,增加该节点。</p>
<p>5.单独启动该节点上的datanode和nodemanager</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux4 hadoop-3.1.3]$ hdfs --daemon start datanode</span><br><span class="line">[vincent@linux4 hadoop-3.1.3]$ yarn --daemon start nodemanager</span><br></pre></td></tr></table></figure>

<p>6.运行start-balancer.sh 进行数据负载均衡</p>
<p>（默认阀值:单个节点的使用率和整个集群的使用率差值是10%以下,超过百分之十集群就会进行负载均衡）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux4 hadoop-3.1.3]$ sbin/start-balancer.sh</span><br></pre></td></tr></table></figure>

<h2 id="退役旧数据节点"><a href="#退役旧数据节点" class="headerlink" title="退役旧数据节点"></a>退役旧数据节点</h2><p>添加到白名单或者黑名单。</p>
<p>添加到白名单的主机节点，都允许访问NameNode，不在白名单的主机节点，都会被直接退出。</p>
<p>添加到黑名单的主机节点，不允许访问NameNode，会在数据迁移后退出。</p>
<p>实际情况下，白名单用于确定允许访问NameNode的DataNode节点，内容配置一般与workers文件内容一致。 黑名单用于在集群运行过程中退役DataNode节点。</p>
<p>配置白名单和黑名单的具体步骤如下：</p>
<p>（1）在NameNode的/opt/module/hadoop-3.1.3/etc/hadoop目录下分别创建whitelist 和 blacklist 文件</p>
<p>在whitelist中添加如下主机名称,假如集群正常工作的节点为linux1，linux2，linux3</p>
<figure class="highlight gcode"><table><tr><td class="code"><pre><span class="line">li<span class="symbol">nux1</span></span><br><span class="line">li<span class="symbol">nux2</span></span><br><span class="line">li<span class="symbol">nux3</span></span><br></pre></td></tr></table></figure>

<p>黑名单暂时为空。</p>
<p>（2）在NameNode的hdfs-site.xml配置文件中增加dfs.hosts 和 dfs.hosts.exclude配置</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- whitelist --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.1.3/etc/hadoop/whitelist<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- blacklist --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.hosts.exclude<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.1.3/etc/hadoop/blacklist<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>（3）配置文件分发</p>
<p>（4）重新启动集群</p>
<p>白名单退役会直接将节点抛弃，没有迁移数据的过程，会造成数据丢失。</p>
<p>黑名单会进行数据迁移，推荐使用黑名单。</p>
<h2 id="Datanode多目录配置"><a href="#Datanode多目录配置" class="headerlink" title="Datanode多目录配置"></a>Datanode多目录配置</h2><ol>
<li><p>DataNode也可以配置成多个目录，每个目录存储的数据不一样。即：数据不是副本。</p>
</li>
<li><p>具体配置如下</p>
</li>
</ol>
<p>（1）在hdfs-site.xml中修改如下内容:</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///$&#123;hadoop.tmp.dir&#125;/data1,file:///$&#123;hadoop.tmp.dir&#125;/data2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>（2）停止集群，删除data和logs中所有数据。</p>
<p>（3）格式化集群并启动。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hadoop-3.1.3]$ bin/hdfs namenode -format</span><br><span class="line">[vincent@linux1 hadoop-3.1.3]$ sbin/start-dfs.sh</span><br></pre></td></tr></table></figure>

<h1 id="小文件存档"><a href="#小文件存档" class="headerlink" title="小文件存档"></a>小文件存档</h1><p><img src="/HDFS/55.png" alt="55"></p>
<p>1.归档文件:</p>
<p>把/user/vincent/input目录里面的所有文件归档成一个叫input.har的归档文件，并把归档后文件存储到/user/vincent/output路径下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hadoop-3.1.3]$ bin/hadoop archive -archiveName input.har -p  /user/vincent/input /user/vincent/output</span><br></pre></td></tr></table></figure>

<p>2.查看归档</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hadoop-3.1.3]$ hadoop fs -lsr /user/vincent/output/input.har</span><br><span class="line">[vincent@linux1 hadoop-3.1.3]$ hadoop fs -lsr har:///user/vincent/output/input.har</span><br></pre></td></tr></table></figure>

<p>3.解归档文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hadoop-3.1.3]$ hadoop fs -cp har:///user/vincent/output/input.har/* /user/vincent</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title>HDFS配置</title>
    <url>/Hadoop%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<h1 id="准备虚拟机"><a href="#准备虚拟机" class="headerlink" title="准备虚拟机"></a>准备虚拟机</h1><h2 id="1-克隆虚拟机"><a href="#1-克隆虚拟机" class="headerlink" title="1. 克隆虚拟机"></a>1. 克隆虚拟机</h2><p>单台虚拟机：内存4G，硬盘50G，安装必要环境(最小化安装)</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo yum install -y epel-release</span><br><span class="line">sudo yum install -y psmisc nc net-tools rsync vim lrzsz ntp libzstd openssl-static tree iotop</span><br></pre></td></tr></table></figure>

<p>修改克隆虚拟机的静态IP(按照自己机器的网络设置进行修改)</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo vim /etc/sysconfig/network-scripts/ifcfg-ens33  或</span><br><span class="line">sudo vim /etc/sysconfig/network-scripts/ifcfg-eth0</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">TYPE=&quot;Ethernet&quot;</span><br><span class="line">PROXY_METHOD=&quot;none&quot;</span><br><span class="line">BROWSER_ONLY=&quot;no&quot;</span><br><span class="line">BOOTPROTO=static</span><br><span class="line">DEFROUTE=&quot;yes&quot;</span><br><span class="line">IPV4_FAILURE_FATAL=&quot;no&quot;</span><br><span class="line">IPV6INIT=&quot;yes&quot;</span><br><span class="line">IPV6_AUTOCONF=&quot;yes&quot;</span><br><span class="line">IPV6_DEFROUTE=&quot;yes&quot;</span><br><span class="line">IPV6_FAILURE_FATAL=&quot;no&quot;</span><br><span class="line">IPV6_ADDR_GEN_MODE=&quot;stable-privacy&quot;</span><br><span class="line">NAME=&quot;eth0&quot;</span><br><span class="line">UUID=&quot;19c440dc-9a12-4fd7-bc46-0a121ef6af96&quot;</span><br><span class="line">DEVICE=&quot;eth0&quot;</span><br><span class="line">ONBOOT=&quot;yes&quot;</span><br><span class="line">IPADDR=10.211.55.10</span><br><span class="line">GATEWAY=10.211.55.1</span><br><span class="line">DNS1=10.211.55.1</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="2-修改主机名"><a href="#2-修改主机名" class="headerlink" title="2. 修改主机名"></a>2. 修改主机名</h2><p>修改主机名称</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo vim /etc/hostname</span><br></pre></td></tr></table></figure>

<p>配置主机名称映射，打开/etc/hosts</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo vim /etc/hosts</span><br></pre></td></tr></table></figure>

<figure class="highlight accesslog"><table><tr><td class="code"><pre><span class="line"><span class="number">10.211.55.10</span>	linux1</span><br><span class="line"><span class="number">10.211.55.11</span>	linux2</span><br><span class="line"><span class="number">10.211.55.12</span>	linux3</span><br></pre></td></tr></table></figure>

<p>修改window7的主机映射文件（hosts文件）</p>
<p>进入C:\Windows\System32\drivers\etc</p>
<h2 id="3-关闭防火墙"><a href="#3-关闭防火墙" class="headerlink" title="3. 关闭防火墙"></a>3. 关闭防火墙</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo systemctl stop firewalld</span><br><span class="line">sudo systemctl disable firewalld</span><br></pre></td></tr></table></figure>

<h2 id="4-创建vincent用户"><a href="#4-创建vincent用户" class="headerlink" title="4. 创建vincent用户"></a>4. 创建vincent用户</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo useradd vincent</span><br><span class="line">sudo passwd vincent</span><br></pre></td></tr></table></figure>

<h2 id="5-配置vincent用户具有root权限"><a href="#5-配置vincent用户具有root权限" class="headerlink" title="5. 配置vincent用户具有root权限"></a>5. 配置vincent用户具有root权限</h2><p>修改/etc/sudoers文件，如下所示</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo vim /etc/sudoers</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># Allows people in group wheel to run all commands</span></span></span><br><span class="line"><span class="meta">%</span><span class="bash">wheel  ALL=(ALL)       ALL</span></span><br><span class="line">vincent ALL=(ALL)       ALL</span><br></pre></td></tr></table></figure>

<h2 id="6-在-opt目录下创建文件夹"><a href="#6-在-opt目录下创建文件夹" class="headerlink" title="6. 在/opt目录下创建文件夹"></a>6. 在/opt目录下创建文件夹</h2><p>在/opt目录下创建module、software文件夹</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo mkdir module</span><br><span class="line">sudo mkdir software</span><br></pre></td></tr></table></figure>

<p>修改module、software文件夹的所有者</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo chown vincent:vincent /opt/module /opt/software</span><br></pre></td></tr></table></figure>

<h2 id="7-安装JDK"><a href="#7-安装JDK" class="headerlink" title="7. 安装JDK"></a>7. 安装JDK</h2><ol>
<li><p>将JDK安装包上传到Linux /opt/software目录下</p>
</li>
<li><p>解压JDK到/opt/module目录下</p>
</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -zxvf jdk-8u212-linux-x64.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>配置JDK环境变量,两种方式:</li>
</ol>
<p>新建/etc/profile.d/my_env.sh文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo vim /etc/profile.d/my_env.sh</span><br></pre></td></tr></table></figure>

<p>添加如下内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">JAVA_HOME</span></span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_212</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br></pre></td></tr></table></figure>

<p>或者</p>
<p>直接将环境变量配置到 /etc/profile 文件中,在/etc/profile文件的末尾追加如下内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">JAVA_HOME=/opt/module/jdk1.8.0_212</span><br><span class="line">PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line">export PATH JAVA_HOME</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">source /etc/profile	</span><br></pre></td></tr></table></figure>

<p>测试JDK是否安装成功</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">java -version</span><br></pre></td></tr></table></figure>

<h2 id="8-安装Hadoop"><a href="#8-安装Hadoop" class="headerlink" title="8. 安装Hadoop"></a>8. 安装Hadoop</h2><p>Hadoop下载地址：<a href="https://archive.apache.org/dist/hadoop/common/hadoop-2.7.2/">https://archive.apache.org/dist/hadoop/common/hadoop-3.1.3/</a></p>
<p>将hadoop安装包上传到/opt/software目录下</p>
<p>解压安装文件到/opt/module下面</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -zxvf hadoop-3.1.3.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>

<p>将Hadoop添加到环境变量</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">HADOOP_HOME</span></span><br><span class="line">HADOOP_HOME=/opt/module/hadoop-3.1.3</span><br><span class="line">PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br><span class="line">export PATH JAVA_HOME HADOOP_HOME</span><br></pre></td></tr></table></figure>

<p>测试是否安装成功</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop version</span><br></pre></td></tr></table></figure>

<h1 id="完全分布式集群搭建"><a href="#完全分布式集群搭建" class="headerlink" title="完全分布式集群搭建"></a>完全分布式集群搭建</h1><h2 id="1-编写集群分发脚本xsync"><a href="#1-编写集群分发脚本xsync" class="headerlink" title="1. 编写集群分发脚本xsync"></a>1. 编写集群分发脚本xsync</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash">1. 判断参数个数</span></span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line">  echo Not Enough Arguement!</span><br><span class="line">  exit;</span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span><span class="bash">2. 遍历集群所有机器</span></span><br><span class="line">for host in linux1 linux2 linux3</span><br><span class="line">do</span><br><span class="line">  echo ====================  $host  ====================</span><br><span class="line"><span class="meta">  #</span><span class="bash">3. 遍历所有目录，挨个发送</span></span><br><span class="line">  for file in $@</span><br><span class="line">  do</span><br><span class="line">    #4 判断文件是否存在</span><br><span class="line">    if [ -e $file ]</span><br><span class="line">    then</span><br><span class="line">    #5. 获取父目录</span><br><span class="line">    # cd -P 的意义是防止文件路径是软连接</span><br><span class="line">      pdir=$(cd -P $(dirname $file); pwd)</span><br><span class="line">      #6. 获取当前文件的名称</span><br><span class="line">      fname=$(basename $file)</span><br><span class="line">      ssh $host &quot;mkdir -p $pdir&quot;</span><br><span class="line">      rsync -av $pdir/$fname $host:$pdir</span><br><span class="line">    else</span><br><span class="line">      echo $file does not exists!</span><br><span class="line">    fi</span><br><span class="line">  done</span><br><span class="line">done</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">chmod +x xsync</span><br></pre></td></tr></table></figure>

<h2 id="2-集群配置"><a href="#2-集群配置" class="headerlink" title="2. 集群配置"></a>2. 集群配置</h2><table>
<thead>
<tr>
<th></th>
<th align="center">linux1</th>
<th align="center">linux2</th>
<th align="center">linux3</th>
</tr>
</thead>
<tbody><tr>
<td>HDFS</td>
<td align="center">NameNode                     DataNode</td>
<td align="center">DataNode</td>
<td align="center">SecondaryNameNode       DataNode</td>
</tr>
<tr>
<td>YARN</td>
<td align="center">NodeManager</td>
<td align="center">ResourceManager  NodeManager</td>
<td align="center">NodeManager</td>
</tr>
</tbody></table>
<h3 id="配置hadoop-env-sh-第54行"><a href="#配置hadoop-env-sh-第54行" class="headerlink" title="配置hadoop-env.sh(第54行)"></a>配置hadoop-env.sh(第54行)</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">:set nu							//设置行号</span><br><span class="line">:set nonu						//取消行号</span><br><span class="line">:noh								//取消高亮</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_212</span><br></pre></td></tr></table></figure>

<h3 id="配置core-site-xml"><a href="#配置core-site-xml" class="headerlink" title="配置core-site.xml"></a>配置core-site.xml</h3><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定NameNode的地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://linux1:9820<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定hadoop数据的存储目录,官方配置文件中的配置项是hadoop.tmp.dir,</span></span><br><span class="line"><span class="comment">     用来指定hadoop数据的存储目录,此次配置用的hadoop.data.dir是自己定义的变量,</span></span><br><span class="line"><span class="comment">     因为在hdfs-site.xml中会使用此配置的值来具体指定namenode 和 datanode存储数据的目录--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.1.3/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.http.staticuser.user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>vincent<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 下面是兼容性配置 --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置该vincent(superUser)允许通过代理访问的主机节点 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.vincent.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置该vincent(superuser)允许代理的用户所属组 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.vincent.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置该vincent(superuser)允许代理的用户--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.vincent.users<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="配置hdfs-site-xml"><a href="#配置hdfs-site-xml" class="headerlink" title="配置hdfs-site.xml"></a>配置hdfs-site.xml</h3><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定NameNode数据的存储目录 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file://$&#123;hadoop.data.dir&#125;/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"> <span class="comment">&lt;!-- 指定Datanode数据的存储目录 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file://$&#123;hadoop.data.dir&#125;/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 备份数 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定SecondaryNameNode数据的存储目录 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file://$&#123;hadoop.data.dir&#125;/namesecondary<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- nn web端访问地址--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>linux1:9870<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">&lt;!-- 2nn web端访问地址--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>linux3:9868<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="配置yarn-site-xml"><a href="#配置yarn-site-xml" class="headerlink" title="配置yarn-site.xml"></a>配置yarn-site.xml</h3><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定mapreduce走shuffle --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="comment">&lt;!-- 指定ResourceManager的地址--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>linux2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 环境变量的继承 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.env-whitelist<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置日志的聚集 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log.server.url<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>http://linux1:19888/jobhistory/logs<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>604800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--node manager 配置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.cpu-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">discription</span>&gt;</span>每个nodemanager可分配的cpu总核数<span class="tag">&lt;/<span class="name">discription</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>4096<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">discription</span>&gt;</span>每个nodemanager可分配的内存总量<span class="tag">&lt;/<span class="name">discription</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- </span></span><br><span class="line"><span class="comment">优化建议：</span></span><br><span class="line"><span class="comment">1. cpu核数=逻辑核数-其他应用数（datanode？work？zk？等）</span></span><br><span class="line"><span class="comment">cat /proc/cpuinfo | grep &quot;processor&quot; | wc -l</span></span><br><span class="line"><span class="comment">可以查看集群的逻辑核数</span></span><br><span class="line"><span class="comment">2. 内存建议是CPU的整数倍，给系统预留好足够用的内存</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--AppMaster 配置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.app.mapreduce.am.resource.cpu-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span>	</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.app.mapreduce.am.resource.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>2048<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">discription</span>&gt;</span>ApplicationMaster的占用的内存大小<span class="tag">&lt;/<span class="name">discription</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- </span></span><br><span class="line"><span class="comment">优化建议</span></span><br><span class="line"><span class="comment">1. cpu和内存比例和 nm的分配比例保持一致</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Container 配置优化 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">discription</span>&gt;</span>单个任务可申请的最多虚拟CPU个数<span class="tag">&lt;/<span class="name">discription</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">discription</span>&gt;</span>单个任务可申请的最小虚拟CPU个数<span class="tag">&lt;/<span class="name">discription</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>512<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">discription</span>&gt;</span>单个任务可申请最小内存<span class="tag">&lt;/<span class="name">discription</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.shceduler.maximum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>4096<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">discription</span>&gt;</span>单个任务可申请最大内存，默认8192MB<span class="tag">&lt;/<span class="name">discription</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">优化建议</span></span><br><span class="line"><span class="comment">1. 在调度器中，很多资源计算部分会转化为这个最小值的N倍进行计算。所以，设定可分配内存等资源的时候，最好是刚好为这个最小值的倍数</span></span><br><span class="line"><span class="comment">2. cpu/内存比例保持一致</span></span><br><span class="line"><span class="comment">3. YARN采用了线程监控的方法判断任务是否超量使用内存，一旦发现超量，则直接将其杀死。由于Cgroups对内存的控制缺乏灵活性（即任务任何时刻不能超过内存上限，如果超过，则直接将其杀死或者报OOM），而Java进程在创建瞬间内存将翻倍，之后骤降到正常值，这种情况下，采用线程监控的方式更加灵活（当发现进程树内存瞬间翻倍超过设定值时，可认为是正常现象，不会将任务杀死），因此YARN未提供Cgroups内存隔离机制来控制容器。</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.fair.assignmultiple<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">discription</span>&gt;</span>是否允许NodeManager一次分配多个容器<span class="tag">&lt;/<span class="name">discription</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.fair.max.assign<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>8<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">discription</span>&gt;</span>如果允许一次分配多个,一次最多可分配多少个,这里按照一个最小分配yarn.scheduler.minimum-allocation-mb0.5gb来计算总共内存4/0.5=8<span class="tag">&lt;/<span class="name">discription</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="配置mapred-site-xml"><a href="#配置mapred-site-xml" class="headerlink" title="配置mapred-site.xml"></a>配置mapred-site.xml</h3><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定MapReduce程序运行在Yarn上 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- mapreduce参数设置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>2048<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">discription</span>&gt;</span>map的内存大小<span class="tag">&lt;/<span class="name">discription</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.java.opts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>-Xmx1024M<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">discription</span>&gt;</span>用户设定的map/reduce阶段申请的container的JVM参数。最大堆设定要比申请的内存少一些，用于JVM的非堆部分使用0.80-0.85建议<span class="tag">&lt;/<span class="name">discription</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>3072<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.java.opts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>-Xmx2048M<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- </span></span><br><span class="line"><span class="comment">优化参考</span></span><br><span class="line"><span class="comment">1. 如果集群主要使用mr进行计算，那么建议map的内存和cpu和容器最小的相等。</span></span><br><span class="line"><span class="comment">2. 一个容器里面最多跑几个map？yarn.scheduler.maximum-allocation-mb/mapreduce.map.memory.mb=2</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">&lt;!-- 历史服务器端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>linux1:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 历史服务器web端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>linux1:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="在集群上分发配置好的hadoop"><a href="#在集群上分发配置好的hadoop" class="headerlink" title="在集群上分发配置好的hadoop"></a>在集群上分发配置好的hadoop</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">xsync /opt/module/hadoop-3.1.3</span><br></pre></td></tr></table></figure>

<h2 id="3-集群单点启动"><a href="#3-集群单点启动" class="headerlink" title="3. 集群单点启动"></a>3. 集群单点启动</h2><p>如果集群是第一次启动，需要格式化NameNode。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure>

<p>在linux1上启动NameNode</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs --daemon start namenode</span><br></pre></td></tr></table></figure>

<p>在linux1、linux2以及linux3上执行如下命令（三台都要执行）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs --daemon start datanode</span><br></pre></td></tr></table></figure>

<h2 id="4-SSH无密登录配置"><a href="#4-SSH无密登录配置" class="headerlink" title="4. SSH无密登录配置"></a>4. SSH无密登录配置</h2><p>生成公钥和私钥</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br></pre></td></tr></table></figure>

<p>然后敲（三个回车），就会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥）</p>
<p>将公钥拷贝到本机</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ssh-copy-id linux1</span><br></pre></td></tr></table></figure>

<p>分发公钥到集群各个节点</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">xsync .ssh</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>known_hosts</th>
<th>记录ssh访问过计算机的公钥(public key)</th>
</tr>
</thead>
<tbody><tr>
<td>id_rsa</td>
<td>生成的私钥</td>
</tr>
<tr>
<td>id_rsa.pub</td>
<td>生成的公钥</td>
</tr>
<tr>
<td>authorized_keys</td>
<td>存放授权过的无密登录服务器公钥</td>
</tr>
</tbody></table>
<h2 id="5-群起集群"><a href="#5-群起集群" class="headerlink" title="5. 群起集群"></a>5. 群起集群</h2><h3 id="配置workers"><a href="#配置workers" class="headerlink" title="配置workers"></a>配置workers</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim /opt/module/hadoop-3.1.3/etc/hadoop/workers</span><br></pre></td></tr></table></figure>

<p>在该文件中增加如下内容：</p>
<figure class="highlight gcode"><table><tr><td class="code"><pre><span class="line">li<span class="symbol">nux1</span></span><br><span class="line">li<span class="symbol">nux2</span></span><br><span class="line">li<span class="symbol">nux3</span></span><br></pre></td></tr></table></figure>

<p>同步所有节点配置文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">xsync /opt/module/hadoop-3.1.3/etc/hadoop/workers</span><br></pre></td></tr></table></figure>

<h3 id="启动集群"><a href="#启动集群" class="headerlink" title="启动集群"></a>启动集群</h3><ol>
<li>如果集群是第一次启动，需要在linux1节点格式化NameNode（注意格式化之前，一定要先停止上次启动的所有namenode和datanode进程，然后再删除data和log数据）</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>启动HDFS</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">start-dfs.sh</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>在配置了ResourceManager的节点(linux2)启动YARN</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">start-yarn.sh</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>集群基本测试</li>
</ol>
<p>上传文件到集群</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop fs -mkdir -p /user/vincent/input</span><br><span class="line">hadoop fs -put $HADOOP_HOME/wcinput/wc.input /user/vincent/input</span><br></pre></td></tr></table></figure>

<h2 id="6-集群启动-停止方式总结"><a href="#6-集群启动-停止方式总结" class="headerlink" title="6. 集群启动/停止方式总结"></a>6. 集群启动/停止方式总结</h2><h3 id="各个服务组件逐一启动-停止"><a href="#各个服务组件逐一启动-停止" class="headerlink" title="各个服务组件逐一启动/停止"></a>各个服务组件逐一启动/停止</h3><p>分别启动/停止HDFS组件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs --daemon start/stop namenode/datanode/secondarynamenode</span><br></pre></td></tr></table></figure>

<p>启动/停止YARN</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yarn --daemon start/stop  resourcemanager/nodemanager</span><br></pre></td></tr></table></figure>



<h3 id="各个模块分开启动-停止（配置ssh是前提）常用"><a href="#各个模块分开启动-停止（配置ssh是前提）常用" class="headerlink" title="各个模块分开启动/停止（配置ssh是前提）常用"></a>各个模块分开启动/停止（配置ssh是前提）常用</h3><p>整体启动/停止HDFS</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">start-dfs.sh/stop-dfs.sh</span><br></pre></td></tr></table></figure>

<p>整体启动/停止YARN</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">start-yarn.sh/stop-yarn.sh</span><br></pre></td></tr></table></figure>

<h2 id="7-配置历史服务器"><a href="#7-配置历史服务器" class="headerlink" title="7. 配置历史服务器"></a>7. 配置历史服务器</h2><p>配置mapred-site.xml</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 历史服务器端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>linux1:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">&lt;!-- 历史服务器web端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>linux1:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>分发配置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">xsync $HADOOP_HOME/etc/hadoop/mapred-site.xml</span><br></pre></td></tr></table></figure>

<p>在linux1上启动历史服务器</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mapred --daemon start historyserver</span><br></pre></td></tr></table></figure>

<p>查看JobHistory</p>
<p><a href="http://linux1:19888/jobhistory">http://linux1:19888/jobhistory</a></p>
<h2 id="8-配置日志的聚集"><a href="#8-配置日志的聚集" class="headerlink" title="8. 配置日志的聚集"></a>8. 配置日志的聚集</h2><p>配置yarn-site.xml</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log.server.url<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>http://linux1:19888/jobhistory/logs<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>604800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>分发配置</p>
<p>重启NodeManager 、ResourceManager和HistoryServer</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">stop-yarn.sh</span><br><span class="line"></span><br><span class="line">mapred --daemon stop historyserver</span><br><span class="line"></span><br><span class="line">start-yarn.sh</span><br><span class="line"></span><br><span class="line">mapred --daemon start historyserver</span><br></pre></td></tr></table></figure>

<p>删除HDFS上已经存在的输出文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs dfs -rm -R /user/vincent/output</span><br></pre></td></tr></table></figure>

<p>执行WordCount程序</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /user/vincent/input /user/vincent/output</span><br></pre></td></tr></table></figure>

<p>查看日志</p>
<p><a href="http://linux1:19888/jobhistory">http://linux1:19888/jobhistory</a></p>
<h2 id="9-集群时间同步"><a href="#9-集群时间同步" class="headerlink" title="9. 集群时间同步"></a>9. 集群时间同步</h2><p>时间同步的方式：找一个机器，作为时间服务器，所有的机器与这台集群时间进行定时的同步，比如，每隔十分钟，同步一次时间。</p>
<h3 id="时间服务器配置（必须root用户）"><a href="#时间服务器配置（必须root用户）" class="headerlink" title="时间服务器配置（必须root用户）"></a>时间服务器配置（必须root用户）</h3><h4 id="1-在所有节点关闭ntp服务和自启动"><a href="#1-在所有节点关闭ntp服务和自启动" class="headerlink" title="1. 在所有节点关闭ntp服务和自启动"></a>1. 在所有节点关闭ntp服务和自启动</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo systemctl stop ntpd</span><br><span class="line">sudo systemctl disable ntpd</span><br></pre></td></tr></table></figure>

<h4 id="2-修改ntp配置文件"><a href="#2-修改ntp配置文件" class="headerlink" title="2. 修改ntp配置文件"></a>2. 修改ntp配置文件</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim /etc/ntp.conf</span><br></pre></td></tr></table></figure>

<p>a）修改1（授权192.168.1.0-192.168.1.255网段上的所有机器可以从这台机器上查询和同步时间）</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">修改</span><br><span class="line">#restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap</span><br><span class="line">为</span><br><span class="line">restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap</span><br></pre></td></tr></table></figure>

<p>b）修改2（集群在局域网中，不使用其他互联网上的时间）</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">修改</span><br><span class="line">server 0.centos.pool.ntp.org iburst</span><br><span class="line">server 1.centos.pool.ntp.org iburst</span><br><span class="line">server 2.centos.pool.ntp.org iburst</span><br><span class="line">server 3.centos.pool.ntp.org iburst</span><br><span class="line">为</span><br><span class="line">#server 0.centos.pool.ntp.org iburst</span><br><span class="line">#server 1.centos.pool.ntp.org iburst</span><br><span class="line">#server 2.centos.pool.ntp.org iburst</span><br><span class="line">#server 3.centos.pool.ntp.org iburst</span><br></pre></td></tr></table></figure>

<p>c）添加3（当该节点丢失网络连接，依然可以采用本地时间作为时间服务器为集群中的其他节点提供时间同步）</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">server 127.127.1.0</span><br><span class="line">fudge 127.127.1.0 stratum 10</span><br></pre></td></tr></table></figure>

<h4 id="3-修改-etc-sysconfig-ntpd-文件"><a href="#3-修改-etc-sysconfig-ntpd-文件" class="headerlink" title="3. 修改/etc/sysconfig/ntpd 文件"></a>3. 修改/etc/sysconfig/ntpd 文件</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim /etc/sysconfig/ntpd</span><br></pre></td></tr></table></figure>

<p>增加内容如下（让硬件时间与系统时间一起同步）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">SYNC_HWCLOCK=yes</span><br></pre></td></tr></table></figure>

<p>Tips:</p>
<p>系统时间: 一般说来就是我们执行 date命令看到的时间，linux系统下所有的时间调用（除了直接访问硬件时间的命令）都是使用的这个时间。 </p>
<p>硬件时间: 主板上BIOS中的时间，由主板电池供电来维持运行，系统开机时要读取这个时间，并根据它来设定系统时间（注意：系统启动时根据硬件时间设定系统时间的过程可能存在时区换算，这要视具体的系统及相关设置而定）</p>
<h4 id="4-重新启动ntpd服务"><a href="#4-重新启动ntpd服务" class="headerlink" title="4. 重新启动ntpd服务"></a>4. 重新启动ntpd服务</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl start ntpd</span><br></pre></td></tr></table></figure>

<h4 id="5-设置ntpd服务开机启动"><a href="#5-设置ntpd服务开机启动" class="headerlink" title="5. 设置ntpd服务开机启动"></a>5. 设置ntpd服务开机启动</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl enable ntpd</span><br></pre></td></tr></table></figure>

<h3 id="其他机器配置（必须root用户）"><a href="#其他机器配置（必须root用户）" class="headerlink" title="其他机器配置（必须root用户）"></a>其他机器配置（必须root用户）</h3><p>在其他机器配置10分钟与时间服务器同步一次</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">crontab -e</span><br></pre></td></tr></table></figure>

<p>编写定时任务如下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">*/10 * * * * /usr/sbin/ntpdate linux1</span><br></pre></td></tr></table></figure>

<p>修改任意机器时间</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">date -s &quot;2017-9-11 11:11:11&quot;</span><br></pre></td></tr></table></figure>

<p>十分钟后查看机器是否与时间服务器同步</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">date</span><br></pre></td></tr></table></figure>

<h2 id="10、HDFS存储多目录"><a href="#10、HDFS存储多目录" class="headerlink" title="10、HDFS存储多目录"></a>10、HDFS存储多目录</h2><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///dfs/data1,file:///hd2/dfs/data2,file:///hd3/dfs/data3,file:///hd4/dfs/data4<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>hd1、hd2、hd3、hd4为一个节点上的多个磁盘。</p>
<h2 id="11、集群数据均衡"><a href="#11、集群数据均衡" class="headerlink" title="11、集群数据均衡"></a>11、集群数据均衡</h2><h3 id="1）节点间数据均衡"><a href="#1）节点间数据均衡" class="headerlink" title="1）节点间数据均衡"></a>1）节点间数据均衡</h3><p>开启数据均衡命令：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">start-balancer.sh -threshold 10</span><br></pre></td></tr></table></figure>

<p>对于参数10，代表的是集群中各个节点的磁盘空间利用率相差不超过10%，可根据实际情况进行调整。</p>
<p>停止数据均衡命令：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">stop-balancer.sh</span><br></pre></td></tr></table></figure>

<h3 id="2）磁盘间数据均衡"><a href="#2）磁盘间数据均衡" class="headerlink" title="2）磁盘间数据均衡"></a>2）磁盘间数据均衡</h3><p>（1）生成均衡计划</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs diskbalancer -plan linux2</span><br></pre></td></tr></table></figure>

<p>在当前路径生成linux2.plan.json</p>
<p>（2）执行均衡计划</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs diskbalancer -execute linux2.plan.json</span><br></pre></td></tr></table></figure>

<p>（3）查看当前均衡任务的执行情况</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs diskbalancer -query linux2</span><br></pre></td></tr></table></figure>

<p>（4）取消均衡任务</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs diskbalancer -cancel linux2.plan.json</span><br></pre></td></tr></table></figure>

<h2 id="12、支持LZO压缩配置"><a href="#12、支持LZO压缩配置" class="headerlink" title="12、支持LZO压缩配置"></a>12、支持LZO压缩配置</h2><p>1）hadoop本身并不支持lzo压缩，故需要使用twitter提供的hadoop-lzo开源组件。hadoop-lzo需依赖hadoop和lzo进行编译。</p>
<p>2）将编译好后的hadoop-lzo-0.4.20.jar 放入hadoop-3.1.3/share/hadoop/common/</p>
<p>3）同步hadoop-lzo-0.4.20.jar到linux2、linux3</p>
<p>4）core-site.xml增加配置支持LZO压缩</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>io.compression.codecs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span></span><br><span class="line">        org.apache.hadoop.io.compress.GzipCodec,</span><br><span class="line">        org.apache.hadoop.io.compress.DefaultCodec,</span><br><span class="line">        org.apache.hadoop.io.compress.BZip2Codec,</span><br><span class="line">        org.apache.hadoop.io.compress.SnappyCodec,</span><br><span class="line">        com.hadoop.compression.lzo.LzoCodec,</span><br><span class="line">        com.hadoop.compression.lzo.LzopCodec</span><br><span class="line">    <span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>io.compression.codec.lzo.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.hadoop.compression.lzo.LzoCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>5）同步core-site.xml到linux2、linux3</p>
<p>6）测试wordcount</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount -Dmapreduce.output.fileoutputformat.compress=true -Dmapreduce.output.fileoutputformat.compress.codec=com.hadoop.compression.lzo.LzopCodec input /output</span><br></pre></td></tr></table></figure>

<h1 id="问题总结"><a href="#问题总结" class="headerlink" title="问题总结"></a>问题总结</h1><p>1）ERROR: Cannot set priority of datanode process</p>
<figure class="highlight gcode"><table><tr><td class="code"><pre><span class="line">将克隆的li<span class="symbol">nux2</span>和li<span class="symbol">nux3</span>上的hadoop卸载，重新安装。</span><br></pre></td></tr></table></figure>

<p>2）linux2，linux3没有没有datanode</p>
<figure class="highlight haskell"><table><tr><td class="code"><pre><span class="line">由于datanode的<span class="type">VERSION</span>不同导致，删除linux2，linux3的<span class="class"><span class="keyword">data</span>目录下文件即可。</span></span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title>Hbase</title>
    <url>/Hbase/</url>
    <content><![CDATA[<p><a href="http://hbase.apache.org/2.0/book.html">http://hbase.apache.org/2.0/book.html</a></p>
<h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>HBase是一种分布式、可扩展、支持海量数据（PB）存储的NoSQL数据库。</p>
<h2 id="数据模型"><a href="#数据模型" class="headerlink" title="数据模型"></a>数据模型</h2><p>传统数据库：数据库-表-行</p>
<p>hbase：命名空间-table-列族-行-列</p>
<p>逻辑上，HBase的数据模型同关系型数据库很类似，数据存储在一张表中，有行有列。但从HBase的<strong>底层物理存储结构（K-V）</strong>来看，HBase更像是一个multi-dimensional map。</p>
<p>hbase<strong>逻辑结构</strong>：</p>
<p><img src="/Hbase/22.png" alt="22"></p>
<p>region存在不同的数据库中。</p>
<p>hbase<strong>物理存储结构</strong>：</p>
<p><img src="/Hbase/23.png" alt="23"></p>
<p><strong>1）Name Space</strong></p>
<p>命名空间，<strong>类似于关系型数据库的database概念</strong>，<strong>每个命名空间下有多个表</strong>。HBase两个自带的命名空间，分别是hbase和default，hbase中存放的是HBase内置的表，default是用户默认使用的命名空间。</p>
<p><strong>2）Table</strong></p>
<p>类似于关系型数据库的表概念。不同的是，<strong>HBase定义表时只需要声明列族即可</strong>，<strong>不需要声明具体的列</strong>。这意味着，往HBase写入数据时，字段可以动态、按需指定。因此，<strong>和关系型数据库相比，HBase能够轻松应对字段变更的场景。</strong></p>
<p><strong>3）Row</strong></p>
<p>HBase表中的每行数据都由一个<strong>RowKey</strong>和多个<strong>Column（列）</strong>组成，数据是按照RowKey的字典顺序存储的，并且<strong>查询数据时只能根据RowKey进行检索，所以RowKey的设计十分重要</strong>。</p>
<p><strong>4）Column</strong></p>
<p>HBase中的每个列都由Column Family(列族)和Column Qualifier（列限定符）进行限定，例如info：name，info：age。<strong>建表时，只需指明列族，而列限定符无需预先定义。列族不建议超过2个。</strong></p>
<p><strong>5）TimeStamp</strong></p>
<p>用于标识数据的不同版本（version），每条数据写入时，系统会自动为其加上该字段，其值为写入HBase的时间。</p>
<p><strong>6）Cell</strong></p>
<p><strong>由{rowkey, column Family：column Qualifier, time Stamp} 确定唯一的单元。cell中的数据全部是字节码形式存贮。</strong></p>
<h2 id="HBase基本架构"><a href="#HBase基本架构" class="headerlink" title="HBase基本架构"></a>HBase基本架构</h2><p><img src="/Hbase/24.png" alt="24"></p>
<p>架构角色：</p>
<p><strong>1）RegionServer</strong></p>
<p>Region Server为 Region的管理者，其实现类为HRegionServer，主要作用如下:</p>
<p>对于数据的操作：get, put, delete；</p>
<p>对于Region的操作：splitRegion、compactRegion。</p>
<p><strong>2）Master</strong></p>
<p>Master是所有Region Server的管理者，其实现类为HMaster，主要作用如下：</p>
<p>对于表的操作：create, delete, alter</p>
<p>对于RegionServer的操作：分配regions到每个RegionServer，监控每个RegionServer的状态，负载均衡和故障转移。</p>
<p><strong>3）Zookeeper</strong></p>
<p>HBase通过Zookeeper来做master的高可用、RegionServer的监控、元数据的入口以及集群配置的维护等工作。</p>
<p><strong>4）HDFS</strong></p>
<p>HDFS为Hbase提供最终的底层数据存储服务，同时为HBase提供高可用的支持。</p>
<h2 id="HBase安装部署"><a href="#HBase安装部署" class="headerlink" title="HBase安装部署"></a>HBase安装部署</h2><h3 id="1、Zookeeper正常部署"><a href="#1、Zookeeper正常部署" class="headerlink" title="1、Zookeeper正常部署"></a>1、Zookeeper正常部署</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 zookeeper-3.5.7]$ zk start</span><br></pre></td></tr></table></figure>

<h3 id="2、Hadoop正常部署"><a href="#2、Hadoop正常部署" class="headerlink" title="2、Hadoop正常部署"></a>2、Hadoop正常部署</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hadoop-3.1.3]$ mycluster start</span><br></pre></td></tr></table></figure>

<h3 id="3、HBase的解压"><a href="#3、HBase的解压" class="headerlink" title="3、HBase的解压"></a>3、HBase的解压</h3><p>解压Hbase到指定目录</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 software]$ tar -zxvf hbase-2.0.5-bin.tar.gz -C /opt/module</span><br><span class="line">[vincent@linux1 software]$ mv /opt/module/hbase-2.0.5 /opt/module/hbase</span><br></pre></td></tr></table></figure>

<p>配置环境变量</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">HBASE_HOME</span></span><br><span class="line">export HBASE_HOME=/opt/module/hbase</span><br><span class="line">export PATH=$PATH:$HBASE_HOME/bin</span><br></pre></td></tr></table></figure>

<h3 id="4、HBase的配置文件"><a href="#4、HBase的配置文件" class="headerlink" title="4、HBase的配置文件"></a>4、HBase的配置文件</h3><p>1.hbase-env.sh修改内容(第125行)</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export HBASE_MANAGES_ZK=false</span><br></pre></td></tr></table></figure>

<p>2.hbase-site.xml修改内容</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://linux1:9820/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>linux1,linux2,linux3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.unsafe.stream.capability.enforce<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.wal.provider<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>filesystem<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.master.maxclockskew<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>180000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Time difference of regionserver from master<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>3.regionservers</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">linux1</span><br><span class="line">linux2</span><br><span class="line">linux3</span><br></pre></td></tr></table></figure>

<h3 id="5、HBase远程发送到其他集群"><a href="#5、HBase远程发送到其他集群" class="headerlink" title="5、HBase远程发送到其他集群"></a>5、HBase远程发送到其他集群</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 module]$ xsync hbase</span><br></pre></td></tr></table></figure>

<h3 id="6、HBase服务的启动"><a href="#6、HBase服务的启动" class="headerlink" title="6、HBase服务的启动"></a>6、HBase服务的启动</h3><p>单点启动</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hbase]$ bin/hbase-daemon.sh start master</span><br><span class="line">[vincent@linux1 hbase]$ bin/hbase-daemon.sh start regionserver</span><br></pre></td></tr></table></figure>

<p>群启群停</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hbase]$ bin/start-hbase.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> bin/start-hbase.sh  在当前节点启动master，其他节点启动region server</span></span><br><span class="line">[vincent@linux1 hbase]$ bin/stop-hbase.sh</span><br></pre></td></tr></table></figure>

<h3 id="7、查看HBase页面"><a href="#7、查看HBase页面" class="headerlink" title="7、查看HBase页面"></a>7、查看HBase页面</h3><figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">http</span>://linux<span class="number">1</span>:<span class="number">16010</span> </span><br></pre></td></tr></table></figure>

<h3 id="8、高可用-可选"><a href="#8、高可用-可选" class="headerlink" title="8、高可用(可选)"></a>8、高可用(可选)</h3><p>在HBase中HMaster负责监控HRegionServer的生命周期，均衡RegionServer的负载，如果HMaster挂掉了，那么整个HBase集群将陷入不健康的状态，并且此时的工作状态并不会维持太久。所以HBase支持对HMaster的高可用配置。</p>
<p>1.关闭HBase集群（如果没有开启则跳过此步）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hbase]$ bin/stop-hbase.sh</span><br></pre></td></tr></table></figure>

<p>2.在conf目录下创建backup-masters文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hbase]$ touch conf/backup-masters</span><br></pre></td></tr></table></figure>

<p>3.在backup-masters文件中配置高可用HMaster节点</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hbase]$ echo linux2 &gt; conf/backup-masters</span><br></pre></td></tr></table></figure>

<p>4.将整个conf目录scp到其他节点</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hbase]$ scp -r conf/ linux2:/opt/module/hbase/</span><br><span class="line">[vincent@linux1 hbase]$ scp -r conf/ linux3:/opt/module/hbase/</span><br></pre></td></tr></table></figure>

<h2 id="HBase-Shell操作"><a href="#HBase-Shell操作" class="headerlink" title="HBase Shell操作"></a>HBase Shell操作</h2><h3 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h3><p>进入HBase客户端命令行</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 hbase]$ bin/hbase shell</span><br></pre></td></tr></table></figure>

<p>查看帮助命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase(main):001:0&gt; help</span><br></pre></td></tr></table></figure>

<p>列出所有用户表</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase(main):002:0&gt; list</span><br></pre></td></tr></table></figure>

<h3 id="表的操作"><a href="#表的操作" class="headerlink" title="表的操作"></a>表的操作</h3><h4 id="1、创建表"><a href="#1、创建表" class="headerlink" title="1、创建表"></a>1、创建表</h4><p>create ‘表名’, ‘列族’</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hbase(main):002:0&gt; create &#x27;student&#x27;,&#x27;info&#x27;</span><br></pre></td></tr></table></figure>

<h4 id="2、插入数据到表"><a href="#2、插入数据到表" class="headerlink" title="2、插入数据到表"></a>2、插入数据到表</h4><p>put ‘表名’, ‘RowKey’, ‘列族:列标识符’, ‘数据’</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hbase(main):003:0&gt; put &#x27;student&#x27;,&#x27;1001&#x27;,&#x27;info:sex&#x27;,&#x27;male&#x27;</span><br><span class="line">hbase(main):004:0&gt; put &#x27;student&#x27;,&#x27;1001&#x27;,&#x27;info:age&#x27;,&#x27;18&#x27;</span><br><span class="line">hbase(main):005:0&gt; put &#x27;student&#x27;,&#x27;1002&#x27;,&#x27;info:name&#x27;,&#x27;Janna&#x27;</span><br><span class="line">hbase(main):006:0&gt; put &#x27;student&#x27;,&#x27;1002&#x27;,&#x27;info:sex&#x27;,&#x27;female&#x27;</span><br><span class="line">hbase(main):007:0&gt; put &#x27;student&#x27;,&#x27;1002&#x27;,&#x27;info:age&#x27;,&#x27;20&#x27;</span><br></pre></td></tr></table></figure>

<h4 id="3、扫描查看表数据"><a href="#3、扫描查看表数据" class="headerlink" title="3、扫描查看表数据"></a>3、扫描查看表数据</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hbase(main):008:0&gt; scan &#x27;student&#x27;</span><br><span class="line">hbase(main):009:0&gt; scan &#x27;student&#x27;,&#123;STARTROW =&gt; &#x27;1001&#x27;, STOPROW  =&gt; &#x27;1002&#x27;&#125;</span><br><span class="line">hbase(main):010:0&gt; scan &#x27;student&#x27;,&#123;STARTROW =&gt; &#x27;1001&#x27;&#125;</span><br></pre></td></tr></table></figure>

<h4 id="4、查看表结构"><a href="#4、查看表结构" class="headerlink" title="4、查看表结构"></a>4、查看表结构</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase(main):011:0&gt; describe &#x27;student&#x27;</span><br></pre></td></tr></table></figure>

<h4 id="5、更新指定字段的数据"><a href="#5、更新指定字段的数据" class="headerlink" title="5、更新指定字段的数据"></a>5、更新指定字段的数据</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hbase(main):012:0&gt; put &#x27;student&#x27;,&#x27;1001&#x27;,&#x27;info:name&#x27;,&#x27;Nick&#x27;</span><br><span class="line">hbase(main):013:0&gt; put &#x27;student&#x27;,&#x27;1001&#x27;,&#x27;info:age&#x27;,&#x27;100&#x27;</span><br></pre></td></tr></table></figure>

<h4 id="6、查看“指定行”或“指定列族-列”的数据"><a href="#6、查看“指定行”或“指定列族-列”的数据" class="headerlink" title="6、查看“指定行”或“指定列族:列”的数据"></a>6、查看“指定行”或“指定列族:列”的数据</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase(main):014:0&gt; get &#x27;student&#x27;,&#x27;1001&#x27;</span><br><span class="line">hbase(main):015:0&gt; get &#x27;student&#x27;,&#x27;1001&#x27;,&#x27;info:name&#x27;</span><br></pre></td></tr></table></figure>

<h4 id="7、统计表数据行数"><a href="#7、统计表数据行数" class="headerlink" title="7、统计表数据行数"></a>7、统计表数据行数</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hbase(main):021:0&gt; count &#x27;student&#x27;</span><br></pre></td></tr></table></figure>

<h4 id="8、删除数据"><a href="#8、删除数据" class="headerlink" title="8、删除数据"></a>8、删除数据</h4><p>删除某rowkey的全部数据：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hbase(main):016:0&gt; deleteall &#x27;student&#x27;,&#x27;1001&#x27;</span><br></pre></td></tr></table></figure>

<p>删除某rowkey的某一列数据：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase(main):017:0&gt; delete &#x27;student&#x27;,&#x27;1002&#x27;,&#x27;info:sex&#x27;</span><br></pre></td></tr></table></figure>

<h4 id="9、清空表数据"><a href="#9、清空表数据" class="headerlink" title="9、清空表数据"></a>9、清空表数据</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hbase(main):018:0&gt; truncate &#x27;student&#x27;</span><br></pre></td></tr></table></figure>

<p>提示：清空表的操作顺序为先disable，然后再truncate。</p>
<h4 id="10、删除表"><a href="#10、删除表" class="headerlink" title="10、删除表"></a>10、删除表</h4><p>首先需要先让该表为disable状态</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hbase(main):019:0&gt; disable &#x27;student&#x27;</span><br></pre></td></tr></table></figure>

<p>然后才能drop这个表</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hbase(main):020:0&gt; drop &#x27;student&#x27;</span><br></pre></td></tr></table></figure>

<p>提示：如果直接drop表，会报错：ERROR: Table student is enabled. Disable it first.</p>
<h4 id="11、变更表信息"><a href="#11、变更表信息" class="headerlink" title="11、变更表信息"></a>11、变更表信息</h4><p>将<strong>info列族</strong>中的数据存放3个版本</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hbase(main):022:0&gt; alter &#x27;student&#x27;,&#123;NAME=&gt;&#x27;info&#x27;,VERSIONS=&gt;3&#125;</span><br><span class="line">hbase(main):022:0&gt; get &#x27;student&#x27;,&#x27;1001&#x27;,&#123;COLUMN=&gt;&#x27;info:name&#x27;,VERSIONS=&gt;3&#125;</span><br></pre></td></tr></table></figure>

<h4 id="12、namespace操作"><a href="#12、namespace操作" class="headerlink" title="12、namespace操作"></a>12、namespace操作</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 列出所有namespace</span><br><span class="line">list_namespace</span><br><span class="line"># 创建namespace</span><br><span class="line">create_namespace &#x27;name_test&#x27;</span><br><span class="line"># 删除namespace</span><br><span class="line">drop_namespace &#x27;name_test&#x27;</span><br><span class="line"># 查看namespace</span><br><span class="line">describe_namespace &#x27;name_test&#x27;</span><br><span class="line"># 在namespace下创建表</span><br><span class="line">create &#x27;name_test:test1&#x27;, &#x27;cf1&#x27;</span><br><span class="line"># 查看namespace下的表</span><br><span class="line">list_namespace_tables &#x27;name_test&#x27;</span><br><span class="line"># 删除命名空间时 必须保证命名空间中没有表</span><br><span class="line">disable &#x27;name_test:test1&#x27;</span><br><span class="line">drop &#x27;name_test:test1&#x27;</span><br><span class="line">drop_namespace &#x27;name_test&#x27;</span><br></pre></td></tr></table></figure>

<h2 id="HBase进阶"><a href="#HBase进阶" class="headerlink" title="HBase进阶"></a>HBase进阶</h2><h3 id="RegionServer-架构"><a href="#RegionServer-架构" class="headerlink" title="RegionServer 架构"></a>RegionServer 架构</h3><p><img src="/Hbase/25.png" alt="25"></p>
<p><strong>1）StoreFile</strong></p>
<p>保存实际数据的物理文件，StoreFile以Hfile的形式存储在HDFS上。每个Store会有一个或多个StoreFile（HFile），<strong>数据在每个StoreFile中都是有序的</strong>。</p>
<p><strong>2）MemStore</strong></p>
<p>写缓存，由于HFile中的数据要求是有序的，所以数据是先存储在MemStore中，排好序后，等到达刷写时机才会刷写到HFile，每次刷写都会形成一个新的HFile。</p>
<p><strong>3）WAL</strong></p>
<p>由于数据要经MemStore排序后才能刷写到HFile，但把数据保存在内存中会有很高的概率导致数据丢失，为了解决这个问题，数据会先写在一个叫做Write-Ahead logfile的文件中，然后再写入MemStore中。所以在系统出现故障的时候，数据可以通过这个日志文件重建。</p>
<p><strong>4）BlockCache</strong></p>
<p>读缓存，每次查询出的数据会缓存在BlockCache中，方便下次查询。</p>
<h3 id="写流程"><a href="#写流程" class="headerlink" title="写流程"></a>写流程</h3><p><img src="/Hbase/26.png" alt="26"></p>
<p>1）Client先访问zookeeper，获取hbase:meta表位于哪个Region Server。</p>
<p>2）访问对应的Region Server，<strong>获取hbase:meta表</strong>，根据读请求的namespace:table/rowkey，查询出目标数据位于哪个Region Server中的哪个Region中。并将该table的region信息以及meta表的位置信息缓存在客户端的meta cache，方便下次访问。</p>
<p>3）与目标Region Server进行通讯；</p>
<p>4）将数据<strong>顺序写入（追加）到WAL</strong>；</p>
<p>5）将数据<strong>写入对应的MemStore</strong>，数据会在MemStore进行<strong>排序</strong>；</p>
<p>6）向客户端发送ack；</p>
<p>7）等达到MemStore的刷写时机后，将<strong>数据刷写到HFile</strong>。</p>
<h3 id="MemStore-Flush"><a href="#MemStore-Flush" class="headerlink" title="MemStore Flush"></a>MemStore Flush</h3><p><img src="/Hbase/27.png" alt="27"></p>
<p><strong>MemStore刷写时机：</strong></p>
<p><strong>所有的flush都是以Region为单位刷新</strong></p>
<p><strong>1）MemStore级别</strong><br>当 Region中 某个 MemStore 的大小达到了hbase.hregion.memstore.flush.size（默认值128M），会触发Region的刷写（若Region中有多个Store，只要有其中一个达到hbase.hregion.memstore.flush.size（默认值128M）值，就会触发flush，每个Store都会生成一个StroeFile文件，可能会生成多个小文件，所以一般情况下，一个Region只设置一个列簇（即一个Store））</p>
<p><strong>2）Region级别</strong><br>当处于写高峰的时候，会延迟触发第一个时机（MemStore级别）</p>
<p>当 Region 中的MemStore的总大小达到了hbase.hregion.memstore.flush.size（默认值128M）* hbase.hregion.memstore.block.multiplier（默认值4）时，会阻塞所有写入该 Region 的写请求，优先flush！这时候如果往 MemStore 写数据，会出现 RegionTooBusyException 异常。</p>
<p>举个例子：当 Region 中的某个 MemStore 占用内存达到128M ，会触发flush ，此时是允许写入操作的；若写入操作大于 flush 的速度，当 Region 中的所有 MemStore 占用内存达到 128 * 4 = 512M 时，阻止所有写入该 Region 的写请求，直到Region 中的所有 MemStore ，flush完毕，才取消阻塞，允许写入。</p>
<p><strong>3）RegionServer级别</strong><br>当RegionServer中，所有Region中的MemStore的总大小达到<br>java_heapsize * hbase.regionserver.global.memstore.size（默认值0.4）* hbase.regionserver.global.memstore.size.lower.limit（默认值0.95）会flush，flush的时候根据每个Region中，总MemStore占用的大小进行降序排序，依次flush；flush的时候优先flush占用空间大的region，每flush一个region，会查看总的占用大小是否小于 java_heapsize * hbase.regionserver.global.memstore.size（默认值0.4）* hbase.regionserver.global.memstore.size.lower.limit（默认值0.95）；如果还是大于，则继续flush region，若小于，则停止flush（注：此情况是允许memStore写入的）</p>
<p>当处于写高峰的时候，会延迟触发第三个时机（HRegionServer级别）</p>
<p>当RegionServer中，所有Region中的MemStore的总大小达到了 java_heapsize * hbase.regionserver.global.memstore.size（默认值0.4）时，会阻塞当前 RegionServer 的所有写请求（无法往Region中的MemStore写入数据），直到RegionServer中，所有Region中的MemStore的总大小 低于 java_heapsize * hbase.regionserver.global.memstore.size（默认值0.4）* hbase.regionserver.global.memstore.size.lower.limit（默认值0.95）时，才取消阻塞（允许写入数据）</p>
<p>举个例子，如果 HBase 堆内存总共是 10G，按照默认的比例，那么触发 RegionServer级别的flush ，是 RegionServer 中所有的 MemStore 占用内存为：10 * 0.4 * 0.95 = 3.8G时触发flush，此时是允许写入操作的，若写入操作大于flush的速度，当占用内存达到 10 * 0.4 = 4G 时，阻止所有写入操作，直到占用内存低于 3.8G ，才取消阻塞，允许写入。</p>
<p><strong>4）HLog级别</strong><br>当WAL文件的数量超过hbase.regionserver.maxlogs，region会按照时间顺序依次进行刷写，直到WAL文件数量减小到hbase.regionserver.maxlogs以下（该属性名已经废弃，现无需手动设置，最大值为32）。</p>
<p>WAL 数量触发的刷写策略是，找到最旧的 un-archived WAL 文件，并找到这个 WAL 文件对应的 Regions， 然后对这些 Regions 进行刷写。</p>
<p><strong>5）定期刷写</strong><br>到达自动刷写的时间，也会触发MemStore flush。自动刷新的时间间隔由该属性进行配置hbase.regionserver.optionalcacheflushinterval（默认1小时），指的是当前 MemStore 1小时会进行一次刷写。如果设定为0，则关闭定时自动刷写。</p>
<p><strong>6）手动刷写</strong><br>可以通过 shell 命令”flush ‘table’”或者”flush ‘regionname’”分别对一个Region或者多个Region进行flush</p>
<h3 id="读流程"><a href="#读流程" class="headerlink" title="读流程"></a>读流程</h3><p><img src="/Hbase/28.png" alt="28"></p>
<p><img src="/Hbase/29.png" alt="29"></p>
<p>1）Client先访问zookeeper，获取hbase:meta表位于哪个Region Server。</p>
<p>2）访问对应的Region Server，获取hbase:meta表，根据读请求的namespace:table/rowkey，查询出目标数据位于哪个Region Server中的哪个Region中。并将该table的region信息以及meta表的位置信息缓存在客户端的meta cache，方便下次访问。</p>
<p>3）与目标Region Server进行通讯；</p>
<p>4）分别在MemStore和Store File（HFile）中查询目标数据，并将查到的所有数据进行合并。此处所有数据是指同一条数据的不同版本（time stamp）或者不同的类型（Put/Delete）。</p>
<p>5）将查询到的新的数据块（Block，HFile数据存储单元，默认大小为64KB）缓存到Block Cache。</p>
<p>6）将合并后的最终结果返回给客户端。</p>
<h3 id="StoreFile-Compaction"><a href="#StoreFile-Compaction" class="headerlink" title="StoreFile Compaction"></a>StoreFile Compaction</h3><p><strong>由于memstore每次刷写都会生成一个新的HFile</strong>，<strong>且同一个字段的不同版本（timestamp）和不同类型（Put/Delete）有可能会分布在不同的HFile中</strong>，因此<strong>查询时需要遍历所有的HFile</strong>。<strong>为了减少HFile的个数，以及清理掉过期和删除的数据，会进行StoreFile Compaction</strong>。</p>
<p>Compaction分为两种，分别是<strong>Minor Compaction</strong>和<strong>Major Compaction</strong>。Minor Compaction会<strong>将临近的若干个较小的HFile合并成一个较大的HFile</strong>，并清理掉部分过期和删除的数据。Major Compaction会将<strong>一个Store下的所有的HFile合并成一个大HFile</strong>，并且清理掉所有过期和删除的数据。</p>
<p><img src="/Hbase/30.png" alt="30"></p>
<h3 id="Region-Split"><a href="#Region-Split" class="headerlink" title="Region Split"></a>Region Split</h3><p>默认情况下，每个Table起初只有一个Region，随着数据的不断写入，Region会自动进行拆分。刚拆分时，两个子Region都位于当前的Region Server，但处于负载均衡的考虑，HMaster有可能会将某个Region转移给其他的Region Server。</p>
<p>Region Split时机：</p>
<p>1.当1个region中的某个Store下所有StoreFile的总大小超过hbase.hregion.max.filesize（默认10G），该Region就会进行拆分（0.94版本之前）。</p>
<p>2.当1个region中的某个Store下所有StoreFile的总大小超过Min(initialSize<em>R^3 ,hbase.hregion.max.filesize”)，该Region就会进行拆分。其中initialSize的默认值为2</em>hbase.hregion.memstore.flush.size，R为当前Region Server中属于该Table的Region个数（0.94版本之后）。</p>
<p>具体的切分策略为：</p>
<p>第一次split：1^3 * 256 = 256MB </p>
<p>第二次split：2^3 * 256 = 2048MB </p>
<p>第三次split：3^3 * 256 = 6912MB </p>
<p>第四次split：4^3 * 256 = 16384MB &gt; 10GB，因此取较小的值10GB </p>
<p>后面每次split的size都是10GB了。</p>
<p>3.Hbase 2.0引入了新的split策略：如果当前RegionServer上该表只有一个Region，按照                                    2 * hbase.hregion.memstore.flush.size（256M）分裂，否则按照hbase.hregion.max.filesize（10G）分裂。</p>
<p><img src="/Hbase/31.png" alt="31"></p>
<h2 id="HBase-API"><a href="#HBase-API" class="headerlink" title="HBase API"></a>HBase API</h2><p>Maven依赖</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.0.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="HbaseDDL"><a href="#HbaseDDL" class="headerlink" title="HbaseDDL"></a>HbaseDDL</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HbaseDDL</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//获取hbase连接</span></span><br><span class="line">  <span class="keyword">val</span> conf: <span class="type">Configuration</span> = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line">  conf.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;linux1,linux2,linux3&quot;</span>)</span><br><span class="line">  <span class="keyword">val</span> conn: <span class="type">Connection</span> = <span class="type">ConnectionFactory</span>.createConnection(conf)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    deleteNS(<span class="string">&quot;xsh&quot;</span>)</span><br><span class="line"></span><br><span class="line">    closeConnection()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 判断表存在</span></span><br><span class="line"><span class="comment">   * @param name</span></span><br><span class="line"><span class="comment">   * @return</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">tableExists</span></span>(name: <span class="type">String</span>) = &#123;</span><br><span class="line">    <span class="comment">//获取管理对象 Admin</span></span><br><span class="line">    <span class="keyword">val</span> admin: <span class="type">Admin</span> = conn.getAdmin</span><br><span class="line">    <span class="comment">//利用admin操作</span></span><br><span class="line">    <span class="keyword">val</span> tableName: <span class="type">TableName</span> = <span class="type">TableName</span>.valueOf(name)</span><br><span class="line">    <span class="keyword">val</span> bool: <span class="type">Boolean</span> = admin.tableExists(tableName)</span><br><span class="line">    <span class="comment">//关闭admin</span></span><br><span class="line">    admin.close()</span><br><span class="line">    bool</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 创建表</span></span><br><span class="line"><span class="comment">   * @param name</span></span><br><span class="line"><span class="comment">   * @param cfs</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createTable</span></span>(name: <span class="type">String</span>, cfs: <span class="type">String</span>*): <span class="type">Boolean</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> admin: <span class="type">Admin</span> = conn.getAdmin</span><br><span class="line">    <span class="keyword">val</span> tableName: <span class="type">TableName</span> = <span class="type">TableName</span>.valueOf(name)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(tableExists(name)) <span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> table: <span class="type">TableDescriptorBuilder</span> = <span class="type">TableDescriptorBuilder</span>.newBuilder(tableName)</span><br><span class="line">    cfs.foreach(cf =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> descriptor: <span class="type">ColumnFamilyDescriptor</span> = <span class="type">ColumnFamilyDescriptorBuilder</span>.newBuilder(<span class="type">Bytes</span>.toBytes(cf)).build()</span><br><span class="line">      table.setColumnFamily(descriptor)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    admin.createTable(table.build())</span><br><span class="line">    admin.close()</span><br><span class="line"></span><br><span class="line">    <span class="literal">true</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 删除表</span></span><br><span class="line"><span class="comment">   * @param name</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">deleteTable</span></span>(name: <span class="type">String</span>) = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> admin: <span class="type">Admin</span> = conn.getAdmin</span><br><span class="line">    <span class="keyword">if</span>(tableExists(name))&#123;</span><br><span class="line">      admin.disableTable(<span class="type">TableName</span>.valueOf(name))</span><br><span class="line">      admin.deleteTable(<span class="type">TableName</span>.valueOf(name))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    admin.close()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 判断namespace存在</span></span><br><span class="line"><span class="comment">   * @param name</span></span><br><span class="line"><span class="comment">   * @return</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">nsExists</span></span>(name: <span class="type">String</span>) = &#123;</span><br><span class="line">    <span class="keyword">val</span> admin: <span class="type">Admin</span> = conn.getAdmin</span><br><span class="line">    <span class="keyword">val</span> descriptors: <span class="type">Array</span>[<span class="type">NamespaceDescriptor</span>] = admin.listNamespaceDescriptors()</span><br><span class="line">    <span class="keyword">val</span> bool: <span class="type">Boolean</span> = descriptors.map(_.getName).contains(name)</span><br><span class="line">    admin.close()</span><br><span class="line">    bool</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 创建namespace</span></span><br><span class="line"><span class="comment">   * @param name</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createNS</span></span>(name: <span class="type">String</span>) = &#123;</span><br><span class="line">    <span class="keyword">val</span> admin: <span class="type">Admin</span> = conn.getAdmin</span><br><span class="line">    <span class="keyword">if</span>(!nsExists(name))&#123;</span><br><span class="line">      <span class="keyword">val</span> descriptor: <span class="type">NamespaceDescriptor</span> = <span class="type">NamespaceDescriptor</span>.create(name).build()</span><br><span class="line">      admin.createNamespace(descriptor)</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">      println(<span class="string">s&quot;您要创建的namespace：<span class="subst">$&#123;name&#125;</span>已经存在.&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    admin.close()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 删除namespace</span></span><br><span class="line"><span class="comment">   * @param name</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">deleteNS</span></span>(name: <span class="type">String</span>) = &#123;</span><br><span class="line">    <span class="keyword">val</span> admin: <span class="type">Admin</span> = conn.getAdmin</span><br><span class="line">    <span class="keyword">if</span> (nsExists(name))&#123;</span><br><span class="line">      admin.deleteNamespace(name)</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">      println(<span class="string">s&quot;<span class="subst">$&#123;name&#125;</span>不存在&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">closeConnection</span></span>() = conn.close()</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="HbaseDML"><a href="#HbaseDML" class="headerlink" title="HbaseDML"></a>HbaseDML</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HbaseDML</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 1. 先获取到hbase的连接</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line">    conf.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;linux1,linux2,linux3&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> conn: <span class="type">Connection</span> = <span class="type">ConnectionFactory</span>.createConnection(conf)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//        putData(&quot;user&quot;, &quot;1001&quot;, &quot;info&quot;, &quot;name&quot;, &quot;lisi&quot;)</span></span><br><span class="line">        <span class="comment">//        putData(&quot;user&quot;, &quot;1002&quot;, &quot;info&quot;, &quot;name&quot;, &quot;ww&quot;)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">//        deleteData(&quot;user1&quot;, &quot;1001&quot;, &quot;info&quot;, &quot;age&quot;)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">//        getData(&quot;user&quot;, &quot;1001&quot;, &quot;info&quot;, &quot;name&quot;)</span></span><br><span class="line">        scanData(<span class="string">&quot;user&quot;</span>)</span><br><span class="line">        closeConnection()</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">scanData</span></span>(tableName: <span class="type">String</span>) = &#123;</span><br><span class="line">        <span class="keyword">val</span> table: <span class="type">Table</span> = conn.getTable(<span class="type">TableName</span>.valueOf(tableName))</span><br><span class="line">        <span class="keyword">val</span> scan = <span class="keyword">new</span> <span class="type">Scan</span>()</span><br><span class="line">        <span class="keyword">val</span> filter =</span><br><span class="line">            <span class="keyword">new</span> <span class="type">SingleColumnValueFilter</span>(<span class="type">Bytes</span>.toBytes(<span class="string">&quot;info&quot;</span>), <span class="type">Bytes</span>.toBytes(<span class="string">&quot;name&quot;</span>), <span class="type">CompareOperator</span>.<span class="type">EQUAL</span>, <span class="type">Bytes</span>.toBytes(<span class="string">&quot;abc&quot;</span>))</span><br><span class="line">        scan.setFilter(filter)</span><br><span class="line">        <span class="keyword">val</span> results: <span class="type">ResultScanner</span> = table.getScanner(scan)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">import</span> scala.collection.<span class="type">JavaConversions</span>._</span><br><span class="line">        <span class="comment">// 从scanner拿到所有数据</span></span><br><span class="line">        <span class="keyword">for</span> (result &lt;- results) &#123;</span><br><span class="line">            <span class="keyword">val</span> cells: util.<span class="type">List</span>[<span class="type">Cell</span>] = result.listCells() <span class="comment">// rawCells</span></span><br><span class="line">            <span class="keyword">if</span> (cells != <span class="literal">null</span>) &#123;</span><br><span class="line">                <span class="keyword">for</span> (cell &lt;- cells) &#123;</span><br><span class="line">                    println(</span><br><span class="line">                        <span class="string">s&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">                           |row = $&#123;Bytes.toString(CellUtil.cloneRow(cell))&#125;</span></span><br><span class="line"><span class="string">                           |cf = $&#123;Bytes.toString(CellUtil.cloneFamily(cell))&#125;</span></span><br><span class="line"><span class="string">                           |name = $&#123;Bytes.toString(CellUtil.cloneQualifier(cell))&#125;</span></span><br><span class="line"><span class="string">                           |value = $&#123;Bytes.toString(CellUtil.cloneValue(cell))&#125;</span></span><br><span class="line"><span class="string">                           |----------------</span></span><br><span class="line"><span class="string">                           |&quot;</span><span class="string">&quot;&quot;</span>.stripMargin)</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        table.close()</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getData</span></span>(tableName: <span class="type">String</span>, rowKey: <span class="type">String</span>, cf: <span class="type">String</span>, columnName: <span class="type">String</span>) = &#123;</span><br><span class="line">        <span class="keyword">val</span> table: <span class="type">Table</span> = conn.getTable(<span class="type">TableName</span>.valueOf(tableName))</span><br><span class="line">        <span class="keyword">val</span> get = <span class="keyword">new</span> <span class="type">Get</span>(<span class="type">Bytes</span>.toBytes(rowKey))</span><br><span class="line">        get.addColumn(<span class="type">Bytes</span>.toBytes(cf), <span class="type">Bytes</span>.toBytes(columnName))</span><br><span class="line">        <span class="keyword">val</span> result: <span class="type">Result</span> = table.get(get)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 这个是用来在java的集合和scala的集合之间互转  (隐式转换)</span></span><br><span class="line">        <span class="keyword">import</span> scala.collection.<span class="type">JavaConversions</span>._</span><br><span class="line">        <span class="keyword">val</span> cells: util.<span class="type">List</span>[<span class="type">Cell</span>] = result.listCells() <span class="comment">// rawCells</span></span><br><span class="line">        <span class="keyword">if</span> (cells != <span class="literal">null</span>) &#123;</span><br><span class="line">            <span class="keyword">for</span> (cell &lt;- cells) &#123;</span><br><span class="line">                </span><br><span class="line">                println(</span><br><span class="line">                    <span class="string">s&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">                       |row = $&#123;Bytes.toString(CellUtil.cloneRow(cell))&#125;</span></span><br><span class="line"><span class="string">                       |cf = $&#123;Bytes.toString(CellUtil.cloneFamily(cell))&#125;</span></span><br><span class="line"><span class="string">                       |name = $&#123;Bytes.toString(CellUtil.cloneQualifier(cell))&#125;</span></span><br><span class="line"><span class="string">                       |value = $&#123;Bytes.toString(CellUtil.cloneValue(cell))&#125;</span></span><br><span class="line"><span class="string">                       |----------------</span></span><br><span class="line"><span class="string">                       |&quot;</span><span class="string">&quot;&quot;</span>.stripMargin)</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        table.close()</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">deleteData</span></span>(tableName: <span class="type">String</span>, rowKey: <span class="type">String</span>, cf: <span class="type">String</span>, columnName: <span class="type">String</span>) = &#123;</span><br><span class="line">        <span class="keyword">val</span> table: <span class="type">Table</span> = conn.getTable(<span class="type">TableName</span>.valueOf(tableName))</span><br><span class="line">        <span class="keyword">val</span> delete = <span class="keyword">new</span> <span class="type">Delete</span>(<span class="type">Bytes</span>.toBytes(rowKey))</span><br><span class="line">        <span class="comment">//        delete.addColumn(Bytes.toBytes(cf), Bytes.toBytes(columnName))</span></span><br><span class="line">        delete.addColumns(<span class="type">Bytes</span>.toBytes(cf), <span class="type">Bytes</span>.toBytes(columnName)) <span class="comment">// 删除所有版本</span></span><br><span class="line">        table.delete(delete)</span><br><span class="line">        </span><br><span class="line">        table.close()</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">putData</span></span>(tableName: <span class="type">String</span>, rowKey: <span class="type">String</span>, cf: <span class="type">String</span>, columnName: <span class="type">String</span>, value: <span class="type">String</span>) = &#123;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 1. 先获取到表对象,客户端到表的连接</span></span><br><span class="line">        <span class="keyword">val</span> table: <span class="type">Table</span> = conn.getTable(<span class="type">TableName</span>.valueOf(tableName))</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 2. 调用表对象的put</span></span><br><span class="line">        <span class="comment">// 2.1 把需要添加的数据封装到一个Put对象   put &#x27;&#x27;, rowKey, &#x27;&#x27;, &#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">val</span> put = <span class="keyword">new</span> <span class="type">Put</span>(<span class="type">Bytes</span>.toBytes(rowKey))</span><br><span class="line">        put.addColumn(<span class="type">Bytes</span>.toBytes(cf), <span class="type">Bytes</span>.toBytes(columnName), <span class="type">Bytes</span>.toBytes(value))</span><br><span class="line">        <span class="comment">//        put.addColumn(Bytes.toBytes(cf), Bytes.toBytes(columnName + &quot;abc&quot;), Bytes.toBytes(value + &quot;efg&quot;))</span></span><br><span class="line">        <span class="comment">// 2.2 提交Put对象</span></span><br><span class="line">        table.put(put)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 3. 关闭到table的连接</span></span><br><span class="line">        table.close()</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">closeConnection</span></span>() = conn.close()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="HBase优化"><a href="#HBase优化" class="headerlink" title="HBase优化"></a>HBase优化</h2><h3 id="预分区"><a href="#预分区" class="headerlink" title="预分区"></a>预分区</h3><p>每一个region维护着startRow与endRowKey，如果加入的数据符合某个region维护的rowKey范围，则该数据交给这个region维护。那么依照这个原则，我们可以将数据所要投放的分区提前大致的规划好，以提高HBase性能。</p>
<p>1.手动设定预分区</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hbase&gt; create &#x27;staff1&#x27;,&#x27;info&#x27;,&#x27;partition1&#x27;,SPLITS =&gt; [&#x27;1000&#x27;,&#x27;2000&#x27;,&#x27;3000&#x27;,&#x27;4000&#x27;]</span><br></pre></td></tr></table></figure>

<p>2.生成16进制序列预分区</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create &#x27;staff2&#x27;,&#x27;info&#x27;,&#x27;partition2&#x27;,&#123;NUMREGIONS =&gt; 15, SPLITALGO =&gt; &#x27;HexStringSplit&#x27;&#125;</span><br></pre></td></tr></table></figure>

<p>3.按照文件中设置的规则预分区</p>
<p>创建splits.txt文件内容如下：</p>
<figure class="highlight ebnf"><table><tr><td class="code"><pre><span class="line"><span class="attribute">aaaa</span></span><br><span class="line"><span class="attribute">bbbb</span></span><br><span class="line"><span class="attribute">cccc</span></span><br><span class="line"><span class="attribute">dddd</span></span><br></pre></td></tr></table></figure>

<p>然后执行：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create &#x27;staff3&#x27;,&#x27;partition3&#x27;,SPLITS_FILE =&gt; &#x27;splits.txt&#x27;</span><br></pre></td></tr></table></figure>

<p>4.使用JavaAPI创建预分区</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//自定义算法，产生一系列Hash散列值存储在二维数组中</span></span><br><span class="line"><span class="keyword">byte</span>[][] splitKeys = 某个散列值函数</span><br><span class="line"><span class="comment">//创建HbaseAdmin实例</span></span><br><span class="line">HBaseAdmin hAdmin = <span class="keyword">new</span> HBaseAdmin(HbaseConfiguration.create());</span><br><span class="line"><span class="comment">//创建HTableDescriptor实例</span></span><br><span class="line">HTableDescriptor tableDesc = <span class="keyword">new</span> HTableDescriptor(tableName);</span><br><span class="line"><span class="comment">//通过HTableDescriptor实例和散列值二维数组创建带有预分区的Hbase表</span></span><br><span class="line">hAdmin.createTable(tableDesc, splitKeys);</span><br></pre></td></tr></table></figure>

<h3 id="RowKey设计"><a href="#RowKey设计" class="headerlink" title="RowKey设计"></a>RowKey设计</h3><p>一条数据的唯一标识就是rowkey，那么这条数据存储于哪个分区，取决于rowkey处于哪个一个预分区的区间内，设计rowkey的主要目的 ，就是让数据均匀的分布于所有的region中，在一定程度上<strong>防止数据倾斜</strong>。接下来我们就谈一谈rowkey常用的设计方案。</p>
<p>1.生成随机数、hash、散列值</p>
<figure class="highlight llvm"><table><tr><td class="code"><pre><span class="line">比如：</span><br><span class="line">原本rowKey为<span class="number">1001</span>的，SHA<span class="number">1</span>后变成：dd<span class="number">01903921</span>ea<span class="number">24941</span><span class="keyword">c</span><span class="number">26</span>a<span class="number">48</span>f<span class="number">2</span>cec<span class="number">24e0</span>bb<span class="number">0e8</span><span class="keyword">cc</span><span class="number">7</span></span><br><span class="line">原本rowKey为<span class="number">3001</span>的，SHA<span class="number">1</span>后变成：<span class="number">49042</span><span class="keyword">c</span><span class="number">54</span>de<span class="number">64</span>a<span class="number">1e9</span>bf<span class="number">0</span>b<span class="number">33e00245660</span>ef<span class="number">92</span>dc<span class="number">7</span>bd</span><br><span class="line">原本rowKey为<span class="number">5001</span>的，SHA<span class="number">1</span>后变成：<span class="number">7</span>b<span class="number">61</span>dec<span class="number">07e02</span><span class="keyword">c</span><span class="number">188790670</span>af<span class="number">43e717</span>f<span class="number">0</span>f<span class="number">46e8913</span></span><br><span class="line">在做此操作之前，一般我们会选择从数据集中抽取样本，来决定什么样的rowKey来Hash后作为每个分区的临界值。</span><br></pre></td></tr></table></figure>

<p>2.字符串反转</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">20170524000001</span>转成<span class="number">10000042507102</span></span><br><span class="line"><span class="attribute">20170524000002</span>转成<span class="number">20000042507102</span></span><br></pre></td></tr></table></figure>

<p>3.字符串拼接</p>
<figure class="highlight asciidoc"><table><tr><td class="code"><pre><span class="line">20170524000001<span class="emphasis">_a12e</span></span><br><span class="line"><span class="emphasis">20170524000001_</span>93i7</span><br></pre></td></tr></table></figure>

<h3 id="内存优化"><a href="#内存优化" class="headerlink" title="内存优化"></a>内存优化</h3><p>HBase操作过程中需要大量的内存开销，毕竟Table是可以缓存在内存中的，但是不建议分配非常大的堆内存，因为GC过程持续太久会导致RegionServer处于长期不可用状态，一般16~36G内存就可以了，如果因为框架占用内存过高导致系统内存不足，框架一样会被系统服务拖死。</p>
<p>1.Zookeeper会话超时时间</p>
<p>hbase-site.xml</p>
<figure class="highlight crmsh"><table><tr><td class="code"><pre><span class="line">属性：zookeeper.session.timeout</span><br><span class="line">解释：默认值为<span class="number">90000</span>毫秒（<span class="number">90s</span>）。当某个RegionServer挂掉，<span class="number">90s</span>之后<span class="literal">Master</span>才能察觉到。可适当减小此值，以加快<span class="literal">Master</span>响应，可调整至<span class="number">600000</span>毫秒。</span><br></pre></td></tr></table></figure>

<p>2.设置RPC监听数量</p>
<p>hbase-site.xml</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">属性：hbase<span class="selector-class">.regionserver</span><span class="selector-class">.handler</span><span class="selector-class">.count</span></span><br><span class="line">解释：默认值为<span class="number">30</span>，用于指定RPC监听的数量，可以根据客户端的请求数进行调整，读写请求较多时，增加此值。</span><br></pre></td></tr></table></figure>

<p>3.手动控制Major Compaction</p>
<p>hbase-site.xml</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">属性：hbase<span class="selector-class">.hregion</span><span class="selector-class">.majorcompaction</span></span><br><span class="line">解释：默认值：<span class="number">604800000</span>秒（<span class="number">7</span>天）， Major Compaction的周期，若关闭自动Major Compaction，可将其设为<span class="number">0</span></span><br></pre></td></tr></table></figure>

<p>4.优化HStore文件大小</p>
<p>hbase-site.xml</p>
<figure class="highlight maxima"><table><tr><td class="code"><pre><span class="line">属性：hbase.hregion.<span class="built_in">max</span>.filesize解释：默认值<span class="number">10737418240</span>（10GB），如果需要运行HBase的MR任务，可以减小此值，因为一个<span class="built_in">region</span>对应一个<span class="built_in">map</span>任务，如果单个<span class="built_in">region</span>过大，会导致<span class="built_in">map</span>任务执行时间过长。该值的意思就是，如果HFile的大小达到这个数值，则这个<span class="built_in">region</span>会被切分为两个Hfile。</span><br></pre></td></tr></table></figure>

<p>5.优化HBase客户端缓存</p>
<p>hbase-site.xml</p>
<figure class="highlight arduino"><table><tr><td class="code"><pre><span class="line">属性：hbase.client.write.buffer解释：默认值<span class="number">2097152b</span>ytes（<span class="number">2</span>M）用于指定HBase客户端缓存，增大该值可以减少RPC调用次数，但是会消耗更多内存，反之则反之。一般我们需要设定一定的缓存大小，以达到减少RPC次数的目的。</span><br></pre></td></tr></table></figure>

<p>6.指定scan.next扫描HBase所获取的行数</p>
<p>hbase-site.xml</p>
<figure class="highlight axapta"><table><tr><td class="code"><pre><span class="line">属性：hbase.<span class="keyword">client</span>.scanner.caching解释：用于指定scan.<span class="keyword">next</span>方法获取的默认行数，值越大，消耗内存越大。</span><br></pre></td></tr></table></figure>

<p>7.BlockCache占用RegionServer堆内存的比例</p>
<p>hbase-site.xml</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">属性：hfile<span class="selector-class">.block</span><span class="selector-class">.cache</span><span class="selector-class">.size</span></span><br><span class="line">解释：默认<span class="number">0.4</span>，读请求比较多的情况下，可适当调大</span><br></pre></td></tr></table></figure>

<p>8.MemStore占用RegionServer堆内存的比例</p>
<p>hbase-site.xml</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">属性：hbase<span class="selector-class">.regionserver</span><span class="selector-class">.global</span><span class="selector-class">.memstore</span><span class="selector-class">.size</span></span><br><span class="line">解释：默认<span class="number">0.4</span>，写请求较多的情况下，可适当调大</span><br></pre></td></tr></table></figure>

<h2 id="HBase与Hive集成使用"><a href="#HBase与Hive集成使用" class="headerlink" title="HBase与Hive集成使用"></a>HBase与Hive集成使用</h2><p>在hive-site.xml中添加zookeeper的属性，如下：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>linux1,linux2,linux3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.zookeeper.client.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>案例1：建立Hive表，关联HBase表，插入数据到Hive表的同时能够影响HBase表。</p>
<p>1.在Hive中创建表同时关联HBase</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> hive_hbase_emp_table(</span><br><span class="line">empno <span class="type">int</span>,</span><br><span class="line">ename string,</span><br><span class="line">job string,</span><br><span class="line">mgr <span class="type">int</span>,</span><br><span class="line">hiredate string,</span><br><span class="line">sal <span class="keyword">double</span>,</span><br><span class="line">comm <span class="keyword">double</span>,</span><br><span class="line">deptno <span class="type">int</span>)</span><br><span class="line">STORED <span class="keyword">BY</span> <span class="string">&#x27;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#x27;</span></span><br><span class="line"><span class="keyword">WITH</span> SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; <span class="operator">=</span> &quot;:key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno&quot;)</span><br><span class="line">TBLPROPERTIES (&quot;hbase.table.name&quot; <span class="operator">=</span> &quot;hbase_emp_table&quot;);</span><br></pre></td></tr></table></figure>

<p>提示：完成之后，可以分别进入Hive和HBase查看，都生成了对应的表</p>
<p>2.在Hive中<strong>创建临时中间表</strong>，用于load文件中的数据</p>
<p>提示：<strong>不能将数据直接load进Hive所关联HBase的那张表中</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CREATE TABLE emp(</span><br><span class="line">empno int,</span><br><span class="line">ename string,</span><br><span class="line">job string,</span><br><span class="line">mgr int,</span><br><span class="line">hiredate string,</span><br><span class="line">sal double,</span><br><span class="line">comm double,</span><br><span class="line">deptno int)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure>

<p>3.向Hive中间表中load数据</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">hive&gt;</span><span class="bash"> load data <span class="built_in">local</span> inpath <span class="string">&#x27;/home/admin/softwares/data/emp.txt&#x27;</span> into table emp;</span></span><br></pre></td></tr></table></figure>

<p>4.通过insert命令将中间表中的数据导入到Hive关联Hbase的那张表中</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive&gt; insert overwrite table hive_hbase_emp_table select * from emp;</span><br></pre></td></tr></table></figure>

<p>5.查看Hive以及关联的HBase表中是否已经成功的同步插入了数据</p>
<p>Hive：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive&gt; select * from hive_hbase_emp_table;</span><br></pre></td></tr></table></figure>

<p>HBase：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">Hbase&gt;</span><span class="bash"> scan <span class="string">&#x27;hbase_emp_table&#x27;</span></span></span><br></pre></td></tr></table></figure>

<p>案例2：在HBase中已经存储了某一张表hbase_emp_table，然后在Hive中创建一个外部表来关联HBase中的hbase_emp_table这张表，使之可以借助Hive来分析HBase这张表中的数据。</p>
<p>1.在Hive中创建外部表</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CREATE EXTERNAL TABLE relevance_hbase_emp(</span><br><span class="line">empno int,</span><br><span class="line">ename string,</span><br><span class="line">job string,</span><br><span class="line">mgr int,</span><br><span class="line">hiredate string,</span><br><span class="line">sal double,</span><br><span class="line">comm double,</span><br><span class="line">deptno int)</span><br><span class="line">STORED BY </span><br><span class="line">&#x27;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#x27;</span><br><span class="line">WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = </span><br><span class="line">&quot;:key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno&quot;) </span><br><span class="line">TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;hbase_emp_table&quot;);</span><br></pre></td></tr></table></figure>

<p>2.关联后就可以使用Hive函数进行一些分析操作了</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from relevance_hbase_emp;</span><br></pre></td></tr></table></figure>

<h2 id="整合Phoenix"><a href="#整合Phoenix" class="headerlink" title="整合Phoenix"></a>整合Phoenix</h2><h3 id="Phoenix定义"><a href="#Phoenix定义" class="headerlink" title="Phoenix定义"></a>Phoenix定义</h3><p>Phoenix是HBase的开源SQL皮肤。可以使用标准JDBC API代替HBase客户端API来创建表，插入数据和查询HBase数据。</p>
<h3 id="Phoenix特点"><a href="#Phoenix特点" class="headerlink" title="Phoenix特点"></a>Phoenix特点</h3><p>1）容易集成：如Spark，Hive，Pig，Flume和Map Reduce；</p>
<p>2）操作简单：DML命令以及通过DDL命令创建和操作表和版本化增量更改；</p>
<p>3）<strong>支持HBase二级索引创建</strong>。</p>
<h3 id="Phoenix架构"><a href="#Phoenix架构" class="headerlink" title="Phoenix架构"></a>Phoenix架构</h3><p><img src="/Hbase/34.png" alt="34"></p>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>1.官网地址</p>
<p><a href="http://phoenix.apache.org/">http://phoenix.apache.org/</a></p>
<p>2.Phoenix部署</p>
<p>1）上传并解压tar包</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 software]$ tar -zxvf apache-phoenix-5.0.0-HBase-2.0-bin.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 module]$ mv apache-phoenix-5.0.0-HBase-2.0-bin phoenix</span><br></pre></td></tr></table></figure>

<p>2）复制server包并拷贝到各个节点的hbase/lib</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 module]$ cd /opt/module/phoenix/</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 phoenix]$ cp /opt/module/phoenix/phoenix-5.0.0-HBase-2.0-server.jar /opt/module/hbase/lib/</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 phoenix]$ xsync /opt/module/hbase/lib/phoenix-5.0.0-HBase-2.0-server.jar</span><br></pre></td></tr></table></figure>

<p>4）配置环境变量</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">phoenix</span></span><br><span class="line">export PHOENIX_HOME=/opt/module/phoenix</span><br><span class="line">export PHOENIX_CLASSPATH=$PHOENIX_HOME</span><br><span class="line">export PATH=$PATH:$PHOENIX_HOME/bin</span><br></pre></td></tr></table></figure>

<p>5）重启HBase</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 ~]$ stop-hbase.sh</span><br><span class="line">[vincent@linux1 ~]$ start-hbase.sh</span><br></pre></td></tr></table></figure>

<ol start="6">
<li>连接Phoenix</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 phoenix]$ sqlline.py linux1,linux2,linux3:2181</span><br></pre></td></tr></table></figure>

<h3 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h3><h4 id="1-表的操作"><a href="#1-表的操作" class="headerlink" title="1.表的操作"></a>1.表的操作</h4><p>1）显示所有表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator">!</span><span class="keyword">table</span> 或 <span class="operator">!</span>tables</span><br></pre></td></tr></table></figure>

<p>2）创建表</p>
<p>直接指定单个列作为RowKey</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CREATE TABLE IF NOT EXISTS student(</span><br><span class="line">id VARCHAR primary key,</span><br><span class="line">name VARCHAR,</span><br><span class="line">addr VARCHAR</span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<p>在phoenix中，表名等会自动转换为大写，若要小写，使用双引号，如”us_population”。</p>
<p>指定多个列的联合作为RowKey</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CREATE TABLE IF NOT EXISTS &quot;us_population&quot; (</span><br><span class="line">State CHAR(2) NOT NULL,</span><br><span class="line">City VARCHAR NOT NULL,</span><br><span class="line">Population BIGINT</span><br><span class="line">CONSTRAINT my_pk PRIMARY KEY (state, city)</span><br><span class="line">);</span><br><span class="line">//联合主键</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table &quot;test&quot;(</span><br><span class="line">  id varchar primary key,</span><br><span class="line">  &quot;info1&quot;.&quot;name&quot; varchar, </span><br><span class="line">  &quot;info2&quot;.&quot;address&quot; varchar</span><br><span class="line">) split on (&#x27;100&#x27;, &#x27;200&#x27;, &#x27;300&#x27;);</span><br><span class="line">//预分区</span><br><span class="line">//列族</span><br></pre></td></tr></table></figure>

<p>3）插入数据</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">upsert into student values(&#x27;1001&#x27;,&#x27;zhangsan&#x27;,&#x27;beijing&#x27;);</span><br></pre></td></tr></table></figure>

<p>4）查询记录</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select * from student;</span><br><span class="line">select * from student where id=&#x27;1001&#x27;;</span><br></pre></td></tr></table></figure>

<p>5）删除记录</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">delete from student where id=&#x27;1001&#x27;;</span><br></pre></td></tr></table></figure>

<p>6）删除表</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">drop table student;</span><br></pre></td></tr></table></figure>

<p>7）退出命令行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">!quit</span><br></pre></td></tr></table></figure>

<h4 id="2-表的映射"><a href="#2-表的映射" class="headerlink" title="2.表的映射"></a>2.表的映射</h4><p>1）表的关系</p>
<p>默认情况下，直接在HBase中创建的表，通过Phoenix是查看不到的。如果要在Phoenix中操作直接在HBase中创建的表，则需要在Phoenix中进行表的映射。映射方式有两种：视图映射和表映射。</p>
<p>2）命令行中创建表test</p>
<p>HBase中test的表结构如下，两个列族info1、info2。</p>
<table>
<thead>
<tr>
<th>Rowkey</th>
<th>info1</th>
<th>info2</th>
</tr>
</thead>
<tbody><tr>
<td>id</td>
<td>name</td>
<td>address</td>
</tr>
</tbody></table>
<p>启动HBase Shell</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 ~]$ hbase shell</span><br></pre></td></tr></table></figure>

<p>创建HBase表test</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hbase(main):001:0&gt; create &#x27;test&#x27;,&#x27;info1&#x27;,&#x27;info2&#x27;</span><br></pre></td></tr></table></figure>

<p>3）视图映射</p>
<p>Phoenix创建的视图是只读的，所以只能用来做查询，无法通过视图对源数据进行修改等操作。在phoenix中创建关联test表的视图</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">0: jdbc:phoenix:linux1,linux2,linux3&gt; create view &quot;test&quot;(id varchar primary key,&quot;info1&quot;.&quot;name&quot; varchar, &quot;info2&quot;.&quot;address&quot; varchar);</span><br></pre></td></tr></table></figure>

<p>删除视图</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">0: jdbc:phoenix:linux1,linux2,linux3&gt; drop view &quot;test&quot;;</span><br></pre></td></tr></table></figure>

<p>4）表映射</p>
<p>使用Apache Phoenix创建对HBase的表映射，有两种方法：</p>
<p>（1）HBase中不存在表时，可以直接使用create table指令创建需要的表,系统将会自动在Phoenix和HBase中创建person_infomation的表，并会根据指令内的参数对表结构进行初始化。</p>
<p>（2）当HBase中已经存在表时，可以以类似创建视图的方式创建关联表，只需要将create view改为create table即可。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">0: jdbc:phoenix:linux1,linux2,linux3&gt; create table &quot;test&quot;(id varchar primary key,&quot;info1&quot;.&quot;name&quot; varchar, &quot;info2&quot;.&quot;address&quot; varchar) column_encoded_bytes=0;</span><br></pre></td></tr></table></figure>

<p><strong>Phoenix双引号””：限制小写</strong></p>
<p><strong>Phoenix单引号’’：字符串</strong></p>
<h3 id="Phoenix-JDBC操作"><a href="#Phoenix-JDBC操作" class="headerlink" title="Phoenix JDBC操作"></a>Phoenix JDBC操作</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">jdbc:phoenix:thin:url<span class="operator">=</span>http:<span class="operator">/</span><span class="operator">/</span>linux1:<span class="number">8765</span>;serialization<span class="operator">=</span>PROTOBUF</span><br></pre></td></tr></table></figure>

<h4 id="1）thin连接"><a href="#1）thin连接" class="headerlink" title="1）thin连接"></a>1）thin连接</h4><p>1.启动queryserver</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 ~]$ queryserver.py start</span><br></pre></td></tr></table></figure>

<p>2.创建项目并导入依赖</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.phoenix/phoenix-queryserver-client --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.phoenix<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>phoenix-queryserver-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.0.0-HBase-2.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">  			<span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.httpcomponents/httpclient --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.httpcomponents<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>httpclient<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.3.4<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- https://mvnrepository.com/artifact/com.google.protobuf/protobuf-java --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.google.protobuf<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>protobuf-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.4.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>3.编写代码</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">phoenixTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> url: <span class="type">String</span> = <span class="type">ThinClientUtil</span>.getConnectionUrl(<span class="string">&quot;linux1&quot;</span>, <span class="number">8765</span>)</span><br><span class="line">    <span class="keyword">val</span> conn: <span class="type">Connection</span> = <span class="type">DriverManager</span>.getConnection(url)</span><br><span class="line">    <span class="keyword">val</span> statement: <span class="type">PreparedStatement</span> = conn.prepareStatement(<span class="string">&quot;select * from student&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> set: <span class="type">ResultSet</span> = statement.executeQuery()</span><br><span class="line">    <span class="keyword">while</span>(set.next())&#123;</span><br><span class="line">      println(set.getString(<span class="number">1</span>) + <span class="string">&quot;\t&quot;</span> + set.getString(<span class="number">2</span>)+ <span class="string">&quot;\t&quot;</span> + set.getString(<span class="number">3</span>))</span><br><span class="line">    &#125;</span><br><span class="line">    conn.close()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="2）thick连接"><a href="#2）thick连接" class="headerlink" title="2）thick连接"></a>2）thick连接</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">phoenixTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> url: <span class="type">String</span> = <span class="string">&quot;jdbc:phoenix:linux1,linux2,linux3:2181&quot;</span></span><br><span class="line">    <span class="keyword">val</span> conn: <span class="type">Connection</span> = <span class="type">DriverManager</span>.getConnection(url)</span><br><span class="line">    <span class="keyword">val</span> statement: <span class="type">PreparedStatement</span> = conn.prepareStatement(<span class="string">&quot;select * from student&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> set: <span class="type">ResultSet</span> = statement.executeQuery()</span><br><span class="line">    <span class="keyword">while</span>(set.next())&#123;</span><br><span class="line">      println(set.getString(<span class="number">1</span>) + <span class="string">&quot;\t&quot;</span> + set.getString(<span class="number">2</span>)+ <span class="string">&quot;\t&quot;</span> + set.getString(<span class="number">3</span>))</span><br><span class="line">    &#125;</span><br><span class="line">    conn.close()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Phoenix二级索引"><a href="#Phoenix二级索引" class="headerlink" title="Phoenix二级索引"></a>Phoenix二级索引</h3><p>添加如下配置到HBase的HRegionserver节点的hbase-site.xml</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- phoenix regionserver 配置参数--&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.regionserver.wal.codec<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.region.server.rpc.scheduler.factory.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hbase.ipc.PhoenixRpcSchedulerFactory<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Factory to create the Phoenix RPC Scheduler that uses separate queues for index and metadata updates<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rpc.controllerfactory.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hbase.ipc.controller.ServerRpcControllerFactory<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Factory to create the Phoenix RPC Scheduler that uses separate queues for index and metadata updates<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="全局二级索引-多读少写"><a href="#全局二级索引-多读少写" class="headerlink" title="全局二级索引(多读少写)"></a>全局二级索引(多读少写)</h4><p>select后面的字段中如果出现未加索引的字段，那么整个语句全局扫描。</p>
<p>Global Index是默认的索引格式，创建全局索引时，会在HBase中建立一张新表。也就是说索引数据和数据表是存放在不同的表中的，因此全局索引适用于多读少写的业务场景。</p>
<p>写数据的时候会消耗大量开销，因为索引表也要更新，而索引表是分布在不同的数据节点上的，跨节点的数据传输带来了较大的性能消耗。</p>
<p>在读数据的时候Phoenix会选择索引表来降低查询消耗的时间。</p>
<p>1.创建单个字段的全局索引</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CREATE INDEX my_index ON my_table (my_col);</span><br></pre></td></tr></table></figure>

<p>2.创建携带其他字段的全局索引</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CREATE INDEX my_index ON my_table (v1) INCLUDE (v2);</span><br></pre></td></tr></table></figure>

<h4 id="局部二级索引-多写"><a href="#局部二级索引-多写" class="headerlink" title="局部二级索引(多写)"></a>局部二级索引(多写)</h4><p>只要where后面的字段添加了索引，那么select后面的字段即使没有索引也不会全局扫描。</p>
<p>Local Index适用于写操作频繁的场景。</p>
<p><strong>索引数据和数据表的数据是存放在同一张表中（且是同一个Region）</strong>，避免了在写操作的时候往不同服务器的索引表中写索引带来的额外开销。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CREATE LOCAL INDEX my_index ON my_table (my_column);</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title>MapReduce</title>
    <url>/MapReduce/</url>
    <content><![CDATA[<h1 id="MapReduce概述"><a href="#MapReduce概述" class="headerlink" title="MapReduce概述"></a>MapReduce概述</h1><p><img src="/MapReduce/56.png" alt="56"></p>
<h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><p><img src="/MapReduce/57.png" alt="57"></p>
<p><img src="/MapReduce/58.png" alt="58"></p>
<p><img src="/MapReduce/59.png" alt="59"></p>
<h2 id="核心编程思想"><a href="#核心编程思想" class="headerlink" title="核心编程思想"></a>核心编程思想</h2><p><img src="/MapReduce/60.png" alt="60"></p>
<p>1）分布式的运算程序往往需要分成至少2个阶段。</p>
<p>2）第一个阶段的MapTask并发实例，完全并行运行，互不相干。</p>
<p>3）第二个阶段的ReduceTask并发实例互不相干，但是他们的数据依赖于上一个阶段的所有MapTask并发实例的输出。</p>
<p>4）MapReduce编程模型只能包含一个Map阶段和一个Reduce阶段，如果用户的业务逻辑非常复杂，那就只能多个MapReduce程序，串行运行。</p>
<h2 id="MapReduce进程"><a href="#MapReduce进程" class="headerlink" title="MapReduce进程"></a>MapReduce进程</h2><p><img src="/MapReduce/61.png" alt="61"></p>
<h2 id="常用数据序列化类型"><a href="#常用数据序列化类型" class="headerlink" title="常用数据序列化类型"></a>常用数据序列化类型</h2><table>
<thead>
<tr>
<th>Java类型</th>
<th>Hadoop Writable类型</th>
</tr>
</thead>
<tbody><tr>
<td>Boolean</td>
<td>BooleanWritable</td>
</tr>
<tr>
<td>Byte</td>
<td>ByteWritable</td>
</tr>
<tr>
<td>Int</td>
<td>IntWritable</td>
</tr>
<tr>
<td>Float</td>
<td>FloatWritable</td>
</tr>
<tr>
<td>Long</td>
<td>LongWritable</td>
</tr>
<tr>
<td>Double</td>
<td>DoubleWritable</td>
</tr>
<tr>
<td>String</td>
<td>Text</td>
</tr>
<tr>
<td>Map</td>
<td>MapWritable</td>
</tr>
<tr>
<td>Array</td>
<td>ArrayWritable</td>
</tr>
</tbody></table>
<h2 id="MapReduce编程规范"><a href="#MapReduce编程规范" class="headerlink" title="MapReduce编程规范"></a>MapReduce编程规范</h2><p>用户编写的程序分成三个部分：Mapper、Reducer和Driver</p>
<p><img src="/MapReduce/62.png" alt="62"></p>
<p><img src="/MapReduce/63.png" alt="63"></p>
<h2 id="WordCount案例实操"><a href="#WordCount案例实操" class="headerlink" title="WordCount案例实操"></a>WordCount案例实操</h2><p>1．需求</p>
<p>在给定的文本文件中统计输出每一个单词出现的总次数。</p>
<p><img src="/MapReduce/64.png" alt="64"></p>
<p>创建maven工程</p>
<p>在pom.xml文件中添加如下依赖：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.12<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.logging.log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j-slf4j-impl<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.12.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>在项目的src/main/resources目录下，新建一个文件，命名为“log4j2.xml”，在文件中填入：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">Configuration</span> <span class="attr">status</span>=<span class="string">&quot;error&quot;</span> <span class="attr">strict</span>=<span class="string">&quot;true&quot;</span> <span class="attr">name</span>=<span class="string">&quot;XMLConfig&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">Appenders</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 类型名为Console，名称为必须属性 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">Appender</span> <span class="attr">type</span>=<span class="string">&quot;Console&quot;</span> <span class="attr">name</span>=<span class="string">&quot;STDOUT&quot;</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!-- 布局为PatternLayout的方式，</span></span><br><span class="line"><span class="comment">            输出样式为[INFO] [2018-01-22 17:34:01][org.test.Console]I&#x27;m here --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">Layout</span> <span class="attr">type</span>=<span class="string">&quot;PatternLayout&quot;</span></span></span><br><span class="line"><span class="tag">                    <span class="attr">pattern</span>=<span class="string">&quot;[%p] [%d&#123;yyyy-MM-dd HH:mm:ss&#125;][%c&#123;10&#125;]%m%n&quot;</span> /&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">Appender</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;/<span class="name">Appenders</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">Loggers</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 可加性为false --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">Logger</span> <span class="attr">name</span>=<span class="string">&quot;test&quot;</span> <span class="attr">level</span>=<span class="string">&quot;info&quot;</span> <span class="attr">additivity</span>=<span class="string">&quot;false&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">AppenderRef</span> <span class="attr">ref</span>=<span class="string">&quot;STDOUT&quot;</span> /&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">Logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!-- root loggerConfig设置 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">Root</span> <span class="attr">level</span>=<span class="string">&quot;info&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">AppenderRef</span> <span class="attr">ref</span>=<span class="string">&quot;STDOUT&quot;</span> /&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">Root</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">Loggers</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">Configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>编写Mapper类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.vincent.mr.wordcount;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *  继承Mapper类，指定4个泛型，4个泛型表示两组KV，一组输入，一组输出</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *  输入的KV：</span></span><br><span class="line"><span class="comment"> *  KEYIN: LongWritable, 表示从文件读取数据的偏移量</span></span><br><span class="line"><span class="comment"> *  VALUEIN: Text, 表示从文件读取一行数据</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *  输出的KV：</span></span><br><span class="line"><span class="comment"> *  KEYOUT: Text, 表示一个单词</span></span><br><span class="line"><span class="comment"> *  VALUEOUT: IntWritable, 表示单词出现1次</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> Vincent</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span> 2020-07-01</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordcountMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">    Text k = <span class="keyword">new</span> Text();</span><br><span class="line">    IntWritable v = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * called once for each key/value pairs in the input split</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key   输入的k，偏移量</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value 输入的v，从文件中读取的一行内容</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context   负责调度Mapper运行</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> InterruptedException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取一行</span></span><br><span class="line">        String line = value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 切割</span></span><br><span class="line">        String[] words = line.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 输出</span></span><br><span class="line">        <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line"></span><br><span class="line">            k.set(word);</span><br><span class="line">            context.write(k, v);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>编写Reducer类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.vincent.mr.wordcount;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 自定义Reducer，需要继承Reducer类，指定4个泛型，表示两组KV，一对输入，一对输出</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 输入的kv</span></span><br><span class="line"><span class="comment"> * KEYIN: Text 对应Map输出的k，表示一个单词</span></span><br><span class="line"><span class="comment"> * VALUEIN: IntWritable 对应Map输出的v，表示单词出现的次数（1次）</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 输出的kv</span></span><br><span class="line"><span class="comment"> * KEYOUT: Text 表示一个单词</span></span><br><span class="line"><span class="comment"> * VALUEOUT: IntWritable 表示这个单词出现的总次数</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> Vincent</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span> 2020-07-01</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordcountReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> sum;</span><br><span class="line">    IntWritable v = <span class="keyword">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * This method is called once for each key. Most applications will define</span></span><br><span class="line"><span class="comment">     * their reduce class by overriding this method. The default implementation</span></span><br><span class="line"><span class="comment">     * is an identity function.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key   表示输入的key，这里是一个单词</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> values 表示封装了当前key对应的所有value的一个迭代器对象</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context   负责调度Reducer运行</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> InterruptedException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 累加求和</span></span><br><span class="line">        sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (IntWritable count : values) &#123;</span><br><span class="line">            sum += count.get();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 输出</span></span><br><span class="line">        v.set(sum);</span><br><span class="line">        context.write(key,v);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>编写Driver驱动类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.vincent.mr.wordcount;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> Vincent</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span> 2020-07-01</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordcountDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 获取配置信息以及封装任务</span></span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        Job job = Job.getInstance(configuration);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 设置jar加载路径</span></span><br><span class="line">        job.setJarByClass(WordcountDriver.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3 设置map和reduce类</span></span><br><span class="line">        job.setMapperClass(WordcountMapper.class);</span><br><span class="line">        job.setReducerClass(WordcountReducer.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4 设置map输出</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5 设置最终输出kv类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 6 设置输入和输出路径</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">//本地模式运行</span></span><br><span class="line">        <span class="comment">//FileInputFormat.setInputPaths(job, new Path(&quot;/Users/vincent/Downloads/inputword&quot;));</span></span><br><span class="line">        <span class="comment">//FileOutputFormat.setOutputPath(job, new Path(&quot;/Users/vincent/Downloads/inputword/output&quot;));</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">//动态传参</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 7 提交</span></span><br><span class="line">        <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">        System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>本地测试：直接运行Driver</p>
<p>集群测试：</p>
<p>1.打包jar并放在集群节点中</p>
<p>2.执行命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop jar wc.jar com.vincent.mr.wordcount.WordcountDriver /input /input/wcOutput</span><br></pre></td></tr></table></figure>

<p>wc.jar 是打好的jar包</p>
<h1 id="Hadoop序列化"><a href="#Hadoop序列化" class="headerlink" title="Hadoop序列化"></a>Hadoop序列化</h1><p><img src="/MapReduce/65.png" alt="65"></p>
<p><img src="/MapReduce/66.png" alt="66"></p>
<h2 id="自定义bean对象实现序列化接口（Writable）"><a href="#自定义bean对象实现序列化接口（Writable）" class="headerlink" title="自定义bean对象实现序列化接口（Writable）"></a>自定义bean对象实现序列化接口（Writable）</h2><p>在企业开发中往往常用的基本序列化类型不能满足所有需求，比如在Hadoop框架内部传递一个bean对象，那么该对象就需要实现序列化接口。</p>
<p>Writable接口：</p>
<p>​    write(DataOutput out):    对象序列化</p>
<p>​    readFields(DataInput in):    对象的反序列化</p>
<p>1.实现Writable接口。</p>
<p>2.在类中提供无参数构造器，反序列化时会反射调用。</p>
<p>3.write方法写出的fields顺序与readFields读取的fields顺序要一致。</p>
<p>4.重写toString()方法，一般将各个属性用’\t’分割打印。</p>
<h2 id="序列化案例实操"><a href="#序列化案例实操" class="headerlink" title="序列化案例实操"></a>序列化案例实操</h2><p>统计每一个手机号耗费的上行流量、下行流量、总流量</p>
<p><img src="/MapReduce/67.png" alt="67"></p>
<p>1.数据文件    phone_data.txt</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">7</span> 	<span class="number">13560436666</span>	<span class="number">120.196.100.99</span>		<span class="number">1116</span>		 <span class="number">954</span>			<span class="number">200</span></span><br><span class="line"><span class="attribute">id</span>	手机号码		 网络ip			        上行流量  下行流量   网络状态码</span><br></pre></td></tr></table></figure>

<p>2.期望输出数据格式</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="number">13560436666</span> 		<span class="number">1116		      </span><span class="number">954</span> 			<span class="number">2070</span></span><br><span class="line"><span class="string">手机号码</span>		     <span class="string">上行流量</span>       <span class="string">下行流量</span>	  <span class="string">总流量</span></span><br></pre></td></tr></table></figure>

<p>3.编写MapReduce程序</p>
<p>FlowBean</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.vincent.mapreduce.flowsum;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Writable;</span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> Vincent</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span> 2020-07-01</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">// 1 实现writable接口</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowBean</span> <span class="keyword">implements</span> <span class="title">Writable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> upFlow;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> downFlow;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> sumFlow;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getUpFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setUpFlow</span><span class="params">(<span class="keyword">long</span> upFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getDownFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setDownFlow</span><span class="params">(<span class="keyword">long</span> downFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.downFlow = downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getSumFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setSumFlow</span><span class="params">(<span class="keyword">long</span> sumFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.sumFlow = sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setSumFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.sumFlow = <span class="keyword">this</span>.getUpFlow() + <span class="keyword">this</span>.getDownFlow();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.upFlow + <span class="string">&quot;\t&quot;</span> + <span class="keyword">this</span>.downFlow + <span class="string">&quot;\t&quot;</span> + <span class="keyword">this</span>.sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 无参数构造器，反序列化调用</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        out.writeLong(upFlow);</span><br><span class="line">        out.writeLong(downFlow);</span><br><span class="line">        out.writeLong(sumFlow);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.upFlow = in.readLong();</span><br><span class="line">        <span class="keyword">this</span>.downFlow = in.readLong();</span><br><span class="line">        <span class="keyword">this</span>.sumFlow = in.readLong();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>FlowCountMapper</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.vincent.mapreduce.flowsum;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> Vincent</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span> 2020-07-01</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowCountMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Text outK = <span class="keyword">new</span> Text();</span><br><span class="line">    <span class="keyword">private</span> FlowBean outV = <span class="keyword">new</span> FlowBean();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//1	13244894052	192.168.1.1	www.baidu.com	2000	3452	200</span></span><br><span class="line">        String line = value.toString();</span><br><span class="line"></span><br><span class="line">        String[] splits = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line"></span><br><span class="line">        outK.set(splits[<span class="number">1</span>]);</span><br><span class="line"></span><br><span class="line">        outV.setUpFlow(Long.parseLong(splits[splits.length - <span class="number">3</span>]));</span><br><span class="line">        outV.setDownFlow(Long.parseLong(splits[splits.length - <span class="number">2</span>]));</span><br><span class="line">        outV.setSumFlow();</span><br><span class="line"></span><br><span class="line">        context.write(outK, outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>FlowCountReducer</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.vincent.mapreduce.flowsum;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> Vincent</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span> 2020-07-01</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowCountReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">FlowBean</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> FlowBean outV = <span class="keyword">new</span> FlowBean();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;FlowBean&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">long</span> totalUpFlow = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">long</span> totalDownFlow = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">long</span> totalSumFlow = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (FlowBean value: values) &#123;</span><br><span class="line">            totalUpFlow += value.getUpFlow();</span><br><span class="line">            totalDownFlow += value.getDownFlow();</span><br><span class="line">            totalSumFlow += value.getSumFlow();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        outV.setUpFlow(totalUpFlow);</span><br><span class="line">        outV.setDownFlow(totalDownFlow);</span><br><span class="line">        outV.setSumFlow(totalSumFlow);</span><br><span class="line"></span><br><span class="line">        context.write(key, outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>FlowDriver</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.vincent.mapreduce.flowsum;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> Vincent</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span> 2020-07-01</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        job.setJarByClass(FlowDriver.class);</span><br><span class="line"></span><br><span class="line">        job.setMapperClass(FlowCountMapper.class);</span><br><span class="line">        job.setReducerClass(FlowCountReducer.class);</span><br><span class="line"></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line"></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="MapReduce框架原理"><a href="#MapReduce框架原理" class="headerlink" title="MapReduce框架原理"></a>MapReduce框架原理</h1><h2 id="InputFormat数据输入"><a href="#InputFormat数据输入" class="headerlink" title="InputFormat数据输入"></a>InputFormat数据输入</h2><p><img src="/MapReduce/68.png" alt="68"></p>
<h3 id="切片与MapTask并行度决定机制"><a href="#切片与MapTask并行度决定机制" class="headerlink" title="切片与MapTask并行度决定机制"></a>切片与MapTask并行度决定机制</h3><p>1．问题引出</p>
<p>MapTask的并行度决定Map阶段的任务处理并发度，进而影响到整个Job的处理速度。</p>
<p>思考：1G的数据，启动8个MapTask，可以提高集群的并发处理能力。那么1K的数据，也启动8个MapTask，会提高集群性能吗？MapTask并行任务是否越多越好呢？哪些因素影响了MapTask并行度？</p>
<p>2．MapTask并行度决定机制</p>
<p><strong>数据块</strong>：Block是HDFS物理上把数据分成一块一块。</p>
<p><strong>数据切片</strong>：数据切片只是在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行存储。</p>
<p><img src="/MapReduce/69.png" alt="69"></p>
<h3 id="Job提交流程源码"><a href="#Job提交流程源码" class="headerlink" title="Job提交流程源码"></a>Job提交流程源码</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">waitForCompletion()</span><br><span class="line"></span><br><span class="line">submit();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1建立连接</span></span><br><span class="line">	connect();	</span><br><span class="line">		<span class="comment">// 1）创建提交Job的代理</span></span><br><span class="line">		<span class="keyword">new</span> Cluster(getConfiguration());</span><br><span class="line">			<span class="comment">// （1）判断是本地yarn还是远程</span></span><br><span class="line">			initialize(jobTrackAddr, conf); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 2 提交job</span></span><br><span class="line">submitter.submitJobInternal(Job.<span class="keyword">this</span>, cluster)</span><br><span class="line">	<span class="comment">// 1）创建给集群提交数据的Stag路径</span></span><br><span class="line">	Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf);</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 2）获取jobid ，并创建Job路径</span></span><br><span class="line">	JobID jobId = submitClient.getNewJobID();</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 3）拷贝jar包到集群</span></span><br><span class="line">copyAndConfigureFiles(job, submitJobDir);	</span><br><span class="line">	rUploader.uploadFiles(job, jobSubmitDir);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4）计算切片，生成切片规划文件</span></span><br><span class="line">writeSplits(job, submitJobDir);</span><br><span class="line">		maps = writeNewSplits(job, jobSubmitDir);</span><br><span class="line">		input.getSplits(job);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 5）向Stag路径写XML配置文件</span></span><br><span class="line">writeConf(conf, submitJobFile);</span><br><span class="line">	conf.writeXml(out);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 6）提交Job,返回提交状态</span></span><br><span class="line">status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials());</span><br></pre></td></tr></table></figure>

<p><img src="/MapReduce/70.png" alt="70"></p>
<h3 id="FileInputFormat切片源码解析-input-getSplits-job"><a href="#FileInputFormat切片源码解析-input-getSplits-job" class="headerlink" title="FileInputFormat切片源码解析(input.getSplits(job))"></a>FileInputFormat切片源码解析(input.getSplits(job))</h3><p><img src="/MapReduce/71.png" alt="71"></p>
<h3 id="FileInputFormat切片机制"><a href="#FileInputFormat切片机制" class="headerlink" title="FileInputFormat切片机制"></a>FileInputFormat切片机制</h3><p><strong>切片默认是128M，与块大小相同，如果切片大小大于块大小，那么切片中的另外一块数据就需要通过网络传输到map任务节点，与使用本地数据运行map任务相比，效率则更低。</strong></p>
<p><img src="/MapReduce/72.png" alt="72"></p>
<p><img src="/MapReduce/73.png" alt="73"></p>
<h3 id="FileInputFormat实现类"><a href="#FileInputFormat实现类" class="headerlink" title="FileInputFormat实现类"></a>FileInputFormat实现类</h3><p><img src="/MapReduce/74.png" alt="74"></p>
<h4 id="1-TextInputFormat"><a href="#1-TextInputFormat" class="headerlink" title="1.TextInputFormat"></a>1.TextInputFormat</h4><p><img src="/MapReduce/75.png" alt="75"></p>
<h4 id="2-KeyValueTextInputFormat"><a href="#2-KeyValueTextInputFormat" class="headerlink" title="2.KeyValueTextInputFormat"></a>2.KeyValueTextInputFormat</h4><p><img src="/MapReduce/76.png" alt="76"></p>
<p>KeyValueTextInputFormat案例</p>
<p><img src="/MapReduce/79.png" alt="79"></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KVTextMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">LongWritable</span>&gt;</span>&#123;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">// 1 设置value等于1</span></span><br><span class="line">   LongWritable v = <span class="keyword">new</span> LongWritable(<span class="number">1</span>);  </span><br><span class="line">    </span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Text key, Text value, Context context)</span></span></span><br><span class="line"><span class="function">			<span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">				<span class="comment">// banzhang ni hao</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 2 写出</span></span><br><span class="line">        context.write(key, v);  </span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KVTextReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">LongWritable</span>&gt;</span>&#123;</span><br><span class="line">	</span><br><span class="line">    LongWritable v = <span class="keyword">new</span> LongWritable();  </span><br><span class="line">    </span><br><span class="line">		<span class="meta">@Override</span></span><br><span class="line">		<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;LongWritable&gt; values,	Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">		</span><br><span class="line">		 <span class="keyword">long</span> sum = <span class="number">0L</span>;  </span><br><span class="line"></span><br><span class="line">		 <span class="comment">// 1 汇总统计</span></span><br><span class="line">        <span class="keyword">for</span> (LongWritable value : values) &#123;  </span><br><span class="line">            sum += value.get();  </span><br><span class="line">        &#125;</span><br><span class="line">         </span><br><span class="line">        v.set(sum);  </span><br><span class="line">         </span><br><span class="line">        <span class="comment">// 2 输出</span></span><br><span class="line">        context.write(key, v);  </span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.KeyValueLineRecordReader;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KVTextDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">		</span><br><span class="line">		Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">		<span class="comment">// 设置切割符</span></span><br><span class="line">		conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, <span class="string">&quot; &quot;</span>);</span><br><span class="line">		<span class="comment">// 1 获取job对象</span></span><br><span class="line">		Job job = Job.getInstance(conf);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 2 设置jar包位置，关联mapper和reducer</span></span><br><span class="line">		job.setJarByClass(KVTextDriver.class);</span><br><span class="line">		job.setMapperClass(KVTextMapper.class);</span><br><span class="line">		job.setReducerClass(KVTextReducer.class);</span><br><span class="line">				</span><br><span class="line">		<span class="comment">// 3 设置map输出kv类型</span></span><br><span class="line">		job.setMapOutputKeyClass(Text.class);</span><br><span class="line">		job.setMapOutputValueClass(LongWritable.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 4 设置最终输出kv类型</span></span><br><span class="line">		job.setOutputKeyClass(Text.class);</span><br><span class="line">		job.setOutputValueClass(LongWritable.class);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 5 设置输入数据路径</span></span><br><span class="line">		FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 设置输入格式</span></span><br><span class="line">		job.setInputFormatClass(KeyValueTextInputFormat.class);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 6 设置输出数据路径</span></span><br><span class="line">		FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 7 提交job</span></span><br><span class="line">		job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="3-NLineInputFormat"><a href="#3-NLineInputFormat" class="headerlink" title="3.NLineInputFormat"></a>3.NLineInputFormat</h4><p><img src="/MapReduce/77.png" alt="77"></p>
<p>NLineInputFormat使用案例</p>
<p><img src="/MapReduce/80.png" alt="80"></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NLineMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">LongWritable</span>&gt;</span>&#123;</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">private</span> Text k = <span class="keyword">new</span> Text();</span><br><span class="line">	<span class="keyword">private</span> LongWritable v = <span class="keyword">new</span> LongWritable(<span class="number">1</span>);</span><br><span class="line">	</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span>	<span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">		</span><br><span class="line">		 <span class="comment">// 1 获取一行</span></span><br><span class="line">        String line = value.toString();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 2 切割</span></span><br><span class="line">        String[] splited = line.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 3 循环写出</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; splited.length; i++) &#123;</span><br><span class="line">        	</span><br><span class="line">        	 k.set(splited[i]);</span><br><span class="line">        	</span><br><span class="line">           context.write(k, v);</span><br><span class="line">        &#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NLineReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">LongWritable</span>&gt;</span>&#123;</span><br><span class="line">	</span><br><span class="line">	LongWritable v = <span class="keyword">new</span> LongWritable();</span><br><span class="line">	</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;LongWritable&gt; values,	Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">		</span><br><span class="line">        <span class="keyword">long</span> sum = <span class="number">0l</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1 汇总</span></span><br><span class="line">        <span class="keyword">for</span> (LongWritable value : values) &#123;</span><br><span class="line">            sum += value.get();</span><br><span class="line">        &#125;  </span><br><span class="line">        </span><br><span class="line">        v.set(sum);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 2 输出</span></span><br><span class="line">        context.write(key, v);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.net.URISyntaxException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.NLineInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NLineDriver</span> </span>&#123;</span><br><span class="line">	</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, URISyntaxException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">		</span><br><span class="line">		 		<span class="comment">// 输入输出路径需要根据自己电脑上实际的输入输出路径设置</span></span><br><span class="line">		 		args = <span class="keyword">new</span> String[] &#123; <span class="string">&quot;e:/input/inputword&quot;</span>, <span class="string">&quot;e:/output1&quot;</span> &#125;;</span><br><span class="line"></span><br><span class="line">		 		<span class="comment">// 1 获取job对象</span></span><br><span class="line">		 		Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        Job job = Job.getInstance(configuration);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 2 设置每个切片InputSplit中划分三条记录</span></span><br><span class="line">        NLineInputFormat.setNumLinesPerSplit(job, <span class="number">3</span>);</span><br><span class="line">          </span><br><span class="line">        <span class="comment">// 3 使用NLineInputFormat处理记录数  </span></span><br><span class="line">        job.setInputFormatClass(NLineInputFormat.class);  </span><br><span class="line">          </span><br><span class="line">        <span class="comment">// 4 设置jar包位置，关联mapper和reducer</span></span><br><span class="line">        job.setJarByClass(NLineDriver.class);  </span><br><span class="line">        job.setMapperClass(NLineMapper.class);  </span><br><span class="line">        job.setReducerClass(NLineReducer.class);  </span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 5 设置map输出kv类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);  </span><br><span class="line">        job.setMapOutputValueClass(LongWritable.class);  </span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 6 设置最终输出kv类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);  </span><br><span class="line">        job.setOutputValueClass(LongWritable.class);  </span><br><span class="line">          </span><br><span class="line">        <span class="comment">// 7 设置输入输出数据路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));  </span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));  </span><br><span class="line">          </span><br><span class="line">        <span class="comment">// 8 提交job</span></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);  </span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="4-CombineTextInputFormat"><a href="#4-CombineTextInputFormat" class="headerlink" title="4.CombineTextInputFormat"></a>4.CombineTextInputFormat</h4><p>框架默认的TextInputFormat切片机制是对任务按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个MapTask，这样如果有大量小文件，就会产生大量的MapTask，处理效率极其低下。</p>
<p>1、应用场景：</p>
<p><strong>CombineTextInputFormat用于小文件过多的场景</strong>，它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个MapTask处理。</p>
<p>2、虚拟存储切片最大值设置</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">CombineTextInputFormat.setMaxInputSplitSize(job, <span class="number">4194304</span>);	<span class="comment">// 4M</span></span><br></pre></td></tr></table></figure>

<p>注意：虚拟存储切片最大值设置最好根据实际的小文件大小情况来设置具体的值。</p>
<p>3、切片机制</p>
<p>生成切片过程包括：虚拟存储过程和切片过程二部分。</p>
<p><img src="/MapReduce/78.png" alt="78"></p>
<p>CombineTextInputFormat案例</p>
<p>WordcountDriver:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 如果不设置InputFormat，它默认用的是TextInputFormat.class</span></span><br><span class="line">job.setInputFormatClass(CombineTextInputFormat.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">//虚拟存储切片最大值设置4m</span></span><br><span class="line">CombineTextInputFormat.setMaxInputSplitSize(job, <span class="number">4194304</span>);</span><br></pre></td></tr></table></figure>

<h4 id="5-自定义InputFormat"><a href="#5-自定义InputFormat" class="headerlink" title="5.自定义InputFormat"></a>5.自定义InputFormat</h4><p><img src="/MapReduce/81.png" alt="81"></p>
<p>无论HDFS还是MapReduce，在处理小文件时效率都非常低，但又难免面临处理大量小文件的场景，此时，就需要有相应解决方案，可以<strong>自定义InputFormat实现小文件的合并</strong>。</p>
<p>案例实操：</p>
<p>将多个小文件合并成一个SequenceFile文件（SequenceFile文件是Hadoop用来存储二进制形式的key-value对的文件格式），SequenceFile里面存储着多个文件，存储的形式为文件路径+名称为key，文件内容为value。</p>
<p>输入为三个小文件：one.txt    two.txt    three.txt</p>
<p>输出为一个SequenceFile文件：part-r-0000</p>
<p><img src="/MapReduce/82.png" alt="82"></p>
<p>自定义InputFromat</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.BytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.InputSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.JobContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.RecordReader;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义类继承FileInputFormat</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WholeFileInputformat</span> <span class="keyword">extends</span> <span class="title">FileInputFormat</span>&lt;<span class="title">Text</span>, <span class="title">BytesWritable</span>&gt;</span>&#123;</span><br><span class="line">	</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">boolean</span> <span class="title">isSplitable</span><span class="params">(JobContext context, Path filename)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> RecordReader&lt;Text, BytesWritable&gt; <span class="title">createRecordReader</span><span class="params">(InputSplit split, TaskAttemptContext context)</span>	<span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">		</span><br><span class="line">    <span class="comment">// 创建自定义RecordReader对象</span></span><br><span class="line">		WholeRecordReader recordReader = <span class="keyword">new</span> WholeRecordReader();	</span><br><span class="line">    </span><br><span class="line">		recordReader.initialize(split, context);</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">return</span> recordReader;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>自定义RecordReader类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataInputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.BytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.InputSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.RecordReader;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WholeRecordReader</span> <span class="keyword">extends</span> <span class="title">RecordReader</span>&lt;<span class="title">Text</span>, <span class="title">BytesWritable</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">private</span> Configuration configuration;</span><br><span class="line">	<span class="keyword">private</span> FileSplit split;</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">boolean</span> isProgress= <span class="keyword">true</span>;</span><br><span class="line">	<span class="keyword">private</span> BytesWritable value = <span class="keyword">new</span> BytesWritable();</span><br><span class="line">	<span class="keyword">private</span> Text k = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">(InputSplit split, TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">this</span>.split = (FileSplit)split;</span><br><span class="line">		configuration = context.getConfiguration();</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">nextKeyValue</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">if</span> (isProgress) &#123;</span><br><span class="line"></span><br><span class="line">			<span class="comment">// 1 定义缓存区</span></span><br><span class="line">			<span class="keyword">byte</span>[] contents = <span class="keyword">new</span> <span class="keyword">byte</span>[(<span class="keyword">int</span>)split.getLength()];</span><br><span class="line">			</span><br><span class="line">			FileSystem fs = <span class="keyword">null</span>;</span><br><span class="line">			FSDataInputStream fis = <span class="keyword">null</span>;</span><br><span class="line">			</span><br><span class="line">			<span class="keyword">try</span> &#123;</span><br><span class="line">				<span class="comment">// 2 获取文件系统</span></span><br><span class="line">				Path path = split.getPath();</span><br><span class="line">				fs = path.getFileSystem(configuration);</span><br><span class="line">				</span><br><span class="line">				<span class="comment">// 3 读取数据</span></span><br><span class="line">				fis = fs.open(path);</span><br><span class="line">				</span><br><span class="line">				<span class="comment">// 4 读取文件内容</span></span><br><span class="line">				IOUtils.readFully(fis, contents, <span class="number">0</span>, contents.length);</span><br><span class="line">				</span><br><span class="line">				<span class="comment">// 5 输出文件内容</span></span><br><span class="line">				value.set(contents, <span class="number">0</span>, contents.length);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 6 获取文件路径及名称</span></span><br><span class="line">        String name = split.getPath().toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 7 设置输出的key值</span></span><br><span class="line">        k.set(name);</span><br><span class="line"></span><br><span class="line">      &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line"></span><br><span class="line">      &#125;<span class="keyword">finally</span> &#123;</span><br><span class="line">        IOUtils.closeStream(fis);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      isProgress = <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Text <span class="title">getCurrentKey</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> k;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> BytesWritable <span class="title">getCurrentValue</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> value;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">float</span> <span class="title">getProgress</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>编写SequenceFileMapper类处理流程</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.BytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SequenceFileMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">Text</span>, <span class="title">BytesWritable</span>, <span class="title">Text</span>, <span class="title">BytesWritable</span>&gt;</span>&#123;</span><br><span class="line">	</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Text key, BytesWritable value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">		context.write(key, value);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>编写SequenceFileReducer类处理流程</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.BytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SequenceFileReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">BytesWritable</span>, <span class="title">Text</span>, <span class="title">BytesWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;BytesWritable&gt; values, Context context)</span>		<span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">		context.write(key, values.iterator().next());</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>编写SequenceFileDriver类处理流程</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.BytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SequenceFileDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">		</span><br><span class="line">       <span class="comment">// 输入输出路径需要根据自己电脑上实际的输入输出路径设置</span></span><br><span class="line">		args = <span class="keyword">new</span> String[] &#123; <span class="string">&quot;e:/input/inputinputformat&quot;</span>, <span class="string">&quot;e:/output1&quot;</span> &#125;;</span><br><span class="line"></span><br><span class="line">       <span class="comment">// 获取job对象</span></span><br><span class="line">		Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">		Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">       <span class="comment">// 设置jar包存储位置、关联自定义的mapper和reducer</span></span><br><span class="line">		job.setJarByClass(SequenceFileDriver.class);</span><br><span class="line">		job.setMapperClass(SequenceFileMapper.class);</span><br><span class="line">		job.setReducerClass(SequenceFileReducer.class);</span><br><span class="line"></span><br><span class="line">       <span class="comment">// 设置输入的inputFormat</span></span><br><span class="line">		job.setInputFormatClass(WholeFileInputformat.class);</span><br><span class="line"></span><br><span class="line">       <span class="comment">// 设置输出的outputFormat</span></span><br><span class="line">	  job.setOutputFormatClass(SequenceFileOutputFormat.class);</span><br><span class="line">       </span><br><span class="line">		   <span class="comment">// 设置map输出端的kv类型</span></span><br><span class="line">		job.setMapOutputKeyClass(Text.class);</span><br><span class="line">		job.setMapOutputValueClass(BytesWritable.class);</span><br><span class="line">		</span><br><span class="line">       <span class="comment">// 设置最终输出端的kv类型</span></span><br><span class="line">		job.setOutputKeyClass(Text.class);</span><br><span class="line">		job.setOutputValueClass(BytesWritable.class);</span><br><span class="line"></span><br><span class="line">       <span class="comment">// 设置输入输出路径</span></span><br><span class="line">		FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">		FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">       <span class="comment">// 提交job</span></span><br><span class="line">		<span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">		System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="MapReduce工作流程"><a href="#MapReduce工作流程" class="headerlink" title="MapReduce工作流程"></a>MapReduce工作流程</h2><p><img src="/MapReduce/83.png" alt="83"></p>
<p><img src="/MapReduce/84.png" alt="84"></p>
<p><strong>Map阶段：溢写过程中对key的索引进行快排（第一次）对溢写文件进行归并（第二次）</strong>combiner对每一个溢写文件相同key的值进行聚合</p>
<p><strong>Reduce阶段：归并（第三次）和分组排序</strong></p>
<p>上面的流程是整个MapReduce最全工作流程，但是Shuffle过程只是从第7步开始到第16步结束，具体Shuffle过程详解，如下：</p>
<p>1）MapTask收集我们的map()方法输出的kv对，放到内存缓冲区中</p>
<p>2）从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件</p>
<p>3）多个溢出文件会被合并成大的溢出文件</p>
<p>4）在溢出过程及合并的过程中，都要调用Partitioner进行分区和针对key进行排序</p>
<p>5）ReduceTask根据自己的分区号，去各个MapTask机器上取相应的结果分区数据</p>
<p>6）ReduceTask会取到同一个分区的来自不同MapTask的结果文件，ReduceTask会将这些文件再进行<strong>合并（归并排序）</strong></p>
<p>7）合并成大文件后，Shuffle的过程也就结束了，后面进入ReduceTask的逻辑运算过程（从文件中取出一个一个的键值对Group，调用用户自定义的reduce()方法）</p>
<p>注意</p>
<p>Shuffle中的缓冲区大小会影响到MapReduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快。</p>
<p>缓冲区的大小可以通过参数调整，参数：io.sort.mb默认100M。</p>
<h2 id="Shuffle机制"><a href="#Shuffle机制" class="headerlink" title="Shuffle机制"></a>Shuffle机制</h2><p>Map方法之后，Reduce方法之前的数据处理过程称之为Shuffle。</p>
<p><img src="/MapReduce/85.png" alt="85"></p>
<h3 id="Partition分区"><a href="#Partition分区" class="headerlink" title="Partition分区"></a>Partition分区</h3><p><img src="/MapReduce/86.png" alt="86"></p>
<p><img src="/MapReduce/87.png" alt="87"></p>
<p><img src="/MapReduce/89.png" alt="89"></p>
<p>案例实操</p>
<p>手机号136、137、138、139开头都分别放到一个独立的4个文件中，其他开头的放到一个文件中。</p>
<p><img src="/MapReduce/88.png" alt="88"></p>
<p>在序列化案例的基础上增加分区类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Partitioner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProvincePartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Text key, FlowBean value, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 1 获取电话号码的前三位</span></span><br><span class="line">		String preNum = key.toString().substring(<span class="number">0</span>, <span class="number">3</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">int</span> partition = <span class="number">5</span>;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 2 判断是哪个省</span></span><br><span class="line">		<span class="keyword">if</span> (<span class="string">&quot;136&quot;</span>.equals(preNum)) &#123;</span><br><span class="line">			partition = <span class="number">0</span>;</span><br><span class="line">		&#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;137&quot;</span>.equals(preNum)) &#123;</span><br><span class="line">			partition = <span class="number">1</span>;</span><br><span class="line">		&#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;138&quot;</span>.equals(preNum)) &#123;</span><br><span class="line">			partition = <span class="number">2</span>;</span><br><span class="line">		&#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;139&quot;</span>.equals(preNum)) &#123;</span><br><span class="line">			partition = <span class="number">3</span>;</span><br><span class="line">		&#125;<span class="keyword">else</span> &#123;</span><br><span class="line">      partition = <span class="number">4</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> partition;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在驱动函数中增加自定义数据分区设置和ReduceTask设置</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowsumDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IllegalArgumentException, IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 输入输出路径需要根据自己电脑上实际的输入输出路径设置</span></span><br><span class="line">		args = <span class="keyword">new</span> String[]&#123;<span class="string">&quot;e:/output1&quot;</span>,<span class="string">&quot;e:/output2&quot;</span>&#125;;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 1 获取配置信息，或者job对象实例</span></span><br><span class="line">		Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">		Job job = Job.getInstance(configuration);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 2 指定本程序的jar包所在的本地路径</span></span><br><span class="line">		job.setJarByClass(FlowsumDriver.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 3 指定本业务job要使用的mapper/Reducer业务类</span></span><br><span class="line">		job.setMapperClass(FlowCountMapper.class);</span><br><span class="line">		job.setReducerClass(FlowCountReducer.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 4 指定mapper输出数据的kv类型</span></span><br><span class="line">		job.setMapOutputKeyClass(Text.class);</span><br><span class="line">		job.setMapOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 5 指定最终输出的数据的kv类型</span></span><br><span class="line">		job.setOutputKeyClass(Text.class);</span><br><span class="line">		job.setOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 8 指定自定义数据分区</span></span><br><span class="line">		job.setPartitionerClass(ProvincePartitioner.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 9 同时指定相应数量的reduce task</span></span><br><span class="line">		job.setNumReduceTasks(<span class="number">5</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 6 指定job的输入原始文件所在目录</span></span><br><span class="line">		FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">		FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 7 将job中配置的相关参数，以及job所用的java类所在的jar包， 提交给yarn去运行</span></span><br><span class="line">		<span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">		System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="WritableComparable排序"><a href="#WritableComparable排序" class="headerlink" title="WritableComparable排序"></a>WritableComparable排序</h3><p><img src="/MapReduce/90.png" alt="90"></p>
<p><img src="/MapReduce/91.png" alt="91"></p>
<p><img src="/MapReduce/92.png" alt="92"></p>
<p>自定义排序WritableComparable</p>
<p>bean对象做为key传输，需要实现WritableComparable接口重写compareTo方法，就可以实现排序。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(FlowBean bean)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">int</span> result;</span><br><span class="line">		</span><br><span class="line">	<span class="comment">// 按照总流量大小，倒序排列</span></span><br><span class="line">	<span class="keyword">if</span> (sumFlow &gt; bean.getSumFlow()) &#123;</span><br><span class="line">		result = -<span class="number">1</span>;</span><br><span class="line">	&#125;<span class="keyword">else</span> <span class="keyword">if</span> (sumFlow &lt; bean.getSumFlow()) &#123;</span><br><span class="line">		result = <span class="number">1</span>;</span><br><span class="line">	&#125;<span class="keyword">else</span> &#123;</span><br><span class="line">		result = <span class="number">0</span>;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="WritableComparable排序案例实操（全排序）"><a href="#WritableComparable排序案例实操（全排序）" class="headerlink" title="WritableComparable排序案例实操（全排序）"></a>WritableComparable排序案例实操（全排序）</h3><p><img src="/MapReduce/93.png" alt="93"></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 实现 WritableComparable接口</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowBean</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">long</span> upFlow;</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">long</span> downFlow;</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">long</span> sumFlow;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 反序列化时，需要反射调用空参构造函数，所以必须有</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">super</span>();</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">(<span class="keyword">long</span> upFlow, <span class="keyword">long</span> downFlow)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">super</span>();</span><br><span class="line">		<span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line">		<span class="keyword">this</span>.downFlow = downFlow;</span><br><span class="line">		<span class="keyword">this</span>.sumFlow = upFlow + downFlow;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">set</span><span class="params">(<span class="keyword">long</span> upFlow, <span class="keyword">long</span> downFlow)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line">		<span class="keyword">this</span>.downFlow = downFlow;</span><br><span class="line">		<span class="keyword">this</span>.sumFlow = upFlow + downFlow;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getSumFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> sumFlow;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setSumFlow</span><span class="params">(<span class="keyword">long</span> sumFlow)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.sumFlow = sumFlow;</span><br><span class="line">	&#125;	</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setSumFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.sumFlow = <span class="keyword">this</span>.getUpFlow() + <span class="keyword">this</span>.getDownFlow();</span><br><span class="line">	&#125;	</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getUpFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> upFlow;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setUpFlow</span><span class="params">(<span class="keyword">long</span> upFlow)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getDownFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> downFlow;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setDownFlow</span><span class="params">(<span class="keyword">long</span> downFlow)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.downFlow = downFlow;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * 序列化方法</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> out</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">		out.writeLong(upFlow);</span><br><span class="line">		out.writeLong(downFlow);</span><br><span class="line">		out.writeLong(sumFlow);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * 反序列化方法 注意反序列化的顺序和序列化的顺序完全一致</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> in</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">		upFlow = in.readLong();</span><br><span class="line">		downFlow = in.readLong();</span><br><span class="line">		sumFlow = in.readLong();</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> upFlow + <span class="string">&quot;\t&quot;</span> + downFlow + <span class="string">&quot;\t&quot;</span> + sumFlow;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(FlowBean bean)</span> </span>&#123;</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">int</span> result;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 按照总流量大小，倒序排列</span></span><br><span class="line">		<span class="keyword">if</span> (sumFlow &gt; bean.getSumFlow()) &#123;</span><br><span class="line">			result = -<span class="number">1</span>;</span><br><span class="line">		&#125;<span class="keyword">else</span> <span class="keyword">if</span> (sumFlow &lt; bean.getSumFlow()) &#123;</span><br><span class="line">			result = <span class="number">1</span>;</span><br><span class="line">		&#125;<span class="keyword">else</span> &#123;</span><br><span class="line">			result = <span class="number">0</span>;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> result;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowCountSortMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>, <span class="title">Text</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">	FlowBean bean = <span class="keyword">new</span> FlowBean();</span><br><span class="line">	Text v = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span>	<span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 1 获取一行</span></span><br><span class="line">		String line = value.toString();</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 2 截取</span></span><br><span class="line">		String[] fields = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 3 封装对象</span></span><br><span class="line">		String phoneNbr = fields[<span class="number">0</span>];</span><br><span class="line">		<span class="keyword">long</span> upFlow = Long.parseLong(fields[<span class="number">1</span>]);</span><br><span class="line">		<span class="keyword">long</span> downFlow = Long.parseLong(fields[<span class="number">2</span>]);</span><br><span class="line">		</span><br><span class="line">		bean.set(upFlow, downFlow);</span><br><span class="line">		v.set(phoneNbr);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 4 输出</span></span><br><span class="line">		context.write(bean, v);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowCountSortReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">FlowBean</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(FlowBean key, Iterable&lt;Text&gt; values, Context context)</span>	<span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 循环输出，避免总流量相同情况</span></span><br><span class="line">    <span class="comment">// 遍历每一个手机号</span></span><br><span class="line">		<span class="keyword">for</span> (Text text : values) &#123;</span><br><span class="line">			context.write(text, key);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowCountSortDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ClassNotFoundException, IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 输入输出路径需要根据自己电脑上实际的输入输出路径设置</span></span><br><span class="line">		args = <span class="keyword">new</span> String[]&#123;<span class="string">&quot;e:/output1&quot;</span>,<span class="string">&quot;e:/output2&quot;</span>&#125;;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 1 获取配置信息，或者job对象实例</span></span><br><span class="line">		Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">		Job job = Job.getInstance(configuration);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 2 指定本程序的jar包所在的本地路径</span></span><br><span class="line">		job.setJarByClass(FlowCountSortDriver.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 3 指定本业务job要使用的mapper/Reducer业务类</span></span><br><span class="line">		job.setMapperClass(FlowCountSortMapper.class);</span><br><span class="line">		job.setReducerClass(FlowCountSortReducer.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 4 指定mapper输出数据的kv类型</span></span><br><span class="line">		job.setMapOutputKeyClass(FlowBean.class);</span><br><span class="line">		job.setMapOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 5 指定最终输出的数据的kv类型</span></span><br><span class="line">		job.setOutputKeyClass(Text.class);</span><br><span class="line">		job.setOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 6 指定job的输入原始文件所在目录</span></span><br><span class="line">		FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">		FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 7 将job中配置的相关参数，以及job所用的java类所在的jar包， 提交给yarn去运行</span></span><br><span class="line">		<span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">		System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="WritableComparable排序案例实操（区内排序）"><a href="#WritableComparable排序案例实操（区内排序）" class="headerlink" title="WritableComparable排序案例实操（区内排序）"></a>WritableComparable排序案例实操（区内排序）</h3><p>要求每个省份手机号输出的文件中按照总流量内部排序</p>
<p><img src="/MapReduce/94.png" alt="94"></p>
<p>增加自定义分区类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Partitioner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProvincePartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">FlowBean</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(FlowBean key, Text value, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 1 获取手机号码前三位</span></span><br><span class="line">		String preNum = value.toString().substring(<span class="number">0</span>, <span class="number">3</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">int</span> partition = <span class="number">4</span>;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 2 根据手机号归属地设置分区</span></span><br><span class="line">		<span class="keyword">if</span> (<span class="string">&quot;136&quot;</span>.equals(preNum)) &#123;</span><br><span class="line">			partition = <span class="number">0</span>;</span><br><span class="line">		&#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;137&quot;</span>.equals(preNum)) &#123;</span><br><span class="line">			partition = <span class="number">1</span>;</span><br><span class="line">		&#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;138&quot;</span>.equals(preNum)) &#123;</span><br><span class="line">			partition = <span class="number">2</span>;</span><br><span class="line">		&#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;139&quot;</span>.equals(preNum)) &#123;</span><br><span class="line">			partition = <span class="number">3</span>;</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      partition = <span class="number">4</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> partition;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在驱动类中添加分区类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 加载自定义分区类</span></span><br><span class="line">job.setPartitionerClass(ProvincePartitioner.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置Reducetask个数</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">5</span>);</span><br></pre></td></tr></table></figure>

<h3 id="Combiner合并"><a href="#Combiner合并" class="headerlink" title="Combiner合并"></a>Combiner合并</h3><p><img src="/MapReduce/95.png" alt="95"></p>
<p>Combiner合并案例实操</p>
<p><img src="/MapReduce/96.png" alt="96"></p>
<p>方案一</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordcountCombiner</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">IntWritable v = <span class="keyword">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1 汇总</span></span><br><span class="line">		<span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">for</span>(IntWritable value :values)&#123;</span><br><span class="line">			sum += value.get();</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		v.set(sum);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 2 写出</span></span><br><span class="line">		context.write(key, v);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 指定需要使用combiner，以及用哪个类作为combiner的逻辑</span></span><br><span class="line">job.setCombinerClass(WordcountCombiner.class);</span><br></pre></td></tr></table></figure>

<p>方案二</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 指定需要使用Combiner，以及用哪个类作为Combiner的逻辑</span></span><br><span class="line">job.setCombinerClass(WordcountReducer.class);</span><br></pre></td></tr></table></figure>

<h3 id="GroupingComparator分组（辅助排序）"><a href="#GroupingComparator分组（辅助排序）" class="headerlink" title="GroupingComparator分组（辅助排序）"></a>GroupingComparator分组（辅助排序）</h3><p>对Reduce阶段的数据根据某一个或几个字段进行分组。</p>
<p>分组排序步骤：</p>
<p>（1）自定义类继承WritableComparator</p>
<p>（2）重写compare()方法</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(WritableComparable a, WritableComparable b)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 比较的业务逻辑</span></span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> result;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（3）创建一个构造将比较对象的类传给父类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="title">OrderGroupingComparator</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">super</span>(OrderBean.class, <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>案例实操</p>
<p>求出每一个订单中最贵的商品</p>
<table>
<thead>
<tr>
<th>订单id</th>
<th>商品id</th>
<th>成交金额</th>
</tr>
</thead>
<tbody><tr>
<td>0000001</td>
<td>Pdt_01</td>
<td>222.8</td>
</tr>
<tr>
<td></td>
<td>Pdt_02</td>
<td>33.8</td>
</tr>
<tr>
<td>0000002</td>
<td>Pdt_03</td>
<td>522.8</td>
</tr>
<tr>
<td></td>
<td>Pdt_04</td>
<td>122.4</td>
</tr>
<tr>
<td></td>
<td>Pdt_05</td>
<td>722.4</td>
</tr>
<tr>
<td>0000003</td>
<td>Pdt_06</td>
<td>232.8</td>
</tr>
<tr>
<td></td>
<td>Pdt_02</td>
<td>33.8</td>
</tr>
</tbody></table>
<p>期望输出</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">1</span>	<span class="number">222</span>.<span class="number">82</span>	<span class="number">722</span>.<span class="number">43</span>	<span class="number">232</span>.<span class="number">8</span></span><br></pre></td></tr></table></figure>

<p>（1）利用“订单id和成交金额”作为key，可以将Map阶段读取到的所有订单数据按照id升序排序，如果id相同再按照金额降序排序，发送到Reduce。</p>
<p>（2）在Reduce端利用groupingComparator将订单id相同的kv聚合成组，然后取第一个即是该订单中最贵商品，如图所示。</p>
<p><img src="/MapReduce/97.png" alt="97"></p>
<p>定义订单信息OrderBean类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderBean</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">OrderBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">int</span> order_id; <span class="comment">// 订单id号</span></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">double</span> price; <span class="comment">// 价格</span></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="title">OrderBean</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">super</span>();</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="title">OrderBean</span><span class="params">(<span class="keyword">int</span> order_id, <span class="keyword">double</span> price)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">super</span>();</span><br><span class="line">		<span class="keyword">this</span>.order_id = order_id;</span><br><span class="line">		<span class="keyword">this</span>.price = price;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">		out.writeInt(order_id);</span><br><span class="line">		out.writeDouble(price);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">		order_id = in.readInt();</span><br><span class="line">		price = in.readDouble();</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> order_id + <span class="string">&quot;\t&quot;</span> + price;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getOrder_id</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> order_id;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setOrder_id</span><span class="params">(<span class="keyword">int</span> order_id)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.order_id = order_id;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">double</span> <span class="title">getPrice</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> price;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setPrice</span><span class="params">(<span class="keyword">double</span> price)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.price = price;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 二次排序</span></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(OrderBean o)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">int</span> result;</span><br><span class="line">		</span><br><span class="line">    <span class="comment">// 订单编号升序排列</span></span><br><span class="line">		<span class="keyword">if</span> (order_id &gt; o.getOrder_id()) &#123;</span><br><span class="line">			result = <span class="number">1</span>;</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (order_id &lt; o.getOrder_id()) &#123;</span><br><span class="line">			result = -<span class="number">1</span>;</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			<span class="comment">// 价格倒序排序</span></span><br><span class="line">			result = price &gt; o.getPrice() ? -<span class="number">1</span> : <span class="number">1</span>;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> result;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>编写OrderSortMapper类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">OrderBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">	OrderBean k = <span class="keyword">new</span> OrderBean();</span><br><span class="line">	</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 1 获取一行</span></span><br><span class="line">		String line = value.toString();</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 2 截取</span></span><br><span class="line">		String[] fields = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 3 封装对象</span></span><br><span class="line">		k.setOrder_id(Integer.parseInt(fields[<span class="number">0</span>]));</span><br><span class="line">		k.setPrice(Double.parseDouble(fields[<span class="number">2</span>]));</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 4 写出</span></span><br><span class="line">		context.write(k, NullWritable.get());</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>编写OrderSortGroupingComparator类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparator;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderGroupingComparator</span> <span class="keyword">extends</span> <span class="title">WritableComparator</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="title">OrderGroupingComparator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">super</span>(OrderBean.class, <span class="keyword">true</span>);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(WritableComparable a, WritableComparable b)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">		OrderBean aBean = (OrderBean) a;</span><br><span class="line">		OrderBean bBean = (OrderBean) b;</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">int</span> result;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 只比较Order_id，相同的Order_id进入一个组</span></span><br><span class="line">		<span class="keyword">if</span> (aBean.getOrder_id() &gt; bBean.getOrder_id()) &#123;</span><br><span class="line">			result = <span class="number">1</span>;</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (aBean.getOrder_id() &lt; bBean.getOrder_id()) &#123;</span><br><span class="line">			result = -<span class="number">1</span>;</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			result = <span class="number">0</span>;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> aBean.compareTo(bBean);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>编写OrderSortReducer类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">OrderBean</span>, <span class="title">NullWritable</span>, <span class="title">OrderBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(OrderBean key, Iterable&lt;NullWritable&gt; values, Context context)</span>		<span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">		</span><br><span class="line">		context.write(key, NullWritable.get());</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>编写OrderSortDriver类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception, IOException </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输入输出路径需要根据自己电脑上实际的输入输出路径设置</span></span><br><span class="line">		args  = <span class="keyword">new</span> String[]&#123;<span class="string">&quot;e:/input/inputorder&quot;</span> , <span class="string">&quot;e:/output1&quot;</span>&#125;;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 1 获取配置信息</span></span><br><span class="line">		Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">		Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 2 设置jar包加载路径</span></span><br><span class="line">		job.setJarByClass(OrderDriver.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 3 加载map/reduce类</span></span><br><span class="line">		job.setMapperClass(OrderMapper.class);</span><br><span class="line">		job.setReducerClass(OrderReducer.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 4 设置map输出数据key和value类型</span></span><br><span class="line">		job.setMapOutputKeyClass(OrderBean.class);</span><br><span class="line">		job.setMapOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 5 设置最终输出数据的key和value类型</span></span><br><span class="line">		job.setOutputKeyClass(OrderBean.class);</span><br><span class="line">		job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 6 设置输入数据和输出数据路径</span></span><br><span class="line">		FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">		FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 8 设置reduce端的分组</span></span><br><span class="line">	  job.setGroupingComparatorClass(OrderGroupingComparator.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 7 提交</span></span><br><span class="line">		<span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">		System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="MapTask工作机制"><a href="#MapTask工作机制" class="headerlink" title="MapTask工作机制"></a>MapTask工作机制</h2><p><img src="/MapReduce/98.png" alt="98"></p>
<p>​    （1）Read阶段：MapTask通过用户编写的RecordReader，<strong>从输入InputSplit中解析出一个个key/value</strong>。</p>
<p>​    （2）Map阶段：该节点主要是将解析出的key/value交给用户编写map()函数处理，并<strong>产生一系列新的key/value</strong>。</p>
<p>​    （3）Collect收集阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的key/value分区（调用Partitioner），并写入一个环形内存缓冲区中。</p>
<p>​    （4）Spill阶段：即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。</p>
<p>​    溢写阶段详情：</p>
<p>​    步骤1：利用<strong>快速排序算法</strong>对缓存区内的数据进行排序，排序方式是，<strong>先按照分区编号Partition进行排序，然后按照key进行排序</strong>。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序。</p>
<p>​    步骤2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件<strong>output/spillN.out</strong>（N表示当前溢写次数）中。<strong>如果用户设置了Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作</strong>。</p>
<p>​    步骤3：将分区数据的元信息写到内存索引数据结构<strong>SpillRecord</strong>中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB，则将内存索引写到文件output/spillN.out.index中。</p>
<p>​    （5）Combine阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最<strong>终只会生成一个数据文件</strong>。</p>
<p>​    当所有数据处理完后，MapTask会将所有临时文件合并成一个大文件，并保存到文件<strong>output/file.out</strong>中，同时生成相应的索引文件<strong>output/file.out.index</strong>。</p>
<p>​    在进行文件合并过程中，MapTask以分区为单位进行合并。对于每个分区，它将采用多轮递归合并的方式。每轮合并io.sort.factor（默认10）个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。</p>
<p>​    <strong>让每个MapTask最终只生成一个数据文件</strong>，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。</p>
<h2 id="ReduceTask工作机制"><a href="#ReduceTask工作机制" class="headerlink" title="ReduceTask工作机制"></a>ReduceTask工作机制</h2><p><img src="/MapReduce/99.png" alt="99"></p>
<p>​    （1）Copy阶段：ReduceTask从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。</p>
<p>​    （2）Merge阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对<strong>内存和磁盘</strong>上的文件进行合并，以防止内存使用过多或磁盘上文件过多。</p>
<p>​    （3）Sort阶段：按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次<strong>归并排序</strong>即可。</p>
<p>​    （4）Reduce阶段：reduce()函数将计算结果写到HDFS上。</p>
<p>设置ReduceTask并行度（个数）</p>
<p>ReduceTask的并行度同样影响整个Job的执行并发度和执行效率，但与<strong>MapTask的并发数由切片数决定</strong>不同，<strong>ReduceTask数量的决定是可以直接手动设置</strong>：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 默认值是1，手动设置为4</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">4</span>);</span><br></pre></td></tr></table></figure>

<p><img src="/MapReduce/100.png" alt="100"></p>
<h2 id="OutputFormat数据输出"><a href="#OutputFormat数据输出" class="headerlink" title="OutputFormat数据输出"></a>OutputFormat数据输出</h2><h3 id="OutputFormat接口实现类"><a href="#OutputFormat接口实现类" class="headerlink" title="OutputFormat接口实现类"></a>OutputFormat接口实现类</h3><p><img src="/MapReduce/101.png" alt="101"></p>
<h3 id="自定义OutputFormat"><a href="#自定义OutputFormat" class="headerlink" title="自定义OutputFormat"></a>自定义OutputFormat</h3><p><img src="/MapReduce/102.png" alt="102"></p>
<p>案例实操</p>
<p><img src="/MapReduce/103.png" alt="103"></p>
<p>编写FilterMapper类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FilterMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt;</span>&#123;</span><br><span class="line">	</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span>	<span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 写出</span></span><br><span class="line">		context.write(value, NullWritable.get());</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>编写FilterReducer类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FilterReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">		Text k = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;NullWritable&gt; values, Context context)</span>		<span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">       <span class="comment">// 1 获取一行</span></span><br><span class="line">		String line = key.toString();</span><br><span class="line"></span><br><span class="line">       <span class="comment">// 2 拼接</span></span><br><span class="line">		line = line + <span class="string">&quot;\n&quot;</span>;</span><br><span class="line"></span><br><span class="line">       <span class="comment">// 3 设置key</span></span><br><span class="line">    k.set(line);</span><br><span class="line"></span><br><span class="line">       <span class="comment">// 4 输出</span></span><br><span class="line">		context.write(k, NullWritable.get());</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>自定义一个OutputFormat类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.RecordWriter;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FilterOutputFormat</span> <span class="keyword">extends</span> <span class="title">FileOutputFormat</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> RecordWriter&lt;Text, NullWritable&gt; <span class="title">getRecordWriter</span><span class="params">(TaskAttemptContext job)</span>			<span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 创建一个RecordWriter</span></span><br><span class="line">		<span class="keyword">return</span> <span class="keyword">new</span> FilterRecordWriter(job);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>编写RecordWriter类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataOutputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.RecordWriter;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FilterRecordWriter</span> <span class="keyword">extends</span> <span class="title">RecordWriter</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">	FSDataOutputStream atguiguOut = <span class="keyword">null</span>;</span><br><span class="line">	FSDataOutputStream otherOut = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="title">FilterRecordWriter</span><span class="params">(TaskAttemptContext job)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 1 获取文件系统</span></span><br><span class="line">		FileSystem fs;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">try</span> &#123;</span><br><span class="line">			fs = FileSystem.get(job.getConfiguration());</span><br><span class="line"></span><br><span class="line">			<span class="comment">// 2 创建输出文件路径</span></span><br><span class="line">			Path atguiguPath = <span class="keyword">new</span> Path(<span class="string">&quot;e:/atguigu.log&quot;</span>);</span><br><span class="line">			Path otherPath = <span class="keyword">new</span> Path(<span class="string">&quot;e:/other.log&quot;</span>);</span><br><span class="line"></span><br><span class="line">			<span class="comment">// 3 创建输出流</span></span><br><span class="line">			atguiguOut = fs.create(atguiguPath);</span><br><span class="line">			otherOut = fs.create(otherPath);</span><br><span class="line">		&#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">			e.printStackTrace();</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(Text key, NullWritable value)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 判断是否包含“atguigu”输出到不同文件</span></span><br><span class="line">		<span class="keyword">if</span> (key.toString().contains(<span class="string">&quot;atguigu&quot;</span>)) &#123;</span><br><span class="line">			atguiguOut.write(key.toString().getBytes());</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			otherOut.write(key.toString().getBytes());</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">(TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 关闭资源</span></span><br><span class="line">		IOUtils.closeStream(atguiguOut);</span><br><span class="line">		IOUtils.closeStream(otherOut);	</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>编写FilterDriver类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FilterDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 输入输出路径需要根据自己电脑上实际的输入输出路径设置</span></span><br><span class="line">		args = <span class="keyword">new</span> String[] &#123; <span class="string">&quot;e:/input/inputoutputformat&quot;</span>, <span class="string">&quot;e:/output2&quot;</span> &#125;;</span><br><span class="line"></span><br><span class="line">		Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">		Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">		job.setJarByClass(FilterDriver.class);</span><br><span class="line">		job.setMapperClass(FilterMapper.class);</span><br><span class="line">		job.setReducerClass(FilterReducer.class);</span><br><span class="line"></span><br><span class="line">		job.setMapOutputKeyClass(Text.class);</span><br><span class="line">		job.setMapOutputValueClass(NullWritable.class);</span><br><span class="line">		</span><br><span class="line">		job.setOutputKeyClass(Text.class);</span><br><span class="line">		job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 要将自定义的输出格式组件设置到job中</span></span><br><span class="line">		job.setOutputFormatClass(FilterOutputFormat.class);</span><br><span class="line"></span><br><span class="line">		FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 虽然我们自定义了outputformat，但是因为我们的outputformat继承自fileoutputformat</span></span><br><span class="line">		<span class="comment">// 而fileoutputformat要输出一个_SUCCESS文件，所以，在这还得指定一个输出目录</span></span><br><span class="line">		FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">		<span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">		System.exit(result ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Join多种应用"><a href="#Join多种应用" class="headerlink" title="Join多种应用"></a>Join多种应用</h2><h3 id="Reduce-Join"><a href="#Reduce-Join" class="headerlink" title="Reduce Join"></a>Reduce Join</h3><p><img src="/MapReduce/104.png" alt="104"></p>
<p>案例实操</p>
<p>将商品信息表中数据根据商品pid合并到订单数据表中</p>
<p><img src="/MapReduce/105.png" alt="105"></p>
<p>创建商品和订合并后的Bean类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Writable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 封装  order.txt  及  pd.txt中的字段信息</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OrderBean</span> <span class="keyword">implements</span> <span class="title">Writable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String orderId;  <span class="comment">//订单id</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String pid ;     <span class="comment">//商品id</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Integer amount ; <span class="comment">//购买数量</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String pname ;   <span class="comment">// 商品名字</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String flag ;    <span class="comment">// 用于标记数据来自于哪个文件</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getOrderId</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> orderId;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setOrderId</span><span class="params">(String orderId)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.orderId = orderId;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getPid</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> pid;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setPid</span><span class="params">(String pid)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.pid = pid;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">getAmount</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> amount;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setAmount</span><span class="params">(Integer amount)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.amount = amount;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getPname</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> pname;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setPname</span><span class="params">(String pname)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.pname = pname;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getFlag</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> flag;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setFlag</span><span class="params">(String flag)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.flag = flag;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">OrderBean</span><span class="params">()</span></span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        out.writeUTF(orderId);</span><br><span class="line">        out.writeUTF(pid);</span><br><span class="line">        out.writeInt(amount);</span><br><span class="line">        out.writeUTF(pname);</span><br><span class="line">        out.writeUTF(flag);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.orderId = in.readUTF();</span><br><span class="line">        <span class="keyword">this</span>.pid = in.readUTF();</span><br><span class="line">        <span class="keyword">this</span>.amount = in.readInt();</span><br><span class="line">        <span class="keyword">this</span>.pname = in.readUTF();</span><br><span class="line">        <span class="keyword">this</span>.flag = in.readUTF();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">       <span class="keyword">return</span> <span class="keyword">this</span>.orderId + <span class="string">&quot;\t&quot;</span> + <span class="keyword">this</span>.pname + <span class="string">&quot;\t&quot;</span> + <span class="keyword">this</span>.amount;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>编写ReduceJoinMapper类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.InputSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReduceJoinMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>,<span class="title">Text</span>,<span class="title">OrderBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String currentSplitFileName ;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> OrderBean outV = <span class="keyword">new</span> OrderBean() ;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Text outK = <span class="keyword">new</span> Text() ;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 在MapTask执行开始时执行一次</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * 获取当前处理的切片对应的文件是哪个</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">       <span class="comment">//获取当前的切片对象</span></span><br><span class="line">        InputSplit inputSplit = context.getInputSplit();</span><br><span class="line">       <span class="comment">//转换成FileSplit</span></span><br><span class="line">        FileSplit currentSplit = (FileSplit)inputSplit;</span><br><span class="line">       <span class="comment">//获取当前处理的文件名</span></span><br><span class="line">        currentSplitFileName = currentSplit.getPath().getName();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">//处理一行数据</span></span><br><span class="line">        String line  = value.toString();</span><br><span class="line">        String[] split = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">        <span class="keyword">if</span>(currentSplitFileName.contains(<span class="string">&quot;order&quot;</span>))&#123;</span><br><span class="line">            <span class="comment">//数据来自于 order.txt</span></span><br><span class="line">            <span class="comment">//  1001	01	1</span></span><br><span class="line">            <span class="comment">//封装key</span></span><br><span class="line">            outK.set(split[<span class="number">1</span>]);</span><br><span class="line">            <span class="comment">//封装value</span></span><br><span class="line">            outV.setOrderId(split[<span class="number">0</span>]);</span><br><span class="line">            outV.setPid(split[<span class="number">1</span>]);</span><br><span class="line">            outV.setAmount(Integer.parseInt(split[<span class="number">2</span>]));</span><br><span class="line">            outV.setPname(<span class="string">&quot;&quot;</span>);</span><br><span class="line">            outV.setFlag(<span class="string">&quot;order&quot;</span>);</span><br><span class="line"></span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="comment">//数据来自于 pd.txt</span></span><br><span class="line">            <span class="comment">// 01	小米</span></span><br><span class="line">            <span class="comment">//封装key</span></span><br><span class="line">            outK.set(split[<span class="number">0</span>]);</span><br><span class="line">            <span class="comment">//封装value</span></span><br><span class="line">            outV.setPid(split[<span class="number">0</span>]);</span><br><span class="line">            outV.setPname(split[<span class="number">1</span>]);</span><br><span class="line">            outV.setOrderId(<span class="string">&quot;&quot;</span>);</span><br><span class="line">            outV.setAmount(<span class="number">0</span>);</span><br><span class="line">            outV.setFlag(<span class="string">&quot;pd&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//写出</span></span><br><span class="line">        context.write(outK,outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>编写ReduceJoinReducer类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.commons.beanutils.BeanUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.lang.reflect.InvocationTargetException;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReduceJoinReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>,<span class="title">OrderBean</span>,<span class="title">OrderBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//定义存储Order数据的OrderBean集合</span></span><br><span class="line">    List&lt;OrderBean&gt; orders = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//定义OrderBean， 存储pd的数据</span></span><br><span class="line">    OrderBean pdBean = <span class="keyword">new</span> OrderBean();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;OrderBean&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">//pid相同的数据会进入到一个reduce方法</span></span><br><span class="line">        <span class="comment">//  1001	01	1</span></span><br><span class="line">        <span class="comment">//  1004	01	4</span></span><br><span class="line">        <span class="comment">//  01	小米</span></span><br><span class="line">        <span class="comment">//思路: 将所有order的数据全部都获取到，保存到一个集合中. 把pd的数据获取到，保存到一个对象中</span></span><br><span class="line">        <span class="comment">//      迭代保存order数据的集合， 获取到每个order数据的OrderBean对象， 把Pd对象中的						//			pname设置到每个order数据的OrderBean对象中</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (OrderBean orderBean : values) &#123;</span><br><span class="line">          <span class="keyword">if</span>(<span class="string">&quot;order&quot;</span>.equals(orderBean.getFlag()))&#123;</span><br><span class="line">              <span class="comment">//order数据</span></span><br><span class="line">              <span class="comment">//深拷贝</span></span><br><span class="line">              OrderBean currentOrderbean = <span class="keyword">new</span> OrderBean();</span><br><span class="line">              <span class="keyword">try</span> &#123;</span><br><span class="line">                  BeanUtils.copyProperties(currentOrderbean,orderBean);</span><br><span class="line">              &#125; <span class="keyword">catch</span> (IllegalAccessException e) &#123;</span><br><span class="line">                  e.printStackTrace();</span><br><span class="line">              &#125; <span class="keyword">catch</span> (InvocationTargetException e) &#123;</span><br><span class="line">                  e.printStackTrace();</span><br><span class="line">              &#125;</span><br><span class="line">              orders.add(currentOrderbean);</span><br><span class="line">          &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">              <span class="comment">//pd数据</span></span><br><span class="line">              <span class="keyword">try</span> &#123;</span><br><span class="line">                  BeanUtils.copyProperties(pdBean,orderBean);</span><br><span class="line">              &#125; <span class="keyword">catch</span> (IllegalAccessException e) &#123;</span><br><span class="line">                  e.printStackTrace();</span><br><span class="line">              &#125; <span class="keyword">catch</span> (InvocationTargetException e) &#123;</span><br><span class="line">                  e.printStackTrace();</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//写出</span></span><br><span class="line">        <span class="keyword">for</span> (OrderBean orderBean : orders) &#123;</span><br><span class="line">            <span class="comment">// 给OrderBean的pname赋值</span></span><br><span class="line">            orderBean.setPname(pdBean.getPname());</span><br><span class="line"></span><br><span class="line">            context.write(orderBean,NullWritable.get());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//清空集合</span></span><br><span class="line">        orders.clear();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>编写ReduceJoinDriver类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReduceJoinDriver</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">//1. 创建Job对象</span></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line">        <span class="comment">//2. 关联jar</span></span><br><span class="line">        job.setJarByClass(ReduceJoinDriver.class);</span><br><span class="line">        <span class="comment">//3. 关联mapper和reducer</span></span><br><span class="line">        job.setMapperClass(ReduceJoinMapper.class);</span><br><span class="line">        job.setReducerClass(ReduceJoinReducer.class);</span><br><span class="line">        <span class="comment">//4. 设置map输出的k 和 v的类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(OrderBean.class);</span><br><span class="line">        <span class="comment">//5. 设置最终输出的k 和 v的类型</span></span><br><span class="line">        job.setOutputKeyClass(OrderBean.class);</span><br><span class="line">        job.setOutputValueClass(NullWritable.class);</span><br><span class="line">        <span class="comment">//6. 设置输入和输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job,<span class="keyword">new</span> Path(<span class="string">&quot;D:/input/inputtable&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job,<span class="keyword">new</span> Path(<span class="string">&quot;D:/output&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//7. 提交job</span></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Map-Join"><a href="#Map-Join" class="headerlink" title="Map Join"></a>Map Join</h3><p>Map Join适用于一张表十分小、一张表很大的场景。</p>
<p>思考：在Reduce端处理过多的表，非常容易产生数据倾斜。怎么办？</p>
<p>在Map端缓存多张表，提前处理业务逻辑，这样增加Map端业务，减少Reduce端数据的压力，尽可能的减少数据倾斜。</p>
<p><strong>具体办法：采用DistributedCache</strong></p>
<p>​    （1）在Mapper的setup阶段，将文件读取到缓存集合中。</p>
<p>​    （2）在驱动函数中加载缓存。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 缓存普通文件到Task运行节点。</span></span><br><span class="line">job.addCacheFile(<span class="keyword">new</span> URI(<span class="string">&quot;file://e:/cache/pd.txt&quot;</span>));</span><br></pre></td></tr></table></figure>

<p><img src="/MapReduce/106.png" alt="106"></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataInputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> java.io.BufferedReader;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.io.InputStreamReader;</span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * MapJoin的思想:</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *    将小表的文件提前加载到内存中，接下来每读取一条大表的数据，就与内存中的小表的数据进行join,join完成后直接写出.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MapJoinMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>,<span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Map&lt;String ,String&gt; pdMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Text outK = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        context.getCounter(<span class="string">&quot;Map Join&quot;</span>,<span class="string">&quot;setup&quot;</span>).increment(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将小表的数据加载到内存中</span></span><br><span class="line">        <span class="comment">// 获取在driver中设置的缓存文件</span></span><br><span class="line">        URI[] cacheFiles = context.getCacheFiles();</span><br><span class="line">        URI currentCacheFile = cacheFiles[<span class="number">0</span>];</span><br><span class="line">        <span class="comment">//读取文件</span></span><br><span class="line">        FileSystem fs = FileSystem.get(context.getConfiguration());</span><br><span class="line">        <span class="comment">//获取输入流</span></span><br><span class="line">        FSDataInputStream in = fs.open(<span class="keyword">new</span> Path(currentCacheFile.getPath()));</span><br><span class="line">        <span class="comment">//一次读取文件中的一行数据</span></span><br><span class="line">        BufferedReader reader  = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(in));</span><br><span class="line">        String line ;</span><br><span class="line">        <span class="keyword">while</span>((line = reader.readLine())!=<span class="keyword">null</span>)&#123;</span><br><span class="line">            <span class="comment">//切割</span></span><br><span class="line">            <span class="comment">// 01	小米</span></span><br><span class="line">            String[] split = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">            pdMap.put(split[<span class="number">0</span>],split[<span class="number">1</span>]);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//pdMap : 01 - 小米    02 - 华为  03 - 格力</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// 计数器</span></span><br><span class="line">        context.getCounter(<span class="string">&quot;Map Join&quot;</span>,<span class="string">&quot;map&quot;</span>).increment(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 读取大表的数据</span></span><br><span class="line">        String line = value.toString() ;</span><br><span class="line">        <span class="comment">// 切割</span></span><br><span class="line">        <span class="comment">// 1001	01	1</span></span><br><span class="line">        String[] split = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">        String currentPanme = pdMap.get(split[<span class="number">1</span>]);</span><br><span class="line">        String resultLine = split[<span class="number">0</span>] +<span class="string">&quot;\t&quot;</span> +currentPanme +<span class="string">&quot;\t&quot;</span> +split[<span class="number">2</span>];</span><br><span class="line"></span><br><span class="line">        <span class="comment">//封装key</span></span><br><span class="line">        outK.set(resultLine);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//写出</span></span><br><span class="line">        context.write(outK, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"><span class="keyword">import</span> java.net.URISyntaxException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MapJoinDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, URISyntaxException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        Job job  = Job.getInstance(conf);</span><br><span class="line">        <span class="comment">//设置缓存文件</span></span><br><span class="line">        job.addCacheFile(<span class="keyword">new</span> URI(<span class="string">&quot;file:///D:/input/inputcache/pd.txt&quot;</span>));</span><br><span class="line">        <span class="comment">// job.addCacheFile();  可以设置多个缓存文件</span></span><br><span class="line">        <span class="comment">//.....</span></span><br><span class="line">        job.setJarByClass(MapJoinDriver.class);</span><br><span class="line">        job.setMapperClass(MapJoinMapper.class);</span><br><span class="line"></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置ReduceTask的个数为0</span></span><br><span class="line">        job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">        FileInputFormat.setInputPaths(job,<span class="keyword">new</span> Path(<span class="string">&quot;d:/input/inputtable2&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job,<span class="keyword">new</span> Path(<span class="string">&quot;d:/output3&quot;</span>));</span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="计数器应用"><a href="#计数器应用" class="headerlink" title="计数器应用"></a>计数器应用</h2><p><img src="/MapReduce/107.png" alt="107"></p>
<h2 id="数据清洗（ETL）"><a href="#数据清洗（ETL）" class="headerlink" title="数据清洗（ETL）"></a>数据清洗（ETL）</h2><p>在运行核心业务MapReduce程序之前，往往要先对数据进行清洗，清理掉不符合用户要求的数据。<strong>清理的过程往往只需要运行Mapper程序，不需要运行Reduce程序。</strong></p>
<p>需求：去除日志中字段个数小于等于11的日志。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt;</span>&#123;</span><br><span class="line">	</span><br><span class="line">	Text k = <span class="keyword">new</span> Text();</span><br><span class="line">	</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 1 获取1行数据</span></span><br><span class="line">		String line = value.toString();</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 2 解析日志</span></span><br><span class="line">		<span class="keyword">boolean</span> result = parseLog(line,context);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 3 日志不合法退出</span></span><br><span class="line">		<span class="keyword">if</span> (!result) &#123;</span><br><span class="line">			<span class="keyword">return</span>;</span><br><span class="line">		&#125;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 4 设置key</span></span><br><span class="line">		k.set(line);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 5 写出数据</span></span><br><span class="line">		context.write(k, NullWritable.get());</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 2 解析日志</span></span><br><span class="line">	<span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">parseLog</span><span class="params">(String line, Context context)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 1 截取</span></span><br><span class="line">		String[] fields = line.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 2 日志长度大于11的为合法</span></span><br><span class="line">		<span class="keyword">if</span> (fields.length &gt; <span class="number">11</span>) &#123;</span><br><span class="line"></span><br><span class="line">			<span class="comment">// 系统计数器</span></span><br><span class="line">			context.getCounter(<span class="string">&quot;map&quot;</span>, <span class="string">&quot;true&quot;</span>).increment(<span class="number">1</span>);</span><br><span class="line">			<span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">		&#125;<span class="keyword">else</span> &#123;</span><br><span class="line">			context.getCounter(<span class="string">&quot;map&quot;</span>, <span class="string">&quot;false&quot;</span>).increment(<span class="number">1</span>);</span><br><span class="line">			<span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>编写LogDriver类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 输入输出路径需要根据自己电脑上实际的输入输出路径设置</span></span><br><span class="line">    args = <span class="keyword">new</span> String[] &#123; <span class="string">&quot;e:/input/inputlog&quot;</span>, <span class="string">&quot;e:/output1&quot;</span> &#125;;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 1 获取job信息</span></span><br><span class="line">		Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">		Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 2 加载jar包</span></span><br><span class="line">		job.setJarByClass(LogDriver.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 3 关联map</span></span><br><span class="line">		job.setMapperClass(LogMapper.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 4 设置最终输出类型</span></span><br><span class="line">		job.setOutputKeyClass(Text.class);</span><br><span class="line">		job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 设置reducetask个数为0</span></span><br><span class="line">		job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 5 设置输入和输出路径</span></span><br><span class="line">		FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">		FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 6 提交</span></span><br><span class="line">		job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>需求：对Web访问日志中的各字段识别切分，去除日志中不合法的记录。根据清洗规则，输出过滤后的数据。</p>
<p>定义一个bean，用来记录日志数据中的各数据字段</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogBean</span> </span>&#123;</span><br><span class="line">	<span class="keyword">private</span> String remote_addr;<span class="comment">// 记录客户端的ip地址</span></span><br><span class="line">	<span class="keyword">private</span> String remote_user;<span class="comment">// 记录客户端用户名称,忽略属性&quot;-&quot;</span></span><br><span class="line">	<span class="keyword">private</span> String time_local;<span class="comment">// 记录访问时间与时区</span></span><br><span class="line">	<span class="keyword">private</span> String request;<span class="comment">// 记录请求的url与http协议</span></span><br><span class="line">	<span class="keyword">private</span> String status;<span class="comment">// 记录请求状态；成功是200</span></span><br><span class="line">	<span class="keyword">private</span> String body_bytes_sent;<span class="comment">// 记录发送给客户端文件主体内容大小</span></span><br><span class="line">	<span class="keyword">private</span> String http_referer;<span class="comment">// 用来记录从那个页面链接访问过来的</span></span><br><span class="line">	<span class="keyword">private</span> String http_user_agent;<span class="comment">// 记录客户浏览器的相关信息</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">boolean</span> valid = <span class="keyword">true</span>;<span class="comment">// 判断数据是否合法</span></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> String <span class="title">getRemote_addr</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> remote_addr;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setRemote_addr</span><span class="params">(String remote_addr)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.remote_addr = remote_addr;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> String <span class="title">getRemote_user</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> remote_user;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setRemote_user</span><span class="params">(String remote_user)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.remote_user = remote_user;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> String <span class="title">getTime_local</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> time_local;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setTime_local</span><span class="params">(String time_local)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.time_local = time_local;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> String <span class="title">getRequest</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> request;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setRequest</span><span class="params">(String request)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.request = request;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> String <span class="title">getStatus</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> status;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setStatus</span><span class="params">(String status)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.status = status;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> String <span class="title">getBody_bytes_sent</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> body_bytes_sent;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setBody_bytes_sent</span><span class="params">(String body_bytes_sent)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.body_bytes_sent = body_bytes_sent;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> String <span class="title">getHttp_referer</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> http_referer;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setHttp_referer</span><span class="params">(String http_referer)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.http_referer = http_referer;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> String <span class="title">getHttp_user_agent</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> http_user_agent;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setHttp_user_agent</span><span class="params">(String http_user_agent)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.http_user_agent = http_user_agent;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isValid</span><span class="params">()</span> </span>&#123;</span><br><span class="line">		<span class="keyword">return</span> valid;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setValid</span><span class="params">(<span class="keyword">boolean</span> valid)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">this</span>.valid = valid;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">		StringBuilder sb = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">		sb.append(<span class="keyword">this</span>.valid);</span><br><span class="line">		sb.append(<span class="string">&quot;\001&quot;</span>).append(<span class="keyword">this</span>.remote_addr);</span><br><span class="line">		sb.append(<span class="string">&quot;\001&quot;</span>).append(<span class="keyword">this</span>.remote_user);</span><br><span class="line">		sb.append(<span class="string">&quot;\001&quot;</span>).append(<span class="keyword">this</span>.time_local);</span><br><span class="line">		sb.append(<span class="string">&quot;\001&quot;</span>).append(<span class="keyword">this</span>.request);</span><br><span class="line">		sb.append(<span class="string">&quot;\001&quot;</span>).append(<span class="keyword">this</span>.status);</span><br><span class="line">		sb.append(<span class="string">&quot;\001&quot;</span>).append(<span class="keyword">this</span>.body_bytes_sent);</span><br><span class="line">		sb.append(<span class="string">&quot;\001&quot;</span>).append(<span class="keyword">this</span>.http_referer);</span><br><span class="line">		sb.append(<span class="string">&quot;\001&quot;</span>).append(<span class="keyword">this</span>.http_user_agent);</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">return</span> sb.toString();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>编写LogMapper类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt;</span>&#123;</span><br><span class="line">	Text k = <span class="keyword">new</span> Text();</span><br><span class="line">	</span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span>	<span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 1 获取1行</span></span><br><span class="line">		String line = value.toString();</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 2 解析日志是否合法</span></span><br><span class="line">		LogBean bean = parseLog(line);</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">if</span> (!bean.isValid()) &#123;</span><br><span class="line">			<span class="keyword">return</span>;</span><br><span class="line">		&#125;</span><br><span class="line">		</span><br><span class="line">		k.set(bean.toString());</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 3 输出</span></span><br><span class="line">		context.write(k, NullWritable.get());</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 解析日志</span></span><br><span class="line">	<span class="function"><span class="keyword">private</span> LogBean <span class="title">parseLog</span><span class="params">(String line)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">		LogBean logBean = <span class="keyword">new</span> LogBean();</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 1 截取</span></span><br><span class="line">		String[] fields = line.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">if</span> (fields.length &gt; <span class="number">11</span>) &#123;</span><br><span class="line"></span><br><span class="line">			<span class="comment">// 2封装数据</span></span><br><span class="line">			logBean.setRemote_addr(fields[<span class="number">0</span>]);</span><br><span class="line">			logBean.setRemote_user(fields[<span class="number">1</span>]);</span><br><span class="line">			logBean.setTime_local(fields[<span class="number">3</span>].substring(<span class="number">1</span>));</span><br><span class="line">			logBean.setRequest(fields[<span class="number">6</span>]);</span><br><span class="line">			logBean.setStatus(fields[<span class="number">8</span>]);</span><br><span class="line">			logBean.setBody_bytes_sent(fields[<span class="number">9</span>]);</span><br><span class="line">			logBean.setHttp_referer(fields[<span class="number">10</span>]);</span><br><span class="line">			</span><br><span class="line">			<span class="keyword">if</span> (fields.length &gt; <span class="number">12</span>) &#123;</span><br><span class="line">				logBean.setHttp_user_agent(fields[<span class="number">11</span>] + <span class="string">&quot; &quot;</span>+ fields[<span class="number">12</span>]);</span><br><span class="line">			&#125;<span class="keyword">else</span> &#123;</span><br><span class="line">				logBean.setHttp_user_agent(fields[<span class="number">11</span>]);</span><br><span class="line">			&#125;</span><br><span class="line">			</span><br><span class="line">			<span class="comment">// 大于400，HTTP错误</span></span><br><span class="line">			<span class="keyword">if</span> (Integer.parseInt(logBean.getStatus()) &gt;= <span class="number">400</span>) &#123;</span><br><span class="line">				logBean.setValid(<span class="keyword">false</span>);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;<span class="keyword">else</span> &#123;</span><br><span class="line">			logBean.setValid(<span class="keyword">false</span>);</span><br><span class="line">		&#125;</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">return</span> logBean;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>编写LogDriver类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogDriver</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 1 获取job信息</span></span><br><span class="line">		Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">		Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 2 加载jar包</span></span><br><span class="line">		job.setJarByClass(LogDriver.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 3 关联map</span></span><br><span class="line">		job.setMapperClass(LogMapper.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 4 设置最终输出类型</span></span><br><span class="line">		job.setOutputKeyClass(Text.class);</span><br><span class="line">		job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 5 设置输入和输出路径</span></span><br><span class="line">		FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">		FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 6 提交</span></span><br><span class="line">		job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="MapReduce开发总结"><a href="#MapReduce开发总结" class="headerlink" title="MapReduce开发总结"></a>MapReduce开发总结</h2><p><img src="/MapReduce/108.png" alt="108"></p>
<p><img src="/MapReduce/109.png" alt="109"></p>
<p><img src="/MapReduce/110.png" alt="110"></p>
<p><img src="/MapReduce/111.png" alt="111"></p>
<p><img src="/MapReduce/112.png" alt="112"></p>
<h1 id="Hadoop数据压缩"><a href="#Hadoop数据压缩" class="headerlink" title="Hadoop数据压缩"></a>Hadoop数据压缩</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p><img src="/MapReduce/113.png" alt="113"></p>
<p><img src="/MapReduce/114.png" alt="114"></p>
<h2 id="MR支持的压缩编码"><a href="#MR支持的压缩编码" class="headerlink" title="MR支持的压缩编码"></a>MR支持的压缩编码</h2><table>
<thead>
<tr>
<th>压缩格式</th>
<th>hadoop自带？</th>
<th>算法</th>
<th>文件扩展名</th>
<th>是否可切分</th>
<th>换成压缩格式后，原来的程序是否需要修改</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>是，直接使用</td>
<td>DEFLATE</td>
<td>.deflate</td>
<td>否</td>
<td>和文本处理一样，不需要修改</td>
</tr>
<tr>
<td>Gzip</td>
<td>是，直接使用</td>
<td>DEFLATE</td>
<td>.gz</td>
<td>否</td>
<td>和文本处理一样，不需要修改</td>
</tr>
<tr>
<td>bzip2</td>
<td>是，直接使用</td>
<td>bzip2</td>
<td>.bz2</td>
<td>是</td>
<td>和文本处理一样，不需要修改</td>
</tr>
<tr>
<td>LZO</td>
<td>否，需要安装</td>
<td>LZO</td>
<td>.lzo</td>
<td>是</td>
<td>需要建索引，还需要指定输入格式</td>
</tr>
<tr>
<td>Snappy</td>
<td>否，需要安装</td>
<td>Snappy</td>
<td>.snappy</td>
<td>否</td>
<td>和文本处理一样，不需要修改</td>
</tr>
</tbody></table>
<p>为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示：</p>
<table>
<thead>
<tr>
<th>压缩格式</th>
<th>对应的编码/解码器</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
</tr>
<tr>
<td>gzip</td>
<td>org.apache.hadoop.io.compress.GzipCodec</td>
</tr>
<tr>
<td>bzip2</td>
<td>org.apache.hadoop.io.compress.BZip2Codec</td>
</tr>
<tr>
<td>LZO</td>
<td>com.hadoop.compression.lzo.LzopCodec</td>
</tr>
<tr>
<td>Snappy</td>
<td>org.apache.hadoop.io.compress.SnappyCodec</td>
</tr>
</tbody></table>
<p>压缩性能的比较：</p>
<table>
<thead>
<tr>
<th>压缩算法</th>
<th>原始文件大小</th>
<th>压缩文件大小</th>
<th>压缩速度</th>
<th>解压速度</th>
</tr>
</thead>
<tbody><tr>
<td>gzip</td>
<td>8.3GB</td>
<td>1.8GB</td>
<td>17.5MB/s</td>
<td>58MB/s</td>
</tr>
<tr>
<td>bzip2</td>
<td>8.3GB</td>
<td>1.1GB</td>
<td>2.4MB/s</td>
<td>9.5MB/s</td>
</tr>
<tr>
<td>LZO</td>
<td>8.3GB</td>
<td>2.9GB</td>
<td>49.3MB/s</td>
<td>74.6MB/s</td>
</tr>
</tbody></table>
<h3 id="Gzip压缩"><a href="#Gzip压缩" class="headerlink" title="Gzip压缩"></a>Gzip压缩</h3><p><img src="/MapReduce/115.png" alt="115"></p>
<h3 id="Bzip2压缩"><a href="#Bzip2压缩" class="headerlink" title="Bzip2压缩"></a>Bzip2压缩</h3><p><img src="/MapReduce/116.png" alt="116"></p>
<h3 id="Lzo压缩"><a href="#Lzo压缩" class="headerlink" title="Lzo压缩"></a>Lzo压缩</h3><p><img src="/MapReduce/117.png" alt="117"></p>
<h3 id="Snappy压缩"><a href="#Snappy压缩" class="headerlink" title="Snappy压缩"></a>Snappy压缩</h3><p><img src="/MapReduce/118.png" alt="118"></p>
<h2 id="压缩位置选择"><a href="#压缩位置选择" class="headerlink" title="压缩位置选择"></a>压缩位置选择</h2><p>压缩可以在MapReduce作用的任意阶段启用</p>
<p><img src="/MapReduce/119.png" alt="119"></p>
<h2 id="压缩参数配置"><a href="#压缩参数配置" class="headerlink" title="压缩参数配置"></a>压缩参数配置</h2><table>
<thead>
<tr>
<th align="left">参数</th>
<th>默认值</th>
<th>阶段</th>
<th>建议</th>
</tr>
</thead>
<tbody><tr>
<td align="left">io.compression.codecs    （在core-site.xml中配置）</td>
<td>org.apache.hadoop.io.compress.DefaultCodec,  org.apache.hadoop.io.compress.GzipCodec,  org.apache.hadoop.io.compress.BZip2Codec</td>
<td>输入压缩</td>
<td>Hadoop使用文件扩展名判断是否支持某种编解码器</td>
</tr>
<tr>
<td align="left">mapreduce.map.output.compress（在mapred-site.xml中配置）</td>
<td>false</td>
<td>mapper输出</td>
<td>这个参数设为true启用压缩</td>
</tr>
<tr>
<td align="left">mapreduce.map.output.compress.codec（在mapred-site.xml中配置）</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
<td>mapper输出</td>
<td>企业多使用LZO或Snappy编解码器在此阶段压缩数据</td>
</tr>
<tr>
<td align="left">mapreduce.output.fileoutputformat.compress（在mapred-site.xml中配置）</td>
<td>false</td>
<td>reducer输出</td>
<td>这个参数设为true启用压缩</td>
</tr>
<tr>
<td align="left">mapreduce.output.fileoutputformat.compress.codec（在mapred-site.xml中配置）</td>
<td>org.apache.hadoop.io.compress.  DefaultCodec</td>
<td>reducer输出</td>
<td>使用标准工具或者编解码器，如gzip和bzip2</td>
</tr>
<tr>
<td align="left">mapreduce.output.fileoutputformat.compress.type（在mapred-site.xml中配置）</td>
<td>RECORD</td>
<td>reducer输出</td>
<td>SequenceFile输出使用的压缩类型：NONE和BLOCK</td>
</tr>
</tbody></table>
<h2 id="压缩实操案例"><a href="#压缩实操案例" class="headerlink" title="压缩实操案例"></a>压缩实操案例</h2><h3 id="数据流的压缩和解压缩"><a href="#数据流的压缩和解压缩" class="headerlink" title="数据流的压缩和解压缩"></a>数据流的压缩和解压缩</h3><p><img src="/MapReduce/120.png" alt="120"></p>
<table>
<thead>
<tr>
<th>DEFLATE</th>
<th>org.apache.hadoop.io.compress.DefaultCodec</th>
</tr>
</thead>
<tbody><tr>
<td>gzip</td>
<td>org.apache.hadoop.io.compress.GzipCodec</td>
</tr>
<tr>
<td>bzip2</td>
<td>org.apache.hadoop.io.compress.BZip2Codec</td>
</tr>
</tbody></table>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.File;</span><br><span class="line"><span class="keyword">import</span> java.io.FileInputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.FileNotFoundException;</span><br><span class="line"><span class="keyword">import</span> java.io.FileOutputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.CompressionCodec;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.CompressionCodecFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.CompressionInputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.CompressionOutputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ReflectionUtils;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestCompress</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">		compress(<span class="string">&quot;e:/hello.txt&quot;</span>,<span class="string">&quot;org.apache.hadoop.io.compress.BZip2Codec&quot;</span>);</span><br><span class="line">	<span class="comment">//		decompress(&quot;e:/hello.txt.bz2&quot;,&quot;e:/hello.txt&quot;);</span></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 1、压缩</span></span><br><span class="line">	<span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">compress</span><span class="params">(String filename, String method)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// （1）获取输入流</span></span><br><span class="line">		FileInputStream fis = <span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(filename));</span><br><span class="line">		</span><br><span class="line">		Class codecClass = Class.forName(method);</span><br><span class="line">		</span><br><span class="line">		CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, <span class="keyword">new</span> Configuration());</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// （2）获取输出流</span></span><br><span class="line">		FileOutputStream fos = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(filename + codec.getDefaultExtension()));</span><br><span class="line">		CompressionOutputStream cos = codec.createOutputStream(fos);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// （3）流的对拷</span></span><br><span class="line">		IOUtils.copyBytes(fis, cos, <span class="number">1024</span>*<span class="number">1024</span>*<span class="number">5</span>, <span class="keyword">false</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// （4）关闭资源</span></span><br><span class="line">		cos.close();</span><br><span class="line">		fos.close();</span><br><span class="line">		fis.close();</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 2、解压缩</span></span><br><span class="line">	<span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">decompress</span><span class="params">(String filename,String dest)</span> <span class="keyword">throws</span> FileNotFoundException, IOException </span>&#123;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// （0）校验是否能解压缩</span></span><br><span class="line">		CompressionCodecFactory factory = <span class="keyword">new</span> CompressionCodecFactory(<span class="keyword">new</span> Configuration());</span><br><span class="line"></span><br><span class="line">		CompressionCodec codec = factory.getCodec(<span class="keyword">new</span> Path(filename));</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">if</span> (codec == <span class="keyword">null</span>) &#123;</span><br><span class="line">			System.out.println(<span class="string">&quot;cannot find codec for file &quot;</span> + filename);</span><br><span class="line">			<span class="keyword">return</span>;</span><br><span class="line">		&#125;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// （1）获取输入流</span></span><br><span class="line">		CompressionInputStream cis = codec.createInputStream(<span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(filename)));</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// （2）获取输出流</span></span><br><span class="line">		FileOutputStream fos = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(dest));</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// （3）流的对拷</span></span><br><span class="line">		IOUtils.copyBytes(cis, fos, <span class="number">1024</span>*<span class="number">1024</span>*<span class="number">5</span>, <span class="keyword">false</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// （4）关闭资源</span></span><br><span class="line">		cis.close();</span><br><span class="line">		fos.close();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="Map输出端采用压缩"><a href="#Map输出端采用压缩" class="headerlink" title="Map输出端采用压缩"></a>Map输出端采用压缩</h3><p>即使你的MapReduce的输入输出文件都是未压缩的文件，你仍然可以对Map任务的中间结果输出做压缩，因为它要写在硬盘并且通过网络传输到Reduce节点，对其压缩可以提高很多性能，这些工作只要设置两个属性即可，我们来看下代码怎么设置。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.BZip2Codec;	</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.CompressionCodec;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.GzipCodec;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">		Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 开启map端输出压缩</span></span><br><span class="line">	  configuration.setBoolean(<span class="string">&quot;mapreduce.map.output.compress&quot;</span>, <span class="keyword">true</span>);</span><br><span class="line">		<span class="comment">// 设置map端输出压缩方式</span></span><br><span class="line">	  configuration.setClass(<span class="string">&quot;mapreduce.map.output.compress.codec&quot;</span>, BZip2Codec.class, CompressionCodec.class);</span><br><span class="line"></span><br><span class="line">		Job job = Job.getInstance(configuration);</span><br><span class="line"></span><br><span class="line">		job.setJarByClass(WordCountDriver.class);</span><br><span class="line"></span><br><span class="line">		job.setMapperClass(WordCountMapper.class);</span><br><span class="line">		job.setReducerClass(WordCountReducer.class);</span><br><span class="line"></span><br><span class="line">		job.setMapOutputKeyClass(Text.class);</span><br><span class="line">		job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">		job.setOutputKeyClass(Text.class);</span><br><span class="line">		job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">		FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">		FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">		<span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">		System.exit(result ? <span class="number">1</span> : <span class="number">0</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Mapper和Reducer保持不变。</p>
<h3 id="Reduce输出端采用压缩"><a href="#Reduce输出端采用压缩" class="headerlink" title="Reduce输出端采用压缩"></a>Reduce输出端采用压缩</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.BZip2Codec;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.DefaultCodec;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.GzipCodec;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.Lz4Codec;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.SnappyCodec;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountDriver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">		</span><br><span class="line">		Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">		</span><br><span class="line">		Job job = Job.getInstance(configuration);</span><br><span class="line">		</span><br><span class="line">		job.setJarByClass(WordCountDriver.class);</span><br><span class="line">		</span><br><span class="line">		job.setMapperClass(WordCountMapper.class);</span><br><span class="line">		job.setReducerClass(WordCountReducer.class);</span><br><span class="line">		</span><br><span class="line">		job.setMapOutputKeyClass(Text.class);</span><br><span class="line">		job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line">		</span><br><span class="line">		job.setOutputKeyClass(Text.class);</span><br><span class="line">		job.setOutputValueClass(IntWritable.class);</span><br><span class="line">		</span><br><span class="line">		FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">		FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 设置reduce端输出压缩开启</span></span><br><span class="line">		FileOutputFormat.setCompressOutput(job, <span class="keyword">true</span>);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 设置压缩的方式</span></span><br><span class="line">	  FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class); </span><br><span class="line"><span class="comment">//	    FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class); </span></span><br><span class="line"><span class="comment">//	    FileOutputFormat.setOutputCompressorClass(job, DefaultCodec.class); </span></span><br><span class="line">	    </span><br><span class="line">		<span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">		</span><br><span class="line">		System.exit(result?<span class="number">1</span>:<span class="number">0</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title>Redis</title>
    <url>/Redis/</url>
    <content><![CDATA[<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>1、下载tar包并解压</p>
<p>2、安装gcc环境</p>
<p>我们需要将源码编译后再安装，因此需要安装c语言的编译环境！不能直接make！</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum install –y gcc-c++</span><br><span class="line"><span class="meta">#</span><span class="bash"> 之后查看安装是否成功：</span></span><br><span class="line">rpm -qa | grep gcc</span><br></pre></td></tr></table></figure>

<p>常见错误：在没有安装gcc环境下，如果执行了make，不会成功！安装环境后，第二次make有可能报错</p>
<figure class="highlight gauss"><table><tr><td class="code"><pre><span class="line">Jemalloc/jemalloc.h:没有那个文件</span><br><span class="line">解决： 运行 <span class="built_in">make</span> distclean之后再<span class="built_in">make</span></span><br></pre></td></tr></table></figure>

<p>3、编译，执行make命令！</p>
<p>4、编译完成后，安装，执行make install命令！</p>
<p>5、文件会被安装到 /usr/local/bin目录</p>
<p>6、可以将redis的bin目录，加入到环境变量中</p>
<table>
<thead>
<tr>
<th>Redis-benchmark</th>
<th>压力测试。标准是每秒80000次写操作，110000次读操作 (服务启动起来后执行,类似安兔兔跑分)</th>
</tr>
</thead>
<tbody><tr>
<td>Redis-check-aof</td>
<td>修复有问题的AOF文件</td>
</tr>
<tr>
<td>Redis-check-dump</td>
<td>修复有问题的dump.rdb文件</td>
</tr>
<tr>
<td>Redis-sentinel</td>
<td>启动哨兵，集群使用</td>
</tr>
<tr>
<td>redis-server</td>
<td>启动服务器</td>
</tr>
<tr>
<td>redis-cli</td>
<td>启动客户端</td>
</tr>
</tbody></table>
<h1 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h1><h2 id="服务端启动"><a href="#服务端启动" class="headerlink" title="服务端启动"></a>服务端启动</h2><p>将配置文件，保留一份副本，进行启动。</p>
<p>命令： </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">redis-server /opt/module/redis-3.2.5/redis.conf</span><br></pre></td></tr></table></figure>

<p>修改配置文件，改为守护进程，在后台运行</p>
<figure class="highlight nginx"><table><tr><td class="code"><pre><span class="line"><span class="attribute">daemonize</span> <span class="literal">yes</span></span><br></pre></td></tr></table></figure>

<p>后台启动后，查看服务</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">netstat -anp | grep 6379</span><br><span class="line"><span class="meta">#</span><span class="bash"> 或者</span></span><br><span class="line">ps -aux | grep 6379</span><br><span class="line">ps -ef | grep redis</span><br></pre></td></tr></table></figure>

<h2 id="客户端登录"><a href="#客户端登录" class="headerlink" title="客户端登录"></a>客户端登录</h2><table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
<th>举例</th>
<th>备注</th>
</tr>
</thead>
<tbody><tr>
<td>redis-cli</td>
<td>启动客户端</td>
<td>redis-cli -h 主机名 –p 端口号 -a 密码 –raw(查看原始数据)</td>
<td>直接执行的话，默认端口号就是6379；</td>
</tr>
<tr>
<td>ping</td>
<td>测试联通</td>
<td></td>
<td>回复pong代表联通</td>
</tr>
<tr>
<td>exit</td>
<td>退出客户端</td>
<td></td>
<td></td>
</tr>
<tr>
<td>redis-cli shutdown</td>
<td>停止服务器</td>
<td>redis-cli -h 127.0.0.1 -p 6379 shutdown  停止指定ip指定端口号的服务器</td>
<td>redis是通过客户端发送停止服务器的命令</td>
</tr>
</tbody></table>
<h1 id="Redis基本操作"><a href="#Redis基本操作" class="headerlink" title="Redis基本操作"></a>Redis基本操作</h1><h2 id="redis命令参考"><a href="#redis命令参考" class="headerlink" title="redis命令参考"></a>redis命令参考</h2><p><a href="http://doc.redisfans.com/">http://doc.redisfans.com/</a></p>
<h2 id="数据库连接操作"><a href="#数据库连接操作" class="headerlink" title="数据库连接操作"></a>数据库连接操作</h2><table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
<th>举例</th>
<th>备注</th>
</tr>
</thead>
<tbody><tr>
<td>select <dbid></dbid></td>
<td>切换数据库</td>
<td>select 1：切换到1号库</td>
<td>开启redis服务后，一共有16（0-15）个库，默认在0号库</td>
</tr>
<tr>
<td>flushdb</td>
<td>清空当前库</td>
<td></td>
<td></td>
</tr>
<tr>
<td>dbsize</td>
<td>查看数据库数据个数</td>
<td></td>
<td></td>
</tr>
<tr>
<td>flushall</td>
<td>通杀全部库</td>
<td></td>
<td></td>
</tr>
</tbody></table>
<h2 id="key的操作"><a href="#key的操作" class="headerlink" title="key的操作"></a>key的操作</h2><p><strong>Redis中的数据以键值对（key-value）为基本存储方式，其中key都是字符串。</strong></p>
<table>
<thead>
<tr>
<th>表达式</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>KEYS  pattern</td>
<td>查询符合指定表达式的所有key，支持*，？等</td>
</tr>
<tr>
<td>TYPE  key</td>
<td>查看key对应值的类型</td>
</tr>
<tr>
<td>EXISTS key</td>
<td>指定的key是否存在，0代表不存在，dr</td>
</tr>
<tr>
<td>DEL key</td>
<td>删除指定key</td>
</tr>
<tr>
<td>RANDOMKEY</td>
<td>在现有的KEY中随机返回一个</td>
</tr>
<tr>
<td>EXPIRE key seconds</td>
<td>为键值设置过期时间，单位是秒，过期后key会被redis移除</td>
</tr>
<tr>
<td>TTL key</td>
<td>查看key还有多少秒过期，-1表示永不过期，-2表示已过期</td>
</tr>
<tr>
<td>RENAME key newkey</td>
<td>重命名一个key，NEWKEY不管是否是已经存在的都会执行，如果NEWKEY已经存在则会被覆盖</td>
</tr>
<tr>
<td>RENAMENX   key newkey</td>
<td>只有在NEWKEY不存在时能够执行成功，否则失败</td>
</tr>
</tbody></table>
<h2 id="常用五大数据类型"><a href="#常用五大数据类型" class="headerlink" title="常用五大数据类型"></a>常用五大数据类型</h2><p>Redis中的数据以键值对（key-value）为基本存储方式，其中<strong>key都是字符串</strong>，这里探讨数据类型都是探讨value的类型。</p>
<table>
<thead>
<tr>
<th>value</th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>string</td>
<td>字符串</td>
</tr>
<tr>
<td>list</td>
<td>可以重复的集合</td>
</tr>
<tr>
<td>set</td>
<td>不可以重复的集合</td>
</tr>
<tr>
<td>hash</td>
<td>类似于Map&lt;String,String&gt;</td>
</tr>
<tr>
<td>zset（sorted  set）</td>
<td>带分数的set</td>
</tr>
</tbody></table>
<h2 id="string操作"><a href="#string操作" class="headerlink" title="string操作"></a>string操作</h2><p>String类型是Redis中最基本的类型，它是key对应的一个单一值。</p>
<p>二进制安全，不必担心由于编码等问题导致二进制数据变化。所以<strong>redis的string可以包含任何数据，比如jpg图片或者序列化的对象</strong>。</p>
<p>Redis中一个字符串值的最大容量是512M。</p>
<table>
<thead>
<tr>
<th>SET  key value</th>
<th>添加键值对，如果存在就更新value</th>
</tr>
</thead>
<tbody><tr>
<td>GET  key</td>
<td>查询指定key的值</td>
</tr>
<tr>
<td>APPEND key value</td>
<td>将给定的value追加到原值的末尾</td>
</tr>
<tr>
<td>STRLEN key</td>
<td>获取值的长度</td>
</tr>
<tr>
<td>SETNX key value</td>
<td>只有在 key 不存在时设置 key 的值</td>
</tr>
<tr>
<td>INCR key</td>
<td>指定key的值自增1，只对数字有效</td>
</tr>
<tr>
<td>DECR key</td>
<td>指定key的值自减1，只对数字有效</td>
</tr>
<tr>
<td>INCRBY key num</td>
<td>自增num</td>
</tr>
<tr>
<td>DECRBY key num</td>
<td>自减num</td>
</tr>
<tr>
<td>MSET key1 value1  key2 value2…</td>
<td>同时设置多个key-value对</td>
</tr>
<tr>
<td>MGET key1 key2</td>
<td>同时获取一个或多个value</td>
</tr>
<tr>
<td>MSETNX key1 value1 key2 value2</td>
<td>当key不存在时，设置多个key-value对；其中一个失败全失败</td>
</tr>
<tr>
<td>GETRANGE key起始索引 结束索引</td>
<td>获取指定范围的值，都是闭区间</td>
</tr>
<tr>
<td>SETRANGE key起始索引 value</td>
<td>从起始位置开始覆写指定的值，给多少覆盖多少</td>
</tr>
<tr>
<td>GETSET key value</td>
<td>以新换旧，同时获取旧值，先get再set</td>
</tr>
<tr>
<td>SETEX  key 过期时间  value</td>
<td>设置键值的同时，设置过期时间，单位秒</td>
</tr>
</tbody></table>
<h2 id="list操作"><a href="#list操作" class="headerlink" title="list操作"></a>list操作</h2><p>在Java中list 一般是单向链表，如常见的Arraylist，只能从一侧插入。</p>
<p><strong>在Redis中，list是双向链表。可以从两侧插入。</strong></p>
<p>常见操作：</p>
<p> 遍历：遍历的时候，是从左往右取值；</p>
<p>删除：弹栈，POP；</p>
<p>添加：压栈，PUSH ；</p>
<p>Redis 列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素导列表的头部（左边）或者尾部（右边）。它的底层实际是个<strong>双向链表</strong>，对两端的操作性能很高，通过索引下标的操作中间的节点性能会较差。</p>
<table>
<thead>
<tr>
<th>LPUSH/RPUSH key value1 value2…</th>
<th>从左边/右边压入一个或多个值  头尾效率高，中间效率低</th>
</tr>
</thead>
<tbody><tr>
<td>LPOP/RPOP key</td>
<td>从左边/右边弹出一个值  值在键在，值光键亡  弹出=返回+删除</td>
</tr>
<tr>
<td>LRANGE key start stop</td>
<td>查看指定区间的元素  正着数：0,1,2,3,…  倒着数：-1,-2,-3,…</td>
</tr>
<tr>
<td>LINDEX key index</td>
<td>按照索引下标获取元素（从左到右）</td>
</tr>
<tr>
<td>LLEN key</td>
<td>获取列表长度</td>
</tr>
<tr>
<td>LINSERT key BEFORE|AFTER value newvalue</td>
<td>在指定value的前后插入newvalue</td>
</tr>
<tr>
<td>LREM key n value</td>
<td>从左边删除n个value</td>
</tr>
<tr>
<td>LSET key index value</td>
<td>把指定索引位置的元素替换为另一个值</td>
</tr>
<tr>
<td>LTRIM key start stop</td>
<td>仅保留指定区间的数据</td>
</tr>
<tr>
<td>RPOPLPUSH list1 list2</td>
<td>从list1右边弹出一个值，左侧压入到list2，list1可以和list2相同</td>
</tr>
</tbody></table>
<h2 id="set操作"><a href="#set操作" class="headerlink" title="set操作"></a>set操作</h2><p>set是无序的，且是不可重复的。</p>
<table>
<thead>
<tr>
<th>SADD key member [member …]</th>
<th>将一个或多个 member 元素加入到集合 key 当中，已经存在于集合的  member 元素将被忽略。</th>
</tr>
</thead>
<tbody><tr>
<td>SMEMBERS key</td>
<td>取出该集合的所有值</td>
</tr>
<tr>
<td>SISMEMBER key value</td>
<td>判断集合<key>是否为含有该<value>值，有返回1，没有返回0</value></key></td>
</tr>
<tr>
<td>SCARD key</td>
<td>返回集合中元素的数量</td>
</tr>
<tr>
<td>SREM key member [member …]</td>
<td>从集合中删除元素</td>
</tr>
<tr>
<td>SPOP key [count]</td>
<td>从集合中随机弹出count个数量的元素，count不指定就弹出1个</td>
</tr>
<tr>
<td>SRANDMEMBER key [count]</td>
<td>从集合中随机返回count个数量的元素，count不指定就返回1个</td>
</tr>
<tr>
<td>SINTER key [key …]</td>
<td>将指定的集合进行“交集”操作</td>
</tr>
<tr>
<td>SINTERSTORE dest key [key …]</td>
<td>取交集，另存为一个set</td>
</tr>
<tr>
<td>SUNION key [key …]</td>
<td>将指定的集合执行“并集”操作</td>
</tr>
<tr>
<td>SUNIONSTORE dest key [key …]</td>
<td>取并集，另存为set</td>
</tr>
<tr>
<td>SDIFF key [key …]</td>
<td>将指定的集合执行“差集”操作</td>
</tr>
<tr>
<td>SDIFFSTORE dest key [key …]</td>
<td>取差集，另存为set</td>
</tr>
</tbody></table>
<h2 id="hash操作"><a href="#hash操作" class="headerlink" title="hash操作"></a>hash操作</h2><p><strong>Hash数据类型的键值对中的值是“单列”的，不支持进一步的层次结构。</strong></p>
<table>
<thead>
<tr>
<th></th>
<th align="center">field:value</th>
</tr>
</thead>
<tbody><tr>
<td>key</td>
<td align="center">“k01”:”v01”, “k02”:”v02”</td>
</tr>
</tbody></table>
<p>从前到后的数据对应关系</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">stu</span>:&#123;<span class="string">&quot;stu_id&quot;</span>:<span class="number">10</span>,<span class="string">&quot;stu_name&quot;</span>:<span class="string">&quot;tom&quot;</span>,<span class="string">&quot;stu_age&quot;</span>:<span class="number">30</span>&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Java</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Student</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> Integer stuId;<span class="comment">//10</span></span><br><span class="line"><span class="keyword">private</span> String stuName;<span class="comment">//&quot;tom&quot;</span></span><br><span class="line"><span class="keyword">private</span> Integer stuAge;<span class="comment">//30</span></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Student stu=<span class="keyword">new</span> Student()’</span><br><span class="line">stu.setStuId=<span class="number">10</span>;</span><br><span class="line">stu.setStuName=”tom”;</span><br><span class="line">stu.setStuAge=<span class="number">30</span>;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>Redis hash</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">key</span>	       value(hash)</span><br><span class="line">	         <span class="attribute">stu_id</span>	   <span class="number">10</span></span><br><span class="line"><span class="attribute">stu</span>	       stu_name	 tom</span><br><span class="line">	         <span class="attribute">stu_age</span>	 <span class="number">30</span></span><br></pre></td></tr></table></figure>

<p>常用操作：</p>
<table>
<thead>
<tr>
<th>HDEL key field1 [field2]</th>
<th>删除一个或多个哈希表字段</th>
</tr>
</thead>
<tbody><tr>
<td>HEXISTS key field</td>
<td>查看哈希表 key 中，指定的字段是否存在。</td>
</tr>
<tr>
<td>HGET key field</td>
<td>获取存储在哈希表中指定字段的值。</td>
</tr>
<tr>
<td>HGETALL key</td>
<td>获取在哈希表中指定 key 的所有字段和值</td>
</tr>
<tr>
<td>HINCRBY key field increment</td>
<td>为哈希表 key 中的指定字段的整数值加上增量 increment 。</td>
</tr>
<tr>
<td>HINCRBYFLOAT key field increment</td>
<td>为哈希表 key 中的指定字段的浮点数值加上增量 increment 。</td>
</tr>
<tr>
<td>HKEYS key</td>
<td>获取所有哈希表中的字段</td>
</tr>
<tr>
<td>HLEN key</td>
<td>获取哈希表中字段的数量</td>
</tr>
<tr>
<td>HMGET key field1 [field2]</td>
<td>获取所有给定字段的值</td>
</tr>
<tr>
<td>HMSET key field1 value1 [field2 value2 ]</td>
<td>同时将多个 field-value (域-值)对设置到哈希表 key 中。</td>
</tr>
<tr>
<td>HSET key field value</td>
<td>将哈希表 key 中的字段 field 的值设为 value 。</td>
</tr>
<tr>
<td>HSETNX key field value</td>
<td>只有在字段 field 不存在时，设置哈希表字段的值。</td>
</tr>
<tr>
<td>HVALS key</td>
<td>获取哈希表中所有值。</td>
</tr>
<tr>
<td>HSCAN key cursor [MATCH pattern] [COUNT count]</td>
<td>迭代哈希表中的键值对。当Redis操作Hash中的大量数据时，需要用到分页操作hscan。数据量太小无法使用。</td>
</tr>
</tbody></table>
<h2 id="zset操作"><a href="#zset操作" class="headerlink" title="zset操作"></a>zset操作</h2><p>zset是一种特殊的set(sorted set)，在保存value的时候，<strong>为每个value多保存了一个score信息。根据score信息，可以进行排序</strong>。</p>
<p>这个评分（score）被用来按照<strong>从最低分到最高分</strong>的方式排序集合中的成员。集合的成员是唯一的，但是评分可以是重复的。</p>
<table>
<thead>
<tr>
<th>ZADD key [score  member …]</th>
<th>添加</th>
</tr>
</thead>
<tbody><tr>
<td>ZSCORE key member</td>
<td>返回指定值的分数</td>
</tr>
<tr>
<td>ZRANGE key start stop [WITHSCORES]</td>
<td>返回指定区间的值，可选择是否一起返回scores</td>
</tr>
<tr>
<td>ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset  count]</td>
<td>在分数的指定区间内返回数据，从小到大排列</td>
</tr>
<tr>
<td>ZREVRANGEBYSCORE key max min [WITHSCORES] [LIMIT offset  count]</td>
<td>在分数的指定区间内返回数据，从大到小排列</td>
</tr>
<tr>
<td>ZCARD key</td>
<td>返回集合中所有的元素的数量</td>
</tr>
<tr>
<td>ZCOUNT key min max</td>
<td>统计分数区间内的元素个数</td>
</tr>
<tr>
<td>ZREM key member</td>
<td>删除该集合下，指定值的元素</td>
</tr>
<tr>
<td>ZRANK key member</td>
<td>返回该值在集合中的排名，从0开始</td>
</tr>
<tr>
<td>ZINCRBY key increment member</td>
<td>为元素的score加上增量</td>
</tr>
</tbody></table>
<h1 id="持久化"><a href="#持久化" class="headerlink" title="持久化"></a>持久化</h1><p>Redis主要是工作在内存中。内存本身就不是一个持久化设备，断电后数据会清空。所以Redis在工作过程中，如果发生了意外停电事故，如何尽可能减少数据丢失。</p>
<h2 id="RDB"><a href="#RDB" class="headerlink" title="RDB"></a>RDB</h2><p>RDB：在指定的时间间隔内将内存中的数据集快照写入磁盘，也就是行话讲的Snapshot快照，它恢复时是将快照文件直接读到内存里。</p>
<p>工作机制：<strong>每隔一段时间，就把内存中的数据保存到硬盘上的指定文件中</strong>。</p>
<p>​    存储方式：</p>
<ol>
<li><p>隔段时间自动存储</p>
</li>
<li><p>使用save或者bgsave时也会持久化</p>
</li>
<li><p>flushall也会持久化（flushdb不会持久化）</p>
</li>
<li><p>shutdown也会持久化</p>
</li>
</ol>
<p>RDB是默认开启的！</p>
<p>Redis会单独创建（fork）一个子进程来进行持久化，会先将数据写入到一个临时文件中，待持久化过程都结束了，再用这个临时文件替换上次持久化好的文件。整个过程中，主进程是不进行任何IO操作的，这就确保了极高的性能如果需要进行大规模数据的恢复，且对于数据恢复的完整性不是非常敏感，那RDB方式要比AOF方式更加的高效。</p>
<p>RDB的缺点是最后一次持久化后的数据可能丢失。</p>
<h3 id="RDB保存策略"><a href="#RDB保存策略" class="headerlink" title="RDB保存策略"></a>RDB保存策略</h3><p>save 900 1                       900 秒内如果至少有 1 个 key 的值变化，则保存</p>
<p>save 300 10                     300 秒内如果至少有 10 个 key 的值变化，则保存</p>
<p>save 60 10000                 60 秒内如果至少有 10000 个 key 的值变化，则保存</p>
<p>save “”                              就是禁用RDB模式；</p>
<h3 id="RDB常用属性配置"><a href="#RDB常用属性配置" class="headerlink" title="RDB常用属性配置"></a>RDB常用属性配置</h3><table>
<thead>
<tr>
<th>属性</th>
<th>含义</th>
<th>备注</th>
</tr>
</thead>
<tbody><tr>
<td>save</td>
<td>保存策略</td>
<td></td>
</tr>
<tr>
<td>dbfilename</td>
<td>RDB快照文件名</td>
<td></td>
</tr>
<tr>
<td>dir</td>
<td>RDB快照保存的目录</td>
<td>必须是一个目录，不能是文件名。最好改为固定目录。默认为./代表执行redis-server命令时的当前目录！</td>
</tr>
<tr>
<td>stop-writes-on-bgsave-error</td>
<td>是否在备份出错时，继续接受写操作</td>
<td>如果用户开启了RDB快照功能，那么在redis持久化数据到磁盘时如果出现失败，默认情况下，redis会停止接受所有的写请求</td>
</tr>
<tr>
<td>rdbcompression</td>
<td>对于存储到磁盘中的快照，可以设置是否进行压缩存储。</td>
<td>如果是的话，redis会采用LZF算法进行压缩。如果你不想消耗CPU来进行压缩的话，可以设置为关闭此功能，但是存储在磁盘上的快照会比较大。</td>
</tr>
<tr>
<td>rdbchecksum</td>
<td>是否进行数据校验</td>
<td>在存储快照后，我们还可以让redis使用CRC64算法来进行数据校验，但是这样做会增加大约10%的性能消耗，   如果希望获取到最大的性能提升，可以关闭此功能。</td>
</tr>
</tbody></table>
<h3 id="RDB数据丢失的情况"><a href="#RDB数据丢失的情况" class="headerlink" title="RDB数据丢失的情况"></a>RDB数据丢失的情况</h3><p>两次保存的时间间隔内，服务器宕机，或者发生断电问题。</p>
<h3 id="RDB的触发"><a href="#RDB的触发" class="headerlink" title="RDB的触发"></a>RDB的触发</h3><p>1、基于自动保存的策略</p>
<p>2、执行save，或者bgsave命令！执行时，是阻塞状态。</p>
<p>3、执行flushall命令，也会产生dump.rdb，但里面是空的，没有意义。</p>
<p>4、当执行shutdown命令时，也会主动地备份数据。</p>
<h3 id="RDB的优缺点"><a href="#RDB的优缺点" class="headerlink" title="RDB的优缺点"></a>RDB的优缺点</h3><p>RDB的优点:</p>
<ul>
<li>RDB是一个紧凑压缩的二进制文件，代表Redis在某一个时间点上的数据快照。非常适合用于备份，全量复制等场景。比如每6小时执行bgsave备份，并把RDB文件拷贝到远程机器或者文件系统中（如hdfs），用于灾难恢复。</li>
<li>Redis加载RDB恢复数据远远快于AOF方式。</li>
</ul>
<p>RDB的缺点:</p>
<ul>
<li><p>RDB方式数据没办法做到实时持久化/秒级持久化。因为bgsave每次运行都要执行fork操作创建子进程，属于重量级操作，频繁执行成本过高。</p>
</li>
<li><p>RDB文件使用特定二进制格式保存，Redis版本演进过程中有多个格式的RDB笨笨，存在老版本Redis服务无法兼容新版RDB格式的问题。</p>
<p>针对RDB不适合实时持久化的问题，Redis提供了AOF持久化方式来解决</p>
</li>
</ul>
<h2 id="AOF"><a href="#AOF" class="headerlink" title="AOF"></a>AOF</h2><p><strong>AOF是以日志的形式来记录每个写操作</strong>，将每一次对数据进行修改，都把新建、修改数据的命令保存到指定文件中。<strong>Redis重新启动时读取这个文件，重新执行新建、修改数据的命令恢复数据</strong>。</p>
<p>默认不开启，需要手动开启</p>
<p>AOF文件的保存路径，同RDB的路径一致。</p>
<p>AOF在保存命令的时候，只会保存对数据有修改的命令，也就是写操作！</p>
<p>当RDB和AOF存的不一致的情况下，按照AOF来恢复。因为AOF是对RDB的补充。备份周期更短，也就更可靠。</p>
<h3 id="AOF保存策略"><a href="#AOF保存策略" class="headerlink" title="AOF保存策略"></a>AOF保存策略</h3><p>appendfsync always：每次产生一条新的修改数据的命令都执行保存操作；效率低，但是安全！</p>
<p>appendfsync everysec：每秒执行一次保存操作。如果在未保存当前秒内操作时发生了断电，仍然会导致一部分数据丢失（即1秒钟的数据）。</p>
<p>appendfsync no：从不保存，将数据交给操作系统来处理。更快，也更不安全的选择。</p>
<p>推荐（并且也是默认）的措施为每秒 fsync 一次， 这种 fsync 策略可以兼顾速度和安全性。</p>
<h3 id="AOF常用属性"><a href="#AOF常用属性" class="headerlink" title="AOF常用属性"></a>AOF常用属性</h3><table>
<thead>
<tr>
<th>属性</th>
<th>含义</th>
<th>备注</th>
</tr>
</thead>
<tbody><tr>
<td>appendonly</td>
<td>是否开启AOF功能</td>
<td>默认是关闭的</td>
</tr>
<tr>
<td>appendfilename</td>
<td>AOF文件名称</td>
<td></td>
</tr>
<tr>
<td>appendfsync</td>
<td>AOF保存策略</td>
<td>官方建议everysec</td>
</tr>
<tr>
<td>no-appendfsync-on-rewrite</td>
<td>在重写时，是否执行保存策略</td>
<td>执行重写，可以节省AOF文件的体积；而且在恢复的时候效率也更高。</td>
</tr>
<tr>
<td>auto-aof-rewrite-percentage</td>
<td>重写的触发条件</td>
<td>当目前aof文件大小超过上一次重写的aof文件大小的百分之多少进行重写</td>
</tr>
<tr>
<td>auto-aof-rewrite-min-size</td>
<td>设置允许重写的最小aof文件大小</td>
<td>避免了达到约定百分比但尺寸仍然很小的情况还要重写</td>
</tr>
<tr>
<td>aof-load-truncated</td>
<td>截断设置</td>
<td>如果选择的是yes，当截断的aof文件被导入的时候，会自动发布一个log给客户端然后load</td>
</tr>
</tbody></table>
<h3 id="AOF文件的修复"><a href="#AOF文件的修复" class="headerlink" title="AOF文件的修复"></a>AOF文件的修复</h3><p>如果AOF文件中出现了残余命令，会导致服务器无法重启。此时需要借助redis-check-aof工具来修复！</p>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><span class="line">命令： redis-<span class="keyword">check</span>-aof  <span class="comment">--fix 文件</span></span><br></pre></td></tr></table></figure>

<h3 id="AOF的优缺点"><a href="#AOF的优缺点" class="headerlink" title="AOF的优缺点"></a>AOF的优缺点</h3><p>优点：</p>
<p>备份机制更稳健，丢失数据概率更低</p>
<p>可读的日志文本，通过操作AOF稳健，可以处理误操作</p>
<p>缺点：</p>
<p>比起RDB占用更多的磁盘空间</p>
<p>恢复备份速度要慢</p>
<p>每次读写都同步的话，有一定的性能压力</p>
<p>存在个别Bug，造成恢复不能</p>
<h2 id="备份建议"><a href="#备份建议" class="headerlink" title="备份建议"></a>备份建议</h2><p>Redis作为内存数据库从本质上来说，如果不想牺牲性能，就不可能做到数据的“绝对”安全。</p>
<p>RDB和AOF都只是尽可能在兼顾性能的前提下降低数据丢失的风险，如果真的发生数据丢失问题，尽可能减少损失。</p>
<p>在整个项目的架构体系中，<strong>Redis大部分情况是扮演“二级缓存”角色</strong>。</p>
<p>二级缓存适合保存的数据</p>
<p>经常要查询，很少被修改的数据。</p>
<p>不是非常重要，允许出现偶尔的并发问题。</p>
<p>不会被其他应用程序修改。</p>
<p>如果Redis是作为缓存服务器，那么说明数据在MySQL这样的传统关系型数据库中是有正式版本的。数据最终以MySQL中的为准。</p>
<p>官方推荐两个都用；如果对数据不敏感，可以选单独用RDB；不建议单独用AOF，因为可能出现Bug；如果只是做纯内存缓存，可以都不用。</p>
<h1 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h1><p>ACID一个都不具有，原子性一致性持久性都没。</p>
<p>把多个指令封装到一个队列中按顺序执行。</p>
<h2 id="事务简介"><a href="#事务简介" class="headerlink" title="事务简介"></a>事务简介</h2><p>Redis中事务，不同于传统的关系型数据库中的事务。</p>
<p>Redis中的事务指的是一个单独的隔离操作。</p>
<p>Redis的事务中的所有命令都会序列化、按顺序地执行且不会被其他客户端发送来的命令请求所打断。</p>
<p>Redis事务的主要作用是串联多个命令防止别的命令插队</p>
<h2 id="事务常用命令"><a href="#事务常用命令" class="headerlink" title="事务常用命令"></a>事务常用命令</h2><table>
<thead>
<tr>
<th>MULTI</th>
<th>标记一个事务块的开始</th>
</tr>
</thead>
<tbody><tr>
<td>EXEC</td>
<td>执行事务中所有在排队等待的指令并将链接状态恢复到正常 当使用WATCH 时，只有当被监视的键没有被修改，且允许检查设定机制时，EXEC会被执行</td>
</tr>
<tr>
<td>DISCARD</td>
<td>刷新一个事务中所有在排队等待的指令，并且将连接状态恢复到正常。  如果已使用WATCH，DISCARD将释放所有被WATCH的key。  取消组队。</td>
</tr>
<tr>
<td>WATCH</td>
<td>标记所有指定的key 被监视起来，在事务中有条件的执行（乐观锁）</td>
</tr>
</tbody></table>
<h2 id="锁"><a href="#锁" class="headerlink" title="锁"></a>锁</h2><h3 id="悲观锁"><a href="#悲观锁" class="headerlink" title="悲观锁"></a>悲观锁</h3><p>适合写多读少场景。</p>
<p>执行操作前假设当前的操作肯定（或有很大几率）会被打断（悲观）。基于这个假设，我们在做操作前就会把相关资源锁定，不允许自己执行期间有其他操作干扰。</p>
<p>Redis不支持悲观锁。Redis作为缓存服务器使用时，以读操作为主，很少写操作，相应的操作被打断的几率较少。不采用悲观锁是为了防止降低性能。</p>
<h3 id="乐观锁"><a href="#乐观锁" class="headerlink" title="乐观锁"></a>乐观锁</h3><p>适合读多写少场景。</p>
<p>执行操作前假设当前操作不会被打断（乐观）。基于这个假设，我们在做操作前不会锁定资源，万一发生了其他操作的干扰，那么本次操作将被放弃。</p>
<h2 id="Redis中的锁策略"><a href="#Redis中的锁策略" class="headerlink" title="Redis中的锁策略"></a>Redis中的锁策略</h2><p>Redis采用了乐观锁策略（通过watch操作）。乐观锁支持读操作，适用于多读少写的情况！</p>
<p>在事务中，可以通过watch命令来加锁；使用 UNWATCH可以取消加锁；</p>
<p>如果在事务之前，执行了WATCH（加锁），那么执行EXEC 命令或 DISCARD 命令后，锁对自动释放，即不需要再执行 UNWATCH 了</p>
<h1 id="主从复制"><a href="#主从复制" class="headerlink" title="主从复制"></a>主从复制</h1><h2 id="主从简介"><a href="#主从简介" class="headerlink" title="主从简介"></a>主从简介</h2><p>配置多台Redis服务器，以主机和备机的身份分开。主机数据更新后，根据配置和策略，自动同步到备机的master/salver机制，<strong>Master以写为主，Slave以读为主，二者之间自动同步数据</strong>。</p>
<p>目的：</p>
<p>读写分离提高Redis性能</p>
<p>避免单点故障，容灾快速恢复</p>
<p>原理：</p>
<p>每次从机联通后，都会给主机发送sync指令，主机立刻进行存盘操作，发送RDB文件，给从机</p>
<p>从机收到RDB文件后，进行全盘加载。之后每次主机的写操作命令，都会立刻发送给从机，从机执行相同的命令来保证主从的数据一致！</p>
<p>注意：主库接收到SYNC的命令时会执行RDB过程，即使在配置文件中禁用RDB持久化也会生成，但是如果主库所在的服务器磁盘IO性能较差，那么这个复制过程就会出现瓶颈，庆幸的是，Redis在2.8.18版本开始实现了无磁盘复制功能（不过该功能还是处于试验阶段），设置repl-diskless-sync yes。即Redis在与从数据库进行复制初始化时将不会将快照存储到磁盘，而是直接通过网络发送给从数据库，避免了IO性能差问题。</p>
<h2 id="主从准备"><a href="#主从准备" class="headerlink" title="主从准备"></a>主从准备</h2><p>除非是不同的主机配置不同的Redis服务，否则在一台机器上面跑多个Redis服务，需要配置多个Redis配置文件。</p>
<p>1、准备多个Redis配置文件，每个配置文件，需要配置以下属性</p>
<figure class="highlight elm"><table><tr><td class="code"><pre><span class="line"><span class="title">daemonize</span> yes: 服务在后台运行<span class="keyword">port</span>：端口号pidfile:pid保存文件logfile：日志文件(如果没有指定的话，就不需要)dump.rdb: RDB appendonly 关掉，或者是更改appendonly文件的名称。</span><br></pre></td></tr></table></figure>

<p>样本</p>
<figure class="highlight gradle"><table><tr><td class="code"><pre><span class="line"><span class="keyword">include</span> <span class="regexp">/root/</span>redis_repilication<span class="regexp">/redis.confport 6379pidfile /</span>var<span class="regexp">/run/</span>redis_6379.piddbfilename dump_6379.rdb</span><br></pre></td></tr></table></figure>

<p>2、根据多个配置文件，启动多个Redis服务</p>
<p>原则是配从不配主。</p>
<h2 id="主从建立"><a href="#主从建立" class="headerlink" title="主从建立"></a>主从建立</h2><h3 id="临时建立"><a href="#临时建立" class="headerlink" title="临时建立"></a>临时建立</h3><p>原则：配从不配主。</p>
<p>配置：在从服务器上执行SLAVEOF ip:port命令；</p>
<p>查看：执行info replication命令；</p>
<h3 id="永久建立"><a href="#永久建立" class="headerlink" title="永久建立"></a>永久建立</h3><p>在从机的配置文件中，编写slaveof属性配置！</p>
<h3 id="恢复身份"><a href="#恢复身份" class="headerlink" title="恢复身份"></a>恢复身份</h3><p>执行命令slaveof no one恢复自由身！</p>
<h2 id="哨兵模式"><a href="#哨兵模式" class="headerlink" title="哨兵模式"></a>哨兵模式</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p><strong>作用</strong>：</p>
<ol>
<li>主从状态检测 </li>
<li>如果Master异常，则会进行Master-Slave切换，将其中一个Slave作为Master，将之前的Master作为Slave(如果重启成功 )的多个哨兵，不仅同时监控主从状态，且哨兵之间也互相监控！</li>
</ol>
<p><strong>下线</strong>：</p>
<ol>
<li><p>主观下线：Subjectively Down，简称 SDOWN，指的是当前 Sentinel 实例对某个redis服务器做出的下线判断。</p>
</li>
<li><p>客观下线：Objectively Down， 简称 ODOWN，指的是多个 Sentinel 实例在对Master Server做出 SDOWN 判断，并且通过 SENTINEL is-master-down-by-addr 命令互相交流之后，得出的Master Server下线判断，然后开启failover.</p>
</li>
</ol>
<p><strong>工作原理</strong>：</p>
<ol>
<li><p>每个Sentinel以每秒钟一次的频率向它所知的Master，Slave以及其他 Sentinel 实例发送一个 PING 命令 ；</p>
</li>
<li><p>如果一个实例（instance）距离最后一次有效回复 PING 命令的时间超过 down-after-milliseconds 选项所指定的值， 则这个实例会被 Sentinel 标记为主观下线；</p>
</li>
<li><p>如果一个Master被标记为主观下线，则正在监视这个Master的所有 Sentinel 要以每秒一次的频率确认Master的确进入了主观下线状态；</p>
</li>
<li><p>当有足够数量的 Sentinel（大于等于配置文件指定的值）在指定的时间范围内确认Master的确进入了主观下线状态， 则Master会被标记为客观下线 ；</p>
</li>
<li><p>在一般情况下， 每个 Sentinel 会以每 10 秒一次的频率向它已知的所有Master，Slave发送 INFO 命令</p>
</li>
<li><p>当Master被 Sentinel 标记为客观下线时，Sentinel 向下线的 Master 的所有 Slave 发送 INFO 命令的频率会从 10 秒一次改为每秒一次 ；</p>
</li>
<li><p>若没有足够数量的 Sentinel 同意 Master 已经下线， Master 的主观下线状态就会被移除； 若 Master 重新向 Sentinel 的 PING 命令返回有效回复， Master 的客观下线状态就会被移除；主机宕机后，新主机选举条件依次为：优先级靠前的、偏移量最大的、runid最小的，若新主复活后则变为从机。</p>
</li>
</ol>
<h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>哨兵模式需要配置哨兵的配置文件！</p>
<figure class="highlight smali"><table><tr><td class="code"><pre><span class="line">sentinel<span class="built_in"> monitor </span>mymaster 10.211.55.10 6379 1</span><br></pre></td></tr></table></figure>

<p>启动哨兵：</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><span class="line">redis-sentinel <span class="regexp">/opt/m</span>odule<span class="regexp">/redis-3.2.5/</span>sentinel.conf</span><br></pre></td></tr></table></figure>

<h1 id="集群"><a href="#集群" class="headerlink" title="集群"></a>集群</h1><p>Redis 集群实现了对Redis的水平扩容，即启动N个redis节点，将整个数据库分布存储在这N个节点中，每个节点存储总数据的1/N。</p>
<p>Redis 集群通过分区（partition）来提供一定程度的可用性（availability）： 即使集群中有一部分节点失效或者无法进行通讯， 集群也可以继续处理命令请求。</p>
<h2 id="安装ruby环境"><a href="#安装ruby环境" class="headerlink" title="安装ruby环境"></a>安装ruby环境</h2><p>本身redis集群的安装是很麻烦了，通过ruby工具，可以非常方便的将一系列命令打包为一个脚本！</p>
<p><strong>依次执行</strong>在安装光盘下的Package目录(/media/CentOS_6.8_Final/Packages)下的rpm包：</p>
<table>
<thead>
<tr>
<th>rpm -ivh compat-readline5-5.2-17.1.el6.x86_64.rpm</th>
</tr>
</thead>
<tbody><tr>
<td>rpm -ivh ruby-libs-1.8.7.374-4.el6_6.x86_64.rpm</td>
</tr>
<tr>
<td>rpm -ivh ruby-1.8.7.374-4.el6_6.x86_64.rpm</td>
</tr>
<tr>
<td>rpm -ivh ruby-irb-1.8.7.374-4.el6_6.x86_64.rpm</td>
</tr>
<tr>
<td>rpm -ivh ruby-rdoc-1.8.7.374-4.el6_6.x86_64.rpm</td>
</tr>
<tr>
<td>rpm -ivh rubygems-1.3.7-5.el6.noarch.rpm</td>
</tr>
</tbody></table>
<p>也可以在联网状态下，执行yum安装，执行</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum -y install ruby</span><br></pre></td></tr></table></figure>

<p>之后安装rubygem，rubygem是ruby的包管理框架。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum -y install rubygems</span><br></pre></td></tr></table></figure>

<h2 id="安装redis-gem"><a href="#安装redis-gem" class="headerlink" title="安装redis gem"></a>安装redis gem</h2><p>redis-3.2.0.gem是一个通过ruby操作redis的插件！</p>
<p>拷贝redis-3.2.0.gem到/opt/software目录下，在/opt/software目录下执行 </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">gem install --local redis-3.2.0.gem</span><br></pre></td></tr></table></figure>

<h2 id="制作6个redis配置文件"><a href="#制作6个redis配置文件" class="headerlink" title="制作6个redis配置文件"></a>制作6个redis配置文件</h2><p>端口号分别是：6379,6380,6381,6382,6383,6384</p>
<p>注意：每个配置文件中需要指定</p>
<figure class="highlight stata"><table><tr><td class="code"><pre><span class="line">daemonize yes: 服务在后台运行port：端口号pidfile:pid保存文件logfile：日志文件(如果没有指定的话，就不需要)dump.rdb: RDB备份文件的名称appendonly 关掉，或者是更改appendonly文件的名称。	<span class="keyword">cluster</span>-enabled yes    打开集群模式<span class="keyword">cluster</span>-config-<span class="keyword">file</span>  nodes-6379.<span class="keyword">conf</span>  设定节点配置文件名<span class="keyword">cluster</span>-node-timeout 15000   设定节点失联时间，超过该时间（毫秒），集群自动进行主从切换。</span><br></pre></td></tr></table></figure>

<p>样例</p>
<figure class="highlight gradle"><table><tr><td class="code"><pre><span class="line"><span class="keyword">include</span> <span class="regexp">/opt/m</span>odule<span class="regexp">/my_redis/</span>base.confpidfile <span class="string">&quot;/opt/module/my_redis/redis-6379.pid&quot;</span>port <span class="number">6379</span>dbfilename <span class="string">&quot;dump_6379.rdb&quot;</span>cluster-enabled yescluster-config-<span class="keyword">file</span> nodes-<span class="number">6379</span>.confcluster-node-timeout <span class="number">15000</span></span><br></pre></td></tr></table></figure>

<p>注意在创建集群的时候，初始化的时候，把所有节点的dump文件全部删掉。</p>
<h2 id="开启集群"><a href="#开启集群" class="headerlink" title="开启集群"></a>开启集群</h2><p>1、首先依次启动6个节点，启动后，会在当前文件夹生成nodes-xxxx.conf文件</p>
<p>2、配置集群</p>
<p>执行命令：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/opt/module/redis-3.2.5/src/redis-trib.rb create --replicas 1 10.211.55.10:6379 10.211.55.10:6380 10.211.55.10:6381 10.211.55.10:6382 10.211.55.10:6383 10.211.55.10:6384</span><br></pre></td></tr></table></figure>

<p>注意，此处不要用127.0.0.1和域名，请用真实IP地址！</p>
<p>3、之后登录到客户端，通过 cluster nodes 命令查看集群信息</p>
<p>4、6个节点，为什么是三主三从？</p>
<p>命令create,代表创建一个集群。参数–replicas 1 表示我们希望为集群中的每个主节点创建一个从节点。一个集群至少要有<strong>三个主节点</strong>，分配原则尽量保证每个主数据库运行在不同的IP地址，每个从库和主库不在一个IP地址上。</p>
<h2 id="slot"><a href="#slot" class="headerlink" title="slot"></a>slot</h2><p>进入集群后，如果我们，直接写入数据，可能会看到报错信息</p>
<p>这是因为，集群中多了slot(插槽)的设计。一个 Redis 集群包含 16384 个插槽（hash slot）， 数据库中的每个键都属于这 16384 个插槽的其中一个， 集群使用公式 CRC16(key) % 16384 来计算键 key 属于哪个槽， 其中 CRC16(key) 语句用于计算键 key 的 CRC16 校验和 。</p>
<p>集群中的每个节点负责处理一部分插槽。 举个例子， 如果一个集群可以有主节点， 其中：</p>
<p>​    节点 A 负责处理 0 号至 5500 号插槽。</p>
<p>​    节点 B 负责处理 5501 号至 11000 号插槽。</p>
<p>​    节点 C 负责处理 11001 号至 16383 号插槽。</p>
<h2 id="集群中写入数据"><a href="#集群中写入数据" class="headerlink" title="集群中写入数据"></a>集群中写入数据</h2><h3 id="客户端重定向"><a href="#客户端重定向" class="headerlink" title="客户端重定向"></a>客户端重定向</h3><p>1、在redis-cli每次录入、查询键值，redis都会计算出该key应该送往的插槽，如果不是该客户端对应服务器插槽，redis会报错，并告知应前往的redis实例地址和端口。</p>
<p>2、redis-cli客户端提供了 –c 参数实现自动重定向。如 redis-cli -c –p 6379 登入后，再录入、查询键值对可以自动重定向。</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">redis</span>-cli -h linux<span class="number">1</span> –p <span class="number">6379</span> -c</span><br></pre></td></tr></table></figure>

<p>3、每个slot可以存储一批键值对。</p>
<h3 id="如何多键操作"><a href="#如何多键操作" class="headerlink" title="如何多键操作"></a>如何多键操作</h3><p>采用哈希算法后，会自动地分配slot，而不在一个slot下的键值，是不能使用mget，mset等多键操作。</p>
<p>如果有需求，需要将一批业务数据一起插入呢？</p>
<p>解决：可以通过{}来定义组的概念，从而使key中{}内相同内容的键值对放到一个slot中去。</p>
<figure class="highlight puppet"><table><tr><td class="code"><pre><span class="line"><span class="keyword">mset</span> &#123;a&#125;<span class="keyword">k1</span> <span class="keyword">v1</span> &#123;a&#125;<span class="keyword">k2</span> <span class="keyword">v2</span> &#123;a&#125;<span class="keyword">k3</span> <span class="keyword">v3mget</span> &#123;a&#125;<span class="keyword">k1</span> <span class="keyword">v1</span> &#123;a&#125;<span class="keyword">k2</span> <span class="keyword">v2</span> &#123;a&#125;<span class="keyword">k3</span> <span class="keyword">v3</span></span><br></pre></td></tr></table></figure>

<h2 id="集群中读取数据"><a href="#集群中读取数据" class="headerlink" title="集群中读取数据"></a>集群中读取数据</h2><p>CLUSTER KEYSLOT <key> 计算键 key 应该被放置在哪个槽上</key></p>
<p>CLUSTER COUNTKEYSINSLOT <slot> 返回槽 slot 目前包含的键值对数量</slot></p>
<p>CLUSTER GETKEYSINSLOT <slot> <count> 返回 count 个 slot 槽中的键。</count></slot></p>
<h2 id="集群中故障恢复"><a href="#集群中故障恢复" class="headerlink" title="集群中故障恢复"></a>集群中故障恢复</h2><p>问题1：如果主节点下线？从节点能否自动升为主节点？</p>
<p>答：主节点下线，从节点自动升为主节点。</p>
<p>问题2：主节点恢复后，主从关系会如何？</p>
<p>主节点恢复后，主节点变为从节点！</p>
<p>问题3：如果所有某一段插槽的主从节点都宕掉，redis服务是否还能继续?</p>
<p>答：服务是否继续，可以通过redis.conf中的cluster-require-full-coverage参数(默认关闭)进行控制。</p>
<p>主从都宕掉，意味着有一片数据，会变成真空，没法再访问了！</p>
<p>如果无法访问的数据，是连续的业务数据，我们需要停止集群，避免缺少此部分数据，造成整个业务的异常。此时可以通过配置cluster-require-full-coverage为yes.</p>
<p>如果无法访问的数据，是相对独立的，对于其他业务的访问，并不影响，那么可以继续开启集群体提供服务。此时，可以配置cluster-require-full-coverage为no。</p>
<h2 id="集群的优缺点"><a href="#集群的优缺点" class="headerlink" title="集群的优缺点"></a>集群的优缺点</h2><p>优点：</p>
<p>实现扩容</p>
<p>分摊压力</p>
<p>无中心配置相对简单</p>
<p>缺点：</p>
<p>多键操作是不被支持的</p>
<p>多键的Redis事务是不被支持的。lua脚本不被支持。</p>
<p>由于集群方案出现较晚，很多公司已经采用了其他的集群方案，而代理或者客户端分片的方案想要迁移至redis cluster，需要整体迁移而不是逐步过渡，复杂度较大。</p>
<h1 id="Redis-Java客户端操作"><a href="#Redis-Java客户端操作" class="headerlink" title="Redis Java客户端操作"></a>Redis Java客户端操作</h1><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/redis.clients/jedis --&gt;</span><span class="tag">&lt;<span class="name">dependency</span>&gt;</span>    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>redis.clients<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span>    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jedis<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span>    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.3.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.utilimport redis.clients.jedis.<span class="type">Jedisimport</span> scala.collection.<span class="type">JavaConversions</span>._<span class="comment">/** * Author vincent * Date 2019/7/13 11:28   */</span>   <span class="class"><span class="keyword">object</span> <span class="title">Demo1</span> </span>&#123;   <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;       <span class="comment">// 1. 创建一个redis的客户端       val client = new Jedis(&quot;linux1&quot;, 8000)       // 2. 使用客户端进行各种操作       //        client.set(&quot;k1&quot;, &quot;v1&quot;)       //        client.set(&quot;name&quot;, &quot;李四&quot;)       //        client.lpush(&quot;list1&quot;, &quot;a&quot;, &quot;b&quot;, &quot;c&quot;)       //        val data: util.List[String] = client.lrange(&quot;list1&quot;, 0, -1)       //        import scala.collection.JavaConversions._       //        import scala.collection.JavaConverters._       //        for (ele &lt;- data.asScala) &#123;       //            println(ele)       //        &#125;   //        client.sadd(&quot;set1&quot;, &quot;a&quot;, &quot;b&quot;, &quot;c&quot;)   //        val set: util.Set[String] = client.smembers(&quot;set1&quot;)       /*for(e &lt;- set)&#123;           println(e)       &#125;*/       /*val it: util.Iterator[String] = set.iterator()       while(it.hasNext)&#123;           println(it.next())       &#125;*/   //        client.hset(&quot;h1&quot;, &quot;k1&quot;, &quot;v1&quot;)              /*val map = Map(&quot;k1&quot;-&gt; &quot;v1&quot;, &quot;k2&quot; -&gt; &quot;v2&quot;)       client.hmset(&quot;s2&quot;, map)*/              /*val map: util.Map[String, String] = client.hgetAll(&quot;s2&quot;)       for((k, v) &lt;- map)&#123;           println(v)       &#125;*/              // 3. 关闭客户端       client.close()   &#125;   &#125;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.redisimport redis.clients.jedis.&#123;<span class="type">HostAndPort</span>, <span class="type">Jedis</span>, <span class="type">JedisCluster</span>&#125;<span class="comment">/** * Author vincent * Date 2019/7/13 11:28   */</span>   <span class="class"><span class="keyword">object</span> <span class="title">Demo2</span> </span>&#123;   <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;       <span class="keyword">val</span> hosts = <span class="keyword">new</span> java.util.<span class="type">HashSet</span>[<span class="type">HostAndPort</span>]()       hosts.add(<span class="keyword">new</span> <span class="type">HostAndPort</span>(<span class="string">&quot;linux1&quot;</span>, <span class="number">6379</span>))       hosts.add(<span class="keyword">new</span> <span class="type">HostAndPort</span>(<span class="string">&quot;linux2&quot;</span>, <span class="number">6380</span>))            <span class="keyword">val</span> cluster = <span class="keyword">new</span> <span class="type">JedisCluster</span>(hosts)              cluster.set(<span class="string">&quot;c2&quot;</span>, <span class="string">&quot;v1&quot;</span>)              cluster.close()   &#125;   &#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Nosql</category>
      </categories>
  </entry>
  <entry>
    <title>Maven</title>
    <url>/Maven/</url>
    <content><![CDATA[<p><a href="https://maven.apache.org/">https://maven.apache.org/</a></p>
<h1 id="功能"><a href="#功能" class="headerlink" title="功能"></a>功能</h1><p>1、项目的自动构建：项目代码的清理、编译、测试、报告、打包、安装、部署等</p>
<p>①清理：删除以前的编译结果，为重新编译做好准备。</p>
<p>②编译：将Java源程序编译为字节码文件。</p>
<p>③测试：针对项目中的关键点进行测试，确保项目在迭代开发过程中关键点的正确性。</p>
<p>④报告：在每一次测试后以标准的格式记录和展示测试结果。</p>
<p>⑤打包：将一个包含诸多文件的工程封装为一个压缩文件用于安装或部署。Java 工程对应jar 包，Web工程对应war包。</p>
<p>⑥安装：在Maven环境下特指将打包的结果——jar包或war包安装到本地仓库中。</p>
<p>⑦部署：将打包的结果部署到远程仓库或将war包部署到服务器上运行</p>
<p>2、管理依赖：项目中常见的其他资源，常见的是jar</p>
<h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><p>①POM</p>
<p>②约定的目录结构</p>
<p>③坐标</p>
<p>④依赖管理</p>
<p>⑤仓库管理</p>
<p>⑥生命周期</p>
<p>⑦插件和目标</p>
<p>⑧继承</p>
<p>⑨聚合</p>
<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>1、确保安装了java环境:maven本身就是java写的，所以要求必须安装JDK。查看java环境变量：echo $JAVA_HOME</p>
<p>2、下载并解压maven安装程序：<a href="http://maven.apache.org/download.cgi">http://maven.apache.org/download.cgi</a></p>
<p>3、配置Maven的环境变量：MAVEN_HOME 或者M2_HOME path=$MAVEN_HOME/bin或者$M2_HOME/bin</p>
<p>4、验证是否安装成功:mvn –v</p>
]]></content>
      <categories>
        <category>工具</category>
      </categories>
  </entry>
  <entry>
    <title>Zookeeper</title>
    <url>/Zookeeper/</url>
    <content><![CDATA[<h1 id="Zookeeper入门"><a href="#Zookeeper入门" class="headerlink" title="Zookeeper入门"></a>Zookeeper入门</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Zookeeper是一个开源的分布式的，为分布式应用提供<strong>协调服务</strong>的Apache项目。</p>
<p>Zookeeper从设计模式角度来理解，是一个基于<strong>观察者模式</strong>设计的分布式服务管理框架，它负责<strong>存储和管理大家都关心的数据</strong>，然后接受观察者的注册，一旦这些数据的状态发生了变化，Zookeeper就负责通知已经在Zookeeper上注册的那些观察者做出相应的反应.</p>
<p>Zookeeper = 文件系统 + 通知机制</p>
<h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><p><img src="/Zookeeper/130.png" alt="130"></p>
<h2 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h2><p><img src="/Zookeeper/131.png" alt="131"></p>
<h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><p>提供的服务包括：统一命名服务、统一配置管理、统一集群管理、服务器节点动态上下线、软负载均衡等。</p>
<p>统一命名服务</p>
<p><img src="/Zookeeper/132.png" alt="132"></p>
<p>统一配置管理</p>
<p><img src="/Zookeeper/133.png" alt="133"></p>
<p>统一集群管理</p>
<p><img src="/Zookeeper/134.png" alt="134"></p>
<p>服务器节点动态上下线</p>
<p><img src="/Zookeeper/135.png" alt="135"></p>
<p>软负载均衡</p>
<p><img src="/Zookeeper/136.png" alt="136"></p>
<h1 id="Zookeeper安装部署"><a href="#Zookeeper安装部署" class="headerlink" title="Zookeeper安装部署"></a>Zookeeper安装部署</h1><h2 id="Zookeeper安装"><a href="#Zookeeper安装" class="headerlink" title="Zookeeper安装"></a>Zookeeper安装</h2><p>1）安装前准备</p>
<p>（1）安装Jdk</p>
<p>（2）拷贝Zookeeper安装包到Linux系统下</p>
<p>（3）解压到指定目录并改名</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 software]$ tar -zxvf apache-zookeeper-3.6.1-bin.tar.gz -C /opt/module/</span><br><span class="line">[vincent@linux1 software]$ mv apache-zookeeper-3.6.1-bin zookeeper</span><br></pre></td></tr></table></figure>

<p>2）配置修改</p>
<p>（1）将/opt/module/zookeeper/conf这个路径下的zoo_sample.cfg修改为zoo.cfg；</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 conf]$ cd /opt/module/zookeeper/conf</span><br><span class="line">[vincent@linux1 conf]$ mv zoo_sample.cfg zoo.cfg</span><br></pre></td></tr></table></figure>

<p>​    （2）打开zoo.cfg文件，修改dataDir路径：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 zookeeper-3.5.7]$ vim zoo.cfg</span><br></pre></td></tr></table></figure>

<p>修改如下内容：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">dataDir=/opt/module/zookeeper/zkData</span><br></pre></td></tr></table></figure>

<p>​    （3）在/opt/module/zookeeper/这个目录上创建zkData文件夹</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 zookeeper]$ cd /opt/module/zookeeper</span><br><span class="line">[vincent@linux1 zookeeper]$ mkdir zkData</span><br></pre></td></tr></table></figure>

<p>3）操作Zookeeper</p>
<p>（1）启动Zookeeper</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 zookeeper]$ bin/zkServer.sh start</span><br></pre></td></tr></table></figure>

<p>（2）查看进程是否启动</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 zookeeper]$ jps</span><br><span class="line">4020 Jps</span><br><span class="line">4001 QuorumPeerMain</span><br></pre></td></tr></table></figure>

<p>（3）查看状态：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 zookeeper]$ bin/zkServer.sh status</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper/bin/../conf/zoo.cfg</span><br><span class="line">Client port found: 2181. Client address: localhost.</span><br><span class="line">Mode: standalone</span><br></pre></td></tr></table></figure>

<p>（4）启动客户端：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 zookeeper]$ bin/zkCli.sh</span><br></pre></td></tr></table></figure>

<p>（5）退出客户端：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 0] quit</span><br></pre></td></tr></table></figure>

<p>（6）停止Zookeeper</p>
<h2 id="配置参数解读"><a href="#配置参数解读" class="headerlink" title="配置参数解读"></a>配置参数解读</h2><p>Zookeeper中的配置文件zoo.cfg中参数含义解读如下：</p>
<p>1）tickTime =2000：通信心跳数，Zookeeper服务器*与客户端心跳时间，单位毫秒</p>
<p>Zookeeper使用的基本时间，服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个tickTime时间就会发送一个心跳，时间单位为毫秒。</p>
<p>它用于心跳机制，并且设置最小的session超时时间为两倍心跳时间。(session的最小超时时间是2*tickTime)</p>
<p>2）initLimit =10：LF初始通信时限</p>
<p>集群中的Follower跟随者服务器与Leader领导者服务器之间初始连接时能容忍的最多心跳数（tickTime的数量），用它来限定集群中的Zookeeper服务器连接到Leader的时限。</p>
<p>3）syncLimit =5：LF同步通信时限</p>
<p>集群中Leader与Follower之间的最大响应时间单位，假如响应超过syncLimit * tickTime，Leader认为Follwer死掉，从服务器列表中删除Follwer。</p>
<p>4）dataDir：数据文件目录+数据持久化路径</p>
<p>主要用于保存Zookeeper中的数据。</p>
<p>5）clientPort =2181：客户端连接端口</p>
<p>监听客户端连接的端口。</p>
<h2 id="Zookeeper的四字命令"><a href="#Zookeeper的四字命令" class="headerlink" title="Zookeeper的四字命令"></a>Zookeeper的四字命令</h2><p>Zookeeper支持某些特定的四字命令(The Four Letter Words) 与其进行交互，它们大多是查询命令，用来获取Zookeeper服务的当前状态及相关信息，用户在客户端可以通过telnet</p>
<p>或nc 向Zookeeper提交相应的命令。</p>
<p>​    需要在Zookeeper的配置文件中加入如下配置:</p>
<figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="attr">4lw.commands.whitelist</span>=*</span><br></pre></td></tr></table></figure>

<p>​    Zookeeper常用四字命令主要如下:</p>
<table>
<thead>
<tr>
<th>ruok</th>
<th>测试服务是否处于正确状态，如果确实如此，那么服务返回 imok ,否则不做任何响应。</th>
</tr>
</thead>
<tbody><tr>
<td>conf</td>
<td>3.3.0版本引入的，打印出服务相关配置的详细信息</td>
</tr>
<tr>
<td>cons</td>
<td>列出所有连接到这台服务器的客户端全部会话详细信息。包括 接收/发送的包数量，会话id，操作延迟、最后的操作执行等等信息</td>
</tr>
<tr>
<td>crst</td>
<td>重置所有连接的连接和会话统计信息</td>
</tr>
<tr>
<td>dump</td>
<td>列出那些比较重要的会话和临时节点。这个命令只能在leader节点上有用</td>
</tr>
<tr>
<td>envi</td>
<td>打印出服务环境的详细信息</td>
</tr>
</tbody></table>
<h1 id="Zookeeper内部原理"><a href="#Zookeeper内部原理" class="headerlink" title="Zookeeper内部原理"></a>Zookeeper内部原理</h1><h2 id="节点类型"><a href="#节点类型" class="headerlink" title="节点类型"></a>节点类型</h2><p><img src="/Zookeeper/137.png" alt="137"></p>
<h2 id="Stat结构体"><a href="#Stat结构体" class="headerlink" title="Stat结构体"></a>Stat结构体</h2><p>（1）czxid-创建节点的事务zxid</p>
<p>每次修改ZooKeeper状态都会收到一个zxid形式的时间戳，也就是ZooKeeper事务ID。</p>
<p>事务ID是ZooKeeper中所有修改总的次序。每个修改都有唯一的zxid，如果zxid1小于zxid2，那么zxid1在zxid2之前发生。</p>
<p>（2）ctime - znode被创建的毫秒数(从1970年开始)</p>
<p>（3）mzxid - znode最后更新的事务zxid</p>
<p>（4）mtime - znode最后修改的毫秒数(从1970年开始)</p>
<p>（5）pZxid-znode最后更新的子节点zxid</p>
<p>（6）cversion - znode子节点变化号，znode子节点修改次数</p>
<p>（7）dataversion - znode数据变化号</p>
<p>（8）aclVersion - znode访问控制列表的变化号</p>
<p>（9）ephemeralOwner- 如果是临时节点，这个是znode拥有者的session id。如果不是临时节点则是0。</p>
<p>（10）dataLength- znode的数据长度</p>
<p>（11）numChildren - znode子节点数量</p>
<h2 id="监听器原理"><a href="#监听器原理" class="headerlink" title="监听器原理"></a>监听器原理</h2><p><img src="/Zookeeper/138.png" alt="138"></p>
<h2 id="选举机制"><a href="#选举机制" class="headerlink" title="选举机制"></a>选举机制</h2><ol>
<li>半数机制：集群中半数以上机器存活，集群可用。所以Zookeeper适合安装奇数台服务器。</li>
<li>Zookeeper虽然在配置文件中并没有指定Master和Slave。但是，Zookeeper工作时，是有一个节点为Leader，其他则为Follower，Leader是通过内部的选举机制临时产生的。</li>
<li>以一个简单的例子来说明整个选举的过程。</li>
</ol>
<p>假设有五台服务器组成的Zookeeper集群，它们的id从1-5，同时它们都是最新启动的，也就是没有历史数据，在存放数据量这一点上，都是一样的。假设这些服务器依序启动，来看看会发生什么。</p>
<p><img src="/Zookeeper/139.png" alt="139"></p>
<p>（1）服务器1启动，发起一次选举。服务器1投自己一票。此时服务器1票数一票，不够半数以上（3票），选举无法完成，服务器1状态保持为LOOKING；</p>
<p>（2）服务器2启动，再发起一次选举。服务器1和2分别投自己一票并交换选票信息：此时服务器1发现服务器2的ID比自己目前投票推举的（服务器1）大，更改选票为推举服务器2。此时服务器1票数0票，服务器2票数2票，没有半数以上结果，选举无法完成，服务器1，2状态保持LOOKING</p>
<p>（3）服务器3启动，发起一次选举。此时服务器1和2都会更改选票为服务器3。此次投票结果：服务器1为0票，服务器2为0票，服务器3为3票。此时服务器3的票数已经超过半数，服务器3当选Leader。服务器1，2更改状态为FOLLOWING，服务器3更改状态为LEADING；</p>
<p>（4）服务器4启动，发起一次选举。此时服务器1，2，3已经不是LOOKING状态，不会更改选票信息。交换选票信息结果：服务器3为3票，服务器4为1票。此时服务器4服从多数，更改选票信息为服务器3，并更改状态为FOLLOWING；</p>
<p>（5）服务器5启动，同4一样当小弟。</p>
<p>如果节点上有数据且zxid不一样，选举zxid最大的节点做leader。</p>
<h2 id="写数据流程"><a href="#写数据流程" class="headerlink" title="写数据流程"></a>写数据流程</h2><p><img src="/Zookeeper/140.png" alt="140"></p>
<h1 id="Zookeeper实战"><a href="#Zookeeper实战" class="headerlink" title="Zookeeper实战"></a>Zookeeper实战</h1><h2 id="分布式安装部署"><a href="#分布式安装部署" class="headerlink" title="分布式安装部署"></a>分布式安装部署</h2><p>1）集群规划</p>
<p>在linux1、linux2和linux3三个节点上部署Zookeeper。</p>
<p>2）解压安装</p>
<p>（1）在linux1解压Zookeeper安装包到/opt/module/目录下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 software]$ tar -zxvf apache-zookeeper-3.6.1-bin.tar.gz -C /opt/module/</span><br><span class="line">[vincent@linux1 software]$ mv apache-zookeeper-3.6.1-bin zookeeper</span><br></pre></td></tr></table></figure>

<p>3）配置服务器编号</p>
<p>（1）在/opt/module/zookeeper/这个目录下创建zkData</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 zookeeper]$ mkdir -p zkData</span><br></pre></td></tr></table></figure>

<p>（2）在/opt/module/zookeeper/zkData目录下创建一个myid的文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 zkData]$ touch myid</span><br></pre></td></tr></table></figure>

<p>添加myid文件，注意一定要在linux里面创建，在notepad++里面很可能乱码</p>
<p>（3）编辑myid文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 zkData]$ vim myid</span><br></pre></td></tr></table></figure>

<p>在文件中添加与server对应的编号：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">1</span><br></pre></td></tr></table></figure>

<p>（4）拷贝配置好的zookeeper到其他机器上</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 module ]$ xsync zookeeper</span><br></pre></td></tr></table></figure>

<p>并分别在linux2、linux3上修改myid文件中内容为2、3</p>
<p>4）配置zoo.cfg文件</p>
<p>（1）重命名/opt/module/zookeeper/conf这个目录下的zoo_sample.cfg为zoo.cfg</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 conf]$ mv zoo_sample.cfg zoo.cfg</span><br></pre></td></tr></table></figure>

<p>（2）打开zoo.cfg文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 conf]$ vim zoo.cfg</span><br></pre></td></tr></table></figure>

<p>修改数据存储路径配置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">dataDir=/opt/module/zookeeper/zkData</span><br></pre></td></tr></table></figure>

<p>增加如下配置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">######################cluster##########################</span></span></span><br><span class="line">server.1=linux1:2888:3888</span><br><span class="line">server.2=linux2:2888:3888</span><br><span class="line">server.3=linux3:2888:3888</span><br></pre></td></tr></table></figure>

<p>（3）同步zoo.cfg配置文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 conf]$ xsync zoo.cfg</span><br></pre></td></tr></table></figure>

<p>（4）配置参数解读</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">server.A=B:C:D</span><br></pre></td></tr></table></figure>

<p>A是一个数字，表示这个是第几号服务器；</p>
<p>集群模式下配置一个文件myid，这个文件在dataDir目录下，这个文件里面有一个数据就是A的值，Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server。</p>
<p>B是这个服务器的地址；</p>
<p>C是这个服务器Follower与集群中的Leader服务器交换信息的端口；</p>
<p>D是万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。</p>
<p>5）集群操作</p>
<p>（1）分别启动Zookeeper</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 zookeeper]$ bin/zkServer.sh start</span><br><span class="line">[vincent@linux2 zookeeper]$ bin/zkServer.sh start</span><br><span class="line">[vincent@linux3 zookeeper]$ bin/zkServer.sh start</span><br></pre></td></tr></table></figure>

<p>（2）查看状态</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 zookeeper]$ bin/zkServer.sh status</span><br><span class="line">[atguigu@linux2 zookeeper]$ bin/zkServer.sh status</span><br><span class="line">[atguigu@linux3 zookeeper]$ bin/zkServer.sh status</span><br></pre></td></tr></table></figure>

<p>（3）群起脚本zk</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line"> then</span><br><span class="line">        echo &quot;No Args Input!!!!&quot;</span><br><span class="line">        exit;</span><br><span class="line">fi</span><br><span class="line">for i in linux1 linux2 linux3</span><br><span class="line">do</span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)</span><br><span class="line">        echo &quot;=============== start $i zookeeper ==============&quot;</span><br><span class="line">        ssh $i /opt/module/zookeeper/bin/zkServer.sh start</span><br><span class="line">;;</span><br><span class="line">&quot;status&quot;)</span><br><span class="line">        echo &quot;=============== status $i zookeeper =============&quot;</span><br><span class="line">        ssh $i /opt/module/zookeeper/bin/zkServer.sh status</span><br><span class="line">;;</span><br><span class="line">&quot;stop&quot;)</span><br><span class="line">        echo &quot;=============== stop $i zookeeper ===============&quot;</span><br><span class="line">        ssh $i /opt/module/zookeeper/bin/zkServer.sh stop</span><br><span class="line">;;</span><br><span class="line">*)</span><br><span class="line">        echo &quot;Input Args Error!!!!&quot;</span><br><span class="line">;;</span><br><span class="line">esac</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<h2 id="客户端命令行操作"><a href="#客户端命令行操作" class="headerlink" title="客户端命令行操作"></a>客户端命令行操作</h2><table>
<thead>
<tr>
<th>命令基本语法</th>
<th>功能描述</th>
</tr>
</thead>
<tbody><tr>
<td>help</td>
<td>显示所有操作命令</td>
</tr>
<tr>
<td>ls path</td>
<td>使用 ls 命令来查看当前znode的子节点 [可监听]  -w 监听子节点变化  -s  附加次级信息</td>
</tr>
<tr>
<td>create</td>
<td>普通创建  -s 含有序列  -e 临时（重启或者超时消失）</td>
</tr>
<tr>
<td>get path</td>
<td>获得节点的值 [可监听]  -w 监听节点内容变化  -s  附加次级信息</td>
</tr>
<tr>
<td>set</td>
<td>设置节点的具体值</td>
</tr>
<tr>
<td>stat</td>
<td>查看节点状态</td>
</tr>
<tr>
<td>delete</td>
<td>删除节点</td>
</tr>
<tr>
<td>deleteall</td>
<td>递归删除节点</td>
</tr>
</tbody></table>
<p>1）启动客户端</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[vincent@linux1 zookeeper]$ bin/zkCli.sh</span><br></pre></td></tr></table></figure>

<p>2）显示所有操作命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 0] help</span><br></pre></td></tr></table></figure>

<p>3）查看当前znode中所包含的内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 1] ls /</span><br><span class="line"></span><br><span class="line">[zookeeper]</span><br></pre></td></tr></table></figure>

<p>4）查看当前节点详细数据</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 1] ls -s /</span><br><span class="line"></span><br><span class="line">&#x27;ls2&#x27; has been deprecated. Please use &#x27;ls [-s] path&#x27; instead.</span><br><span class="line">[zookeeper]</span><br><span class="line">cZxid = 0x0</span><br><span class="line">ctime = Thu Jan 01 08:00:00 CST 1970</span><br><span class="line">mZxid = 0x0</span><br><span class="line">mtime = Thu Jan 01 08:00:00 CST 1970</span><br><span class="line">pZxid = 0x0</span><br><span class="line">cversion = -1</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 0</span><br><span class="line">numChildren = 1</span><br></pre></td></tr></table></figure>

<p>5）分别创建2个普通节点</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 3] create /sanguo &quot;jinlian&quot;</span><br><span class="line">Created /sanguo</span><br><span class="line">[zk: localhost:2181(CONNECTED) 4] create /sanguo/shuguo &quot;liubei&quot;</span><br><span class="line">Created /sanguo/shuguo</span><br></pre></td></tr></table></figure>

<p>6）获得节点的值</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 5] get /sanguo</span><br><span class="line">jinlian</span><br><span class="line">cZxid = 0x100000003</span><br><span class="line">ctime = Wed Aug 29 00:03:23 CST 2018</span><br><span class="line">mZxid = 0x100000003</span><br><span class="line">mtime = Wed Aug 29 00:03:23 CST 2018</span><br><span class="line">pZxid = 0x100000004</span><br><span class="line">cversion = 1</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 7</span><br><span class="line">numChildren = 1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[zk: localhost:2181(CONNECTED) 6] get /sanguo/shuguo</span><br><span class="line">liubei</span><br><span class="line">cZxid = 0x100000004</span><br><span class="line">ctime = Wed Aug 29 00:04:35 CST 2018</span><br><span class="line">mZxid = 0x100000004</span><br><span class="line">mtime = Wed Aug 29 00:04:35 CST 2018</span><br><span class="line">pZxid = 0x100000004</span><br><span class="line">cversion = 0</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 6</span><br><span class="line">numChildren = 0</span><br></pre></td></tr></table></figure>

<p>7）创建短暂节点</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 7] create -e /sanguo/wuguo &quot;zhouyu&quot;</span><br><span class="line">Created /sanguo/wuguo</span><br></pre></td></tr></table></figure>

<p>8）创建带序号的节点</p>
<p>（1）先创建一个普通的根节点/sanguo/weiguo</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 1] create /sanguo/weiguo &quot;caocao&quot;</span><br><span class="line">Created /sanguo/weiguo</span><br></pre></td></tr></table></figure>

<p>（2）创建带序号的节点</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 2] create -s /sanguo/weiguo/xiaoqiao &quot;jinlian&quot;</span><br><span class="line">Created /sanguo/weiguo/xiaoqiao0000000000</span><br><span class="line"></span><br><span class="line">[zk: localhost:2181(CONNECTED) 3] create -s /sanguo/weiguo/daqiao &quot;jinlian&quot;</span><br><span class="line">Created /sanguo/weiguo/daqiao0000000001</span><br><span class="line"></span><br><span class="line">[zk: localhost:2181(CONNECTED) 4] create -s /sanguo/weiguo/diaocan &quot;jinlian&quot;</span><br><span class="line">Created /sanguo/weiguo/diaocan0000000002</span><br></pre></td></tr></table></figure>

<p>如果原来没有序号节点，序号从0开始依次递增。如果原节点下已有2个节点，则再排序时从2开始，以此类推。</p>
<p>9）修改节点数据值</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 6] set /sanguo/weiguo &quot;simayi&quot;</span><br></pre></td></tr></table></figure>

<p>10）节点的值变化监听</p>
<p>（1）在hadoop104主机上注册监听/sanguo节点数据变化</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 26] [zk: localhost:2181(CONNECTED) 8] get -w /sanguo </span><br></pre></td></tr></table></figure>

<p>（2）在hadoop103主机上修改/sanguo节点的数据</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 1] set /sanguo &quot;xisi&quot;</span><br></pre></td></tr></table></figure>

<p>​    （3）观察hadoop104主机收到数据变化的监听</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">WATCHER::</span><br><span class="line">WatchedEvent state:SyncConnected type:NodeDataChanged path:/sanguo</span><br></pre></td></tr></table></figure>

<p>11）节点的子节点变化监听（路径变化）</p>
<p>（1）在hadoop104主机上注册监听/sanguo节点的子节点变化</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 1] ls -w /sanguo</span><br><span class="line">[aa0000000001, server101]</span><br></pre></td></tr></table></figure>

<p>​    （2）在hadoop103主机/sanguo节点上创建子节点</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 2] create /sanguo/jin &quot;simayi&quot;</span><br><span class="line">Created /sanguo/jin</span><br></pre></td></tr></table></figure>

<p>​    （3）观察hadoop104主机收到子节点变化的监听</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">WATCHER::</span><br><span class="line">WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/sanguo</span><br></pre></td></tr></table></figure>

<p>12）删除节点</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 4] delete /sanguo/jin</span><br></pre></td></tr></table></figure>

<p>13）递归删除节点</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 15] deleteall /sanguo/shuguo</span><br></pre></td></tr></table></figure>

<p>14）查看节点状态</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 17] stat /sanguo</span><br><span class="line">cZxid = 0x100000003</span><br><span class="line">ctime = Wed Aug 29 00:03:23 CST 2018</span><br><span class="line">mZxid = 0x100000011</span><br><span class="line">mtime = Wed Aug 29 00:21:23 CST 2018</span><br><span class="line">pZxid = 0x100000014</span><br><span class="line">cversion = 9</span><br><span class="line">dataVersion = 1</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 4</span><br><span class="line">numChildren = 1</span><br></pre></td></tr></table></figure>

<h2 id="API应用"><a href="#API应用" class="headerlink" title="API应用"></a>API应用</h2><p>1.创建maven工程</p>
<p>2.修改pom文件</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">version</span>&gt;</span>RELEASE<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">		<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.logging.log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">version</span>&gt;</span>2.8.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">		<span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.zookeeper/zookeeper --&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.zookeeper<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>zookeeper<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">version</span>&gt;</span>3.5.7<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>3.拷贝log4j.properties文件到项目根目录</p>
<p>需要在项目的src/main/resources目录下，新建一个文件，命名为“log4j.properties”，在文件中填入：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">log4j.rootLogger=INFO, stdout  </span><br><span class="line">log4j.appender.stdout=org.apache.log4j.ConsoleAppender  </span><br><span class="line">log4j.appender.stdout.layout=org.apache.log4j.PatternLayout  </span><br><span class="line">log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n  </span><br><span class="line">log4j.appender.logfile=org.apache.log4j.FileAppender  </span><br><span class="line">log4j.appender.logfile.File=target/spring.log  </span><br><span class="line">log4j.appender.logfile.layout=org.apache.log4j.PatternLayout  </span><br><span class="line">log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n  </span><br></pre></td></tr></table></figure>

<p>4.创建ZooKeeper客户端</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//获取客户端连接对象</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> String connectString = <span class="string">&quot;linux1:2181,linux2:2181,linux3:2181&quot;</span>;</span><br><span class="line">	</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> sessionTimeout = <span class="number">10000</span>;</span><br><span class="line"><span class="keyword">private</span> ZooKeeper zkClient = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Before</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">	zkClient = <span class="keyword">new</span> ZooKeeper(connectString, sessionTimeout, <span class="keyword">new</span> Watcher() &#123;</span><br><span class="line">			<span class="comment">//回调方法，当watcher对象监听到感兴趣事件后，会调用prcess方法</span></span><br><span class="line">			<span class="comment">//WatchEvent：事件对象，封装了所发生的的事</span></span><br><span class="line">      <span class="meta">@Override</span></span><br><span class="line">			<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent event)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">				<span class="comment">// 收到事件通知后的回调函数（用户的业务逻辑）</span></span><br><span class="line">				System.out.println(event.getType() + <span class="string">&quot;--&quot;</span> + event.getPath());</span><br><span class="line"></span><br><span class="line">				<span class="comment">// 再次启动监听</span></span><br><span class="line">				<span class="keyword">try</span> &#123;</span><br><span class="line">					zkClient.getChildren(<span class="string">&quot;/&quot;</span>, <span class="keyword">true</span>);</span><br><span class="line">				&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">					e.printStackTrace();</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">   );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>5.创建子节点</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 创建子节点</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">create</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 参数1：要创建的节点的路径； 参数2：节点数据 ； 参数3：节点权限 ；参数4：节点的类型</span></span><br><span class="line">		String nodeCreated = zkClient.create(<span class="string">&quot;/vincent&quot;</span>, <span class="string">&quot;jinlian&quot;</span>.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>6.获取子节点并监听节点变化</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取子节点</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getChildren</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">		List&lt;String&gt; children = zkClient.getChildren(<span class="string">&quot;/&quot;</span>, <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">		<span class="keyword">for</span> (String child : children) &#123;</span><br><span class="line">			System.out.println(child);</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 延时阻塞</span></span><br><span class="line">  	<span class="comment">// 只管一次监听</span></span><br><span class="line">		Thread.sleep(Long.MAX_VALUE);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>7.判断Znode是否存在</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 判断znode是否存在</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">exist</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">	Stat stat = zkClient.exists(<span class="string">&quot;/eclipse&quot;</span>, <span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line">	System.out.println(stat == <span class="keyword">null</span> ? <span class="string">&quot;not exist&quot;</span> : <span class="string">&quot;exist&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="监听服务器节点动态上下线案例"><a href="#监听服务器节点动态上下线案例" class="headerlink" title="监听服务器节点动态上下线案例"></a>监听服务器节点动态上下线案例</h2><p>1）需求</p>
<p>某分布式系统中，主节点可以有多台，可以动态上下线，任意一台客户端都能实时感知到主节点服务器的上下线。</p>
<p>2）需求分析</p>
<p><img src="/Zookeeper/141.png" alt="141"></p>
<p>3）具体实现</p>
<p>0）先在集群上创建/servers节点</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 10] create /servers &quot;servers&quot;</span><br><span class="line">Created /servers</span><br></pre></td></tr></table></figure>

<p>1）服务器端向Zookeeper注册代码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.CreateMode;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.WatchedEvent;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.Watcher;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.ZooKeeper;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.ZooDefs.Ids;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DistributeServer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">static</span> String connectString = <span class="string">&quot;linux1:2181,linux2:2181,linux3:2181&quot;</span>;</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> sessionTimeout = <span class="number">10000</span>;</span><br><span class="line">	<span class="keyword">private</span> ZooKeeper zk = <span class="keyword">null</span>;</span><br><span class="line">	<span class="keyword">private</span> String parentNode = <span class="string">&quot;/servers&quot;</span>;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">// 创建到zk的客户端连接</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getConnect</span><span class="params">()</span> <span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line">		</span><br><span class="line">		zk = <span class="keyword">new</span> ZooKeeper(connectString, sessionTimeout, <span class="keyword">new</span> Watcher() &#123;</span><br><span class="line"></span><br><span class="line">			<span class="meta">@Override</span></span><br><span class="line">			<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent event)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">			&#125;</span><br><span class="line">		&#125;);</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">// 注册服务器</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">registServer</span><span class="params">(String hostname)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line"></span><br><span class="line">		String create = zk.create(parentNode + <span class="string">&quot;/server&quot;</span>, hostname.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL);</span><br><span class="line">		</span><br><span class="line">		System.out.println(hostname +<span class="string">&quot; is online &quot;</span>+ create);</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">// 业务功能</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">business</span><span class="params">(String hostname)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">		System.out.println(hostname+<span class="string">&quot; is working ...&quot;</span>);</span><br><span class="line">		</span><br><span class="line">		Thread.sleep(Long.MAX_VALUE);</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 1获取zk连接</span></span><br><span class="line">		DistributeServer server = <span class="keyword">new</span> DistributeServer();</span><br><span class="line">		server.getConnect();</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 2 利用zk连接注册服务器信息</span></span><br><span class="line">		server.registServer(args[<span class="number">0</span>]);</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 3 启动业务功能</span></span><br><span class="line">		server.business(args[<span class="number">0</span>]);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>2）客户端代码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.WatchedEvent;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.Watcher;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.ZooKeeper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DistributeClient</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">static</span> String connectString = <span class="string">&quot;hadoop102:2181,hadoop103:2181,hadoop104:2181&quot;</span>;</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> sessionTimeout = <span class="number">10000</span>;</span><br><span class="line">	<span class="keyword">private</span> ZooKeeper zk = <span class="keyword">null</span>;</span><br><span class="line">	<span class="keyword">private</span> String parentNode = <span class="string">&quot;/servers&quot;</span>;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 创建到zk的客户端连接</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getConnect</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">		zk = <span class="keyword">new</span> ZooKeeper(connectString, sessionTimeout, <span class="keyword">new</span> Watcher() &#123;</span><br><span class="line"></span><br><span class="line">			<span class="meta">@Override</span></span><br><span class="line">			<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent event)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">				<span class="comment">// 再次启动监听</span></span><br><span class="line">				<span class="keyword">try</span> &#123;</span><br><span class="line">					getServerList();</span><br><span class="line">				&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">					e.printStackTrace();</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 获取服务器列表信息</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getServerList</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// 1获取服务器子节点信息，并且对父节点进行监听</span></span><br><span class="line">		List&lt;String&gt; children = zk.getChildren(parentNode, <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2存储服务器信息列表</span></span><br><span class="line">		ArrayList&lt;String&gt; servers = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">		</span><br><span class="line">        <span class="comment">// 3遍历所有节点，获取节点中的主机名称信息</span></span><br><span class="line">		<span class="keyword">for</span> (String child : children) &#123;</span><br><span class="line">			<span class="keyword">byte</span>[] data = zk.getData(parentNode + <span class="string">&quot;/&quot;</span> + child, <span class="keyword">false</span>, <span class="keyword">null</span>);</span><br><span class="line"></span><br><span class="line">			servers.add(<span class="keyword">new</span> String(data));</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4打印服务器列表信息</span></span><br><span class="line">		System.out.println(servers);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 业务功能</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">business</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line"></span><br><span class="line">		System.out.println(<span class="string">&quot;client is working ...&quot;</span>);</span><br><span class="line">		Thread.sleep(Long.MAX_VALUE);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 1获取zk连接</span></span><br><span class="line">		DistributeClient client = <span class="keyword">new</span> DistributeClient();</span><br><span class="line">		client.getConnect();</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 2获取servers的子节点信息，从中获取服务器信息列表</span></span><br><span class="line">		client.getServerList();</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 3业务进程启动</span></span><br><span class="line">		client.business();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>分布式</category>
      </categories>
  </entry>
</search>
