<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/cute.jpeg">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/cute.jpeg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/cute.jpeg">
  <link rel="mask-icon" href="/images/cute.jpeg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="精通Kafka">
<meta property="og:type" content="article">
<meta property="og:title" content="Kafka">
<meta property="og:url" content="http://example.com/Kafka/index.html">
<meta property="og:site_name" content="Vincent&#39;s Learning Journey">
<meta property="og:description" content="精通Kafka">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/Kafka/18.png">
<meta property="og:image" content="http://example.com/Kafka/19.png">
<meta property="og:image" content="http://example.com/Kafka/20.png">
<meta property="og:image" content="http://example.com/Kafka/21.png">
<meta property="og:image" content="http://example.com/Kafka/159.png">
<meta property="og:image" content="http://example.com/Kafka/160.png">
<meta property="og:image" content="http://example.com/Kafka/161.png">
<meta property="og:image" content="http://example.com/Kafka/%E6%88%AA%E5%B1%8F2021-12-28%20%E4%B8%8B%E5%8D%882.34.50.png">
<meta property="og:image" content="http://example.com/Kafka/%E6%88%AA%E5%B1%8F2021-12-28%20%E4%B8%8B%E5%8D%882.39.32.png">
<meta property="og:image" content="http://example.com/Kafka/%E6%88%AA%E5%B1%8F2021-12-28%20%E4%B8%8B%E5%8D%882.53.13.png">
<meta property="og:image" content="http://example.com/Kafka/%E6%88%AA%E5%B1%8F2021-12-28%20%E4%B8%8B%E5%8D%882.58.53.png">
<meta property="og:image" content="http://example.com/Kafka/%E6%88%AA%E5%B1%8F2021-12-28%20%E4%B8%8B%E5%8D%883.01.35.png">
<meta property="og:image" content="http://example.com/Kafka/162.png">
<meta property="og:image" content="http://example.com/Kafka/163.png">
<meta property="og:image" content="http://example.com/Kafka/164.png">
<meta property="article:published_time" content="2021-12-01T04:42:37.000Z">
<meta property="article:modified_time" content="2021-12-28T13:59:17.785Z">
<meta property="article:author" content="Vincent">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/Kafka/18.png">

<link rel="canonical" href="http://example.com/Kafka/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Kafka | Vincent's Learning Journey</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Vincent's Learning Journey</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">DayDayUp</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section">Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section">Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section">Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger">Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/Kafka/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cute.jpeg">
      <meta itemprop="name" content="Vincent">
      <meta itemprop="description" content="nice">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Vincent's Learning Journey">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Kafka
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-12-01 12:42:37" itemprop="dateCreated datePublished" datetime="2021-12-01T12:42:37+08:00">2021-12-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-12-28 21:59:17" itemprop="dateModified" datetime="2021-12-28T21:59:17+08:00">2021-12-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/" itemprop="url" rel="index"><span itemprop="name">实时计算</span></a>
                </span>
            </span>

          
            <div class="post-description">精通Kafka</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>Kafka是一个分布式的基于发布/订阅模式的消息队列，主要应用于大数据实时处理领域。</p>
<p>官网：<a target="_blank" rel="noopener" href="https://kafka.apache.org/">https://kafka.apache.org</a></p>
<h2 id="Kafka基础架构"><a href="#Kafka基础架构" class="headerlink" title="Kafka基础架构"></a>Kafka基础架构</h2><p><img src="/Kafka/18.png" alt="18"></p>
<p>（1）Producer ：消息生产者，就是向kafka broker发消息的客户端；</p>
<p>（2）Consumer ：消息消费者，向kafka broker取消息的客户端；</p>
<p>（3）Consumer Group （CG）：消费者组，由多个consumer组成。<strong>消费者组内每个消费者负责消费不同分区的数据</strong>，一个分区只能由一个消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。</p>
<p>（4）Broker ：一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic。</p>
<p>（5）Topic ：可以理解为一个队列，生产者和消费者面向的都是一个topic；</p>
<p>（6）Partition：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列；</p>
<p>（7）Replica：副本，为保证集群中的某个节点发生故障时，该节点上的partition数据不丢失，且kafka仍然能够继续工作，kafka提供了副本机制，一个topic的每个分区都有若干个副本，一个leader和若干个follower。</p>
<p>（8）leader：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是leader。</p>
<p>（9）follower：每个分区多个副本中的“从”，实时从leader中同步数据，保持和leader数据的同步。leader发生故障时，某个follower会成为新的leader。</p>
<h2 id="Kafka架构深入"><a href="#Kafka架构深入" class="headerlink" title="Kafka架构深入"></a>Kafka架构深入</h2><h3 id="Kafka工作流程及文件存储机制"><a href="#Kafka工作流程及文件存储机制" class="headerlink" title="Kafka工作流程及文件存储机制"></a>Kafka工作流程及文件存储机制</h3><p><img src="/Kafka/19.png" alt="19"></p>
<p>Kafka中消息是以topic进行分类的，生产者生产消息，消费者消费消息，都是面向topic的。</p>
<p><strong>topic是逻辑上的概念，而partition是物理上的概念，每个partition对应于一个log文件，该log文件中存储的就是producer生产的数据</strong>。Producer生产的数据会被不断追加到该log文件末端，且每条数据都有自己的offset。消费者组中的每个消费者，都会实时记录自己消费到了哪个offset，以便出错恢复时，从上次的位置继续消费。</p>
<p><img src="/Kafka/20.png" alt="20"></p>
<p>由于生产者生产的消息会不断追加到log文件末尾，为防止log文件过大导致数据定位效率低下，Kafka采取了<strong>分片和索引</strong>机制，将<strong>每个partition分为多个segment</strong>。每个<strong>segment对应两个文件——.index文件和.log文件</strong>。这些文件位于一个文件夹下，该文件夹的命名规则为：<strong>topic名称+分区序号</strong>。例如，first这个topic有三个分区，则其对应的文件夹为first-0,first-1,first-2。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">--first-0</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#第一个segment</span></span></span><br><span class="line">00000000000000000000.index</span><br><span class="line">00000000000000000000.log</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#第二个segment</span></span></span><br><span class="line">00000000000000170410.index</span><br><span class="line">00000000000000170410.log</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#第三个segment</span></span></span><br><span class="line">00000000000000239430.index</span><br><span class="line">00000000000000239430.log</span><br></pre></td></tr></table></figure>

<p><strong>index和log文件以当前segment的第一条消息的offset命名</strong>。下图为index文件和log文件的结构示意图。</p>
<p><img src="/Kafka/21.png" alt="21"></p>
<p>.index文件存储大量的索引信息，.log文件存储大量的数据，索引文件中的元数据指向对应数据文件中message的物理偏移地址。</p>
<h3 id="Kafka生产者"><a href="#Kafka生产者" class="headerlink" title="Kafka生产者"></a>Kafka生产者</h3><h4 id="分区策略"><a href="#分区策略" class="headerlink" title="分区策略"></a>分区策略</h4><p><strong>1）分区的原因</strong></p>
<p>（1）方便在集群中<strong>扩展</strong>，每个Partition可以通过调整以适应它所在的机器，而一个topic又可以有多个Partition组成，因此整个集群就可以适应任意大小的数据了；</p>
<p>（2）可以提高<strong>并发</strong>，因为可以<strong>以Partition为单位读写</strong>了。</p>
<p><strong>2）分区的原则</strong></p>
<p>我们需要将producer发送的数据封装成一个<strong>ProducerRecord</strong>对象。</p>
<p>（1）指明partition的情况下，直接将指明的值作为 partiton 值；</p>
<p>（2）没有指明partition值但有key的情况下，将key的hash值与topic的partition数进行取余得到partition值；</p>
<p>（3）既没有partition值又没有 key 的情况下，第一次调用时随机生成一个整数（后面每次调用在这个整数上自增），将这个值与topic可用的partition总数取余得到partition值，也就是常说的round-robin算法。</p>
<h4 id="数据可靠性保证"><a href="#数据可靠性保证" class="headerlink" title="数据可靠性保证"></a>数据可靠性保证</h4><p>为保证producer发送的数据，能可靠的发送到指定的topic，topic的每个partition收到producer发送的数据后，都需要向producer发送ack（acknowledgement确认收到），如果producer收到ack，就会进行下一轮的发送，否则重新发送数据。</p>
<h5 id="1）副本数据同步策略"><a href="#1）副本数据同步策略" class="headerlink" title="1）副本数据同步策略"></a>1）副本数据同步策略</h5><table>
<thead>
<tr>
<th>方案</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody><tr>
<td>半数以上完成同步，就发送ack</td>
<td>延迟低</td>
<td>选举新的leader时，容忍n台节点的故障，需要2n+1个副本</td>
</tr>
<tr>
<td>全部完成同步，才发送ack</td>
<td>选举新的leader时，容忍n台节点的故障，需要n+1个副本</td>
<td>延迟高</td>
</tr>
</tbody></table>
<p>Kafka选择了第二种方案，原因如下：</p>
<p>（1）同样为了容忍n台节点的故障，第一种方案需要2n+1个副本，而第二种方案只需要n+1个副本，而Kafka的每个分区都有大量的数据，第一种方案会造成大量数据的冗余。</p>
<p>（2）虽然第二种方案的网络延迟会比较高，但网络延迟对Kafka的影响较小。</p>
<h5 id="2）ISR"><a href="#2）ISR" class="headerlink" title="2）ISR"></a>2）ISR</h5><p>采用第二种方案之后，设想以下情景：leader收到数据，所有follower都开始同步数据，但有一个follower，因为某种故障，迟迟不能与leader进行同步，那leader就要一直等下去，直到它完成同步，才能发送ack。这个问题怎么解决呢？</p>
<p>Leader维护了一个动态的in-sync replica set (ISR)，意为和leader保持同步的follower集合。当ISR中的follower完成数据的同步之后，leader就会给producer发送ack。如果follower长时间未向leader同步数据，则该follower将被踢出ISR，该时间阈值由<strong>replica.lag.time.max.ms</strong>参数设定。Leader发生故障之后，就会从ISR中选举新的leader。</p>
<h5 id="3）ack应答机制"><a href="#3）ack应答机制" class="headerlink" title="3）ack应答机制"></a>3）ack应答机制</h5><p>对于某些不太重要的数据，对数据的可靠性要求不是很高，能够容忍数据的少量丢失，所以没必要等ISR中的follower全部接收成功。</p>
<p>所以Kafka为用户提供了三种可靠性级别，用户根据对可靠性和延迟的要求进行权衡，选择以下的配置。</p>
<p>acks参数配置：</p>
<p>acks：</p>
<p>0：producer不等待broker的ack，这一操作提供了一个最低的延迟，broker一接收到还没有写入磁盘就已经返回，当broker故障时有可能<strong>丢失数据</strong>；</p>
<p>1：producer等待broker的ack，partition的leader落盘成功后返回ack，如果在follower同步成功之前leader故障，那么将会<strong>丢失数据</strong>；</p>
<p><img src="/Kafka/159.png" alt="159"></p>
<p>-1（all）：producer等待broker的ack，partition的leader和follower全部落盘成功后才返回ack。但是如果在follower同步完成后，broker发送ack之前，leader发生故障，那么会造成<strong>数据重复</strong>。</p>
<p><img src="/Kafka/160.png" alt="160"></p>
<h5 id="4）故障处理细节"><a href="#4）故障处理细节" class="headerlink" title="4）故障处理细节"></a>4）故障处理细节</h5><p><img src="/Kafka/161.png" alt="161"></p>
<p>（1）follower故障</p>
<p>follower发生故障后会被临时踢出ISR，待该follower恢复后，follower会读取本地磁盘记录的上次的HW，并将log文件高于HW的部分截取掉，从HW开始向leader进行同步。等该follower的LEO大于等于该Partition的HW，即follower追上leader之后，就可以重新加入ISR了。</p>
<p>（2）leader故障</p>
<p>leader发生故障之后，会从ISR中选出一个新的leader，之后，为保证多个副本之间的数据一致性，其余的follower会先将各自的log文件高于HW的部分截掉，然后从新的leader同步数据。</p>
<p>注意：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。</p>
<h4 id="Exactly-Once语义"><a href="#Exactly-Once语义" class="headerlink" title="Exactly Once语义"></a>Exactly Once语义</h4><p><strong>将服务器的ACK级别设置为-1，可以保证Producer到Server之间不会丢失数据，即At Least Once语义。相对的，将服务器ACK级别设置为0，可以保证生产者每条消息只会被发送一次，即At Most Once语义。</strong></p>
<p>At Least Once可以保证数据不丢失，但是不能保证数据不重复；相对的，At Most Once可以保证数据不重复，但是不能保证数据不丢失。但是，对于一些非常重要的信息，比如说交易数据，下游数据消费者要求数据既不重复也不丢失，即Exactly Once语义。在0.11版本以前的Kafka，对此是无能为力的，只能保证数据不丢失，再在下游消费者对数据做全局去重。对于多个下游应用的情况，每个都需要单独做全局去重，这就对性能造成了很大影响。</p>
<p>0.11版本的Kafka，引入了一项重大特性：幂等性。<strong>所谓的幂等性就是指Producer不论向Server发送多少次重复数据，Server端都只会持久化一条</strong>。幂等性结合At Least Once语义，就构成了Kafka的Exactly Once语义。即：</p>
<p>At Least Once + 幂等性 = Exactly Once</p>
<p><strong>要启用幂等性，只需要将Producer的参数中enable.idompotence设置为true即可</strong>。Kafka的幂等性实现其实就是将原来下游需要做的去重放在了数据上游。开启幂等性的Producer在初始化的时候会被分配一个PID，发往同一Partition的消息会附带Sequence Number。而Broker端会对&lt;PID, Partition, SeqNumber&gt;做缓存，当具有相同主键的消息提交时，Broker只会持久化一条。</p>
<p>但是PID重启就会变化，同时不同的Partition也具有不同主键，所以幂等性无法保证跨分区跨会话的Exactly Once。</p>
<h3 id="Kafka消费者"><a href="#Kafka消费者" class="headerlink" title="Kafka消费者"></a>Kafka消费者</h3><h4 id="消费方式"><a href="#消费方式" class="headerlink" title="消费方式"></a>消费方式</h4><p>consumer采用pull（拉）模式从broker中读取数据。</p>
<p>push（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而pull模式则可以根据consumer的消费能力以适当的速率消费消息。</p>
<p>pull模式不足之处是，如果kafka没有数据，消费者可能会陷入循环中，一直返回空数据。针对这一点，Kafka的消费者在消费数据时会传入一个时长参数timeout，<strong>如果当前没有数据可供消费，consumer会等待一段时间之后再返回，这段时长即为timeout</strong>。</p>
<h4 id="分区分配策略"><a href="#分区分配策略" class="headerlink" title="分区分配策略"></a>分区分配策略</h4><p>一个consumer group中有多个consumer，一个 topic有多个partition，所以必然会涉及到partition的分配问题，即确定那个partition由哪个consumer来消费。</p>
<p><strong>Kafka有3种分配策略，RangeAssignor，RoundRobinAssignor，StickyAssignor。</strong></p>
<h5 id="RangeAssignor"><a href="#RangeAssignor" class="headerlink" title="RangeAssignor"></a><strong>RangeAssignor</strong></h5><p>RangeAssignor对每个Topic进行独立的分区分配。对于每一个Topic，首先对分区按照分区ID进行排序，然后订阅这个Topic的消费组的消费者再进行排序，之后尽量均衡的将分区分配给消费者。这里只能是尽量均衡，因为分区数可能无法被消费者数量整除，那么有一些消费者就会多分配到一些分区。</p>
<p><img src="/Kafka/%E6%88%AA%E5%B1%8F2021-12-28%20%E4%B8%8B%E5%8D%882.34.50.png" alt="截屏2021-12-28 下午2.34.50"></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">assign(topic, consumers) &#123;</span><br><span class="line">  <span class="comment">// 对分区和Consumer进行排序</span></span><br><span class="line">  List&lt;Partition&gt; partitions = topic.getPartitions();</span><br><span class="line">  sort(partitions);</span><br><span class="line">  sort(consumers);</span><br><span class="line">  <span class="comment">// 计算每个Consumer分配的分区数</span></span><br><span class="line">  <span class="keyword">int</span> numPartitionsPerConsumer = partition.size() / consumers.size();</span><br><span class="line">  <span class="comment">// 额外有一些Consumer会多分配到分区</span></span><br><span class="line">  <span class="keyword">int</span> consumersWithExtraPartition = partition.size() % consumers.size();</span><br><span class="line">  <span class="comment">// 计算分配结果</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>, n = consumers.size(); i &lt; n; i++) &#123;</span><br><span class="line">    <span class="comment">// 第i个Consumer分配到的分区的index</span></span><br><span class="line">        <span class="keyword">int</span> start = numPartitionsPerConsumer * i + Math.min(i, consumersWithExtraPartition);</span><br><span class="line">        <span class="comment">// 第i个Consumer分配到的分区数</span></span><br><span class="line">        <span class="keyword">int</span> length = numPartitionsPerConsumer + (i + <span class="number">1</span> &gt; consumersWithExtraPartition ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">        <span class="comment">// 分装分配结果</span></span><br><span class="line">        assignment.get(consumersForTopic.get(i)).addAll(partitions.subList(start, start + length));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>RangeAssignor策略的原理是按照消费者总数和分区总数进行整除运算来获得一个跨度，然后将分区按照跨度进行平均分配，以保证分区尽可能均匀地分配给所有的消费者。对于每一个Topic，RangeAssignor策略会将消费组内所有订阅这个Topic的消费者按照名称的字典序排序，然后为每个消费者划分固定的分区范围，如果<strong>不够平均分配，那么字典序靠前的消费者会被多分配一个分区</strong>。</p>
<p>这种分配方式明显的一个问题是随着消费者订阅的Topic的数量的增加，不均衡的问题会越来越严重，比如上图中4个分区3个消费者的场景，C0会多分配一个分区。如果此时再订阅一个分区数为4的Topic，那么C0又会比C1、C2多分配一个分区，这样C0总共就比C1、C2多分配两个分区了，而且随着Topic的增加，这个情况会越来越严重。</p>
<p>分配结果：</p>
<p>订阅2个Topic，每个Topic4个分区，共3个Consumer</p>
<p>C0：[T0P0，T0P1，T1P0，T1P1]</p>
<p>C1：[T0P2，T1P2]</p>
<p>C2：[T0P3，T1P3]</p>
<h5 id="RoundRobinAssignor"><a href="#RoundRobinAssignor" class="headerlink" title="RoundRobinAssignor"></a>RoundRobinAssignor</h5><p>RoundRobinAssignor的分配策略是将消费组内订阅的所有Topic的分区及所有消费者进行排序后尽量均衡的分配（RangeAssignor是针对单个Topic的分区进行排序分配的）。如果消费组内，消费者订阅的Topic列表是相同的（每个消费者都订阅了相同的Topic），那么分配结果是尽量均衡的（消费者之间分配到的分区数的差值不会超过1）。如果订阅的Topic列表是不同的，那么分配结果是不保证“尽量均衡”的，因为某些消费者不参与一些Topic的分配。</p>
<p>RangeAssignor和RoundRobinAssignor(消费相同的topic)对比：</p>
<p><img src="/Kafka/%E6%88%AA%E5%B1%8F2021-12-28%20%E4%B8%8B%E5%8D%882.39.32.png" alt="截屏2021-12-28 下午2.39.32"></p>
<p>相对于RangeAssignor，在订阅多个Topic的情况下，RoundRobinAssignor的方式能使消费者之间尽量均衡的分配到分区（分配到的分区数的差值不会超过1——RangeAssignor的分配策略可能随着订阅的Topic越来越多，差值越来越大）。</p>
<p>对于组内消费者订阅Topic不一致的情况：假设有三个消费者分别为C0、C1、C2，有3个Topic T0、T1、T2，分别拥有1、2、3个分区，并且C0订阅T0，C1订阅T0和T1，C2订阅T0、T1、T0，那么RoundRobinAssignor的分配结果如下：</p>
<p><img src="/Kafka/%E6%88%AA%E5%B1%8F2021-12-28%20%E4%B8%8B%E5%8D%882.53.13.png" alt="截屏2021-12-28 下午2.53.13"></p>
<h5 id="StickyAssignor"><a href="#StickyAssignor" class="headerlink" title="StickyAssignor"></a>StickyAssignor</h5><p>尽管RoundRobinAssignor已经在RangeAssignor上做了一些优化来更均衡的分配分区，但是在一些情况下依旧会产生严重的分配偏差，<strong>比如消费组中订阅的Topic列表不相同的情况下</strong>（这个情况可能更多的发生在发布阶段，但是这真的是一个问题吗？——可以参照Kafka官方的说明：KIP-49 Fair Partition Assignment Strategy）。更核心的问题是<strong>无论是RangeAssignor，还是RoundRobinAssignor，当前的分区分配算法都没有考虑上一次的分配结果</strong>。显然，在执行一次新的分配之前，如果能考虑到上一次分配的结果，尽量少的调整分区分配的变动，显然是能节省很多开销的。</p>
<p>从字面意义上看，Sticky是“粘性的”，可以理解为分配结果是带“粘性的”——每一次分配变更相对上一次分配做最少的变动（上一次的结果是有粘性的），其目标有两点：</p>
<ol>
<li><p><strong>分区的分配尽量的均衡</strong></p>
</li>
<li><p><strong>每一次重分配的结果尽量与上一次分配结果保持一致</strong></p>
</li>
</ol>
<p>当这两个目标发生冲突时，优先保证第一个目标。第一个目标是每个分配算法都尽量尝试去完成的，而第二个目标才真正体现出StickyAssignor特性的。 </p>
<p>我们先来看预期分配的结构，后续再具体分析StickyAssignor的算法实现。</p>
<p>例如：</p>
<ul>
<li>有3个Consumer：C0、C1、C2</li>
<li>有4个Topic：T0、T1、T2、T3，每个Topic有2个分区</li>
<li>所有Consumer都订阅了这4个分区</li>
</ul>
<p>StickyAssignor的分配结果如下图所示（增加RoundRobinAssignor分配作为对比）：</p>
<p><img src="/Kafka/%E6%88%AA%E5%B1%8F2021-12-28%20%E4%B8%8B%E5%8D%882.58.53.png" alt="截屏2021-12-28 下午2.58.53"></p>
<p>上面的例子中，Sticky模式原来分配给C0、C2的分区都没有发生变动，且最终C0、C1达到的均衡的目的。</p>
<p>再举一个例子：</p>
<ul>
<li>有3个Consumer：C0、C1、C2</li>
<li>3个Topic：T0、T1、T2，它们分别有1、2、3个分区</li>
<li>C0订阅T0；C1订阅T0、T1；C2订阅T0、T1、T2</li>
</ul>
<p>分配结果如下图所示：</p>
<p><img src="/Kafka/%E6%88%AA%E5%B1%8F2021-12-28%20%E4%B8%8B%E5%8D%883.01.35.png" alt="截屏2021-12-28 下午3.01.35"></p>
<h4 id="offset的维护"><a href="#offset的维护" class="headerlink" title="offset的维护"></a>offset的维护</h4><p>由于consumer在消费过程中可能会出现断电宕机等故障，consumer恢复后，需要从故障前的位置的继续消费，所以<strong>consumer需要实时记录自己消费到了哪个offset，以便故障恢复后继续消费</strong>。</p>
<p>Kafka 0.9版本之前，consumer默认将offset保存在Zookeeper中，从0.9版本开始，consumer默认将offset保存在Kafka一个内置的topic中，该topic为<strong>consumer_offsets</strong>。</p>
<h3 id="Kafka高效读写数据"><a href="#Kafka高效读写数据" class="headerlink" title="Kafka高效读写数据"></a>Kafka高效读写数据</h3><p><strong>1）顺序写磁盘</strong></p>
<p>Kafka的producer生产数据，要写入到log文件中，写的过程是一直追加到文件末端，为顺序写。官网有数据表明，同样的磁盘，顺序写能到到600M/s，而随机写只有100k/s。这与磁盘的机械机构有关，顺序写之所以快，是因为其省去了大量磁头寻址的时间。</p>
<p><strong>2）应用Pagecache</strong></p>
<p>Kafka数据持久化是直接持久化到Pagecache中，这样会产生以下几个好处： </p>
<p>I/O Scheduler 会将连续的小块写组装成大块的物理写从而提高性能</p>
<p>I/O Scheduler 会尝试将一些写操作重新按顺序排好，从而减少磁盘头的移动时间</p>
<p>充分利用所有空闲内存（非 JVM 内存）。如果使用应用层 Cache（即 JVM 堆内存），会增加 GC 负担</p>
<p>读操作可直接在 Page Cache 内进行。如果消费和生产速度相当，甚至不需要通过物理磁盘（直接通过 Page Cache）交换数据</p>
<p>如果进程重启，JVM 内的 Cache 会失效，但 Page Cache 仍然可用</p>
<p>尽管持久化到Pagecache上可能会造成宕机丢失数据的情况，但这可以被Kafka的Replication机制解决。如果为了保证这种情况下数据不丢失而强制将 Page Cache 中的数据 Flush 到磁盘，反而会降低性能。</p>
<p><strong>3）零复制技术</strong></p>
<p><img src="/Kafka/162.png" alt="162"></p>
<h3 id="Zookeeper在Kafka中的作用"><a href="#Zookeeper在Kafka中的作用" class="headerlink" title="Zookeeper在Kafka中的作用"></a>Zookeeper在Kafka中的作用</h3><p>Kafka集群中有一个broker会被选举为Controller，负责管理集群broker的上下线，所有topic的分区副本分配和leader选举等工作。</p>
<p>Controller的管理工作都是依赖于Zookeeper的。</p>
<p>以下为partition的leader选举过程：</p>
<p><img src="/Kafka/163.png" alt="163"></p>
<h3 id="Kafka事务"><a href="#Kafka事务" class="headerlink" title="Kafka事务"></a>Kafka事务</h3><p>Kafka从0.11版本开始引入了事务支持。事务可以保证Kafka在Exactly Once语义的基础上，生产和消费可以跨分区和会话，要么全部成功，要么全部失败。</p>
<h4 id="Producer事务"><a href="#Producer事务" class="headerlink" title="Producer事务"></a>Producer事务</h4><p>为了实现跨分区跨会话的事务，需要引入一个<strong>全局唯一的Transaction ID</strong>，并ack。这样当Producer重启后就可以通过正在进行的Transaction ID获得原来的PID。</p>
<p>为了管理Transaction，Kafka引入了一个新的组件<strong>Transaction Coordinator</strong>。Producer就是通过和Transaction Coordinator交互获得Transaction ID对应的任务状态。Transaction Coordinator还负责将所有事务写入Kafka的一个内部Topic，这样即使整个服务重启，由于事务状态得到保存，进行中的事务状态可以得到恢复，从而继续进行。</p>
<h4 id="Consumer事务（精准一次性消费）"><a href="#Consumer事务（精准一次性消费）" class="headerlink" title="Consumer事务（精准一次性消费）"></a>Consumer事务（精准一次性消费）</h4><p>上述事务机制主要是从Producer方面考虑，对于Consumer而言，事务的保证就会相对较弱，尤其是无法保证Commit的信息被精确消费。这是由于Consumer可以通过offset访问任意信息，而且不同的Segment File生命周期不同，同一事务的消息可能会出现重启后被删除的情况。</p>
<p>如果想完成Consumer端的精准一次性消费，那么需要<strong>kafka消费端将消费过程和提交offset过程做原子绑定</strong>。此时我们需<strong>要将kafka的offset保存到支持事务的自定义介质中（比如mysql）</strong>。这部分知识会在后续项目部分涉及。</p>
<h2 id="Kafka-API"><a href="#Kafka-API" class="headerlink" title="Kafka API"></a>Kafka API</h2><h3 id="Producer-API"><a href="#Producer-API" class="headerlink" title="Producer API"></a>Producer API</h3><h4 id="消息发送流程"><a href="#消息发送流程" class="headerlink" title="消息发送流程"></a>消息发送流程</h4><p>Kafka的Producer发送消息采用的是<strong>异步发送</strong>的方式。在消息发送的过程中，涉及到了两个线程——main线程和Sender线程，以及一个线程共享变量——RecordAccumulator。main线程将消息发送给RecordAccumulator，Sender线程不断从RecordAccumulator中拉取消息发送到Kafka broker。</p>
<p><img src="/Kafka/164.png" alt="164"></p>
<p>相关参数：</p>
<p><strong>batch.size</strong>: 只有数据积累到batch.size之后，sender才会发送数据。</p>
<p><strong>linger.ms</strong>: 如果数据迟迟未达到batch.size，sender等待linger.time之后就会发送数据。</p>
<h4 id="异步发送API"><a href="#异步发送API" class="headerlink" title="异步发送API"></a>异步发送API</h4><p>1）导入依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>2）编写代码</p>
<p>需要用到的类：</p>
<p><strong>KafkaProducer</strong>：需要创建一个生产者对象，用来发送数据</p>
<p><strong>ProducerConfig</strong>：获取所需的一系列配置参数</p>
<p><strong>ProducerRecord</strong>：每条数据都要封装成一个ProducerRecord对象</p>
<p>（1）不带回调函数的API</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.*;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ExecutionException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducer</span> </span>&#123;    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ExecutionException, InterruptedException </span>&#123;  </span><br><span class="line"></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();     </span><br><span class="line"></span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;linux1:9092&quot;</span>);    <span class="comment">//kafka集群，broker-list        </span></span><br><span class="line">        props.put(<span class="string">&quot;acks&quot;</span>, <span class="string">&quot;all&quot;</span>);                         <span class="comment">//-1        </span></span><br><span class="line">        props.put(<span class="string">&quot;retries&quot;</span>, <span class="number">1</span>);                          <span class="comment">//重试次数        </span></span><br><span class="line">        props.put(<span class="string">&quot;batch.size&quot;</span>, <span class="number">16384</span>);                   <span class="comment">//批次大小 16Kb        </span></span><br><span class="line">        props.put(<span class="string">&quot;linger.ms&quot;</span>, <span class="number">1</span>);                        <span class="comment">//等待时间        </span></span><br><span class="line">        props.put(<span class="string">&quot;buffer.memory&quot;</span>, <span class="number">33554432</span>);             <span class="comment">//RecordAccumulator缓冲区大小,缓冲区存放RecordBatch    </span></span><br><span class="line">        props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        Producer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;            </span><br><span class="line">            producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(<span class="string">&quot;first&quot;</span>, Integer.toString(i), Integer.toString(i)));            </span><br><span class="line">            <span class="comment">// topic, Partition, k, v                </span></span><br><span class="line">            <span class="comment">// Partition 和 k 可以省略        </span></span><br><span class="line">        &#125;      </span><br><span class="line"></span><br><span class="line">        producer.close();    </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>（2）带回调函数的API</p>
<p>回调函数会在producer收到ack时调用，为异步调用，该方法有两个参数，分别是RecordMetadata和Exception，如果Exception为null，说明消息发送成功，如果Exception不为null，说明消息发送失败。</p>
<p>注意：消息发送失败会自动重试，不需要我们在回调函数中手动重试。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.*;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ExecutionException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducer</span> </span>&#123;    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ExecutionException, InterruptedException </span>&#123;        </span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();        </span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;linux1:9092&quot;</span>);<span class="comment">//kafka集群，broker-list        </span></span><br><span class="line">        props.put(<span class="string">&quot;acks&quot;</span>, <span class="string">&quot;all&quot;</span>);        </span><br><span class="line">        props.put(<span class="string">&quot;retries&quot;</span>, <span class="number">1</span>);<span class="comment">//重试次数        </span></span><br><span class="line">        props.put(<span class="string">&quot;batch.size&quot;</span>, <span class="number">16384</span>);<span class="comment">//批次大小        </span></span><br><span class="line">        props.put(<span class="string">&quot;linger.ms&quot;</span>, <span class="number">1</span>);<span class="comment">//等待时间        </span></span><br><span class="line">        props.put(<span class="string">&quot;buffer.memory&quot;</span>, <span class="number">33554432</span>);<span class="comment">//RecordAccumulator缓冲区大小        </span></span><br><span class="line">        props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);        </span><br><span class="line">        props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);        </span><br><span class="line"></span><br><span class="line">        Producer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);        </span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;            </span><br><span class="line">            producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(<span class="string">&quot;first&quot;</span>, Integer.toString(i), Integer.toString(i)), <span class="keyword">new</span> Callback() &#123;                </span><br><span class="line">                <span class="comment">//回调函数，该方法会在Producer收到ack时调用，为异步调用                </span></span><br><span class="line">                <span class="meta">@Override</span>                </span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCompletion</span><span class="params">(RecordMetadata metadata, Exception exception)</span> </span>&#123;                    </span><br><span class="line">                    <span class="keyword">if</span> (exception == <span class="keyword">null</span>) &#123;                        </span><br><span class="line">                        System.out.println(<span class="string">&quot;success-&gt;&quot;</span> + metadata.offset());                    </span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;                        </span><br><span class="line">                        exception.printStackTrace();                    </span><br><span class="line">                    &#125;                </span><br><span class="line">                &#125;            </span><br><span class="line">            &#125;);        </span><br><span class="line">        &#125;        </span><br><span class="line"></span><br><span class="line">        producer.close();    </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="同步发送API"><a href="#同步发送API" class="headerlink" title="同步发送API"></a>同步发送API</h4><p><strong>同步发送的意思就是，一条消息发送之后，会阻塞当前线程，直至返回ack。</strong></p>
<p>由于send方法返回的是一个Future对象，根据Futrue对象的特点，我们也可以实现同步发送的效果，<strong>只需调用Future对象的get方法即可</strong>。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.Producer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties; </span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ExecutionException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducer</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ExecutionException, InterruptedException </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();        </span><br><span class="line"></span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;linux1:9092&quot;</span>);                  <span class="comment">//kafka集群，broker-list        </span></span><br><span class="line">        props.put(<span class="string">&quot;acks&quot;</span>, <span class="string">&quot;all&quot;</span>);        props.put(<span class="string">&quot;retries&quot;</span>, <span class="number">1</span>);       <span class="comment">//重试次数        </span></span><br><span class="line">        props.put(<span class="string">&quot;batch.size&quot;</span>, <span class="number">16384</span>);                                 <span class="comment">//批次大小        </span></span><br><span class="line">        props.put(<span class="string">&quot;linger.ms&quot;</span>, <span class="number">1</span>);                                      <span class="comment">//等待时间        </span></span><br><span class="line">        props.put(<span class="string">&quot;buffer.memory&quot;</span>, <span class="number">33554432</span>);                           <span class="comment">//RecordAccumulator缓冲区大小        </span></span><br><span class="line">        props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);        </span><br><span class="line">        props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);        </span><br><span class="line">        Producer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);        </span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;            </span><br><span class="line">            producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(<span class="string">&quot;first&quot;</span>, Integer.toString(i), Integer.toString(i))).get();        </span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        producer.close();    </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Consumer-API"><a href="#Consumer-API" class="headerlink" title="Consumer API"></a>Consumer API</h3><p>Consumer消费数据时的可靠性是很容易保证的，因为数据在Kafka中是持久化的，故不用担心数据丢失问题。</p>
<p>由于consumer在消费过程中可能会出现断电宕机等故障，consumer恢复后，需要从故障前的位置继续消费，<strong>所以consumer需要实时记录自己消费到了哪个offset</strong>，以便故障恢复后继续消费。</p>
<p>所以offset的维护是Consumer消费数据是必须考虑的问题。</p>
<h4 id="自动提交offset"><a href="#自动提交offset" class="headerlink" title="自动提交offset"></a>自动提交offset</h4><p><strong>1）导入依赖</strong></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka&lt;/groupId &gt;</span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><strong>2）编写代码</strong></p>
<p>需要用到的类：</p>
<p><strong>KafkaConsumer</strong>：需要创建一个消费者对象，用来消费数据</p>
<p><strong>ConsumerConfig</strong>：获取所需的一系列配置参数</p>
<p><strong>ConsuemrRecord</strong>：每条数据都要封装成一个ConsumerRecord对象</p>
<p>为了使我们能够专注于自己的业务逻辑，Kafka提供了自动提交offset的功能。 </p>
<p><strong>自动提交offset的相关参数</strong>：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">enable.auto.commit：是否开启自动提交offset功能</span><br><span class="line">auto.commit.interval.ms：自动提交offset的时间间隔</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;<span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomConsumer</span> </span>&#123;    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;        </span><br><span class="line"></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();        </span><br><span class="line"></span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;linux1:9092&quot;</span>);        </span><br><span class="line">        props.put(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;test&quot;</span>);        </span><br><span class="line">        props.put(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;true&quot;</span>);        </span><br><span class="line">        props.put(<span class="string">&quot;auto.commit.interval.ms&quot;</span>, <span class="string">&quot;1000&quot;</span>);        </span><br><span class="line">        props.put(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);        </span><br><span class="line">        props.put(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);              </span><br><span class="line"></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);        </span><br><span class="line"></span><br><span class="line">        consumer.subscribe(Arrays.asList(<span class="string">&quot;first&quot;</span>));        </span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;            </span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);            </span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records)                </span><br><span class="line">                System.out.printf(<span class="string">&quot;offset = %d, key = %s, value = %s%n&quot;</span>, record.offset(), record.key(), record.value());        </span><br><span class="line">        &#125;    </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="手动提交offset"><a href="#手动提交offset" class="headerlink" title="手动提交offset"></a>手动提交offset</h4><p>虽然自动提交offset十分简介便利，但由于其是基于时间提交的，开发人员难以把握offset提交的时机。因此Kafka还提供了手动提交offset的API。</p>
<p>手动提交offset的方法有两种：分别是<strong>commitSync（同步提交）</strong>和<strong>commitAsync（异步提交）</strong>。两者的相同点是，都会将本次poll的一批数据最高的偏移量提交；不同点是，commitSync阻塞当前线程，一直到提交成功，并且会自动失败重试（由不可控因素导致，也会出现提交失败）；而commitAsync则没有失败重试机制，故有可能提交失败。</p>
<h5 id="1）同步提交offset"><a href="#1）同步提交offset" class="headerlink" title="1）同步提交offset"></a>1）同步提交offset</h5><p>由于同步提交offset有<strong>失败重试机制</strong>，故更加可靠，以下为同步提交offset的示例。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomComsumer</span> </span>&#123;    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;        </span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();        </span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;linux1:9092&quot;</span>);<span class="comment">//Kafka集群        </span></span><br><span class="line">        props.put(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;test&quot;</span>);<span class="comment">//消费者组，只要group.id相同，就属于同一个消费者组        </span></span><br><span class="line">        props.put(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;false&quot;</span>);<span class="comment">//关闭自动提交offset        </span></span><br><span class="line">        props.put(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);        </span><br><span class="line">        props.put(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);        </span><br><span class="line">        </span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);  </span><br><span class="line"></span><br><span class="line">        consumer.subscribe(Arrays.asList(<span class="string">&quot;first&quot;</span>));<span class="comment">//消费者订阅主题        </span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;      </span><br><span class="line"></span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);<span class="comment">//消费者拉取数据            </span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;                </span><br><span class="line">                System.out.printf(<span class="string">&quot;offset = %d, key = %s, value = %s%n&quot;</span>, record.offset(), record.key(), record.value());            </span><br><span class="line">            &#125;            </span><br><span class="line"></span><br><span class="line">            consumer.commitSync();<span class="comment">//同步提交，当前线程会阻塞直到offset提交成功        </span></span><br><span class="line">        &#125;    </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="2）异步提交offset"><a href="#2）异步提交offset" class="headerlink" title="2）异步提交offset"></a>2）异步提交offset</h5><p>虽然同步提交offset更可靠一些，但是由于其会阻塞当前线程，直到提交成功。因此吞吐量会收到很大的影响。因此更多的情况下，会选用异步提交offset的方式。</p>
<p>以下为异步提交offset的示例：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.TopicPartition;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays; </span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomConsumer</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();        </span><br><span class="line"></span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;linux1:9092&quot;</span>);<span class="comment">//Kafka集群        </span></span><br><span class="line">        props.put(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;test&quot;</span>);<span class="comment">//消费者组，只要group.id相同，就属于同一个消费者组        </span></span><br><span class="line">        props.put(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;false&quot;</span>);<span class="comment">//关闭自动提交offset        </span></span><br><span class="line">        props.put(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);        </span><br><span class="line">        props.put(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);        </span><br><span class="line"></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);        </span><br><span class="line"></span><br><span class="line">        consumer.subscribe(Arrays.asList(<span class="string">&quot;first&quot;</span>));<span class="comment">//消费者订阅主题        </span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;            </span><br><span class="line"></span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);<span class="comment">//消费者拉取数据            </span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;  </span><br><span class="line">                System.out.printf(<span class="string">&quot;offset = %d, key = %s, value = %s%n&quot;</span>, record.offset(), record.key(), record.value());            </span><br><span class="line">            &#125;            </span><br><span class="line"></span><br><span class="line">            consumer.commitAsync(<span class="keyword">new</span> OffsetCommitCallback() &#123;                </span><br><span class="line">                <span class="meta">@Override</span>                </span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onComplete</span><span class="params">(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, Exception exception)</span> </span>&#123;                    </span><br><span class="line">                    <span class="keyword">if</span> (exception != <span class="keyword">null</span>) &#123;                        </span><br><span class="line">                        System.err.println(<span class="string">&quot;Commit failed for&quot;</span> + offsets);                    </span><br><span class="line">                    &#125;                </span><br><span class="line">                &#125;            </span><br><span class="line">            &#125;);<span class="comment">//异步提交        </span></span><br><span class="line">        &#125;    </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="数据漏消费和重复消费"><a href="#数据漏消费和重复消费" class="headerlink" title="数据漏消费和重复消费"></a>数据漏消费和重复消费</h5><p>无论是同步提交还是异步提交offset，都有可能会造成数据的漏消费或者重复消费。<strong>先提交offset后消费，有可能造成数据的漏消费；而先消费后提交offset，有可能会造成数据的重复消费。</strong></p>
<h2 id="安装部署"><a href="#安装部署" class="headerlink" title="安装部署"></a>安装部署</h2><h3 id="jar包下载"><a href="#jar包下载" class="headerlink" title="jar包下载"></a>jar包下载</h3><p><a target="_blank" rel="noopener" href="http://kafka.apache.org/downloads">http://kafka.apache.org/downloads</a></p>
<h3 id="集群部署"><a href="#集群部署" class="headerlink" title="集群部署"></a>集群部署</h3><p>1）解压安装包</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[vincent@linux1 software]$ tar -zxvf kafka_2.11-2.4.1.tgz -C /opt/module/</span><br></pre></td></tr></table></figure>

<p>2）修改解压后的文件名称</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[vincent@linux1 module]$ mv kafka_2.11-2.4.1 kafka</span><br></pre></td></tr></table></figure>

<p>3）在/opt/module/kafka目录下创建logs文件夹</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[vincent@linux1 kafka]$ mkdir logs</span><br></pre></td></tr></table></figure>

<p>4）修改配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[vincent@linux1 kafka]$ cd config</span><br><span class="line">[vincent@linux1 config]$ vim server.properties</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">broker的全局唯一编号，不能重复</span></span><br><span class="line">broker.id=0</span><br><span class="line"><span class="meta">#</span><span class="bash">删除topic功能使能</span></span><br><span class="line">delete.topic.enable=true</span><br><span class="line"><span class="meta">#</span><span class="bash">处理网络请求的线程数量</span></span><br><span class="line">num.network.threads=3</span><br><span class="line"><span class="meta">#</span><span class="bash">用来处理磁盘IO的线程数量</span></span><br><span class="line">num.io.threads=8</span><br><span class="line"><span class="meta">#</span><span class="bash">发送套接字的缓冲区大小</span></span><br><span class="line">socket.send.buffer.bytes=102400</span><br><span class="line"><span class="meta">#</span><span class="bash">接收套接字的缓冲区大小</span></span><br><span class="line">socket.receive.buffer.bytes=102400</span><br><span class="line"><span class="meta">#</span><span class="bash">请求套接字的缓冲区大小</span></span><br><span class="line">socket.request.max.bytes=104857600</span><br><span class="line"><span class="meta">#</span><span class="bash">kafka运行日志存放的路径</span></span><br><span class="line">log.dirs=/opt/module/kafka/logs</span><br><span class="line"><span class="meta">#</span><span class="bash">topic在当前broker上的分区个数</span></span><br><span class="line">num.partitions=1</span><br><span class="line"><span class="meta">#</span><span class="bash">用来恢复和清理data下数据的线程数量</span></span><br><span class="line">num.recovery.threads.per.data.dir=1</span><br><span class="line"><span class="meta">#</span><span class="bash">segment文件保留的最长时间，超时将被删除</span></span><br><span class="line">log.retention.hours=168</span><br><span class="line"><span class="meta">#</span><span class="bash">配置连接Zookeeper集群地址</span></span><br><span class="line">zookeeper.connect=linux1:2181,linux2:2181,linux3:2181/kafka</span><br></pre></td></tr></table></figure>

<p>5）配置环境变量</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[vincent@linux1 module]$ sudo vim /etc/profile.d/my_env.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">KAFKA_HOME</span></span><br><span class="line">export KAFKA_HOME=/opt/module/kafka</span><br><span class="line">export PATH=$PATH:$KAFKA_HOME/bin</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[vincent@linux1 module]$ source /etc/profile</span><br></pre></td></tr></table></figure>

<p>6）分发安装包</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[vincent@linux1 module]$ xsync kafka</span><br></pre></td></tr></table></figure>

<p>注意：分发之后记得配置其他机器的环境变量</p>
<p>7）分别在linux2和linux3上修改配置文件/opt/module/kafka/config/server.properties中的broker.id=1、broker.id=2</p>
<p>注：broker.id不得重复</p>
<p>8）启动集群</p>
<p>依次在linux1、linux2、linux3节点上启动kafka</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[vincent@linux1 kafka]$ kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties</span><br><span class="line">[vincent@linux2 kafka]$ kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties</span><br><span class="line">[vincent@linux3 kafka]$ kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties</span><br></pre></td></tr></table></figure>

<p>9）关闭集群</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[vincent@linux1 kafka]$ bin/kafka-server-stop.sh</span><br><span class="line">[vincent@linux2 kafka]$ bin/kafka-server-stop.sh</span><br><span class="line">[vincent@linux3 kafka]$ bin/kafka-server-stop.sh</span><br></pre></td></tr></table></figure>

<p>10）kafka群起脚本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">if [ $# -lt 1 ] then        </span><br><span class="line">    echo &quot;No Args Input!!!!&quot;        </span><br><span class="line">    exit;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">for i in linux1 linux2 linux3</span><br><span class="line">do</span><br><span class="line">    case $1 in</span><br><span class="line">    &quot;start&quot;)        </span><br><span class="line">        echo &quot;=============== start $i kafka ==============&quot;        </span><br><span class="line">        ssh $i /opt/module/kafka/bin/kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties</span><br><span class="line">        ;;</span><br><span class="line">    &quot;stop&quot;)        </span><br><span class="line">        echo &quot;=============== stop $i kafka ===============&quot;        </span><br><span class="line">        ssh $i /opt/module/kafka/bin/kafka-server-stop.sh</span><br><span class="line">        ;;</span><br><span class="line">    *)        </span><br><span class="line">        echo &quot;Input Args Error!!!!&quot;</span><br><span class="line">        ;;</span><br><span class="line">    esac</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<h3 id="kafka命令行操作"><a href="#kafka命令行操作" class="headerlink" title="kafka命令行操作"></a>kafka命令行操作</h3><p>1）查看当前服务器中的所有topic</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[vincent@linux1 kafka]$ bin/kafka-topics.sh --zookeeper linux1:2181/kafka --list</span><br></pre></td></tr></table></figure>

<p>2）创建topic</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[vincent@linux1 kafka]$ bin/kafka-topics.sh --zookeeper linux1:2181/kafka --create --topic second --replication-factor 2 --partitions 2</span><br></pre></td></tr></table></figure>

<p>选项说明：</p>
<p>–topic 定义topic名</p>
<p>–replication-factor  定义副本数</p>
<p>–partitions  定义分区数</p>
<p>3）删除topic</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[vincent@linux1 kafka]$ bin/kafka-topics.sh --zookeeper linux1:2181/kafka --delete --topic first</span><br></pre></td></tr></table></figure>

<p>需要server.properties中设置delete.topic.enable=true否则只是标记删除。</p>
<p>4）发送消息</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[vincent@linux1 kafka]$ bin/kafka-console-producer.sh --broker-list linux1:9092 --topic first</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">hello world</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">vincent  vincent</span></span><br></pre></td></tr></table></figure>

<p>5）消费消息</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[vincent@linux1 kafka]$ bin/kafka-console-consumer.sh --bootstrap-server linux1:9092 --from-beginning --topic first</span><br></pre></td></tr></table></figure>

<p>–from-beginning：会把主题中以往所有的数据都读取出来。</p>
<p>6）查看某个Topic的详情</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[vincent@linux1 kafka]$ bin/kafka-topics.sh --zookeeper linux1:2181/kafka --describe --topic first</span><br></pre></td></tr></table></figure>

<p>7）修改分区数</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[vincent@linux1 kafka]$bin/kafka-topics.sh --zookeeper linux1:2181/kafka --alter --topic first --partitions 6</span><br></pre></td></tr></table></figure>

<h2 id="Kafka监控（Kafka-Eagle）"><a href="#Kafka监控（Kafka-Eagle）" class="headerlink" title="Kafka监控（Kafka Eagle）"></a>Kafka监控（Kafka Eagle）</h2><p>1）修改kafka启动命令</p>
<p>修改kafka-server-start.sh命令中</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">if [ &quot;x$KAFKA_HEAP_OPTS&quot; = &quot;x&quot; ]; </span><br><span class="line">    then export KAFKA_HEAP_OPTS=&quot;-Xmx1G -Xms1G&quot;</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<p>为</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">if [ &quot;x$KAFKA_HEAP_OPTS&quot; = &quot;x&quot; ]; then</span><br><span class="line">  export KAFKA_HEAP_OPTS=&quot;-server -Xms2G -Xmx2G -XX:PermSize=128m -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:ParallelGCThreads=8 -XX:ConcGCThreads=5 -XX:InitiatingHeapOccupancyPercent=70&quot;</span><br><span class="line">  export JMX_PORT=&quot;9999&quot;</span><br><span class="line"><span class="meta">  #</span><span class="bash"><span class="built_in">export</span> KAFKA_HEAP_OPTS=<span class="string">&quot;-Xmx1G -Xms1G&quot;</span></span></span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<p>注意：修改之后在启动Kafka之前要分发至其他节点</p>
<p>2）上传压缩包kafka-eagle-bin-1.4.5.tar.gz到集群/opt/software目录</p>
<p>3）解压到本地</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[vincent@linux1 software]$ tar -zxvf kafka-eagle-bin-1.4.5.tar.gz</span><br></pre></td></tr></table></figure>

<p>4）进入刚才解压的目录</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[vincent@linux1 kafka-eagle-bin-1.4.5]$ ll</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">总用量 82932</span><br><span class="line">-rw-rw-r--. 1 vincent vincent 84920710 8月  13 23:00 kafka-eagle-web-1.4.5-bin.tar.gz</span><br></pre></td></tr></table></figure>

<p>5）将kafka-eagle-web-1.3.7-bin.tar.gz解压至/opt/module</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[vincent@linux1 kafka-eagle-bin-1.3.7]$ tar -zxvf kafka-eagle-web-1.4.5-bin.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>

<p>6）修改名称</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[vincent@linux1 module]$ mv kafka-eagle-web-1.4.5 eagle</span><br></pre></td></tr></table></figure>

<p>7）给启动文件执行权限</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[vincent@linux1 eagle]$ cd bin</span><br><span class="line">[vincent@linux1 bin]$ ll</span><br><span class="line"></span><br><span class="line">总用量 12</span><br><span class="line">-rw-r--r--. 1 vincent vincent 1848 8月  22 2017 ke.bat</span><br><span class="line">-rw-r--r--. 1 vincent vincent 7190 7月  30 20:12 ke.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[vincent@linux1 bin]$ chmod 777 ke.sh</span><br></pre></td></tr></table></figure>

<p>8）修改配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#####################################</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> multi zookeeper&amp;kafka cluster list</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#####################################</span></span></span><br><span class="line"></span><br><span class="line">kafka.eagle.zk.cluster.alias=cluster1</span><br><span class="line">cluster1.zk.list=linux1:2181,linux2:2181,linux3:2181</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#####################################</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> kafka offset storage</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#####################################</span></span></span><br><span class="line"></span><br><span class="line">cluster1.kafka.eagle.offset.storage=kafka</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#####################################</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">enable</span> kafka metrics</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#####################################</span></span></span><br><span class="line"></span><br><span class="line">kafka.eagle.metrics.charts=true</span><br><span class="line">kafka.eagle.sql.fix.error=false</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#####################################</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> kafka jdbc driver address</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#####################################</span></span></span><br><span class="line"></span><br><span class="line">kafka.eagle.driver=com.mysql.jdbc.Driver</span><br><span class="line">kafka.eagle.url=jdbc:mysql://linux1:3306/ke?useUnicode=true&amp;characterEncoding=UTF-8&amp;zeroDateTimeBehavior=convertToNull</span><br><span class="line">kafka.eagle.username=root</span><br><span class="line">kafka.eagle.password=211819</span><br></pre></td></tr></table></figure>

<p>9）添加环境变量</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export KE_HOME=/opt/module/eagle</span><br><span class="line">export PATH=$PATH:$KE_HOME/bin</span><br></pre></td></tr></table></figure>

<p>注意：source /etc/profile</p>
<p>10）启动</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[vincent@linux1 eagle]$ bin/ke.sh start</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">... ...</span><br><span class="line">... ...</span><br><span class="line"></span><br><span class="line">*******************************************************************</span><br><span class="line"></span><br><span class="line">* Kafka Eagle Service has started success.</span><br><span class="line">* Welcome, Now you can visit &#x27;http://10.211.55.10:8048/ke&#x27;</span><br><span class="line">* Account:admin ,Password:211819</span><br><span class="line">*******************************************************************</span><br><span class="line">* &lt;Usage&gt; ke.sh [start|status|stop|restart|stats] &lt;/Usage&gt;</span><br><span class="line">* &lt;Usage&gt; https://www.kafka-eagle.org/ &lt;/Usage&gt;</span><br><span class="line">*******************************************************************</span><br><span class="line"></span><br><span class="line">[vincent@linux1 eagle]$</span><br></pre></td></tr></table></figure>

<p>注意：启动之前需要先启动ZK以及KAFKA</p>
<p>11）登录页面查看监控数据</p>
<p><a target="_blank" rel="noopener" href="http://10.211.55.10:8048/ke">http://10.211.55.10:8048/ke</a></p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/Linux/" rel="prev" title="Linux">
      <i class="fa fa-chevron-left"></i> Linux
    </a></div>
      <div class="post-nav-item">
    <a href="/Kylin/" rel="next" title="Kylin">
      Kylin <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Kafka%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84"><span class="nav-number">1.</span> <span class="nav-text">Kafka基础架构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Kafka%E6%9E%B6%E6%9E%84%E6%B7%B1%E5%85%A5"><span class="nav-number">2.</span> <span class="nav-text">Kafka架构深入</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Kafka%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%E5%8F%8A%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E6%9C%BA%E5%88%B6"><span class="nav-number">2.1.</span> <span class="nav-text">Kafka工作流程及文件存储机制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Kafka%E7%94%9F%E4%BA%A7%E8%80%85"><span class="nav-number">2.2.</span> <span class="nav-text">Kafka生产者</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%86%E5%8C%BA%E7%AD%96%E7%95%A5"><span class="nav-number">2.2.1.</span> <span class="nav-text">分区策略</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%8F%AF%E9%9D%A0%E6%80%A7%E4%BF%9D%E8%AF%81"><span class="nav-number">2.2.2.</span> <span class="nav-text">数据可靠性保证</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1%EF%BC%89%E5%89%AF%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A5"><span class="nav-number">2.2.2.1.</span> <span class="nav-text">1）副本数据同步策略</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2%EF%BC%89ISR"><span class="nav-number">2.2.2.2.</span> <span class="nav-text">2）ISR</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3%EF%BC%89ack%E5%BA%94%E7%AD%94%E6%9C%BA%E5%88%B6"><span class="nav-number">2.2.2.3.</span> <span class="nav-text">3）ack应答机制</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4%EF%BC%89%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E7%BB%86%E8%8A%82"><span class="nav-number">2.2.2.4.</span> <span class="nav-text">4）故障处理细节</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Exactly-Once%E8%AF%AD%E4%B9%89"><span class="nav-number">2.2.3.</span> <span class="nav-text">Exactly Once语义</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Kafka%E6%B6%88%E8%B4%B9%E8%80%85"><span class="nav-number">2.3.</span> <span class="nav-text">Kafka消费者</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B6%88%E8%B4%B9%E6%96%B9%E5%BC%8F"><span class="nav-number">2.3.1.</span> <span class="nav-text">消费方式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%86%E5%8C%BA%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5"><span class="nav-number">2.3.2.</span> <span class="nav-text">分区分配策略</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#RangeAssignor"><span class="nav-number">2.3.2.1.</span> <span class="nav-text">RangeAssignor</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#RoundRobinAssignor"><span class="nav-number">2.3.2.2.</span> <span class="nav-text">RoundRobinAssignor</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#StickyAssignor"><span class="nav-number">2.3.2.3.</span> <span class="nav-text">StickyAssignor</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#offset%E7%9A%84%E7%BB%B4%E6%8A%A4"><span class="nav-number">2.3.3.</span> <span class="nav-text">offset的维护</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Kafka%E9%AB%98%E6%95%88%E8%AF%BB%E5%86%99%E6%95%B0%E6%8D%AE"><span class="nav-number">2.4.</span> <span class="nav-text">Kafka高效读写数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Zookeeper%E5%9C%A8Kafka%E4%B8%AD%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="nav-number">2.5.</span> <span class="nav-text">Zookeeper在Kafka中的作用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Kafka%E4%BA%8B%E5%8A%A1"><span class="nav-number">2.6.</span> <span class="nav-text">Kafka事务</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Producer%E4%BA%8B%E5%8A%A1"><span class="nav-number">2.6.1.</span> <span class="nav-text">Producer事务</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Consumer%E4%BA%8B%E5%8A%A1%EF%BC%88%E7%B2%BE%E5%87%86%E4%B8%80%E6%AC%A1%E6%80%A7%E6%B6%88%E8%B4%B9%EF%BC%89"><span class="nav-number">2.6.2.</span> <span class="nav-text">Consumer事务（精准一次性消费）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Kafka-API"><span class="nav-number">3.</span> <span class="nav-text">Kafka API</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Producer-API"><span class="nav-number">3.1.</span> <span class="nav-text">Producer API</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B6%88%E6%81%AF%E5%8F%91%E9%80%81%E6%B5%81%E7%A8%8B"><span class="nav-number">3.1.1.</span> <span class="nav-text">消息发送流程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BC%82%E6%AD%A5%E5%8F%91%E9%80%81API"><span class="nav-number">3.1.2.</span> <span class="nav-text">异步发送API</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%90%8C%E6%AD%A5%E5%8F%91%E9%80%81API"><span class="nav-number">3.1.3.</span> <span class="nav-text">同步发送API</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Consumer-API"><span class="nav-number">3.2.</span> <span class="nav-text">Consumer API</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%87%AA%E5%8A%A8%E6%8F%90%E4%BA%A4offset"><span class="nav-number">3.2.1.</span> <span class="nav-text">自动提交offset</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%89%8B%E5%8A%A8%E6%8F%90%E4%BA%A4offset"><span class="nav-number">3.2.2.</span> <span class="nav-text">手动提交offset</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1%EF%BC%89%E5%90%8C%E6%AD%A5%E6%8F%90%E4%BA%A4offset"><span class="nav-number">3.2.2.1.</span> <span class="nav-text">1）同步提交offset</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2%EF%BC%89%E5%BC%82%E6%AD%A5%E6%8F%90%E4%BA%A4offset"><span class="nav-number">3.2.2.2.</span> <span class="nav-text">2）异步提交offset</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E6%BC%8F%E6%B6%88%E8%B4%B9%E5%92%8C%E9%87%8D%E5%A4%8D%E6%B6%88%E8%B4%B9"><span class="nav-number">3.2.2.3.</span> <span class="nav-text">数据漏消费和重复消费</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2"><span class="nav-number">4.</span> <span class="nav-text">安装部署</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#jar%E5%8C%85%E4%B8%8B%E8%BD%BD"><span class="nav-number">4.1.</span> <span class="nav-text">jar包下载</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2"><span class="nav-number">4.2.</span> <span class="nav-text">集群部署</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#kafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C"><span class="nav-number">4.3.</span> <span class="nav-text">kafka命令行操作</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Kafka%E7%9B%91%E6%8E%A7%EF%BC%88Kafka-Eagle%EF%BC%89"><span class="nav-number">5.</span> <span class="nav-text">Kafka监控（Kafka Eagle）</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Vincent"
      src="/images/cute.jpeg">
  <p class="site-author-name" itemprop="name">Vincent</p>
  <div class="site-description" itemprop="description">nice</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">17</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Vincent</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>
